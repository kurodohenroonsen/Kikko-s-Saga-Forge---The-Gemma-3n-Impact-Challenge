Passer au contenu principal
Android Developers
Plus
Recherche
/


Fran√ßais
Android Studio
Connexion
AI
Aper√ßu
Exemples
Guides
R√©f√©rence


Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Android Developers
D√©velopper
AI
Guides
Ce contenu vous a-t-il √©t√© utile ?

Premiers pas avec l'acc√®s exp√©rimental √† Gemini Nano

bookmark_border
L'acc√®s exp√©rimental √† Gemini Nano est con√ßu pour les d√©veloppeurs qui souhaitent tester l'am√©lioration de leurs applications gr√¢ce √† des fonctionnalit√©s d'IA de pointe sur l'appareil. Ce guide explique comment tester Gemini Nano √† l'aide du SDK Google AI Edge dans votre propre application.

Remarque : En utilisant l'acc√®s exp√©rimental √† Gemini Nano, vous acceptez les pr√©sentes conditions d'utilisation qui s'appliquent √† votre utilisation des produits Google et √† votre participation au programme d'acc√®s exp√©rimental √† Gemini Nano.

Obtenir l'application exemple
Si vous souhaitez suivre une d√©monstration pr√©par√©e, consultez notre application exemple sur GitHub.

Pr√©requis
Pour tester Gemini Nano, vous devez disposer d'un appareil de la gamme Pixel 9. Assurez-vous d'en avoir un sous la main avant de continuer et de n'√™tre connect√© qu'avec le compte que vous souhaitez utiliser pour les tests.

Rejoignez le groupe Google aicore-experimental.
S'inscrire au programme de test Android AICore
Une fois ces √©tapes termin√©es, le nom de l'application sur le Play Store (sous "G√©rer les applications et l'appareil") devrait passer de "Android AICore" √† "Android AICore (Beta)".

Mettre √† jour les APK et t√©l√©charger les binaires
Mettez √† jour l'APK AICore :
En haut √† droite, appuyez sur l'ic√¥ne de votre profil.
Appuyez sur G√©rer les applications et l'appareil > G√©rer.
Appuyez sur Android AICore.
Si une mise √† jour est disponible, appuyez sur Mettre √† jour.
Mettez √† jour l'APK Private Compute Services :
En haut √† droite, appuyez sur l'ic√¥ne de votre profil.
Appuyez sur G√©rer les applications et l'appareil > G√©rer.
Appuyez sur Private Compute Services.
Si une mise √† jour est disponible, appuyez sur Mettre √† jour.
V√©rifiez la version sous l'onglet √Ä propos de cette application et assurez-vous qu'elle est 1.0.release.658389993 ou ult√©rieure.
Red√©marrez votre appareil et patientez quelques minutes pour que l'enregistrement du test soit pris en compte.
V√©rifiez la version de l'APK AICore sur le Play Store (dans l'onglet "√Ä propos de cette application") pour confirmer qu'elle commence par 0.thirdpartyeap.
Configurer Gradle
Ajoutez les √©l√©ments suivants au bloc de d√©pendances de votre configuration build.gradle :


implementation("com.google.ai.edge.aicore:aicore:0.0.1-exp01")

Dans la configuration build.gradle, d√©finissez la version cible minimale du SDK sur 31 :

defaultConfig {
    ...
    minSdk = 31
    ...
}
Obtenir AICore et ex√©cuter une inf√©rence
Cr√©ez un objet GenerationConfig, qui comporte des param√®tres permettant de personnaliser les propri√©t√©s de la mani√®re dont le mod√®le doit ex√©cuter l'inf√©rence.

Les param√®tres incluent :

Temp√©rature : contr√¥le le caract√®re al√©atoire. Des valeurs sup√©rieures augmentent la diversit√©.
Top K : nombre de jetons √† prendre en compte parmi ceux ayant le rang le plus √©lev√©.
Nombre de candidats : nombre maximal de r√©ponses √† renvoyer
Nombre maximal de jetons de sortie : longueur de la r√©ponse
Ex√©cuteur de n≈ìud de calcul : ExecutorService sur lequel les t√¢ches en arri√®re-plan doivent √™tre ex√©cut√©es
Ex√©cuteur de rappel : Executor sur lequel les rappels doivent √™tre appel√©s
Kotlin
Java
val generationConfig = generationConfig {
  context = ApplicationProvider.getApplicationContext() // required
  temperature = 0.2f
  topK = 16
  maxOutputTokens = 256
}
Cr√©ez un downloadCallback facultatif. Il s'agit d'une fonction de rappel utilis√©e pour le t√©l√©chargement du mod√®le. Les messages renvoy√©s sont destin√©s au d√©bogage.

Cr√©ez l'objet GenerativeModel avec les configurations de g√©n√©ration et de t√©l√©chargement facultatives que vous avez cr√©√©es pr√©c√©demment.

Kotlin
Java
val downloadConfig = DownloadConfig(downloadCallback)
val generativeModel = GenerativeModel(
   generationConfig = generationConfig,
   downloadConfig = downloadConfig // optional
)
Ex√©cutez l'inf√©rence avec le mod√®le et transmettez votre requ√™te. Comme GenerativeModel.generateContent() est une fonction de suspension, nous devons nous assurer qu'elle se trouve dans la port√©e de coroutine appropri√©e pour le lancement.

Remarque : Si vous avez plusieurs cha√Ænes pour votre entr√©e, vous pouvez √©galement cr√©er un objet de contenu avec chaque ligne de texte et le transmettre √† generateContent() en tant que param√®tre.
Kotlin
Java
scope.launch {
  // Single string input prompt
  val input = "I want you to act as an English proofreader. I will provide you
    texts, and I would like you to review them for any spelling, grammar, or
    punctuation errors. Once you have finished reviewing the text, provide me
    with any necessary corrections or suggestions for improving the text: These
    arent the droids your looking for."
  val response = generativeModel.generateContent(input)
  print(response.text)

  // Or multiple strings as input
  val response = generativeModel.generateContent(
  content {
    text("I want you to act as an English proofreader. I will provide you texts
      and I would like you to review them for any spelling, grammar, or
      punctuation errors.")
    text("Once you have finished reviewing the text, provide me with any
      necessary corrections or suggestions for improving the text:")
    text("These arent the droids your looking for.")
    }
  )
  print(response.text)
}
Si vous avez des commentaires sur le SDK Google AI Edge ou d'autres commentaires √† faire parvenir √† notre √©quipe, envoyez une demande.

Conseils concernant les requ√™tes
La conception de requ√™tes est le processus de cr√©ation de requ√™tes qui d√©clenchent une r√©ponse optimale des mod√®les de langage. Pour obtenir des r√©ponses pr√©cises et de haute qualit√© √† partir d'un mod√®le de langage, il est essentiel de r√©diger des requ√™tes bien structur√©es. Nous avons inclus quelques exemples pour vous aider √† vous lancer dans les cas d'utilisation courants de Gemini Nano. Pour en savoir plus, consultez les strat√©gies de requ√™te Gemini.

Remarque : Gemini Nano autorise jusqu'√† 12 000 jetons d'entr√©e.
Pour les r√©√©critures :

I want you to act as an English proofreader. I will provide you texts, and I
would like you to review them for any spelling, grammar, or punctuation errors.
Once you have finished reviewing the text, provide me with any necessary
corrections or suggestions for improving the text: These arent the droids your
looking for
Pour les cas d'utilisation des r√©ponses sugg√©r√©es :

Prompt: Predict up to 5 emojis as a response to a text chat message. The output
should only include emojis.

input: The new visual design is blowing my mind ü§Ø
output: ‚ûï,üíò, ‚ù§‚Äçüî•

input: Well that looks great regardless
output: üíó,ü™Ñ

input: Unfortunately this won't work
output: üíî,üòî

input: sounds good, I'll look into that
output: üôè,üëç

input: 10hr cut of jeff goldblum laughing URL
output: üòÇ,üíÄ,‚ö∞Ô∏è

input: Woo! Launch time!
Output:
Pour la synth√®se :

Summarize this text as bullet points of key information.
Text: A quantum computer exploits quantum mechanical phenomena to perform
  calculations exponentially faster than any modern traditional computer. At
  very tiny scales, physical matter acts as both particles and as waves, and
  quantum computing uses specialized hardware to leverage this behavior. The
  operating principles of quantum devices are beyond the scope of classical
  physics. When deployed at scale, quantum computers could be used in a wide
  variety of applications such as: in cybersecurity to break existing encryption
  methods while helping researchers create new ones, in meteorology to develop
  better weather forecasting etc. However, the current state-of-the-art quantum
  computers are still largely experimental and impractical.
Ce contenu vous a-t-il √©t√© utile ?

Le contenu et les exemples de code de cette page sont soumis aux licences d√©crites dans la Licence de contenu. Java et OpenJDK sont des marques ou des marques d√©pos√©es d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/08/02 (UTC).

XX
Suivez @AndroidDev sur X
YouTubeYouTube
Retrouvez la cha√Æne Android Developers sur YouTube
LinkedInLinkedIn
Connect with the Android Developers community on LinkedIn
En savoir plus sur Android
Android
Android pour les entreprises
S√©curit√©
Projet Android Open Source
Actualit√©s
Blog
Podcasts
D√©couvrir
Jeux vid√©o
Machine learning
Sant√© et remise en forme
Appareil photo et contenus multim√©dias
R√®gles de confidentialit√©
5G
Appareils Android
Grands √©crans
Wear OS
Appareils ChromeOS
Android for Cars
Android TV
Versions
Android 15
Android 14
Android 13
Android 12
Android 11
Android 10
Pie
Documentation et t√©l√©chargements
Guide Android Studio
Guides pour les d√©veloppeurs
Document de r√©f√©rence sur les API
T√©l√©charger Studio
Android NDK
Support
Signaler un bug de la plate-forme
Signaler un bug relatif √† la documentation
Google Play support
Participer aux √©tudes sur l'exp√©rience utilisateur
Google Developers
Android
Chrome
Firebase
Google Cloud Platform
Tous les produits
R√®gles de confidentialit√©
Licence
Consignes relatives √† la marque
Recevoir des actualit√©s et des conseils par e-mail
S‚Äôabonner

Fran√ßais

Passer au contenu principal
ML Kit
ML Kit
Guides
R√©f√©rence
Exemples
√âtudes de cas
Plus
Recherche
/

Fran√ßais


Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Produits
ML Kit
Ce contenu vous a-t-il √©t√© utile ?

API GenAI Summarization

bookmark_border
Cette API est propos√©e en version b√™ta et n'est soumise √† aucun contrat de niveau de service ni √† aucune r√®gle d'obsolescence. Des modifications peuvent √™tre apport√©es √† cette API et entra√Æner une rupture de la r√©trocompatibilit√©.


L'API de synth√®se d'IA g√©n√©rative de ML Kit vous permet de g√©n√©rer automatiquement des r√©sum√©s d'articles et de conversations sous forme de liste √† puces. Cela aide les utilisateurs √† comprendre de longs textes.

La synth√®se b√©n√©ficie de l'IA g√©n√©rative sur l'appareil, car elle r√©pond aux pr√©occupations concernant la confidentialit√© des donn√©es et l'efficacit√© des co√ªts. Les applications qui r√©sument les discussions priv√©es, les e-mails, les notes et les rappels g√®rent souvent des informations sensibles. Il est donc important de traiter les donn√©es sur l'appareil pour prot√©ger la confidentialit√© des utilisateurs. De plus, les t√¢ches de synth√®se, en particulier celles avec des contextes longs ou de nombreux √©l√©ments, peuvent n√©cessiter une puissance de traitement importante. Le traitement de ce contenu sur l'appareil r√©duit la charge du serveur et les co√ªts de diffusion, tout en pr√©servant la confidentialit√© des donn√©es utilisateur.

Capacit√©s cl√©s
L'API GenAI Summarization couvre les fonctionnalit√©s suivantes :

R√©sumer un texte, class√© comme article ou conversation.
R√©sume la sortie en un, deux ou trois points.
Commencer
Remarque : Cette API n√©cessite Android au niveau d'API 26 ou sup√©rieur.
Ajoutez l'API de r√©sum√© ML Kit en tant que d√©pendance dans votre configuration build.gradle.


implementation("com.google.mlkit:genai-summarization:1.0.0-beta1")
Ensuite, impl√©mentez le code dans votre projet :

Cr√©ez un objet Summarizer.
T√©l√©chargez la fonctionnalit√© si elle est t√©l√©chargeable.
Cr√©ez une demande de synth√®se.
Ex√©cutez l'inf√©rence et r√©cup√©rez le r√©sultat.
Kotlin
Java

val articleToSummarize = "Announcing a set of on-device GenAI APIs..."

// Define task with required input type, output type, and language
val summarizerOptions = SummarizerOptions.builder(context)
    .setInputType(InputType.ARTICLE)
    .setOutputType(OutputType.ONE_BULLET)
    .setLanguage(Language.ENGLISH)
    .build()
val summarizer = Summarization.getClient(summarizerOptions)

suspend fun prepareAndStartSummarization() {
    // Check feature availability. Status will be one of the following:
    // UNAVAILABLE, DOWNLOADABLE, DOWNLOADING, AVAILABLE
    val featureStatus = summarizer.checkFeatureStatus().await()

    if (featureStatus == FeatureStatus.DOWNLOADABLE) {
        // Download feature if necessary. If downloadFeature is not called,
        // the first inference request will also trigger the feature to be
        // downloaded if it's not already downloaded.
        summarizer.downloadFeature(object : DownloadCallback {
            override fun onDownloadStarted(bytesToDownload: Long) { }

            override fun onDownloadFailed(e: GenAiException) { }

            override fun onDownloadProgress(totalBytesDownloaded: Long) {}

            override fun onDownloadCompleted() {
                startSummarizationRequest(articleToSummarize, summarizer)
            }
        })
    } else if (featureStatus == FeatureStatus.DOWNLOADING) {
        // Inference request will automatically run once feature is
        // downloaded. If Gemini Nano is already downloaded on the device,
        // the feature-specific LoRA adapter model will be downloaded
        // quickly. However, if Gemini Nano is not already downloaded, the
        // download process may take longer.
        startSummarizationRequest(articleToSummarize, summarizer)
    } else if (featureStatus == FeatureStatus.AVAILABLE) {
        startSummarizationRequest(articleToSummarize, summarizer)
    }
}

fun startSummarizationRequest(text: String, summarizer: Summarizer) {
    // Create task request
    val summarizationRequest = SummarizationRequest.builder(text).build()

    // Start summarization request with streaming response
    summarizer.runInference(summarizationRequest) { newText ->
        // Show new text in UI
    }

    // You can also get a non-streaming response from the request
    // val summarizationResult = summarizer.runInference(
    //     summarizationRequest).get().summary
}

// Be sure to release the resource when no longer needed
// For example, on viewModel.onCleared() or activity.onDestroy()
summarizer.close()
Comment le mod√®le g√®re-t-il les diff√©rents types d'entr√©es ?
Lorsque l'entr√©e de texte est sp√©cifi√©e comme InputType.CONVERSATION, le mod√®le attend une entr√©e au format suivant :


<name>: <message>
<name2>: <message2>
<name>: <message3>
<name3>: <message4>
Cela permet au mod√®le de produire un r√©sum√© plus pr√©cis en comprenant mieux la conversation et les interactions.

Fonctionnalit√©s et limites
L'entr√©e ne doit pas d√©passer 4 000 jetons (environ 3 000 mots en anglais). Si l'entr√©e d√©passe 4 000 jetons, envisagez les options suivantes :

Priorit√© √† la synth√®se des 4 000 premiers jetons. Les tests indiquent que cela donne g√©n√©ralement de bons r√©sultats pour les entr√©es plus longues. Envisagez d'activer la troncature automatique en appelant setLongInputAutoTruncationEnabled afin que l'entr√©e suppl√©mentaire soit automatiquement tronqu√©e.
Segmente l'entr√©e en groupes de 4 000 jetons et r√©sume-les individuellement.
Envisagez une solution cloud plus adapt√©e aux entr√©es plus volumineuses.
Pour InputType.ARTICLE, l'entr√©e doit √©galement comporter plus de 400 caract√®res. Le mod√®le fonctionne mieux lorsque l'article comporte au moins 300 mots.

L'API GenAI Summarization est compatible avec l'anglais, le cor√©en et le japonais, et est d√©finie dans SummarizerOptions.Language.

La disponibilit√© de la configuration de la fonctionnalit√© sp√©cifique (sp√©cifi√©e par SummarizerOptions) peut varier en fonction de la configuration de l'appareil et des mod√®les qui ont √©t√© t√©l√©charg√©s sur l'appareil.

Le moyen le plus fiable pour les d√©veloppeurs de s'assurer que la fonctionnalit√© d'API pr√©vue est compatible avec un appareil dot√© de la SummarizerOptions demand√©e consiste √† appeler la m√©thode checkFeatureStatus(). Cette m√©thode fournit l'√©tat d√©finitif de la disponibilit√© des fonctionnalit√©s sur l'appareil au moment de l'ex√©cution.

Remarque : Les variations mat√©rielles entre les diff√©rents types d'appareils peuvent entra√Æner des diff√©rences dans les versions du mod√®le de base Gemini Nano et, par cons√©quent, dans les r√©sultats de l'API ML Kit GenAI. Les adaptateurs LoRA sont appliqu√©s pour att√©nuer les diff√©rences.
Probl√®mes de configuration courants
Remarque : Cette API n'est pas compatible avec les appareils dont le bootloader est d√©verrouill√©.
Les API d'IA g√©n√©rative de ML Kit s'appuient sur l'application Android AICore pour acc√©der √† Gemini Nano. Lorsqu'un appareil vient d'√™tre configur√© (y compris r√©initialis√©) ou que l'application AICore vient d'√™tre r√©initialis√©e (par exemple, en effa√ßant les donn√©es, puis en la d√©sinstallant et en la r√©installant), il se peut que l'application AICore n'ait pas eu suffisamment de temps pour terminer l'initialisation (y compris le t√©l√©chargement des derni√®res configurations depuis le serveur). Par cons√©quent, il est possible que les API ML Kit GenAI ne fonctionnent pas comme pr√©vu. Voici les messages d'erreur de configuration courants que vous pouvez rencontrer et comment les r√©soudre :

Exemple de message d'erreur	Comment r√©pondre
AICore a √©chou√© avec le type d'erreur 4-CONNECTION_ERROR et le code d'erreur 601-BINDING_FAILURE : √©chec de la liaison du service AICore.	Cela peut se produire lorsque vous installez l'application √† l'aide des API ML Kit GenAI imm√©diatement apr√®s la configuration de l'appareil ou lorsque AICore est d√©sinstall√© apr√®s l'installation de votre application. Mettre √† jour l'application AICore, puis r√©installer votre application devrait r√©soudre le probl√®me.
AICore a √©chou√© avec le type d'erreur 3-PREPARATION_ERROR et le code d'erreur 606-FEATURE_NOT_FOUND : la fonctionnalit√© ... n'est pas disponible.	Cela peut se produire lorsque AICore n'a pas termin√© de t√©l√©charger les derni√®res configurations. Lorsque l'appareil est connect√© √† Internet, la mise √† jour prend g√©n√©ralement de quelques minutes √† quelques heures. Red√©marrer l'appareil peut acc√©l√©rer la mise √† jour.

Notez que si le bootloader de l'appareil est d√©verrouill√©, cette erreur s'affichera √©galement. Cette API n'est pas compatible avec les appareils dont le bootloader est d√©verrouill√©.
AICore a √©chou√© avec le type d'erreur 1-DOWNLOAD_ERROR et le code d'erreur 0-UNKNOWN : la fonctionnalit√© ... a √©chou√© avec l'√©tat d'√©chec 0 et l'erreur esz : UNAVAILABLE : impossible de r√©soudre l'h√¥te ...	Maintenez la connexion r√©seau, patientez quelques minutes, puis r√©essayez.
Remarque : Les d√©veloppeurs sont habitu√©s √† r√©initialiser les appareils, mais les utilisateurs le font beaucoup moins souvent. De plus, tous ces cas d'√©chec de la configuration peuvent √™tre d√©tect√©s par l'API checkFeatureStatus(). Veillez √† appeler checkFeatureStatus() avant d'afficher une UI associ√©e, afin que les utilisateurs de votre application ne voient jamais ces erreurs.
Ce contenu vous a-t-il √©t√© utile ?

Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/08/08 (UTC).

Communiquer
Blog
Bluesky
Instagram
LinkedIn
X (Twitter)
YouTube
Programmes
Google Developer Program
Google Developer Groups
Google Developer Experts
Accelerators
Women Techmakers
Google Cloud & NVIDIA
Consoles pour d√©veloppeurs
Google API Console
Google Cloud Platform Console
Google Play Console
Firebase Console
Actions on Google Console
Cast SDK Developer Console
Chrome Web Store Dashboard
Google Home Developer Console
Google Developers
Android
Chrome
Firebase
Google Cloud Platform
Google AI
Tous les produits
Conditions d'utilisation
R√®gles de confidentialit√©
S'inscrire √† la newsletter Google pour les d√©veloppeurs
S‚Äôabonner

Fran√ßais

Passer au contenu principal
ML Kit
ML Kit
Guides
R√©f√©rence
Exemples
√âtudes de cas
Plus
Recherche
/

Fran√ßais


Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Produits
ML Kit
Ce contenu vous a-t-il √©t√© utile ?

API GenAI Proofreading

bookmark_border
Cette API est propos√©e en version b√™ta et n'est soumise √† aucun contrat de niveau de service ni √† aucune r√®gle d'obsolescence. Des modifications peuvent √™tre apport√©es √† cette API et entra√Æner une rupture de la r√©trocompatibilit√©.


L'API GenAI Proofreading de ML Kit vous permet d'aider les utilisateurs √† v√©rifier la grammaire et l'orthographe de courts textes.

Capacit√©s cl√©s
Relire le texte saisi au clavier ou par saisie vocale
Les requ√™tes renvoient au moins une suggestion. Si plusieurs suggestions sont renvoy√©es, les r√©sultats sont tri√©s par ordre d√©croissant de confiance.
Exemples de r√©sultats
Entr√©e

Type d'entr√©e

Sortie

Ceci est un court message

Clavier

Ceci est un court message

Le projet est presque termin√©, mais doit √™tre examin√©.

Clavier

Le projet est presque termin√©, mais doit √™tre examin√©

Please meat me at the bear,

Voice

Veuillez me rejoindre au bar.

Premiers pas
Remarque : Cette API n√©cessite Android au niveau d'API 26 ou sup√©rieur.
Pour commencer √† utiliser l'API GenAI Proofreading, ajoutez cette d√©pendance au fichier de compilation de votre projet.


implementation("com.google.mlkit:genai-proofreading:1.0.0-beta1")
Ensuite, configurez et obtenez un client Proofreader avec des param√®tres de langue et de type d'entr√©e sp√©cifiques. V√©rifiez et assurez-vous que les fonctionnalit√©s de mod√®le sur l'appareil n√©cessaires sont disponibles (en d√©clenchant un t√©l√©chargement si n√©cessaire). Envoyez le texte que vous souhaitez analyser dans un ProofreadingRequest, ex√©cutez l'inf√©rence de relecture, puis traitez les suggestions renvoy√©es pour corriger le texte.

Kotlin
Java

val textToProofread = "The praject is compleet but needs too be reviewd"

// Define task with required input and output format
val options = ProofreaderOptions.builder(context)
    // InputType can be KEYBOARD or VOICE. VOICE indicates that
    // the user generated text based on audio input.
    .setInputType(ProofreaderOptions.InputType.KEYBOARD)
    // Refer to ProofreaderOptions.Language for available
    // languages
    .setLanguage(ProofreaderOptions.Language.ENGLISH)
    .build()
val proofreader = Proofreading.getClient(options)

suspend fun prepareAndStartProofread() {
    // Check feature availability, status will be one of the
    // following: UNAVAILABLE, DOWNLOADABLE, DOWNLOADING, AVAILABLE
    val featureStatus = proofreader.checkFeatureStatus().await()

    if (featureStatus == FeatureStatus.DOWNLOADABLE) {
        // Download feature if necessary.
        // If downloadFeature is not called, the first inference
        // request will also trigger the feature to be downloaded
        // if it's not already downloaded.
        proofreader.downloadFeature(object : DownloadCallback {
            override fun onDownloadStarted(bytesToDownload: Long) { }

            override fun onDownloadFailed(e: GenAiException) { }

            override fun onDownloadProgress(
                totalBytesDownloaded: Long
            ) {}

            override fun onDownloadCompleted() {
                startProofreadingRequest(textToProofread, proofreader)
            }
        })
    } else if (featureStatus == FeatureStatus.DOWNLOADING) {
        // Inference request will automatically run once feature is
        // downloaded.
        // If Gemini Nano is already downloaded on the device, the
        // feature-specific LoRA adapter model will be downloaded
        // very quickly. However, if Gemini Nano is not already
        // downloaded, the download process may take longer.
        startProofreadingRequest(textToProofread, proofreader)
    } else if (featureStatus == FeatureStatus.AVAILABLE) {
        startProofreadingRequest(textToProofread, proofreader)
    }
}

suspend fun startProofreadingRequest(
    text: String, proofreader: Proofreader
) {
    // Create task request
    val proofreadingRequest =
        ProofreadingRequest.builder(text).build()

    // Start proofreading request with non-streaming response
    // More than 1 result may be returned. If multiple suggestions
    // are returned, results will be sorted by descending confidence.
    val proofreadingResults =
        proofreader.runInference(proofreadingRequest).await().results

    // You can also start a streaming request
    // proofreader.runInference(proofreadingRequest) { newText ->
    //     // show new text in UI
    // }
}

// Be sure to release the resource when no longer needed
// For example, on viewModel.onCleared() or activity.onDestroy()
proofreader.close()
Comment le mod√®le g√®re-t-il les diff√©rents types d'entr√©es ?
Lorsque le mod√®le dispose de plus d'informations sur la fa√ßon dont l'utilisateur a saisi le texte (au clavier ou √† la voix), il peut mieux pr√©dire le type d'erreurs qui peuvent √™tre pr√©sentes. Le texte saisi au clavier est plus sujet aux fautes d'orthographe avec les touches voisines, tandis que le texte saisi √† la voix est plus sujet aux fautes d'orthographe des mots ayant la m√™me prononciation.

Fonctionnalit√©s et limites
La relecture est disponible pour les langues suivantes : allemand, anglais, cor√©en, espagnol, fran√ßais, italien et japonais. Elles sont d√©finies dans ProofreaderOptions.Language. La saisie doit comporter moins de 256 jetons.

La disponibilit√© de la configuration de la fonctionnalit√© sp√©cifique (sp√©cifi√©e par ProofreaderOptions) peut varier en fonction de la configuration de l'appareil et des mod√®les qui ont √©t√© t√©l√©charg√©s sur l'appareil.

Le moyen le plus fiable pour les d√©veloppeurs de s'assurer que la fonctionnalit√© d'API pr√©vue est compatible avec un appareil dot√© de la ProofreaderOptions demand√©e consiste √† appeler la m√©thode checkFeatureStatus(). Cette m√©thode fournit l'√©tat d√©finitif de la disponibilit√© des fonctionnalit√©s sur l'appareil au moment de l'ex√©cution.

Remarque : Les variations mat√©rielles entre les diff√©rents types d'appareils peuvent entra√Æner des diff√©rences dans les versions du mod√®le de base Gemini Nano et, par cons√©quent, dans les r√©sultats de l'API ML Kit GenAI. Les adaptateurs LoRA sont appliqu√©s pour att√©nuer les diff√©rences.
Probl√®mes de configuration courants
Remarque : Cette API n'est pas compatible avec les appareils dont le bootloader est d√©verrouill√©.
Les API d'IA g√©n√©rative de ML Kit s'appuient sur l'application Android AICore pour acc√©der √† Gemini Nano. Lorsqu'un appareil vient d'√™tre configur√© (y compris r√©initialis√©) ou que l'application AICore vient d'√™tre r√©initialis√©e (par exemple, en effa√ßant les donn√©es, puis en la d√©sinstallant et en la r√©installant), il se peut que l'application AICore n'ait pas eu suffisamment de temps pour terminer l'initialisation (y compris le t√©l√©chargement des derni√®res configurations depuis le serveur). Par cons√©quent, il est possible que les API ML Kit GenAI ne fonctionnent pas comme pr√©vu. Voici les messages d'erreur de configuration courants que vous pouvez rencontrer et comment les r√©soudre :

Exemple de message d'erreur	Comment r√©pondre
AICore a √©chou√© avec le type d'erreur 4-CONNECTION_ERROR et le code d'erreur 601-BINDING_FAILURE : √©chec de la liaison du service AICore.	Cela peut se produire lorsque vous installez l'application √† l'aide des API ML Kit GenAI imm√©diatement apr√®s la configuration de l'appareil ou lorsque AICore est d√©sinstall√© apr√®s l'installation de votre application. Mettre √† jour l'application AICore, puis r√©installer votre application devrait r√©soudre le probl√®me.
AICore a √©chou√© avec le type d'erreur 3-PREPARATION_ERROR et le code d'erreur 606-FEATURE_NOT_FOUND : la fonctionnalit√© ... n'est pas disponible.	Cela peut se produire lorsque AICore n'a pas termin√© de t√©l√©charger les derni√®res configurations. Lorsque l'appareil est connect√© √† Internet, la mise √† jour prend g√©n√©ralement de quelques minutes √† quelques heures. Red√©marrer l'appareil peut acc√©l√©rer la mise √† jour.

Notez que si le bootloader de l'appareil est d√©verrouill√©, cette erreur s'affichera √©galement. Cette API n'est pas compatible avec les appareils dont le bootloader est d√©verrouill√©.
AICore a √©chou√© avec le type d'erreur 1-DOWNLOAD_ERROR et le code d'erreur 0-UNKNOWN : la fonctionnalit√© ... a √©chou√© avec l'√©tat d'√©chec 0 et l'erreur esz : UNAVAILABLE : impossible de r√©soudre l'h√¥te ...	Maintenez la connexion r√©seau, patientez quelques minutes, puis r√©essayez.
Remarque : Les d√©veloppeurs sont habitu√©s √† r√©initialiser les appareils, mais les utilisateurs le font beaucoup moins souvent. De plus, tous ces cas d'√©chec de la configuration peuvent √™tre d√©tect√©s par l'API checkFeatureStatus(). Veillez √† appeler checkFeatureStatus() avant d'afficher une UI associ√©e, afin que les utilisateurs de votre application ne voient jamais ces erreurs.
Ce contenu vous a-t-il √©t√© utile ?

Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/08/08 (UTC).

Communiquer
Blog
Bluesky
Instagram
LinkedIn
X (Twitter)
YouTube
Programmes
Google Developer Program
Google Developer Groups
Google Developer Experts
Accelerators
Women Techmakers
Google Cloud & NVIDIA
Consoles pour d√©veloppeurs
Google API Console
Google Cloud Platform Console
Google Play Console
Firebase Console
Actions on Google Console
Cast SDK Developer Console
Chrome Web Store Dashboard
Google Home Developer Console
Google Developers
Android
Chrome
Firebase
Google Cloud Platform
Google AI
Tous les produits
Conditions d'utilisation
R√®gles de confidentialit√©
S'inscrire √† la newsletter Google pour les d√©veloppeurs
S‚Äôabonner

Fran√ßais

Passer au contenu principal
ML Kit
ML Kit
Guides
R√©f√©rence
Exemples
√âtudes de cas
Plus
Recherche
/

Fran√ßais


Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Produits
ML Kit
Ce contenu vous a-t-il √©t√© utile ?

API GenAI Rewriting

bookmark_border
Cette API est propos√©e en version b√™ta et n'est soumise √† aucun contrat de niveau de service ni √† aucune r√®gle d'obsolescence. Des modifications peuvent √™tre apport√©es √† cette API et entra√Æner une rupture de la r√©trocompatibilit√©.


L'API GenAI Rewriting de ML Kit vous permet d'aider automatiquement les utilisateurs √† r√©√©crire des messages de chat ou de courts contenus avec un style ou un ton diff√©rents.

Les utilisateurs peuvent trouver utile de recevoir des suggestions sur la fa√ßon de r√©√©crire un contenu dans les cas suivants :

Restructurer un message pour le rendre plus professionnel lorsque vous communiquez avec des parties prenantes
Raccourcir un message pour le rendre plus adapt√© √† la publication sur les plates-formes de r√©seaux sociaux
Reformuler un message pour les locuteurs non natifs qui recherchent d'autres fa√ßons de communiquer le message
Capacit√©s cl√©s
L'API GenAI Rewriting de ML Kit peut r√©√©crire de courts contenus dans l'un des styles suivants :

D√©velopper : d√©veloppe le texte saisi en ajoutant des d√©tails et un langage descriptif.
Emojify : ajoute des emoji pertinents au texte saisi, le rendant plus expressif et amusant.
Raccourcir : condense le texte d'entr√©e en une version plus courte, tout en conservant le message principal.
Amical : r√©√©crit le texte saisi pour le rendre plus d√©contract√© et accessible, en utilisant un ton conversationnel.
Professionnel : r√©√©crit le texte saisi pour le rendre plus formel et plus adapt√© au monde des affaires, en utilisant un ton respectueux.
Reformuler : r√©√©crit le texte saisi en utilisant d'autres mots et structures de phrases, tout en conservant le sens d'origine.
Les requ√™tes renvoient au moins une suggestion. Si plusieurs suggestions sont renvoy√©es, les r√©sultats sont tri√©s par ordre d√©croissant de confiance.

Exemples de r√©sultats
Entr√©e	Style de r√©√©criture	Sortie
Souhaitez-vous que nous nous rencontrions pour en discuter ?	Professionnel	Souhaiteriez-vous que nous nous rencontrions √† nouveau pour en discuter plus en d√©tail ?
J'aimerais avoir le plaisir de votre compagnie lors d'une r√©union informelle chez moi samedi soir prochain.	Raccourcir	√áa te dirait de venir prendre un verre chez moi samedi soir ?
L'√©v√©nement a r√©ussi	D√©velopper	L'√©v√©nement a √©t√© un succ√®s retentissant, d√©passant toutes nos attentes et s'av√©rant √™tre un triomphe retentissant.
Prenons un caf√© bient√¥t	Ajouter des emoji	Allons prendre un caf√© ‚òï bient√¥t üëã.
Fournis le rapport d'ici la fin de la journ√©e	Amical	Pourriez-vous m'envoyer le rapport d'ici la fin de la journ√©e ?
Hey, j'ai besoin de √ßa au plus vite	Professionnel	Pourriez-vous nous fournir le document demand√© d√®s que possible ?
Le projet est en retard	Reformuler	La chronologie du projet doit √™tre ajust√©e pour respecter le d√©lai initial.
Premiers pas
Remarque : Cette API n√©cessite Android au niveau d'API 26 ou sup√©rieur.
Pour commencer √† utiliser l'API GenAI Rewriting, ajoutez cette d√©pendance au fichier de compilation de votre projet.


implementation("com.google.mlkit:genai-rewriting:1.0.0-beta1")
Ensuite, instanciez le client Rewriter avec les options requises, v√©rifiez si les fonctionnalit√©s de mod√®le sur l'appareil requises sont disponibles (et t√©l√©chargez-les si n√©cessaire), pr√©parez votre texte d'entr√©e en tant que requ√™te, ex√©cutez le processus de r√©√©criture pour obtenir des suggestions et lib√©rez les ressources.

Kotlin
Java

val textToRewrite = "The event was successful"

// Define task with selected input and output format
val rewriterOptions = RewriterOptions.builder(context)
    // OutputType can be one of the following: ELABORATE, EMOJIFY, SHORTEN,
    // FRIENDLY, PROFESSIONAL, REPHRASE
    .setOutputType(RewriterOptions.OutputType.ELABORATE)
    // Refer to RewriterOptions.Language for available languages
    .setLanguage(RewriterOptions.Language.ENGLISH)
    .build()
val rewriter = Rewriting.getClient(rewriterOptions)

suspend fun prepareAndStartRewrite() {
    // Check feature availability, status will be one of the following:
    // UNAVAILABLE, DOWNLOADABLE, DOWNLOADING, AVAILABLE
    val featureStatus = rewriter.checkFeatureStatus().await()

    if (featureStatus == FeatureStatus.DOWNLOADABLE) {
        // Download feature if necessary.
        // If downloadFeature is not called, the first inference request will
        // also trigger the feature to be downloaded if it's not already
        // downloaded.
        rewriter.downloadFeature(object : DownloadCallback {
            override fun onDownloadStarted(bytesToDownload: Long) { }

            override fun onDownloadFailed(e: GenAiException) { }

            override fun onDownloadProgress(totalBytesDownloaded: Long) {}

            override fun onDownloadCompleted() {
                startRewritingRequest(textToRewrite, rewriter)
            }
        })
    } else if (featureStatus == FeatureStatus.DOWNLOADING) {
        // Inference request will automatically run once feature is
        // downloaded.
        // If Gemini Nano is already downloaded on the device, the
        // feature-specific LoRA adapter model will be downloaded
        // quickly. However, if Gemini Nano is not already downloaded,
        // the download process may take longer.
        startRewritingRequest(textToRewrite, rewriter)
    } else if (featureStatus == FeatureStatus.AVAILABLE) {
        startRewritingRequest(textToRewrite, rewriter)
    }
}

suspend fun startRewritingRequest(text: String, rewriter: Rewriter) {
    // Create task request
    val rewritingRequest = RewritingRequest.builder(text).build()

    // Start rewriting request with non-streaming response
    // More than 1 result may be returned. If multiple suggestions are
    // returned, results will be sorted by descending confidence.
    val rewriteResults =
        rewriter.runInference(rewritingRequest).await().results

    // You can also start a streaming request
    // rewriter.runInference(rewritingRequest) { newText ->
    //    // Show new text in UI
    // }
}

// Be sure to release the resource when no longer needed
// For example, on viewModel.onCleared() or activity.onDestroy()
rewriter.close()
Fonctionnalit√©s et limites
L'API GenAI Rewriting est compatible avec les langues suivantes : anglais, fran√ßais, allemand, italien, japonais, cor√©en et espagnol. Elles sont d√©finies dans RewriterOptions.Language. La saisie doit comporter moins de 256 jetons.

La disponibilit√© de la configuration de fonctionnalit√© sp√©cifique (sp√©cifi√©e par RewriterOptions) peut varier en fonction de la configuration de l'appareil et des mod√®les qui ont √©t√© t√©l√©charg√©s sur l'appareil.

Le moyen le plus fiable pour les d√©veloppeurs de s'assurer que la fonctionnalit√© d'API pr√©vue est compatible avec un appareil dot√© de la RewriterOptions demand√©e consiste √† appeler la m√©thode checkFeatureStatus(). Cette m√©thode fournit l'√©tat d√©finitif de la disponibilit√© des fonctionnalit√©s sur l'appareil au moment de l'ex√©cution.

Remarque : Les variations mat√©rielles entre les diff√©rents types d'appareils peuvent entra√Æner des diff√©rences dans les versions du mod√®le de base Gemini Nano et, par cons√©quent, dans les r√©sultats de l'API ML Kit GenAI. Les adaptateurs LoRA sont appliqu√©s pour att√©nuer les diff√©rences.
Probl√®mes de configuration courants
Remarque : Cette API n'est pas compatible avec les appareils dont le bootloader est d√©verrouill√©.
Les API d'IA g√©n√©rative de ML Kit s'appuient sur l'application Android AICore pour acc√©der √† Gemini Nano. Lorsqu'un appareil vient d'√™tre configur√© (y compris r√©initialis√©) ou que l'application AICore vient d'√™tre r√©initialis√©e (par exemple, en effa√ßant les donn√©es, puis en la d√©sinstallant et en la r√©installant), il se peut que l'application AICore n'ait pas eu suffisamment de temps pour terminer l'initialisation (y compris le t√©l√©chargement des derni√®res configurations depuis le serveur). Par cons√©quent, il est possible que les API ML Kit GenAI ne fonctionnent pas comme pr√©vu. Voici les messages d'erreur de configuration courants que vous pouvez rencontrer et comment les r√©soudre :

Exemple de message d'erreur	Comment r√©pondre
AICore a √©chou√© avec le type d'erreur 4-CONNECTION_ERROR et le code d'erreur 601-BINDING_FAILURE : √©chec de la liaison du service AICore.	Cela peut se produire lorsque vous installez l'application √† l'aide des API ML Kit GenAI imm√©diatement apr√®s la configuration de l'appareil ou lorsque AICore est d√©sinstall√© apr√®s l'installation de votre application. Mettre √† jour l'application AICore, puis r√©installer votre application devrait r√©soudre le probl√®me.
AICore a √©chou√© avec le type d'erreur 3-PREPARATION_ERROR et le code d'erreur 606-FEATURE_NOT_FOUND : la fonctionnalit√© ... n'est pas disponible.	Cela peut se produire lorsque AICore n'a pas termin√© de t√©l√©charger les derni√®res configurations. Lorsque l'appareil est connect√© √† Internet, la mise √† jour prend g√©n√©ralement de quelques minutes √† quelques heures. Red√©marrer l'appareil peut acc√©l√©rer la mise √† jour.

Notez que si le bootloader de l'appareil est d√©verrouill√©, cette erreur s'affichera √©galement. Cette API n'est pas compatible avec les appareils dont le bootloader est d√©verrouill√©.
AICore a √©chou√© avec le type d'erreur 1-DOWNLOAD_ERROR et le code d'erreur 0-UNKNOWN : la fonctionnalit√© ... a √©chou√© avec l'√©tat d'√©chec 0 et l'erreur esz : UNAVAILABLE : impossible de r√©soudre l'h√¥te ...	Maintenez la connexion r√©seau, patientez quelques minutes, puis r√©essayez.
Remarque : Les d√©veloppeurs sont habitu√©s √† r√©initialiser les appareils, mais les utilisateurs le font beaucoup moins souvent. De plus, tous ces cas d'√©chec de la configuration peuvent √™tre d√©tect√©s par l'API checkFeatureStatus(). Veillez √† appeler checkFeatureStatus() avant d'afficher une UI associ√©e, afin que les utilisateurs de votre application ne voient jamais ces erreurs.
Ce contenu vous a-t-il √©t√© utile ?

Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/08/08 (UTC).

Communiquer
Blog
Bluesky
Instagram
LinkedIn
X (Twitter)
YouTube
Programmes
Google Developer Program
Google Developer Groups
Google Developer Experts
Accelerators
Women Techmakers
Google Cloud & NVIDIA
Consoles pour d√©veloppeurs
Google API Console
Google Cloud Platform Console
Google Play Console
Firebase Console
Actions on Google Console
Cast SDK Developer Console
Chrome Web Store Dashboard
Google Home Developer Console
Google Developers
Android
Chrome
Firebase
Google Cloud Platform
Google AI
Tous les produits
Conditions d'utilisation
R√®gles de confidentialit√©
S'inscrire √† la newsletter Google pour les d√©veloppeurs
S‚Äôabonner

Fran√ßais

Passer au contenu principal
ML Kit
ML Kit
Guides
R√©f√©rence
Exemples
√âtudes de cas
Plus
Recherche
/

Fran√ßais


Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Produits
ML Kit
Ce contenu vous a-t-il √©t√© utile ?

API GenAI Image Description

bookmark_border
Cette API est propos√©e en version b√™ta et n'est soumise √† aucun contrat de niveau de service ni √† aucune r√®gle d'obsolescence. Des modifications peuvent √™tre apport√©es √† cette API et entra√Æner une rupture de la r√©trocompatibilit√©.


L'API GenAI Image Description de ML Kit vous permet de g√©n√©rer de courtes descriptions de contenu pour les images. Cela peut √™tre utile dans les cas d'utilisation suivants :

G√©n√©rer des titres d'images
G√©n√©ration de texte alternatif pour aider les utilisateurs malvoyants √† mieux comprendre le contenu des images
Utiliser les descriptions g√©n√©r√©es comme m√©tadonn√©es pour aider les utilisateurs √† rechercher ou organiser des images
Utiliser de courtes descriptions d'images lorsque l'utilisateur ne peut pas regarder son √©cran, par exemple lorsqu'il conduit ou √©coute un podcast
Capacit√©s cl√©s
Retourner une br√®ve description pour une image d'entr√©e
Exemples de r√©sultats
Entr√©e	Sortie
Un petit robot Android vert avec un design en forme de cactus est assis sur une surface noire.	Un petit robot Android vert avec un design en forme de cactus est assis sur une surface noire.
Un petit chien blanc avec un nez noir et une langue rose court dans un champ herbeux avec un pont en arri√®re-plan.	Un petit chien blanc avec un nez noir et une langue rose court dans un champ herbeux avec un pont en arri√®re-plan.
Premiers pas
Remarque : Cette API n√©cessite Android au niveau d'API 26 ou sup√©rieur.
Pour commencer √† utiliser l'API GenAI Image Description, ajoutez cette d√©pendance au fichier de compilation de votre projet.


implementation("com.google.mlkit:genai-image-description:1.0.0-beta1")
Pour int√©grer l'API Image Description √† votre application, vous devez d'abord obtenir un client ImageDescriber. Vous devez ensuite v√©rifier l'√©tat des fonctionnalit√©s de mod√®le sur l'appareil n√©cessaires et t√©l√©charger le mod√®le s'il n'est pas d√©j√† sur l'appareil. Apr√®s avoir pr√©par√© votre entr√©e d'image dans un ImageDescriptionRequest, ex√©cutez l'inf√©rence √† l'aide du client pour obtenir le texte de description de l'image. Enfin, n'oubliez pas de fermer le client pour lib√©rer les ressources.

Kotlin
Java

// Create an image describer
val options = ImageDescriberOptions.builder(context).build()
val imageDescriber = ImageDescription.getClient(options)

suspend fun prepareAndStartImageDescription(
    bitmap: Bitmap
) {
  // Check feature availability, status will be one of the following:
  // UNAVAILABLE, DOWNLOADABLE, DOWNLOADING, AVAILABLE
  val featureStatus = imageDescriber.checkFeatureStatus().await()

  if (featureStatus == FeatureStatus.DOWNLOADABLE) {
      // Download feature if necessary.
      // If downloadFeature is not called, the first inference request
      // will also trigger the feature to be downloaded if it's not
      // already downloaded.
      imageDescriber.downloadFeature(object : DownloadCallback {
          override fun onDownloadStarted(bytesToDownload: Long) { }

          override fun onDownloadFailed(e: GenAiException) { }

          override fun onDownloadProgress(totalBytesDownloaded: Long) {}

          override fun onDownloadCompleted() {
              startImageDescriptionRequest(bitmap, imageDescriber)
          }
      })
  } else if (featureStatus == FeatureStatus.DOWNLOADING) {
      // Inference request will automatically run once feature is
      // downloaded.
      // If Gemini Nano is already downloaded on the device, the
      // feature-specific LoRA adapter model will be downloaded
      // very quickly. However, if Gemini Nano is not already
      // downloaded, the download process may take longer.
      startImageDescriptionRequest(bitmap, imageDescriber)
  } else if (featureStatus == FeatureStatus.AVAILABLE) {
      startImageDescriptionRequest(bitmap, imageDescriber)
  }
}

fun startImageDescriptionRequest(
    bitmap: Bitmap,
    imageDescriber: ImageDescriber
) {
    // Create task request
    val imageDescriptionRequest = ImageDescriptionRequest
        .builder(bitmap)
        .build()
}

  // Run inference with a streaming callback
  val imageDescriptionResultStreaming =
      imageDescriber.runInference(imageDescriptionRequest) { outputText ->
          // Append new output text to show in UI
          // This callback is called incrementally as the description
          // is generated
      }

  // You can also get a non-streaming response from the request
  // val imageDescription = imageDescriber.runInference(
  //        imageDescriptionRequest).await().description
}

// Be sure to release the resource when no longer needed
// For example, on viewModel.onCleared() or activity.onDestroy()
imageDescriber.close()
Fonctionnalit√©s et limites
L'API GenAI Image Description est compatible avec l'anglais. D'autres langues seront ajout√©es √† l'avenir. L'API renvoie une br√®ve description de l'image.

La disponibilit√© de la configuration de la fonctionnalit√© sp√©cifique (sp√©cifi√©e par ImageDescriberOptions) peut varier en fonction de la configuration de l'appareil et des mod√®les qui ont √©t√© t√©l√©charg√©s sur l'appareil.

Le moyen le plus fiable pour les d√©veloppeurs de s'assurer que la fonctionnalit√© d'API pr√©vue est compatible avec un appareil dot√© de la ImageDescriberOptions demand√©e consiste √† appeler la m√©thode checkFeatureStatus(). Cette m√©thode fournit l'√©tat d√©finitif de la disponibilit√© des fonctionnalit√©s sur l'appareil au moment de l'ex√©cution.

Remarque : Les variations mat√©rielles entre les diff√©rents types d'appareils peuvent entra√Æner des diff√©rences dans les versions du mod√®le de base Gemini Nano et, par cons√©quent, dans les r√©sultats de l'API ML Kit GenAI. Les adaptateurs LoRA sont appliqu√©s pour att√©nuer les diff√©rences.
Probl√®mes de configuration courants
Remarque : Cette API n'est pas compatible avec les appareils dont le bootloader est d√©verrouill√©.
Les API d'IA g√©n√©rative de ML Kit s'appuient sur l'application Android AICore pour acc√©der √† Gemini Nano. Lorsqu'un appareil vient d'√™tre configur√© (y compris r√©initialis√©) ou que l'application AICore vient d'√™tre r√©initialis√©e (par exemple, en effa√ßant les donn√©es, puis en la d√©sinstallant et en la r√©installant), il se peut que l'application AICore n'ait pas eu suffisamment de temps pour terminer l'initialisation (y compris le t√©l√©chargement des derni√®res configurations depuis le serveur). Par cons√©quent, il est possible que les API ML Kit GenAI ne fonctionnent pas comme pr√©vu. Voici les messages d'erreur de configuration courants que vous pouvez rencontrer et comment les r√©soudre :

Exemple de message d'erreur	Comment r√©pondre
AICore a √©chou√© avec le type d'erreur 4-CONNECTION_ERROR et le code d'erreur 601-BINDING_FAILURE : √©chec de la liaison du service AICore.	Cela peut se produire lorsque vous installez l'application √† l'aide des API ML Kit GenAI imm√©diatement apr√®s la configuration de l'appareil ou lorsque AICore est d√©sinstall√© apr√®s l'installation de votre application. Mettre √† jour l'application AICore, puis r√©installer votre application devrait r√©soudre le probl√®me.
AICore a √©chou√© avec le type d'erreur 3-PREPARATION_ERROR et le code d'erreur 606-FEATURE_NOT_FOUND : la fonctionnalit√© ... n'est pas disponible.	Cela peut se produire lorsque AICore n'a pas termin√© de t√©l√©charger les derni√®res configurations. Lorsque l'appareil est connect√© √† Internet, la mise √† jour prend g√©n√©ralement de quelques minutes √† quelques heures. Red√©marrer l'appareil peut acc√©l√©rer la mise √† jour.

Notez que si le bootloader de l'appareil est d√©verrouill√©, cette erreur s'affichera √©galement. Cette API n'est pas compatible avec les appareils dont le bootloader est d√©verrouill√©.
AICore a √©chou√© avec le type d'erreur 1-DOWNLOAD_ERROR et le code d'erreur 0-UNKNOWN : la fonctionnalit√© ... a √©chou√© avec l'√©tat d'√©chec 0 et l'erreur esz : UNAVAILABLE : impossible de r√©soudre l'h√¥te ...	Maintenez la connexion r√©seau, patientez quelques minutes, puis r√©essayez.
Remarque : Les d√©veloppeurs sont habitu√©s √† r√©initialiser les appareils, mais les utilisateurs le font beaucoup moins souvent. De plus, tous ces cas d'√©chec de la configuration peuvent √™tre d√©tect√©s par l'API checkFeatureStatus(). Veillez √† appeler checkFeatureStatus() avant d'afficher une UI associ√©e, afin que les utilisateurs de votre application ne voient jamais ces erreurs.
Ce contenu vous a-t-il √©t√© utile ?

Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/08/08 (UTC).

Communiquer
Blog
Bluesky
Instagram
LinkedIn
X (Twitter)
YouTube
Programmes
Google Developer Program
Google Developer Groups
Google Developer Experts
Accelerators
Women Techmakers
Google Cloud & NVIDIA
Consoles pour d√©veloppeurs
Google API Console
Google Cloud Platform Console
Google Play Console
Firebase Console
Actions on Google Console
Cast SDK Developer Console
Chrome Web Store Dashboard
Google Home Developer Console
Google Developers
Android
Chrome
Firebase
Google Cloud Platform
Google AI
Tous les produits
Conditions d'utilisation
R√®gles de confidentialit√©
S'inscrire √† la newsletter Google pour les d√©veloppeurs
S‚Äôabonner

Fran√ßais

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide sur les solutions MediaPipe
MediaPipe Solutions fournit une suite de biblioth√®ques et d'outils qui vous permettent d'appliquer rapidement des techniques d'intelligence artificielle (IA) et de machine learning (ML) dans vos applications. Vous pouvez int√©grer ces solutions √† vos applications imm√©diatement, les personnaliser en fonction de vos besoins et les utiliser sur plusieurs plates-formes de d√©veloppement. MediaPipe Solutions fait partie du projet Open Source MediaPipe. Vous pouvez donc personnaliser davantage le code des solutions pour r√©pondre aux besoins de votre application. La suite MediaPipe Solutions comprend les √©l√©ments suivants:

MediaPipe Solutions, Studio et Model Maker

Ces biblioth√®ques et ressources fournissent les fonctionnalit√©s de base de chaque solution MediaPipe:

MediaPipe Tasks: API et biblioth√®ques multiplates-formes pour le d√©ploiement de solutions. En savoir plus
Mod√®les MediaPipe: mod√®les pr√©-entra√Æn√©s et pr√™ts √† l'emploi √† utiliser avec chaque solution.
Ces outils vous permettent de personnaliser et d'√©valuer des solutions:

MediaPipe Model Maker: personnalisez des mod√®les pour des solutions avec vos donn√©es. En savoir plus
MediaPipe Studio: visualisez, √©valuez et comparez les solutions dans votre navigateur. En savoir plus
Solutions disponibles
MediaPipe Solutions est disponible sur plusieurs plates-formes. Chaque solution comprend un ou plusieurs mod√®les, et vous pouvez √©galement personnaliser les mod√®les pour certaines solutions. La liste suivante indique les solutions disponibles pour chaque plate-forme compatible et si vous pouvez utiliser Model Maker pour personnaliser le mod√®le:

Solution	Android	Web	Python	iOS	Personnaliser le mod√®le
API d'inf√©rence LLM	cercle plein	cercle plein		cercle plein	cercle plein
D√©tection d'objets	cercle plein	cercle plein	cercle plein	cercle plein	cercle plein
Classification d'images	cercle plein	cercle plein	cercle plein	cercle plein	cercle plein
Segmentation d'image	cercle plein	cercle plein	cercle plein		
Segmentation interactive	cercle plein	cercle plein	cercle plein		
D√©tection des points de rep√®re de la main	cercle plein	cercle plein	cercle plein	cercle plein	
Reconnaissance de gestes	cercle plein	cercle plein	cercle plein	cercle plein	cercle plein
Embedding d'images	cercle plein	cercle plein	cercle plein		
D√©tection de visages	cercle plein	cercle plein	cercle plein	cercle plein	
D√©tection des points de rep√®re sur le visage	cercle plein	cercle plein	cercle plein		
—Å—Ç–∏–ª–∏–∑–∞—Ü–∏–∏ –ª–∏—Ü–∞	cercle plein	cercle plein	cercle plein		cercle plein
D√©tection des points de rep√®re de la pose	cercle plein	cercle plein	cercle plein		
G√©n√©ration d'images	cercle plein				cercle plein
Classification de texte	cercle plein	cercle plein	cercle plein	cercle plein	cercle plein
Embedding textuel	cercle plein	cercle plein	cercle plein		
D√©tecteur de langue	cercle plein	cercle plein	cercle plein		
Classification audio	cercle plein	cercle plein	cercle plein		
Commencer
Pour commencer √† utiliser MediaPipe Solutions, s√©lectionnez l'une des t√¢ches list√©es dans l'arborescence de navigation de gauche, y compris les t√¢ches de vision, de texte et d'audio. Si vous avez besoin d'aide pour configurer un environnement de d√©veloppement √† utiliser avec MediaPipe Tasks, consultez les guides de configuration pour Android, les applications Web et Python.

Anciennes solutions
Les anciennes solutions MediaPipe list√©es ci-dessous ne sont plus prises en charge depuis le 1er mars 2023. Toutes les autres anciennes solutions MediaPipe seront migr√©es vers une nouvelle solution MediaPipe. Pour en savoir plus, consultez la liste ci-dessous. Le d√©p√¥t de code et les binaires pr√©compil√©s de toutes les anciennes solutions MediaPipe continueront d'√™tre fournis tels quels.

Ancienne solution	√âtat	Nouvelle solution MediaPipe
D√©tection de visages (infos)	Mis √† niveau	D√©tection de visages
Face Mesh (infos)	Mis √† niveau	D√©tection des points de rep√®re sur le visage
Iris (infos)	Mis √† niveau	D√©tection des points de rep√®re sur le visage
Mains (infos)	Mis √† niveau	D√©tection des points de rep√®re de la main
Posture (infos)	Mis √† niveau	D√©tection des points de rep√®re de la posture
Global (infos)	Mettre √† niveau	D√©tection globale des points de rep√®re
Segmentation des selfies (infos)	Mis √† niveau	Segmentation d'images
Segmentation des cheveux (infos)	Mis √† niveau	Segmentation d'images
D√©tection d'objets (infos)	Mis √† niveau	D√©tection d'objets
Suivi des colis (infos)	Fin de la compatibilit√©	
Suivi du mouvement instantan√© (infos)	Fin de la compatibilit√©	
Objectron (infos)	Fin de la compatibilit√©	
KNIFT (infos)	Fin de la compatibilit√©	
Inversion automatique (infos)	Fin de la compatibilit√©	
MediaSequence (info)	Fin de la compatibilit√©	
YouTube 8 millions (infos)	Fin de la compatibilit√©	
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires√Ä propos de l'aper√ßu
Bienvenue dans l'aper√ßu des solutions MediaPipe. Nous avons h√¢te de d√©couvrir ce que vous allez d√©velopper avec cette nouvelle offre et de lire vos commentaires.

Le machine learning sur appareil √©volue. Les cas d'utilisation sont plus complexes et n√©cessiteront peut-√™tre de nombreux mod√®les multimodaux, des processus sp√©cifiques √† un domaine et des pipelines √©troitement int√©gr√©s pour des performances de bout en bout. Cette nouvelle solution MediaPipe est une unification de plusieurs outils existants: les solutions, la biblioth√®que de t√¢ches TensorFlow Lite et le outil de cr√©ation de mod√®les TensorFlow Lite.

MediaPipe Tasks: API n√©cessitant peu de code pour cr√©er et d√©ployer des solutions de ML avanc√©es sur plusieurs plates-formes.

Nouveaut√©s: va au-del√† de l'inf√©rence de mod√®le unique avec des performances de pipeline optimis√©es de bout en bout
Ce qui reste inchang√©: m√™me workflow facile pour d√©ployer une solution avec quelques lignes de code.
MediaPipe Model Maker: API n√©cessitant peu de code pour personnaliser des solutions √† l'aide de vos propres donn√©es.

Nouveaut√©s: refactoris√© pour prendre en charge des cas d'utilisation robustes et l'infrastructure MediaPipe Studio
Ce qui reste inchang√©: m√™me workflow facile pour personnaliser un mod√®le avec quelques lignes de code.
MediaPipe Studio: visualisez et comparez des solutions.

Nouveaut√©s: nouvelle fa√ßon d'importer et de visualiser les solutions compatibles
Ce qui ne change pas: l'expertise de Google en ML pour am√©liorer les performances et l'√©volutivit√©
Explorez les exemples et commencez √† cr√©er vos solutions de ML √† l'aide des guides du d√©veloppeur.



Conditions d'utilisation de l'aper√ßu des solutions MediaPipe
Derni√®re modification: 2 f√©vrier 2023

L'utilisation de MediaPipe Solutions Preview est soumise aux Conditions d'utilisation des API Google, au R√®glement sur les donn√©es utilisateur dans les services d'API Google, aux Conditions d'utilisation de l'API MediaPipe et aux conditions ci-dessous.

La version preview des solutions MediaPipe est soumise aux limites suivantes: sa compatibilit√© peut √™tre limit√©e, les modifications peuvent ne pas √™tre compatibles avec d'autres versions de pr√©-disponibilit√© g√©n√©rale et la disponibilit√© peut changer sans pr√©avis.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesT√¢ches MediaPipe
MediaPipe Tasks fournit l'interface de programmation principale de MediaPipe Suite de solutions comprenant un ensemble de biblioth√®ques permettant de d√©ployer des mod√®les de ML innovants sur des appareils avec un minimum de code. Il prend en charge plusieurs plateformes, y compris Android, Web / JavaScript, Python et la prise en charge d'iOS sera bient√¥t prise en charge.

API multiplates-formes bien d√©finies et faciles √† utiliser
Ex√©cutez des inf√©rences de ML avec seulement cinq lignes de code. Utilisez l'outil puissant et simple d'utilisation API de solution dans MediaPipe Tasks pour cr√©er vos propres mod√®les de ML caract√©ristiques.

Solutions personnalisables
Vous pouvez profiter de tous les avantages de MediaPipe Tasks et le personnaliser facilement √† l'aide de mod√®les cr√©√©s avec vos propres donn√©es via l'API Model Maker. Par exemple, vous pouvez cr√©er un mod√®le qui reconna√Æt les gestes personnalis√©s que vous avez d√©finis √† l'aide de Model Maker l'API GestReconnaissance, et d√©ployer le mod√®le sur les plates-formes souhait√©es √† l'aide de l'outil de reconnaissance de gestes Tasks API.

Pipelines de ML hautes performances
En g√©n√©ral, les solutions de ML int√©gr√©es √† l'appareil combinent plusieurs blocs de ML et d'autres blocs, ce qui ralentit des performances. MediaPipe Tasks fournit des pipelines de ML optimis√©s avec une approche sur le processeur, le GPU et le TPU pour r√©pondre aux besoins d'une utilisation en temps r√©el sur l'appareil cas d'utilisation.

Plates-formes compatibles
Cette section pr√©sente les t√¢ches MediaPipe pour chaque t√¢che Google Cloud. Pour des impl√©mentations sp√©cifiques, consultez la documentation sur les impl√©mentations guides relatifs √† chaque t√¢che. Pour vous aider √† configurer votre environnement de d√©veloppement pour utiliser MediaPipe Tasks sur une plate-forme, consultez la configuration de la plate-forme .

<ph type="x-smartling-placeholder">
</ph> Attention:Cet aper√ßu de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Android
T√¢ches MediaPipe Java API for Android est divis√© en packages qui ex√©cutent des t√¢ches de ML dans des domaines principaux, comme la vision, le langage naturel et l'audio. Voici une liste des que vous pouvez ajouter √† votre projet de d√©veloppement d'applications Android API:


dependencies {
    implementation 'com.google.mediapipe:tasks-vision:latest.release'
    implementation 'com.google.mediapipe:tasks-text:latest.release'
    implementation 'com.google.mediapipe:tasks-audio:latest.release'
}
Pour en savoir plus sur l'impl√©mentation, consultez la documentation des guides pour chaque solution dans MediaPipe Tasks.

Python
L'API Python MediaPipe Tasks dispose de quelques fonctionnalit√©s des modules d√©di√©s aux solutions qui ex√©cutent des t√¢ches de ML dans des domaines majeurs : vision, vision, en langage naturel et en audio. Vous trouverez ci-dessous la commande d'installation liste des importations que vous pouvez ajouter √† votre projet de d√©veloppement Python pour activer ces API:


$ python -m pip install mediapipe

import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from mediapipe.tasks.python import text
from mediapipe.tasks.python import audio
Pour en savoir plus sur l'impl√©mentation, consultez la documentation des guides pour chaque solution dans MediaPipe Tasks.

Web et JavaScript
L'API Web JavaScript MediaPipe Tasks est divis√© en packages qui ex√©cutent des t√¢ches de ML dans des domaines majeurs : vision, en langage naturel et en audio. Voici une liste d'importations de scripts que vous pouvez √† votre projet de d√©veloppement Web et JavaScript pour activer les API suivantes:


<head>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-text/text_bundle.js"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-audio/audio_bundle.js"
    crossorigin="anonymous"></script>
</head>
Pour en savoir plus sur l'impl√©mentation, consultez la documentation des guides pour chaque solution dans MediaPipe Tasks.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesMediaPipe Model Maker
MediaPipe Model Maker est un outil permettant de personnaliser des mod√®les de machine learning (ML) existants afin de les utiliser avec vos donn√©es et vos applications. Vous pouvez utiliser cet outil comme alternative plus rapide √† la cr√©ation et √† l'entra√Ænement d'un nouveau mod√®le de ML. Model Maker utilise une technique d'entra√Ænement de ML appel√©e apprentissage par transfert, qui r√©entra√Æne des mod√®les existants avec de nouvelles donn√©es. Cette technique r√©utilise une partie significative de la logique du mod√®le existant, ce qui signifie que l'entra√Ænement prend moins de temps que l'entra√Ænement d'un nouveau mod√®le et peut √™tre effectu√© avec moins de donn√©es.

Model Maker fonctionne sur diff√©rents types de mod√®les, y compris la d√©tection d'objets, la reconnaissance de gestes ou les classificateurs d'images, de texte ou de donn√©es audio. L'outil r√©entra√Æne les mod√®les en supprimant les derni√®res couches du mod√®le qui classent les donn√©es dans des cat√©gories sp√©cifiques, puis √† reconstruire ces couches √† l'aide des nouvelles donn√©es que vous fournissez. Model Maker prend √©galement en charge une option permettant d'affiner les couches de mod√®le afin d'am√©liorer la pr√©cision et les performances.

Mod√®le de machine learning montrant des couches de classification supprim√©es et remplac√©es

Figure 1. Model Maker supprime les derni√®res couches d'un mod√®le existant et les recompile avec de nouvelles donn√©es.

Le r√©entra√Ænement d'un mod√®le √† l'aide de Model Maker le r√©duit g√©n√©ralement, en particulier si vous entra√Ænez le nouveau mod√®le pour qu'il reconnaisse moins d'√©l√©ments. Vous pouvez ainsi utiliser Model Maker pour cr√©er des mod√®les plus cibl√©s et mieux adapt√©s √† votre application. Cet outil peut √©galement vous aider √† appliquer des techniques de ML telles que la quantification afin que votre mod√®le utilise moins de ressources et s'ex√©cute plus efficacement.

Exigences relatives aux donn√©es d'entra√Ænement
Model Maker vous permet de r√©entra√Æner des mod√®les avec beaucoup moins de donn√©es que d'entra√Æner un nouveau mod√®le. Lorsque vous r√©entra√Ænez un mod√®le avec de nouvelles donn√©es, vous devez viser √† disposer d'environ 100 √©chantillons de donn√©es pour chaque classe entra√Æn√©e. Par exemple, si vous r√©entra√Ænez un mod√®le de classification d'images pour qu'il reconnaisse les chats, les chiens et les perroquets, vous devriez disposer d'environ 100 images de chats, 100 images de chiens et 100 images de perroquets. En fonction de votre application, vous pourrez peut-√™tre r√©entra√Æner un mod√®le utile avec encore moins de donn√©es par cat√©gorie. Toutefois, un ensemble de donn√©es plus volumineux am√©liore g√©n√©ralement la justesse du mod√®le. Lorsque vous cr√©ez votre ensemble de donn√©es d'entra√Ænement, n'oubliez pas que vos donn√©es d'entra√Ænement sont divis√©es lors du processus de r√©entra√Ænement, g√©n√©ralement 80% pour l'entra√Ænement, 10% pour le test et le reste pour la validation.

Limites de la personnalisation
√âtant donn√© que le processus de r√©entra√Ænement supprime les couches de classification pr√©c√©dentes, le mod√®le r√©sultant ne peut reconna√Ætre que les √©l√©ments, ou classes, fournis dans les nouvelles donn√©es. Si l'ancien mod√®le a √©t√© entra√Æn√© √† reconna√Ætre 30 classes d'√©l√©ments diff√©rentes et que vous utilisez Model Maker pour r√©entra√Æner 10 √©l√©ments diff√©rents avec des donn√©es, le mod√®le obtenu ne pourra reconna√Ætre que ces 10 nouveaux √©l√©ments.

Le r√©entra√Ænement d'un mod√®le avec Model Maker ne peut pas modifier ce √† quoi le mod√®le de ML d'origine a √©t√© con√ßu, m√™me si ces t√¢ches sont similaires. Par exemple, vous ne pouvez pas l'utiliser pour faire en sorte qu'un mod√®le de classification d'images effectue la d√©tection d'objets, m√™me si ces t√¢ches ont des similitudes.

Commencer
Vous pouvez commencer √† utiliser MediaPipe Model Maker en ex√©cutant l'un des tutoriels de personnalisation de la solution pour les solutions MediaPipe, tels que Classification d'images.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesMediaPipe Studio
MediaPipe Studio est une application Web permettant d'√©valuer et de personnaliser les pipelines et les mod√®les de ML sur l'appareil pour vos applications. Cette application vous permet de tester rapidement les solutions MediaPipe dans votre navigateur avec vos propres donn√©es et mod√®les de ML personnalis√©s. Chaque d√©monstration de solution vous permet √©galement de tester les param√®tres du mod√®le pour le nombre total de r√©sultats, le seuil de confiance minimal pour la cr√©ation de rapports sur les r√©sultats, etc.

Essayerarrow_forward

Cette page fournit des instructions rapides pour utiliser des solutions dans MediaPipe Studio.

Capture d&#39;√©cran de l&#39;application Web MediaPipe Studio Figure 1. Page d'accueil de l'application MediaPipe Studio

Mod√®les personnalis√©s
Mod√®le de s√©lection d&#39;application MediaPipe StudioVous pouvez utiliser des mod√®les personnalis√©s avec MediaPipe dans l'option S√©lection du mod√®le. Pour ce faire, cliquez sur S√©lectionner un fichier de mod√®le, puis s√©lectionnez un mod√®le dans votre espace de stockage, comme illustr√© dans la capture d'√©cran.

Le mod√®le que vous choisissez doit √™tre conforme aux exigences d'entr√©e et de sortie du mod√®le de l'API MediaPipe Tasks que vous utilisez et inclure des m√©tadonn√©es de mod√®le compatibles. Le moyen le plus rapide de cr√©er un mod√®le √† utiliser avec une API MediaPipe Tasks consiste √† modifier un mod√®le de solution compatible avec vos propres donn√©es √† l'aide de l'outil MediaPipe Model Maker. Pour en savoir plus, consultez MediaPipe Model Maker.

Donn√©es d'entr√©e personnalis√©es
Vous pouvez utiliser vos propres donn√©es sur chaque page de la solution dans MediaPipe Studio. Pour les t√¢ches textuelles, vous pouvez saisir du texte dans le champ pr√©vu √† cet effet. Les t√¢ches Vision vous permettent d'utiliser une webcam en tant qu'entr√©e. Vous pouvez √©galement importer des images √† l'aide de l'option S√©lectionner un fichier image dans le menu d√©roulant, comme indiqu√© ci-dessous:

Image de s√©lection de l&#39;application MediaPipe Studio

Commencer
Vous pouvez commencer √† utiliser MediaPipe Studio en ex√©cutant l'une des d√©monstrations de la solution, telle que la classification d'images, puis utiliser le guide du d√©veloppeur associ√© pour int√©grer cette fonctionnalit√© dans votre propre application.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide d'inf√©rence LLM


Remarque :L'utilisation de l'API d'inf√©rence LLM MediaPipe est soumise au R√®glement sur les utilisations interdites de l'IA g√©n√©rative.
L'API d'inf√©rence LLM vous permet d'ex√©cuter des grands mod√®les de langage (LLM) enti√®rement sur l'appareil, que vous pouvez utiliser pour effectuer un large √©ventail de t√¢ches, telles que la g√©n√©ration de texte, la r√©cup√©ration d'informations sous forme de langage naturel et le r√©sum√© de documents. La t√¢che est compatible avec plusieurs grands mod√®les de langage de texte √† texte, ce qui vous permet d'appliquer les derniers mod√®les d'IA g√©n√©rative sur l'appareil √† vos applications et produits.

Essayez !arrow_forward

La t√¢che est compatible avec de nombreux LLM. Les mod√®les h√©berg√©s sur la page de la communaut√© LITERAT sont disponibles dans un format compatible avec MediaPipe et ne n√©cessitent aucune √©tape de conversion ou de compilation suppl√©mentaire.

Vous pouvez utiliser AI Edge Torch pour exporter des mod√®les PyTorch vers des mod√®les LiteRT (tflite) multisignature, qui sont regroup√©s avec des param√®tres de tokenizer pour cr√©er des bundles de t√¢ches. Les mod√®les convertis avec AI Edge Torch sont compatibles avec l'API d'inf√©rence LLM et peuvent s'ex√©cuter sur le backend du processeur, ce qui les rend adapt√©s aux applications Android et iOS.

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez l'un de ces guides d'impl√©mentation pour votre plate-forme cible. Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter cette t√¢che de mani√®re basique, avec des exemples de code qui utilisent un mod√®le disponible et les options de configuration recommand√©es:

Web:

Guide
Exemple de code
Android :

Guide
Exemple de code
iOS

Guide
Exemple de code
D√©tails de la t√¢che
Cette section d√©crit les fonctionnalit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Fonctionnalit√©s
L'API LLM Inference contient les principales fonctionnalit√©s suivantes:

G√©n√©ration de texte √† partir de texte : g√©n√®re du texte √† partir d'une requ√™te textuelle.
S√©lection de LLM : appliquez plusieurs mod√®les pour adapter l'application √† vos cas d'utilisation sp√©cifiques. Vous pouvez √©galement r√©entra√Æner le mod√®le et appliquer des poids personnalis√©s.
Compatibilit√© avec LoRA : √©tendez et personnalisez les fonctionnalit√©s du LLM avec le mod√®le LoRA en l'entra√Ænant sur l'ensemble de donn√©es complet ou en utilisant des mod√®les LoRA pr√©d√©finis pr√©par√©s par la communaut√© Open Source (non compatibles avec les mod√®les convertis avec l'API g√©n√©rative Torch Edge AI).
Entr√©es de t√¢che	Sorties de t√¢che
L'API d'inf√©rence LLM accepte les entr√©es suivantes:
Requ√™te textuelle (par exemple, une question, l'objet d'un e-mail ou un document √† r√©sumer)
L'API LLM Inference g√©n√®re les r√©sultats suivants:
Texte g√©n√©r√© en fonction de la requ√™te d'entr√©e (par exemple, une r√©ponse √† la question, un brouillon d'e-mail ou un r√©sum√© du document)
Options de configuration
Cette t√¢che propose les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
modelPath	Chemin d'acc√®s au r√©pertoire du projet dans lequel le mod√®le est stock√©.	CHEMIN	N/A
maxTokens	Nombre maximal de jetons (jetons d'entr√©e + jetons de sortie) g√©r√©s par le mod√®le.	Entier	512
topK	Nombre de jetons que le mod√®le prend en compte √† chaque √©tape de g√©n√©ration. Limite les pr√©dictions aux k jetons les plus probables.	Entier	40
temperature	Quantit√© de hasard introduite lors de la g√©n√©ration. Une temp√©rature plus √©lev√©e g√©n√®re un texte plus cr√©atif, tandis qu'une temp√©rature plus basse g√©n√®re un texte plus pr√©visible.	Float	0,8
randomSeed	Graine al√©atoire utilis√©e lors de la g√©n√©ration de texte.	Entier	0
loraPath	Chemin absolu vers le mod√®le LoRA localement sur l'appareil. Remarque: Cette option n'est compatible qu'avec les mod√®les de GPU.	CHEMIN	N/A
resultListener	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats de mani√®re asynchrone. Ne s'applique que lorsque vous utilisez la m√©thode de g√©n√©ration asynchrone.	N/A	N/A
errorListener	D√©finit un √©couteur d'erreur facultatif.	N/A	N/A
Mod√®les
L'API d'inf√©rence LLM est compatible avec de nombreux grands mod√®les de langage de texte √† texte, y compris la prise en charge int√©gr√©e de plusieurs mod√®les optimis√©s pour s'ex√©cuter sur les navigateurs et les appareils mobiles. Ces mod√®les l√©gers peuvent √™tre utilis√©s pour ex√©cuter des inf√©rences enti√®rement sur l'appareil.

Avant d'initialiser l'API d'inf√©rence LLM, t√©l√©chargez un mod√®le et stockez le fichier dans le r√©pertoire de votre projet. Vous pouvez utiliser un mod√®le pr√©converti √† partir du d√©p√¥t HuggingFace de la communaut√© LITERAT, ou convertir un mod√®le dans un format compatible avec MediaPipe √† l'aide du convertisseur g√©n√©ratif Torch d'AI Edge.

Si vous ne disposez pas encore d'un LLM √† utiliser avec l'API d'inf√©rence LLM, commencez avec l'un des mod√®les suivants.

Gemma-3 1B
Remarque :Gemma-3 est un mod√®le disponible publiquement. Vous √™tes responsable du respect des conditions de licence applicables.
Gemma-3 1B est le dernier mod√®le de la famille Gemma, une gamme de mod√®les ouverts, l√©gers et de pointe con√ßus √† partir des m√™mes recherches et technologies que celles utilis√©es pour cr√©er les mod√®les Gemini. Le mod√®le contient 1 milliard de param√®tres et de pond√©rations ouvertes. La variante 1B est le mod√®le le plus l√©ger de la famille Gemma, ce qui la rend id√©ale pour de nombreux cas d'utilisation sur l'appareil.

T√©l√©charger Gemma-3 1B

Le mod√®le Gemma-3 1B de HuggingFace est disponible au format .task et pr√™t √† l'emploi avec l'API d'inf√©rence LLM pour les applications Android et Web.

Lorsque vous ex√©cutez Gemma-3 1B avec l'API d'inf√©rence LLM, configurez les options suivantes en cons√©quence:

preferredBackend: utilisez cette option pour choisir entre un backend CPU ou GPU. Cette option n'est disponible que pour Android.
supportedLoraRanks: L'API d'inf√©rence LLM ne peut pas √™tre configur√©e pour prendre en charge l'adaptation de rang faible (LoRA) avec le mod√®le Gemma-3 1B. N'utilisez pas les options supportedLoraRanks ou loraRanks.
maxTokens: la valeur de maxTokens doit correspondre √† la taille de contexte int√©gr√©e au mod√®le. On peut √©galement l'appeler cache cl√©-valeur (KV) ou longueur de contexte.
numResponses: doit toujours √™tre d√©fini sur 1. Cette option n'est disponible que pour le Web.
Lorsque vous ex√©cutez Gemma-3 1B sur des applications Web, l'initialisation peut entra√Æner un blocage prolong√© dans le thread en cours. Dans la mesure du possible, ex√©cutez toujours le mod√®le √† partir d'un thread de travail.

Gemma-2 2B
Remarque :Gemma-2 est un mod√®le disponible publiquement. Vous √™tes responsable du respect des conditions de licence applicables.
Gemma-2 2B est une variante 2B de Gemma-2 et fonctionne sur toutes les plates-formes.

T√©l√©charger Gemma-2 2B

Le mod√®le contient 2 milliards de param√®tres et de pond√©rations ouvertes. Gemma-2 2B est r√©put√© pour ses capacit√©s de raisonnement de pointe pour les mod√®les de sa cat√©gorie.

Conversion de mod√®les PyTorch
Remarque :La biblioth√®que AI Edge Torch est en cours de d√©veloppement. L'API est instable et contient des probl√®mes connus.
Les mod√®les g√©n√©ratifs PyTorch peuvent √™tre convertis en format compatible avec MediaPipe avec l'API g√©n√©rative AI Edge Torch. Vous pouvez utiliser l'API pour convertir des mod√®les PyTorch en mod√®les LiteRT (TensorFlow Lite) √† signature multiple. Pour en savoir plus sur la cartographie et l'exportation de mod√®les, consultez la page GitHub d'AI Edge Torch.

Pour convertir un mod√®le PyTorch avec l'API g√©n√©rative Torch d'IA Edge, proc√©dez comme suit:

T√©l√©chargez les points de contr√¥le du mod√®le PyTorch.
Utilisez l'API g√©n√©rative Torch AI Edge pour cr√©er, convertir et quantizer le mod√®le dans un format de fichier compatible avec MediaPipe (.tflite).
Cr√©ez un bundle de t√¢ches (.task) √† partir du fichier tflite et du tokenizer du mod√®le.
Le convertisseur g√©n√©ratif Torch ne convertit que pour le processeur et n√©cessite une machine Linux avec au moins 64 Go de RAM.

Pour cr√©er un groupe de t√¢ches, utilisez le script de regroupement pour cr√©er un groupe de t√¢ches. Le processus de regroupement empaquette le mod√®le mapp√© avec des m√©tadonn√©es suppl√©mentaires (par exemple, Param√®tres du tokenizer) n√©cessaires pour ex√©cuter l'inf√©rence de bout en bout.

Le processus de regroupement de mod√®les n√©cessite le package PyPI MediaPipe. Le script de conversion est disponible dans tous les packages MediaPipe apr√®s 0.10.14.

Remarque :Pour √©viter les probl√®mes de compatibilit√©, utilisez le Colab de regroupement de mod√®les.
Installez et importez les d√©pendances avec la commande suivante:

$ python3 -m pip install mediapipe
Utilisez la biblioth√®que genai.bundler pour regrouper le mod√®le:

import mediapipe as mp
from mediapipe.tasks.python.genai import bundler

config = bundler.BundleConfig(
    tflite_model=TFLITE_MODEL,
    tokenizer_model=TOKENIZER_MODEL,
    start_token=START_TOKEN,
    stop_tokens=STOP_TOKENS,
    output_filename=OUTPUT_FILENAME,
    enable_bytes_to_unicode_mapping=ENABLE_BYTES_TO_UNICODE_MAPPING,
)
bundler.create_bundle(config)
Param√®tre	Description	Valeurs accept√©es
tflite_model	Chemin d'acc√®s au mod√®le TFLite export√© par AI Edge.	CHEMIN
tokenizer_model	Chemin d'acc√®s au mod√®le de tokenizer SentencePiece.	CHEMIN
start_token	Jeton de d√©but sp√©cifique au mod√®le. Le jeton de d√©but doit √™tre pr√©sent dans le mod√®le de tokenizer fourni.	STRING
stop_tokens	Mod√©liser des jetons d'arr√™t sp√©cifiques Les jetons de fin de mot doivent √™tre pr√©sents dans le mod√®le de tokenizer fourni.	LIST[STRING]
output_filename	Nom du fichier de groupe de t√¢ches de sortie.	CHEMIN
Personnalisation LoRA
L'API d'inf√©rence LLM Mediapipe peut √™tre configur√©e pour prendre en charge l'adaptation faible (LoRA) pour les grands mod√®les de langage. Gr√¢ce √† des mod√®les LoRA optimis√©s, les d√©veloppeurs peuvent personnaliser le comportement des LLM via un processus d'entra√Ænement rentable.
La prise en charge de LoRA par l'API d'inf√©rence LLM fonctionne pour toutes les variantes de Gemma et les mod√®les Phi-2 pour le backend GPU, les poids LoRA ne s'appliquant qu'aux couches d'attention. Cette impl√©mentation initiale sert d'API exp√©rimentale pour les futurs d√©veloppements. Nous pr√©voyons de prendre en charge davantage de mod√®les et diff√©rents types de calques dans les prochaines mises √† jour.

Pr√©parer des mod√®les LoRA
Suivez les instructions sur HuggingFace pour entra√Æner un mod√®le LoRA affin√© sur votre propre ensemble de donn√©es avec les types de mod√®les compatibles, Gemma ou Phi-2. Les mod√®les Gemma-2 2B, Gemma 2B et Phi-2 sont tous deux disponibles sur Hugging Face au format safetensors. √âtant donn√© que l'API d'inf√©rence LLM n'est compatible qu'avec LORA sur les couches d'attention, ne sp√©cifiez que les couches d'attention lors de la cr√©ation de LoraConfig comme suit:

# For Gemma
from peft import LoraConfig
config = LoraConfig(
    r=LORA_RANK,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
)

# For Phi-2
config = LoraConfig(
    r=LORA_RANK,
    target_modules=["q_proj", "v_proj", "k_proj", "dense"],
)
Pour les tests, des mod√®les LoRA affin√©s accessibles au public qui correspondent √† l'API d'inf√©rence LLM sont disponibles sur HuggingFace. Par exemple, monsterapi/gemma-2b-lora-maths-orca-200k pour Gemma-2B et lole25/phi-2-sft-ultrachat-lora pour Phi-2.

Apr√®s avoir entra√Æn√© le mod√®le sur l'ensemble de donn√©es pr√©par√© et enregistr√© le mod√®le, vous obtenez un fichier adapter_model.safetensors contenant les poids du mod√®le LoRA affin√©s. Le fichier safetensors est le point de contr√¥le LoRA utilis√© lors de la conversion du mod√®le.

L'√©tape suivante consiste √† convertir les poids du mod√®le en Flatbuffer TensorFlow Lite √† l'aide du package Python MediaPipe. ConversionConfig doit sp√©cifier les options de mod√®le de base ainsi que les options LoRA suppl√©mentaires. Notez que, comme l'API n'est compatible qu'avec l'inf√©rence LoRA avec GPU, le backend doit √™tre d√©fini sur 'gpu'.

import mediapipe as mp
from mediapipe.tasks.python.genai import converter

config = converter.ConversionConfig(
  # Other params related to base model
  ...
  # Must use gpu backend for LoRA conversion
  backend='gpu',
  # LoRA related params
  lora_ckpt=LORA_CKPT,
  lora_rank=LORA_RANK,
  lora_output_tflite_file=LORA_OUTPUT_TFLITE_FILE,
)

converter.convert_checkpoint(config)
Le convertisseur g√©n√®re deux fichiers FlatBuffer TFLite, l'un pour le mod√®le de base et l'autre pour le mod√®le LoRA.

Inf√©rence de mod√®le LoRA
L'API d'inf√©rence LLM pour le Web, Android et iOS est mise √† jour pour prendre en charge l'inf√©rence de mod√®le LoRA.

Android est compatible avec LoRA statique lors de l'initialisation. Pour charger un mod√®le LoRA, les utilisateurs doivent sp√©cifier le chemin d'acc√®s au mod√®le LoRA ainsi que le LLM de base.
// Set the configuration options for the LLM Inference task
val options = LlmInferenceOptions.builder()
        .setModelPath('<path to base model>')
        .setMaxTokens(1000)
        .setTopK(40)
        .setTemperature(0.8)
        .setRandomSeed(101)
        .setLoraPath('<path to LoRA model>')
        .build()

// Create an instance of the LLM Inference task
llmInference = LlmInference.createFromOptions(context, options)
Pour ex√©cuter l'inf√©rence LLM avec LoRA, utilisez les m√™mes m√©thodes generateResponse() ou generateResponseAsync() que le mod√®le de base.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/05/20 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide d'inf√©rence LLM pour Android


Remarque :L'utilisation de l'API d'inf√©rence LLM MediaPipe est soumise au R√®glement sur les utilisations interdites de l'IA g√©n√©rative.
L'API Inference LLM vous permet d'ex√©cuter des grands mod√®les de langage (LLM) enti√®rement sur l'appareil pour les applications Android. Vous pouvez les utiliser pour effectuer un large √©ventail de t√¢ches, comme g√©n√©rer du texte, r√©cup√©rer des informations au format langage naturel et r√©sumer des documents. La t√¢che est compatible avec plusieurs grands mod√®les de langage de texte √† texte, ce qui vous permet d'appliquer les derniers mod√®les d'IA g√©n√©rative sur l'appareil √† vos applications Android.

Pour ajouter rapidement l'API d'inf√©rence LLM √† votre application Android, suivez le guide de d√©marrage rapide. Pour obtenir un exemple de base d'application Android ex√©cutant l'API d'inf√©rence LLM, consultez l'exemple d'application. Pour en savoir plus sur le fonctionnement de l'API d'inf√©rence LLM, consultez les sections Options de configuration, Conversion de mod√®le et R√©glage de LORA.

Vous pouvez voir cette t√¢che en action avec la d√©mo MediaPipe Studio. Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Guide de d√©marrage rapide
Pour ajouter l'API d'inf√©rence LLM √† votre application Android, proc√©dez comme suit : L'API d'inf√©rence LLM est optimis√©e pour les appareils Android haut de gamme, tels que le Pixel 8 et le Samsung S23 ou version ult√©rieure. Elle n'est pas compatible de mani√®re fiable avec les √©mulateurs d'appareils.

Ajouter des d√©pendances
L'API Inference LLM utilise la biblioth√®que com.google.mediapipe:tasks-genai. Ajoutez cette d√©pendance au fichier build.gradle de votre application Android:


dependencies {
    implementation 'com.google.mediapipe:tasks-genai:0.10.24'
}
T√©l√©charger un mod√®le
T√©l√©chargez Gemma-3 1B au format quantifi√© 4 bits sur Hugging Face. Pour en savoir plus sur les mod√®les disponibles, consultez la documentation sur les mod√®les.

Transf√©rez le contenu du dossier output_path sur l'appareil Android.


$ adb shell rm -r /data/local/tmp/llm/ # Remove any previously loaded models
$ adb shell mkdir -p /data/local/tmp/llm/
$ adb push output_path /data/local/tmp/llm/model_version.task
Remarque :Lors du d√©veloppement, vous pouvez utiliser adb pour transf√©rer le mod√®le vers votre appareil de test afin de simplifier le workflow. Pour le d√©ploiement, h√©bergez le mod√®le sur un serveur et t√©l√©chargez-le au moment de l'ex√©cution. Le mod√®le est trop volumineux pour √™tre group√© dans un APK.
Initialiser la t√¢che
Initialisez la t√¢che avec des options de configuration de base:


// Set the configuration options for the LLM Inference task
val taskOptions = LlmInferenceOptions.builder()
        .setModelPath('/data/local/tmp/llm/model_version.task')
        .setMaxTopK(64)
        .build()

// Create an instance of the LLM Inference task
llmInference = LlmInference.createFromOptions(context, taskOptions)
Ex√©cuter la t√¢che
Utilisez la m√©thode generateResponse() pour g√©n√©rer une r√©ponse textuelle. Une seule r√©ponse est g√©n√©r√©e.


val result = llmInference.generateResponse(inputPrompt)
logger.atInfo().log("result: $result")
Pour diffuser la r√©ponse en continu, utilisez la m√©thode generateResponseAsync().


val options = LlmInference.LlmInferenceOptions.builder()
  ...
  .setResultListener { partialResult, done ->
    logger.atInfo().log("partial result: $partialResult")
  }
  .build()

llmInference.generateResponseAsync(inputPrompt)
Exemple d'application
Remarque :La galerie Google AI Edge est une version alpha.
Pour voir les API d'inf√©rence LLM en action et explorer une gamme compl√®te de fonctionnalit√©s d'IA g√©n√©rative sur l'appareil, consultez l'application Google AI Gallery.

La Google AI Edge Gallery est une application Android Open Source qui sert de terrain d'entra√Ænement interactif pour les d√©veloppeurs. Il pr√©sente les √©l√©ments suivants:

Exemples pratiques d'utilisation de l'API d'inf√©rence LLM pour diverses t√¢ches, y compris :
Demander √† une image: importez une image et posez-lui des questions. Obtenez des descriptions, r√©solvez des probl√®mes ou identifiez des objets.
Atelier sur les requ√™tes: r√©sumez, r√©√©crivez, g√©n√©rez du code ou utilisez des requ√™tes au format libre pour explorer les cas d'utilisation des LLM √† un seul tour.
Chat avec IA: engagez des conversations multitours.
Possibilit√© de d√©couvrir, t√©l√©charger et tester diff√©rents mod√®les optimis√©s pour LiteRT de la communaut√© Hugging Face LiteRT et des versions officielles de Google (par exemple, Gemma 3N).
Benchmarks de performances sur l'appareil en temps r√©el pour diff√©rents mod√®les (temps de premier jeton, vitesse de d√©codage, etc.).
Importer et tester vos propres mod√®les .task personnalis√©s
Cette application est une ressource permettant de comprendre l'impl√©mentation pratique de l'API d'inf√©rence LLM et le potentiel de l'IA g√©n√©rative sur l'appareil. Explorez le code source et t√©l√©chargez l'application depuis le d√©p√¥t GitHub de la galerie Google AI Edge.

Options de configuration
Utilisez les options de configuration suivantes pour configurer une application Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
modelPath	Chemin d'acc√®s au r√©pertoire du projet dans lequel le mod√®le est stock√©.	CHEMIN	N/A
maxTokens	Nombre maximal de jetons (jetons d'entr√©e + jetons de sortie) g√©r√©s par le mod√®le.	Entier	512
topK	Nombre de jetons que le mod√®le prend en compte √† chaque √©tape de g√©n√©ration. Limite les pr√©dictions aux k jetons les plus probables.	Entier	40
temperature	Quantit√© de hasard introduite lors de la g√©n√©ration. Une temp√©rature plus √©lev√©e g√©n√®re un texte plus cr√©atif, tandis qu'une temp√©rature plus basse g√©n√®re un texte plus pr√©visible.	Float	0,8
randomSeed	Graine al√©atoire utilis√©e lors de la g√©n√©ration de texte.	Entier	0
loraPath	Chemin absolu vers le mod√®le LoRA localement sur l'appareil. Remarque: Cette option n'est compatible qu'avec les mod√®les de GPU.	CHEMIN	N/A
resultListener	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats de mani√®re asynchrone. Ne s'applique que lorsque vous utilisez la m√©thode de g√©n√©ration asynchrone.	N/A	N/A
errorListener	D√©finit un √©couteur d'erreur facultatif.	N/A	N/A
Requ√™tes multimodales
Les API Android de l'API d'inf√©rence LLM sont compatibles avec les requ√™tes multimodales avec des mod√®les qui acceptent les entr√©es de texte et d'image. Lorsque la multimodalit√© est activ√©e, les utilisateurs peuvent inclure une combinaison d'images et de texte dans leurs requ√™tes, et le LLM fournit une r√©ponse textuelle.

Pour commencer, utilisez une variante de Gemma 3n compatible avec MediaPipe:

Gemma-3n E2B: mod√®le 2B de la famille Gemma-3n.
Gemma-3n E4B: mod√®le 4 B de la famille Gemma-3n.
Pour en savoir plus, consultez la documentation Gemma-3n.

Pour fournir des images dans une requ√™te, convertissez les images ou les frames d'entr√©e en objet com.google.mediapipe.framework.image.MPImage avant de les transmettre √† l'API d'inf√©rence LLM:

import com.google.mediapipe.framework.image.BitmapImageBuilder
import com.google.mediapipe.framework.image.MPImage

// Convert the input Bitmap object to an MPImage object to run inference
val mpImage = BitmapImageBuilder(image).build()
Pour activer la prise en charge de la vision pour l'API d'inf√©rence LLM, d√©finissez l'option de configuration EnableVisionModality sur true dans les options de graphique:

LlmInferenceSession.LlmInferenceSessionOptions sessionOptions =
  LlmInferenceSession.LlmInferenceSessionOptions.builder()
    ...
    .setGraphOptions(GraphOptions.builder().setEnableVisionModality(true).build())
    .build();
Gemma-3n n'accepte qu'une seule image par session. D√©finissez donc MaxNumImages sur 1.

LlmInferenceOptions options = LlmInferenceOptions.builder()
  ...
  .setMaxNumImages(1)
  .build();
Voici un exemple d'impl√©mentation de l'API d'inf√©rence LLM configur√©e pour g√©rer les entr√©es visuelles et textuelles:

Conseil :Les requ√™tes multimodales fonctionnent g√©n√©ralement mieux si le texte pr√©c√®de l'image dans la requ√™te.
MPImage image = getImageFromAsset(BURGER_IMAGE);

LlmInferenceSession.LlmInferenceSessionOptions sessionOptions =
  LlmInferenceSession.LlmInferenceSessionOptions.builder()
    .setTopK(10)
    .setTemperature(0.4f)
    .setGraphOptions(GraphOptions.builder().setEnableVisionModality(true).build())
    .build();

try (LlmInference llmInference =
    LlmInference.createFromOptions(ApplicationProvider.getApplicationContext(), options);
  LlmInferenceSession session =
    LlmInferenceSession.createFromOptions(llmInference, sessionOptions)) {
  session.addQueryChunk("Describe the objects in the image.");
  session.addImage(image);
  String result = session.generateResponse();
}
Personnalisation LoRA
L'API Inference LLM est compatible avec le r√©glage LoRA (Low-Rank Adaptation) √† l'aide de la biblioth√®que PEFT (Parameter-Efficient Fine-Tuning). Le r√©glage LoRA personnalise le comportement des LLM via un processus d'entra√Ænement rentable, en cr√©ant un petit ensemble de poids enregistrables en fonction de nouvelles donn√©es d'entra√Ænement plut√¥t que de r√©entra√Æner l'ensemble du mod√®le.

L'API d'inf√©rence LLM permet d'ajouter des poids LoRA aux couches d'attention des mod√®les Gemma-2 2B, Gemma 2B et Phi-2. T√©l√©chargez le mod√®le au format safetensors.

Le mod√®le de base doit √™tre au format safetensors pour cr√©er des poids LoRA. Apr√®s l'entra√Ænement LoRA, vous pouvez convertir les mod√®les au format FlatBuffers pour les ex√©cuter sur MediaPipe.

Pr√©parer les pond√©rations LoRA
Utilisez le guide M√©thodes LoRA de PEFT pour entra√Æner un mod√®le LoRA affin√© sur votre propre ensemble de donn√©es.

L'API LLM Inference n'est compatible qu'avec LoRA sur les couches d'attention. Par cons√©quent, ne sp√©cifiez que les couches d'attention dans LoraConfig:

# For Gemma
from peft import LoraConfig
config = LoraConfig(
    r=LORA_RANK,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
)

# For Phi-2
config = LoraConfig(
    r=LORA_RANK,
    target_modules=["q_proj", "v_proj", "k_proj", "dense"],
)
Apr√®s avoir entra√Æn√© le mod√®le sur l'ensemble de donn√©es pr√©par√© et enregistr√© le mod√®le, les poids du mod√®le LoRA affin√©s sont disponibles dans adapter_model.safetensors. Le fichier safetensors est le point de contr√¥le LoRA utilis√© lors de la conversion du mod√®le.

Conversion de mod√®les
Utilisez le package Python MediaPipe pour convertir les poids du mod√®le au format Flatbuffer. ConversionConfig sp√©cifie les options de mod√®le de base ainsi que les options LoRA suppl√©mentaires.

Remarque :√âtant donn√© que l'API n'est compatible qu'avec l'inf√©rence LoRA avec GPU, le backend doit √™tre d√©fini sur 'gpu'.
import mediapipe as mp
from mediapipe.tasks.python.genai import converter

config = converter.ConversionConfig(
  # Other params related to base model
  ...
  # Must use gpu backend for LoRA conversion
  backend='gpu',
  # LoRA related params
  lora_ckpt=LORA_CKPT,
  lora_rank=LORA_RANK,
  lora_output_tflite_file=LORA_OUTPUT_FILE,
)

converter.convert_checkpoint(config)
Le convertisseur g√©n√®re deux fichiers Flatbuffer, l'un pour le mod√®le de base et l'autre pour le mod√®le LoRA.

Inf√©rence de mod√®le LoRA
Android est compatible avec LoRA statique lors de l'initialisation. Pour charger un mod√®le LoRA, sp√©cifiez le chemin d'acc√®s au mod√®le LoRA ainsi que le LLM de base.

// Set the configuration options for the LLM Inference task
val options = LlmInferenceOptions.builder()
        .setModelPath(BASE_MODEL_PATH)
        .setMaxTokens(1000)
        .setTopK(40)
        .setTemperature(0.8)
        .setRandomSeed(101)
        .setLoraPath(LORA_MODEL_PATH)
        .build()

// Create an instance of the LLM Inference task
llmInference = LlmInference.createFromOptions(context, options)
Pour ex√©cuter l'inf√©rence LLM avec LoRA, utilisez les m√™mes m√©thodes generateResponse() ou generateResponseAsync() que le mod√®le de base.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide RAG AI Edge
Attention:Le SDK AI Edge RAG est en cours de d√©veloppement.
Le SDK RAG AI Edge fournit les composants de base pour cr√©er un pipeline de g√©n√©ration augment√©e par r√©cup√©ration (RAG) avec l'API d'inf√©rence LLM. Un pipeline RAG fournit aux LLM un acc√®s aux donn√©es fournies par l'utilisateur, qui peuvent inclure des informations √† jour, sensibles ou sp√©cifiques au domaine. Gr√¢ce aux fonctionnalit√©s de r√©cup√©ration d'informations suppl√©mentaires de RAG, les LLM peuvent g√©n√©rer des r√©ponses plus pr√©cises et plus sensibles au contexte pour des cas d'utilisation sp√©cifiques.

Le SDK AI Edge RAG est disponible pour Android et peut √™tre enti√®rement ex√©cut√© sur l'appareil. Commencez √† utiliser le SDK en suivant le guide Android, qui vous explique comment impl√©menter de mani√®re basique un exemple d'application √† l'aide de RAG.

Pipeline RAG
La configuration d'un pipeline RAG avec le SDK RAG AI Edge comprend les √©tapes cl√©s suivantes:

Importez des donn√©es: fournissez les donn√©es textuelles que le LLM utilisera lors de la g√©n√©ration de la sortie.
Diviser et indexer les donn√©es: divisez les donn√©es en petits blocs pour les indexer dans une base de donn√©es.
G√©n√©rer des embeddings: utilisez un outil d'embedding pour vectoriser les segments √† stocker dans une base de donn√©es vectorielle.
R√©cup√©rer des informations: d√©finissez la mani√®re dont les informations pertinentes sont identifi√©es et r√©cup√©r√©es pour r√©pondre aux requ√™tes des utilisateurs. Pour une requ√™te donn√©e, le composant de r√©cup√©ration recherche dans la base de donn√©es vectorielle les informations pertinentes.
G√©n√©rer du texte avec un LLM: utilisez un grand mod√®le de langage pour g√©n√©rer du texte de sortie en fonction des informations r√©cup√©r√©es dans la base de donn√©es de vecteurs.
Modules cl√©s
Le SDK AI Edge RAG fournit les principaux modules et API suivants pour le pipeline RAG:

Mod√®les de langage: mod√®les LLM avec API √† requ√™te ouverte, locaux (sur l'appareil) ou bas√©s sur un serveur. L'API est bas√©e sur l'interface LanguageModel.
Mod√®les d'embedding de texte: convertissez le texte structur√© et non structur√© en vecteurs d'embedding pour la recherche s√©mantique. L'API est bas√©e sur l'interface Embedder.
Magasins de vecteurs: le magasin de vecteurs contient les embeddings et les m√©tadonn√©es d√©riv√©es des blocs de donn√©es. Vous pouvez l'interroger pour obtenir des segments similaires ou des correspondances exactes. L'API est bas√©e sur l'interface VectorStore.
M√©moire s√©mantique: sert de r√©cup√©rateur s√©mantique pour r√©cup√©rer les segments les plus pertinents parmi les k premiers en fonction d'une requ√™te. L'API est bas√©e sur l'interface SemanticMemory.
Division du texte en blocs: permet de diviser les donn√©es utilisateur en √©l√©ments plus petits pour faciliter l'indexation. L'API est bas√©e sur l'interface TextChunker.
Le SDK fournit des cha√Ænes, qui combinent plusieurs composants RAG dans un seul pipeline. Vous pouvez utiliser des cha√Ænes pour orchestrer les mod√®les de r√©cup√©ration et de requ√™te. L'API est bas√©e sur l'interface Chain. Pour commencer, essayez la cha√Æne de r√©cup√©ration et d'inf√©rence ou la cha√Æne de r√©cup√©ration.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/05/26 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de RAG AI Edge pour Android


Attention:Le SDK AI Edge RAG est en cours de d√©veloppement.
Remarque :L'utilisation du SDK AI Edge RAG est soumise au R√®glement sur les utilisations interdites de l'IA g√©n√©rative.
Le SDK RAG AI Edge fournit les composants de base pour cr√©er un pipeline de g√©n√©ration augment√©e par r√©cup√©ration (RAG) avec l'API d'inf√©rence LLM. Un pipeline RAG fournit aux LLM un acc√®s aux donn√©es fournies par l'utilisateur, qui peuvent inclure des informations √† jour, sensibles ou sp√©cifiques au domaine. Gr√¢ce aux fonctionnalit√©s de r√©cup√©ration d'informations suppl√©mentaires de RAG, les LLM peuvent g√©n√©rer des r√©ponses plus pr√©cises et plus sensibles au contexte pour des cas d'utilisation sp√©cifiques.

Ce guide vous explique comment impl√©menter de mani√®re basique un exemple d'application √† l'aide de l'API d'inf√©rence LLM avec le SDK RAG d'IA Edge. Ce guide est ax√© sur la cr√©ation d'un pipeline RAG. Pour en savoir plus sur l'utilisation de l'API LLM Inference, consultez le guide LLM Inference pour Android.

Vous trouverez l'exemple d'application complet sur GitHub. Pour commencer, cr√©ez l'application, lisez les donn√©es fournies par l'utilisateur (sample_context.txt) et posez au LLM des questions sur les informations du fichier texte.

Ex√©cuter l'exemple d'application
Remarque :L'API LLM Inference n'est pas enti√®rement compatible avec les √©mulateurs d'appareils. Utilisez un appareil physique pour √©viter les plantages, les probl√®mes de performances et les comportements inattendus.
Ce guide fait r√©f√©rence √† un exemple d'application de g√©n√©ration de texte de base avec RAG pour Android. Vous pouvez utiliser l'application exemple comme point de d√©part pour votre propre application Android ou vous y r√©f√©rer lorsque vous modifiez une application existante.

L'application est optimis√©e pour les appareils haut de gamme tels que le Pixel 8, le Pixel 9, le S23 et le S24. Connectez un appareil Android √† votre poste de travail et assurez-vous de disposer d'une version √† jour d'Android Studio. Pour en savoir plus, consultez le guide de configuration Android.

T√©l√©charger le code de l'application
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple de code √† l'aide de l'outil de ligne de commande git.

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:


git clone https://github.com/google-ai-edge/ai-edge-apis
Apr√®s avoir cr√©√© une version locale de l'exemple de code, vous pouvez importer le projet dans Android Studio et ex√©cuter l'application.

T√©l√©charger un mod√®le
L'application exemple est configur√©e pour utiliser Gemma-3 1B. Gemma-3 1B fait partie de la famille Gemma de mod√®les ouverts, l√©gers et √† la pointe de la technologie, bas√©s sur les m√™mes recherches et technologies que celles utilis√©es pour cr√©er les mod√®les Gemini. Le mod√®le contient 1 milliard de param√®tres et de pond√©rations ouvertes.

T√©l√©charger Gemma-3 1B

Apr√®s avoir t√©l√©charg√© Gemma-3 1B depuis Hugging Face, transf√©rez le mod√®le sur votre appareil:

cd ~/Downloads
tar -xvzf gemma3-1b-it-int4.tar.gz
$ adb shell rm -r /data/local/tmp/llm/ # Remove any previously loaded models
$ adb shell mkdir -p /data/local/tmp/llm/
$ adb push output_path /data/local/tmp/llm/model_version.task
Vous pouvez √©galement utiliser d'autres mod√®les avec l'application exemple, mais cela peut n√©cessiter des √©tapes de configuration suppl√©mentaires.

Configurer un outil d'int√©gration
L'outil d'embedding extrait des segments de texte des donn√©es fournies par l'utilisateur et les transforme en repr√©sentations num√©riques vectoris√©es qui capturent leur signification s√©mantique. Le LLM se r√©f√®re √† ces embeddings pour identifier les vecteurs pertinents et int√®gre les segments s√©mantiquement les plus pertinents dans la sortie g√©n√©r√©e.

L'application exemple est con√ßue pour fonctionner avec deux int√©grateurs, l'int√©grateur Gemini et l'int√©grateur Gecko.

Configurer avec l'outil d'int√©gration Gecko
Par d√©faut, l'application exemple est configur√©e pour utiliser l'outil d'int√©gration Gecko (GeckoEmbeddingModel) et ex√©cute enti√®rement le mod√®le sur l'appareil.

T√©l√©charger Gecko 110m-en

L'outil d'int√©gration Gecko est disponible en tant que mod√®les √† virgule flottante et quantifi√©s, avec plusieurs versions pour diff√©rentes longueurs de s√©quence. Pour en savoir plus, consultez la fiche de mod√®le Gecko.

Les sp√©cifications du mod√®le se trouvent dans le nom de fichier du mod√®le. Exemple :

Gecko_256_fp32.tflite: mod√®le √† virgule flottante compatible avec des s√©quences de 256 jetons maximum.
Gecko_1024_quant.tflite: mod√®le quantifi√© compatible avec des s√©quences de 1 024 jetons maximum.
La longueur de la s√©quence correspond √† la taille maximale des fragments que le mod√®le peut int√©grer. Par exemple, si le mod√®le Gecko_256_fp32.tflite re√ßoit un segment qui d√©passe la longueur de la s√©quence, il int√©grera les 256 premiers jetons et tronquera le reste du segment.

Transf√©rez le mod√®le de tokenizer (sentencepiece.model) et l'outil d'int√©gration Gecko sur votre appareil:

adb push sentencepiece.model /data/local/tmp/sentencepiece.model
adb push Gecko_256_fp32.tflite /data/local/tmp/gecko.tflite
Le mod√®le d'encapsulation est compatible avec le processeur et le GPU. Par d√©faut, l'application exemple est configur√©e pour extraire des repr√©sentations vectorielles continues avec le mod√®le Gecko sur le GPU.

companion object {
  ...
  private const val USE_GPU_FOR_EMBEDDINGS = true
}
Configurer avec Gemini Embedder
L'outil Gemini Embedder (GeminiEmbedder) cr√©e des embeddings √† l'aide de l'API Gemini Cloud. Pour ex√©cuter l'application, vous avez besoin d'une cl√© API Google Gemini, que vous pouvez obtenir sur la page de configuration de l'API Google Gemini.

Obtenir une cl√© API Gemini dans Google AI Studio

Ajoutez votre cl√© API Gemini et d√©finissez COMPUTE_EMBEDDINGS_LOCALLY sur "false" dans RagPipeline.kt:


companion object {
  ...
  private const val COMPUTE_EMBEDDINGS_LOCALLY = false
  private const val GEMINI_API_KEY = "<API_KEY>"
}
Fonctionnement
Cette section fournit des informations plus d√©taill√©es sur les composants du pipeline RAG de l'application. Vous pouvez consulter la majeure partie du code dans RagPipeline.kt.

D√©pendances
Le SDK RAG utilise la biblioth√®que com.google.ai.edge.localagents:localagents-rag. Ajoutez cette d√©pendance au fichier build.gradle de votre application Android:


dependencies {
    ...
    implementation("com.google.ai.edge.localagents:localagents-rag:0.1.0")
    implementation("com.google.mediapipe:tasks-genai:0.10.22")
}
Donn√©es fournies par l'utilisateur
Les donn√©es fournies par l'utilisateur dans l'application sont un fichier texte nomm√© sample_context.txt, qui est stock√© dans le r√©pertoire assets. L'application prend des segments du fichier texte, cr√©e des repr√©sentations vectorielles continues de ces segments et s'y r√©f√®re lors de la g√©n√©ration du texte de sortie.

L'extrait de code suivant se trouve dans MainActivity.kt:


class MainActivity : ComponentActivity() {
  lateinit var chatViewModel: ChatViewModel
...
    chatViewModel.memorizeChunks("sample_context.txt")
...
}
Fragmentation
Pour plus de simplicit√©, le fichier sample_context.txt inclut des balises <chunk_splitter> que l'application exemple utilise pour cr√©er des segments. Des repr√©sentations vectorielles continues sont ensuite cr√©√©es pour chaque segment. Dans les applications de production, la taille des blocs est un facteur cl√©. Lorsqu'un bloc est trop volumineux, le vecteur ne contient pas suffisamment de sp√©cificit√© pour √™tre utile. Lorsqu'il est trop petit, il ne contient pas suffisamment de contexte.

L'application exemple g√®re le d√©coupage via la fonction memorizeChunks dans RagPipeline.kt.

Embedding
L'application propose deux voies pour l'embedding textuel:

Embedding Gecko : extraction locale (sur l'appareil) des repr√©sentations vectorielles continues de texte avec le mod√®le Gecko.
Gemini Embedder : extraction d'embeddings textuels dans le cloud avec l'API Cloud Language g√©n√©rative.
L'application exemple s√©lectionne l'outil d'int√©gration en fonction de l'intention de l'utilisateur de calculer des repr√©sentations vectorielles continues en local ou via Google Cloud. L'extrait de code suivant se trouve dans RagPipeline.kt:


private val embedder: Embedder<String> = if (COMPUTE_EMBEDDINGS_LOCALLY) {
  GeckoEmbeddingModel(
    GECKO_MODEL_PATH,
    Optional.of(TOKENIZER_MODEL_PATH),
    USE_GPU_FOR_EMBEDDINGS,
    )
  } else {
    GeminiEmbedder(
      GEMINI_EMBEDDING_MODEL,
      GEMINI_API_KEY
      )
  }
Base de donn√©es
L'application exemple utilise SQLite (SqliteVectorStore) pour stocker des repr√©sentations vectorielles continues de texte. Vous pouvez √©galement utiliser la base de donn√©es DefaultVectorStore pour le stockage de vecteurs non persistants.

L'extrait de code suivant se trouve dans RagPipeline.kt:

private val config = ChainConfig.create(
    mediaPipeLanguageModel, PromptBuilder(QA_PROMPT_TEMPLATE1),
    DefaultSemanticTextMemory(
        SqliteVectorStore(768), embedder
    )
)
L'application exemple d√©finit la dimension d'embedding sur 768, ce qui correspond √† la longueur de chaque vecteur dans la base de donn√©es de vecteurs.

Cha√Æne
Le SDK RAG fournit des cha√Ænes, qui combinent plusieurs composants RAG dans un seul pipeline. Vous pouvez utiliser des cha√Ænes pour orchestrer les mod√®les de r√©cup√©ration et de requ√™te. L'API est bas√©e sur l'interface Chain.

L'application exemple utilise la cha√Æne de r√©cup√©ration et d'inf√©rence. L'extrait de code suivant se trouve dans RagPipeline.kt:

private val retrievalAndInferenceChain = RetrievalAndInferenceChain(config)
La cha√Æne est appel√©e lorsque le mod√®le g√©n√®re des r√©ponses:

suspend fun generateResponse(
    prompt: String,
    callback: AsyncProgressListener<LanguageModelResponse>?
): String =
    coroutineScope {
        val retrievalRequest =
            RetrievalRequest.create(
                prompt,
                RetrievalConfig.create(2, 0.0f, TaskType.QUESTION_ANSWERING)
            )
        retrievalAndInferenceChain.invoke(retrievalRequest, callback).await().text
    }
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.


Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de l'appel de fonction Edge d'IA
Attention:Le SDK d'appel de fonction Edge AI est en cours de d√©veloppement.
Le SDK d'appel de fonction Edge AI (SDK FC) est une biblioth√®que qui permet aux d√©veloppeurs d'utiliser l'appel de fonction avec des LLM sur l'appareil. L'appel de fonction vous permet de connecter des mod√®les √† des outils et des API externes, ce qui leur permet d'appeler des fonctions sp√©cifiques avec les param√®tres n√©cessaires pour ex√©cuter des actions r√©elles.

Plut√¥t que de g√©n√©rer simplement du texte, un LLM utilisant le SDK FC peut g√©n√©rer un appel structur√© √† une fonction qui ex√©cute une action, comme rechercher des informations √† jour, d√©finir des alarmes ou effectuer des r√©servations.

Le SDK FC AI Edge est disponible pour Android et peut √™tre enti√®rement ex√©cut√© sur l'appareil avec l'API d'inf√©rence LLM. Commencez √† utiliser le SDK en suivant le guide Android, qui vous explique comment impl√©menter de mani√®re basique un exemple d'application √† l'aide d'un appel de fonction.

Pipeline d'appel de fonction
Pour configurer un LLM sur l'appareil avec des fonctionnalit√©s d'appel de fonction, proc√©dez comme suit:

D√©finir des d√©clarations de fonction: la structure et les param√®tres des fonctions que le LLM peut appeler doivent √™tre d√©finis dans le code de votre application. Cela inclut la sp√©cification des noms, des param√®tres et des types de fonctions.
Mettre en forme les requ√™tes et les sorties: le texte d'entr√©e et de sortie peut contenir du langage naturel et des appels de fonction. Un formateur contr√¥le la mani√®re dont les structures de donn√©es sont converties en cha√Ænes et √† partir de cha√Ænes, ce qui permet au LLM de mettre en forme les informations de mani√®re appropri√©e.
Analyser les sorties: un analyseur d√©tecte si la r√©ponse g√©n√©r√©e contient un appel de fonction et l'analyse en un type de donn√©es structur√© afin que l'application puisse ex√©cuter l'appel de fonction.
Examiner les r√©ponses: si l'analyseur d√©tecte un appel de fonction, l'application appelle la fonction avec les param√®tres et le type de donn√©es structur√©s appropri√©s. Sinon, il renvoie un texte en langage naturel.
Composants cl√©s
Le SDK FC contient les composants cl√©s suivants:

Backend d'inf√©rence: interface permettant d'ex√©cuter une inf√©rence sur un mod√®le d'IA g√©n√©rative. Le SDK FC utilise l'API d'inf√©rence LLM pour ex√©cuter des inf√©rences sur les mod√®les LiteRT (TFLite). L'API utilise l'interface InferenceBackend.
Outil de formatage des requ√™tes: interface permettant de mettre en forme les requ√™tes et les r√©ponses envoy√©es au mod√®le d'IA g√©n√©rative et re√ßues de celui-ci. Le SDK FC fournit un formateur qui convertit les d√©clarations de fonction au format sp√©cifique au mod√®le requis par le LLM et les ins√®re dans l'invite syst√®me. Le formateur g√®re √©galement les jetons sp√©cifiques au mod√®le pour indiquer les tours de l'utilisateur et du mod√®le. L'API utilise l'interface ModelFormatter.
Analyseur de sortie: le SDK FC fournit un analyseur qui d√©tecte si la sortie du mod√®le repr√©sente un appel de fonction et l'analyse en une structure de donn√©es √† utiliser par l'application. L'API utilise l'interface ModelFormatter.
D√©codage contraint: interface permettant de cr√©er et de g√©rer des contraintes pour s'assurer que la sortie g√©n√©r√©e respecte des r√®gles ou des conditions sp√©cifiques. Pour les mod√®les compatibles, le SDK FC configure le backend d'inf√©rence pour utiliser le d√©codage contraint, ce qui garantit que le mod√®le ne renvoie que des noms et des param√®tres de fonction valides. L'API utilise l'interface ConstraintProvider.
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/05/26 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de l'appel de fonction AI Edge pour Android


Attention:Le SDK d'appel de fonction Edge AI est en cours de d√©veloppement.
Remarque :L'utilisation du SDK d'appel de fonction Edge AI est soumise au R√®glement sur les utilisations interdites de l'IA g√©n√©rative.
Le SDK d'appel de fonction Edge AI (SDK FC) est une biblioth√®que qui permet aux d√©veloppeurs d'utiliser l'appel de fonction avec des LLM sur l'appareil. L'appel de fonction vous permet de connecter des mod√®les √† des outils et des API externes, ce qui leur permet d'appeler des fonctions sp√©cifiques avec les param√®tres n√©cessaires pour ex√©cuter des actions r√©elles.

Plut√¥t que de g√©n√©rer simplement du texte, un LLM utilisant le SDK FC peut g√©n√©rer un appel structur√© √† une fonction qui ex√©cute une action, comme rechercher des informations √† jour, d√©finir des alarmes ou effectuer des r√©servations.

Ce guide vous explique comment ajouter l'API d'inf√©rence LLM avec le SDK FC √† une application Android. Ce guide explique comment ajouter des fonctionnalit√©s d'appel de fonction √† un LLM sur l'appareil. Pour en savoir plus sur l'utilisation de l'API LLM Inference, consultez le guide LLM Inference pour Android.

Guide de d√©marrage rapide
Pour utiliser le SDK FC dans votre application Android, proc√©dez comme suit : Ce tutoriel de d√©marrage utilise l'API d'inf√©rence LLM avec Hammer 2.1 (1,5 milliard). L'API d'inf√©rence LLM est optimis√©e pour les appareils Android haut de gamme, tels que le Pixel 8 et le Samsung S23 ou mod√®les ult√©rieurs, et n'est pas fiable avec les √©mulateurs d'appareils.

Ajouter des d√©pendances
Le SDK FC utilise la biblioth√®que com.google.ai.edge.localagents:localagents-fc, et l'API d'inf√©rence LLM utilise la biblioth√®que com.google.mediapipe:tasks-genai. Ajoutez les deux d√©pendances au fichier build.gradle de votre application Android:


dependencies {
    implementation 'com.google.mediapipe:tasks-genai:0.10.24'
    implementation 'com.google.ai.edge.localagents:localagents-fc:0.1.0'
}
Pour les appareils √©quip√©s d'Android 12 (API 31) ou version ult√©rieure, ajoutez la d√©pendance de la biblioth√®que OpenCL native. Pour en savoir plus, consultez la documentation sur la balise uses-native-library.

Ajoutez les balises uses-native-library suivantes au fichier AndroidManifest.xml:


<uses-native-library android:name="libOpenCL.so" android:required="false"/>
<uses-native-library android:name="libOpenCL-car.so" android:required="false"/>
<uses-native-library android:name="libOpenCL-pixel.so" android:required="false"/>
T√©l√©charger un mod√®le
T√©l√©chargez Hammer 1B au format quantifi√© 8 bits depuis Hugging Face. Pour en savoir plus sur les mod√®les disponibles, consultez la documentation sur les mod√®les.

Transf√©rez le contenu du dossier hammer2.1_1.5b_q8_ekv4096.task vers l'appareil Android.

$ adb shell rm -r /data/local/tmp/llm/ # Remove any previously loaded models
$ adb shell mkdir -p /data/local/tmp/llm/
$ adb push hammer2.1_1.5b_q8_ekv4096.task /data/local/tmp/llm/hammer2.1_1.5b_q8_ekv4096.task
Remarque :Lors du d√©veloppement, vous pouvez utiliser adb pour transf√©rer le mod√®le vers votre appareil de test afin de simplifier le workflow. Pour le d√©ploiement, h√©bergez le mod√®le sur un serveur et t√©l√©chargez-le au moment de l'ex√©cution. Le mod√®le est trop volumineux pour √™tre group√© dans un APK.
D√©clarer des d√©finitions de fonction
D√©finissez les fonctions qui seront mises √† la disposition du mod√®le. Pour illustrer le processus, ce guide de d√©marrage rapide inclut deux fonctions en tant que m√©thodes statiques qui renvoient des r√©ponses cod√©es en dur. Une impl√©mentation plus pratique consisterait √† d√©finir des fonctions qui appellent une API REST ou r√©cup√®rent des informations √† partir d'une base de donn√©es.

Le code suivant d√©finit les fonctions getWeather et getTime:

class ToolsForLlm {
    public static String getWeather(String location) {
        return "Cloudy, 56¬∞F";
    }

    public static String getTime(String timezone) {
        return "7:00 PM " + timezone;
    }

    private ToolsForLlm() {}
}
Utilisez FunctionDeclaration pour d√©crire chaque fonction, en lui attribuant un nom et une description, et en sp√©cifiant les types. Cela informe le mod√®le de ce que font les fonctions et du moment o√π les appeler.

var getWeather = FunctionDeclaration.newBuilder()
    .setName("getWeather")
    .setDescription("Returns the weather conditions at a location.")
    .setParameters(
        Schema.newBuilder()
            .setType(Type.OBJECT)
            .putProperties(
                "location",
                Schema.newBuilder()
                    .setType(Type.STRING)
                    .setDescription("The location for the weather report.")
                    .build())
            .build())
    .build();
var getTime = FunctionDeclaration.newBuilder()
    .setName("getTime")
    .setDescription("Returns the current time in the given timezone.")

    .setParameters(
        Schema.newBuilder()
            .setType(Type.OBJECT)
            .putProperties(
                "timezone",
                Schema.newBuilder()
                    .setType(Type.STRING)
                    .setDescription("The timezone to get the time from.")
                    .build())
            .build())
    .build();
Ajoutez les d√©clarations de fonction √† un objet Tool:

var tool = Tool.newBuilder()
    .addFunctionDeclarations(getWeather)
    .addFunctionDeclarations(getTime)
    .build();
Cr√©er le backend d'inf√©rence
Cr√©ez un backend d'inf√©rence √† l'aide de l'API d'inf√©rence LLM et transmettez-lui un objet de formatage pour votre mod√®le. Le formateur du SDK FC (ModelFormatter) agit √† la fois comme un formateur et un analyseur. √âtant donn√© que ce guide de d√©marrage rapide utilise Gemma-3 1B, nous utiliserons GemmaFormatter:

var llmInferenceOptions = LlmInferenceOptions.builder()
    .setModelPath(modelFile.getAbsolutePath())
    .build();
var llmInference = LlmInference.createFromOptions(context, llmInferenceOptions);
var llmInferenceBackend = new llmInferenceBackend(llmInference, new GemmaFormatter());
Pour en savoir plus, consultez les options de configuration de l'inf√©rence LLM.

Instancier le mod√®le
Utilisez l'objet GenerativeModel pour connecter le backend d'inf√©rence, l'invite syst√®me et les outils. Nous disposons d√©j√† du backend d'inf√©rence et des outils. Il ne nous reste donc qu'√† cr√©er l'invite syst√®me:

var systemInstruction = Content.newBuilder()
      .setRole("system")
      .addParts(Part.newBuilder().setText("You are a helpful assistant."))
      .build();
Instancier le mod√®le avec GenerativeModel:

var generativeModel = new GenerativeModel(
    llmInferenceBackend,
    systemInstruction,
    List.of(tool),
)
D√©marrer une session de chat
Pour des raisons de simplicit√©, ce guide de d√©marrage rapide d√©marre une seule session de chat. Vous pouvez √©galement cr√©er plusieurs sessions ind√©pendantes.

√Ä l'aide de la nouvelle instance de GenerativeModel, d√©marrez une session de chat:

var chat = generativeModel.startChat();
Envoyez des requ√™tes au mod√®le via la session de chat √† l'aide de la m√©thode sendMessage:


var response = chat.sendMessage("How's the weather in San Francisco?");
Analyser la r√©ponse du mod√®le
Apr√®s avoir transmis une requ√™te au mod√®le, l'application doit examiner la r√©ponse pour d√©terminer si elle doit effectuer un appel de fonction ou g√©n√©rer du texte en langage naturel.


// Extract the model's message from the response.
var message = response.getCandidates(0).getContent().getParts(0);

// If the message contains a function call, execute the function.
if (message.hasFunctionCall()) {
  var functionCall = message.getFunctionCall();
  var args = functionCall.getArgs().getFieldsMap();
  var result = null;

  // Call the appropriate function.
  switch (functionCall.getName()) {
    case "getWeather":
      result = ToolsForLlm.getWeather(args.get("location").getStringValue());
      break;
    case "getTime":
      result = ToolsForLlm.getWeather(args.get("timezone").getStringValue());
      break;
    default:
      throw new Exception("Function does not exist:" + functionCall.getName());
  }
  // Return the result of the function call to the model.
  var functionResponse =
      FunctionResponse.newBuilder()
          .setName(functionCall.getName())
          .setResponse(
              Struct.newBuilder()
                  .putFields("result", Value.newBuilder().setStringValue(result).build()))
          .build();
  var response = chat.sendMessage(functionResponse);
} else if (message.hasText()) {
  Log.i(message.getText());
}
L'exemple de code est une impl√©mentation trop simplifi√©e. Pour en savoir plus sur la fa√ßon dont une application peut examiner les r√©ponses du mod√®le, consultez la section Mise en forme et analyse.

Fonctionnement
Cette section fournit des informations plus d√©taill√©es sur les concepts et composants cl√©s du SDK d'appel de fonction pour Android.

Mod√®les
Le SDK d'appel de fonction n√©cessite un mod√®le avec un formateur et un analyseur. Le SDK FC contient un formateur et un analyseur int√©gr√©s pour les mod√®les suivants:

Gemma: utilisez GemmaFormatter.
Llama: utilisez LlamaFormatter.
Marteau: utilisez HammerFormatter.
Pour utiliser un autre mod√®le avec le SDK FC, vous devez d√©velopper votre propre formateur et votre propre analyseur compatibles avec l'API d'inf√©rence LLM.

Mise en forme et analyse
Une partie importante de la prise en charge des appels de fonction est la mise en forme des requ√™tes et l'analyse de la sortie du mod√®le. Bien qu'il s'agisse de deux processus distincts, le SDK FC g√®re √† la fois le formatage et l'analyse avec l'interface ModelFormatter.

Le formateur est charg√© de convertir les d√©clarations de fonction structur√©es en texte, de mettre en forme les r√©ponses de fonction et d'ins√©rer des jetons pour indiquer le d√©but et la fin des tours de conversation, ainsi que les r√¥les de ces tours (par exemple, "utilisateur", "mod√®le").

L'analyseur est charg√© de d√©tecter si la r√©ponse du mod√®le contient un appel de fonction. Si l'analyseur d√©tecte un appel de fonction, il l'analyse en type de donn√©es structur√©. Sinon, il traite le texte comme une r√©ponse en langage naturel.

D√©codage contraint
Remarque :Seuls les mod√®les Gemma sont compatibles avec le d√©codage contraint dans le SDK FC.
Le d√©codage contraint est une technique qui guide la g√©n√©ration de sortie des LLM pour s'assurer qu'elle respecte un format structur√© pr√©d√©fini, tel que des objets JSON ou des appels de fonction Python. En appliquant ces contraintes, le mod√®le met en forme ses sorties de mani√®re √† s'aligner sur les fonctions pr√©d√©finies et leurs types de param√®tres correspondants.

Pour activer le d√©codage contraint, d√©finissez les contraintes dans un objet ConstraintOptions et appelez la m√©thode enableConstraint d'une instance ChatSession. Lorsqu'elle est activ√©e, cette contrainte limite la r√©ponse aux outils associ√©s √† GenerativeModel.

L'exemple suivant montre comment configurer le d√©codage contraint pour limiter la r√©ponse aux appels d'outils. Il contraint l'appel de l'outil √† commencer par le pr√©fixe ```tool_code\n et √† se terminer par le suffixe \n```.

ConstraintOptions constraintOptions = ConstraintOptions.newBuilder()
  .setToolCallOnly( ConstraintOptions.ToolCallOnly.newBuilder()
  .setConstraintPrefix("```tool_code\n")
  .setConstraintSuffix("\n```"))
  .build();
chatSession.enableConstraint(constraintOptions);
Pour d√©sactiver la contrainte active dans la m√™me session, utilisez la m√©thode disableConstraint:

chatSession.disableConstraint();
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de g√©n√©ration d'images


Attention : La t√¢che MediaPipe Image Generator est exp√©rimentale et en cours de d√©veloppement.
T√¢che de g√©n√©ration d&#39;images

Remarque : L'utilisation de la t√¢che MediaPipe Image Generator est soumise au R√®glement sur les utilisations interdites de l'IA g√©n√©rative.
La t√¢che MediaPipe Image Generator vous permet de g√©n√©rer des images √† partir d'une requ√™te textuelle. Cette t√¢che utilise un mod√®le texte vers image pour g√©n√©rer des images √† l'aide de techniques de diffusion.

La t√¢che accepte une requ√™te textuelle en entr√©e, ainsi qu'une image de condition facultative que le mod√®le peut augmenter et utiliser comme r√©f√©rence pour la g√©n√©ration. Pour en savoir plus sur la g√©n√©ration d'images √† partir de texte conditionnel, consultez Plugins de diffusion sur l'appareil pour la g√©n√©ration d'images √† partir de texte conditionnel.

Le g√©n√©rateur d'images peut √©galement g√©n√©rer des images en fonction de concepts sp√©cifiques fournis au mod√®le lors de l'entra√Ænement ou du r√©entra√Ænement. Pour en savoir plus, consultez Personnaliser avec LoRA.

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez l'un de ces guides d'impl√©mentation pour votre plate-forme cible. Ces guides sp√©cifiques √† la plate-forme vous guident dans l'impl√©mentation de base de cette t√¢che, avec des exemples de code qui utilisent un mod√®le par d√©faut et les options de configuration recommand√©es :

Android ‚Äì Exemple de code ‚Äì Guide
Personnaliser avec LoRA ‚Äì Exemple de code ‚Äì Colab
D√©tails de la t√¢che
Cette section d√©crit les capacit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Fonctionnalit√©s
Vous pouvez utiliser le g√©n√©rateur d'images pour impl√©menter les √©l√©ments suivants :

G√©n√©ration d'images √† partir de texte : g√©n√©rez des images √† l'aide d'une requ√™te textuelle.
G√©n√©ration d'images avec des images de condition : g√©n√©rez des images avec une requ√™te textuelle et une image de r√©f√©rence. Image Generator utilise des images de condition de mani√®re similaire √† ControlNet.
G√©n√©ration d'images avec des pond√©rations LoRA : g√©n√©rez des images de personnes, d'objets et de styles sp√©cifiques √† l'aide d'une requ√™te textuelle utilisant des pond√©rations de mod√®le personnalis√©es.
Entr√©es de t√¢ches	Sorties de t√¢ches
Le g√©n√©rateur d'images accepte les entr√©es suivantes :
Requ√™te textuelle
Graine
Nombre d'it√©rations g√©n√©ratives
Facultatif : image de l'√©tat
Le g√©n√©rateur d'images produit les r√©sultats suivants :
Image g√©n√©r√©e en fonction des entr√©es.
Facultatif : Instantan√©s it√©ratifs de l'image g√©n√©r√©e.
Options de configuration
Cette t√¢che comporte les options de configuration suivantes :

Nom de l'option	Description	Plage de valeurs
imageGeneratorModelDirectory	R√©pertoire du mod√®le de g√©n√©rateur d'images stockant les pond√©rations du mod√®le.	PATH
loraWeightsFilePath	D√©finit le chemin d'acc√®s au fichier de pond√©rations LoRA. Facultatif et ne s'applique que si le mod√®le a √©t√© personnalis√© avec LoRA.	PATH
errorListener	D√©finit un √©couteur d'erreur facultatif.	N/A
La t√¢che est √©galement compatible avec les mod√®les de plug-in, ce qui permet aux utilisateurs d'inclure des images de condition dans l'entr√©e de la t√¢che, que le mod√®le de fondation peut augmenter et utiliser comme r√©f√©rence pour la g√©n√©ration. Ces images de condition peuvent √™tre des points de rep√®re du visage, des contours et des estimations de profondeur, que le mod√®le utilise comme contexte et informations suppl√©mentaires pour g√©n√©rer des images.

Lorsque vous ajoutez un mod√®le de plug-in au mod√®le de fondation, configurez √©galement les options du plug-in. Le plug-in de rep√®res faciaux utilise faceConditionOptions, le plug-in de d√©tection des contours de Canny utilise edgeConditionOptions et le plug-in de profondeur utilise depthConditionOptions.

Options de contours de Canny
Configurez les options suivantes dans edgeConditionOptions.

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
threshold1	Premier seuil de la proc√©dure d'hyst√©r√©sis.	Float	100
threshold2	Deuxi√®me seuil pour la proc√©dure d'hyst√©r√®se.	Float	200
apertureSize	Taille de l'ouverture pour l'op√©rateur Sobel. La plage habituelle est comprise entre 3 et 7.	Integer	3
l2Gradient	Indique si la norme L2 est utilis√©e pour calculer l'amplitude du gradient de l'image, au lieu de la norme L1 par d√©faut.	BOOLEAN	False
EdgePluginModelBaseOptions	Objet BaseOptions qui d√©finit le chemin d'acc√®s au mod√®le de plug-in.	Objet BaseOptions	N/A
Pour en savoir plus sur le fonctionnement de ces options de configuration, consultez D√©tecteur de contours Canny.

Options de points de rep√®re sur le visage
Configurez les options suivantes dans faceConditionOptions.

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
minFaceDetectionConfidence	Score de confiance minimal pour que la d√©tection du visage soit consid√©r√©e comme r√©ussie.	Float [0.0,1.0]	0.5
minFacePresenceConfidence	Score de confiance minimal de la pr√©sence d'un visage dans la d√©tection des points de rep√®re du visage.	Float [0.0,1.0]	0.5
faceModelBaseOptions	Objet BaseOptions qui d√©finit le chemin d'acc√®s au mod√®le qui cr√©e l'image de condition.	Objet BaseOptions	N/A
FacePluginModelBaseOptions	Objet BaseOptions qui d√©finit le chemin d'acc√®s au mod√®le de plug-in.	Objet BaseOptions	N/A
Pour en savoir plus sur le fonctionnement de ces options de configuration, consultez la section T√¢che de reconnaissance des points de rep√®re du visage.

Options de profondeur
Configurez les options suivantes dans depthConditionOptions.

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
depthModelBaseOptions	Objet BaseOptions qui d√©finit le chemin d'acc√®s au mod√®le qui cr√©e l'image de condition.	Objet BaseOptions	N/A
depthPluginModelBaseOptions	Objet BaseOptions qui d√©finit le chemin d'acc√®s au mod√®le de plug-in.	Objet BaseOptions	N/A
Mod√®les
Le g√©n√©rateur d'images n√©cessite un mod√®le de fondation, qui est un mod√®le d'IA de texte vers image qui utilise des techniques de diffusion pour g√©n√©rer de nouvelles images. Les mod√®les de fondation list√©s dans cette section sont des mod√®les l√©gers optimis√©s pour s'ex√©cuter sur des smartphones haut de gamme.

Les mod√®les de plug-in sont facultatifs et compl√®tent les mod√®les de base. Ils permettent aux utilisateurs de fournir une image de condition suppl√©mentaire avec une requ√™te textuelle, pour une g√©n√©ration d'images plus sp√©cifique. La personnalisation des mod√®les de fondation √† l'aide de pond√©rations LoRA est une option qui permet d'enseigner au mod√®le de fondation un concept sp√©cifique, tel qu'un objet, une personne ou un style, et de l'injecter dans les images g√©n√©r√©es.

Mod√®les de fondation
Remarque : Lorsque vous t√©l√©chargez un mod√®le Open Source √† partir d'un d√©p√¥t externe, vous √™tes tenu de respecter les conditions de licence applicables. Ces mod√®les Open Source ne sont pas un service Google.
Les mod√®les de base sont des mod√®les de diffusion latente de texte vers image qui g√©n√®rent des images √† partir d'une requ√™te textuelle. Le g√©n√©rateur d'images n√©cessite que le mod√®le de fondation corresponde au format de mod√®le stable-diffusion-v1-5/stable-diffusion-v1-5 EMA-only, en fonction du mod√®le suivant :

stable-diffusion-v1-5/stable-diffusion-v1-5
Les mod√®les de fondation suivants sont √©galement compatibles avec le g√©n√©rateur d'images :

justinpinkney/miniSD
hakurei/waifu-diffusion-v1-4
Fictiverse/Stable_Diffusion_PaperCut_Model
Apr√®s avoir t√©l√©charg√© un mod√®le de fondation, utilisez image_generator_converter pour convertir le mod√®le au format appropri√© sur l'appareil pour le g√©n√©rateur d'images.

Installez les d√©pendances n√©cessaires :

$ pip install torch typing_extensions numpy Pillow requests pytorch_lightning absl-py
Ex√©cutez le script convert.py :

$ python3 convert.py --ckpt_path <ckpt_path> --output_path <output_path>
Mod√®les de plug-ins
Les mod√®les de plug-in de cette section sont d√©velopp√©s par Google et doivent √™tre utilis√©s en combinaison avec un mod√®le de fondation. Les mod√®les de plug-in permettent au g√©n√©rateur d'images d'accepter une image de conditionnement ainsi qu'une requ√™te textuelle en entr√©e, ce qui vous permet de contr√¥ler la structure des images g√©n√©r√©es. Les mod√®les de plug-in offrent des fonctionnalit√©s semblables √† ControlNet, avec une nouvelle architecture sp√©cialement con√ßue pour la diffusion sur l'appareil.

Les mod√®les de plug-in doivent √™tre sp√©cifi√©s dans les options de base et peuvent n√©cessiter le t√©l√©chargement de fichiers de mod√®le suppl√©mentaires. Chaque plug-in a des exigences uniques concernant l'image de condition, qui peut √™tre g√©n√©r√©e par le g√©n√©rateur d'images.

Plug-in Canny Edge
Exemple de sortie de deux images g√©n√©r√©es qui utilisent une image de condition fournie avec un contour de briques bien d√©fini et la requ√™te 

Le plug-in Canny Edge accepte une image de condition qui d√©crit les contours souhait√©s de l'image g√©n√©r√©e. Le mod√®le de base utilise les contours impliqu√©s par l'image de condition et g√©n√®re une nouvelle image en fonction de la requ√™te textuelle. Le g√©n√©rateur d'images contient des fonctionnalit√©s int√©gr√©es permettant de cr√©er des images de conditions. Il vous suffit de t√©l√©charger le mod√®le de plug-in.

T√©l√©charger le plug-in Canny Edge

Le plug-in Canny Edge contient les options de configuration suivantes :

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
threshold1	Premier seuil de la proc√©dure d'hyst√©r√©sis.	Float	100
threshold2	Deuxi√®me seuil pour la proc√©dure d'hyst√©r√®se.	Float	200
apertureSize	Taille de l'ouverture pour l'op√©rateur Sobel. La plage habituelle est comprise entre 3 et 7.	Integer	3
l2Gradient	Indique si la norme L2 est utilis√©e pour calculer l'amplitude du gradient de l'image, au lieu de la norme L1 par d√©faut.	BOOLEAN	False
EdgePluginModelBaseOptions	Objet BaseOptions qui d√©finit le chemin d'acc√®s au mod√®le de plug-in.	Objet BaseOptions	N/A
Pour en savoir plus sur le fonctionnement de ces options de configuration, consultez D√©tecteur de contours Canny.

Plug-in Face Landmark
Exemple de deux images g√©n√©r√©es qui utilisent une image de condition fournie d&#39;un visage dessin√© et deux requ√™tes diff√©rentes pour montrer que la m√™me image de condition peut √™tre utilis√©e pour g√©n√©rer des images tr√®s diff√©rentes

Le plug-in Face Landmark accepte la sortie du Face Landmarker MediaPipe comme image de condition. Le rep√®re facial fournit un maillage facial d√©taill√© d'un seul visage, qui mappe la pr√©sence et l'emplacement des caract√©ristiques faciales. Le mod√®le de base utilise le mappage facial impliqu√© par l'image de condition et g√©n√®re un nouveau visage sur le maillage.

T√©l√©charger le plug-in Face landmark

Le plug-in Face landmark n√©cessite √©galement le bundle de mod√®le Face Landmarker pour cr√©er l'image de condition. Ce bundle de mod√®les est le m√™me que celui utilis√© par la t√¢che Face Landmarker.

T√©l√©charger le bundle de mod√®les de points de rep√®re du visage

Le plug-in Face Landmark contient les options de configuration suivantes :

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
minFaceDetectionConfidence	Score de confiance minimal pour que la d√©tection du visage soit consid√©r√©e comme r√©ussie.	Float [0.0,1.0]	0.5
minFacePresenceConfidence	Score de confiance minimal de la pr√©sence d'un visage dans la d√©tection des points de rep√®re du visage.	Float [0.0,1.0]	0.5
faceModelBaseOptions	Objet BaseOptions qui d√©finit le chemin d'acc√®s au mod√®le qui cr√©e l'image de condition.	Objet BaseOptions	N/A
FacePluginModelBaseOptions	Objet BaseOptions qui d√©finit le chemin d'acc√®s au mod√®le de plug-in.	Objet BaseOptions	N/A
Pour en savoir plus sur le fonctionnement de ces options de configuration, consultez la section T√¢che de reconnaissance des points de rep√®re du visage.

Plug-in de profondeur
Exemple de deux images g√©n√©r√©es √† partir d&#39;une image de condition fournie montrant la forme g√©n√©rale d&#39;une voiture, pour illustrer la capacit√© du plug-in Depth √† cr√©er des images qui ajoutent de la profondeur √† une image plate

Le plug-in Depth accepte une image de condition qui sp√©cifie la profondeur monoculaire d'un objet. Le mod√®le de base utilise l'image de condition pour d√©duire la taille et la profondeur de l'objet √† g√©n√©rer, puis g√©n√®re une nouvelle image en fonction de l'invite textuelle.

T√©l√©charger le plug-in Depth

Le plug-in Depth n√©cessite √©galement un mod√®le d'estimation de la profondeur pour cr√©er l'image de condition.

T√©l√©charger le mod√®le d'estimation de la profondeur

Le plug-in Depth contient les options de configuration suivantes :

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
depthModelBaseOptions	Objet BaseOptions qui d√©finit le chemin d'acc√®s au mod√®le qui cr√©e l'image de condition.	Objet BaseOptions	N/A
depthPluginModelBaseOptions	Objet BaseOptions qui d√©finit le chemin d'acc√®s au mod√®le de plug-in.	Objet BaseOptions	N/A
Personnalisation avec LoRA
Remarque : Si un mod√®le est personnalis√© avec des pond√©rations LoRA, il ne doit √™tre utilis√© que pour g√©n√©rer des images du concept tokenis√©. Il n'est plus utile en tant que mod√®le de g√©n√©ration d'images g√©n√©ralis√© et devient incompatible avec les mod√®les de plug-ins.
La personnalisation d'un mod√®le avec LoRA peut permettre au g√©n√©rateur d'images de g√©n√©rer des images bas√©es sur des concepts sp√©cifiques, qui sont identifi√©s par des jetons uniques lors de l'entra√Ænement. Gr√¢ce aux nouveaux poids LoRA apr√®s l'entra√Ænement, le mod√®le est capable de g√©n√©rer des images du nouveau concept lorsque le jeton est sp√©cifi√© dans la requ√™te textuelle.

Pour cr√©er des pond√©rations LoRA, il faut entra√Æner un mod√®le de base sur des images d'un objet, d'une personne ou d'un style sp√©cifiques. Le mod√®le peut ainsi reconna√Ætre le nouveau concept et l'appliquer lors de la g√©n√©ration d'images. Si vous cr√©ez des pond√©rations LoRA pour g√©n√©rer des images de personnes et de visages sp√©cifiques, n'utilisez cette solution que sur votre visage ou sur les visages de personnes qui vous ont donn√© leur autorisation.

Vous trouverez ci-dessous le r√©sultat d'un mod√®le personnalis√© entra√Æn√© sur des images de th√©i√®res de l'ensemble de donn√©es DreamBooth, √† l'aide du jeton "th√©i√®re monadikos" :

Image photor√©aliste g√©n√©r√©e d&#39;une th√©i√®re pos√©e sur une table √† c√¥t√© d&#39;un miroir fix√© au mur

Requ√™te : une th√©i√®re monadikos √† c√¥t√© d'un miroir

Le mod√®le personnalis√© a re√ßu le jeton dans le prompt et a ins√©r√© une th√©i√®re qu'il a appris √† repr√©senter √† partir des pond√©rations LoRA, et l'a plac√©e dans l'image √† c√¥t√© d'un miroir, comme demand√© dans le prompt.

LoRA avec Vertex AI

Pour en savoir plus, consultez le guide de personnalisation, qui utilise Model Garden sur Vertex AI pour personnaliser un mod√®le en appliquant des pond√©rations LoRA √† un mod√®le de fondation.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/16 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de g√©n√©ration d'images pour Android


Attention:La t√¢che du g√©n√©rateur d'images MediaPipe est exp√©rimentale et en cours de d√©veloppement.
Remarque :L'utilisation de la t√¢che MediaPipe Image Generator est soumise au R√®glement sur les utilisations interdites de l'IA g√©n√©rative.
La t√¢che du g√©n√©rateur d'images MediaPipe vous permet de g√©n√©rer des images √† partir d'une requ√™te textuelle. Cette t√¢che utilise un mod√®le texte-vers-image pour g√©n√©rer des images √† l'aide de techniques de diffusion.

La t√¢che accepte une requ√™te textuelle en entr√©e, ainsi qu'une image de condition facultative que le mod√®le peut augmenter et utiliser comme r√©f√©rence pour la g√©n√©ration. Le g√©n√©rateur d'images peut √©galement g√©n√©rer des images en fonction de concepts sp√©cifiques fournis au mod√®le lors de l'entra√Ænement ou du r√©entra√Ænement. Pour en savoir plus, consultez Personnaliser avec LoRA.

L'exemple de code d√©crit dans ces instructions est disponible sur GitHub. Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Exemple de code
L'exemple de code MediaPipe Tasks est une impl√©mentation de base d'une application de g√©n√©rateur d'images pour Android. Vous pouvez utiliser l'application comme point de d√©part pour votre propre application Android ou vous y r√©f√©rer lorsque vous modifiez une application existante. L'exemple de code de l'outil Image Generator est h√©berg√© sur GitHub.

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple de code √† l'aide de l'outil de ligne de commande git.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Vous pouvez √©ventuellement configurer votre instance git pour qu'elle utilise un "checkout sparse", de sorte que vous n'ayez que les fichiers de l'application exemple Image Generator:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/image_generation/android
Apr√®s avoir cr√©√© une version locale de l'exemple de code, vous pouvez importer le projet dans Android Studio et ex√©cuter l'application. Pour obtenir des instructions, consultez le guide de configuration pour Android.

Composants cl√©s
Les fichiers suivants contiennent le code essentiel de cet exemple d'application de g√©n√©ration d'images:

ImageGenerationHelper.kt : initialise la t√¢che et g√®re la g√©n√©ration d'images.
DiffusionActivity.kt : g√©n√®re des images lorsque les plug-ins ou les poids LoRA ne sont pas activ√©s.
PluginActivity.kt : impl√©mente les mod√®les de plug-in, ce qui permet aux utilisateurs de fournir une image de condition en entr√©e.
LoRAWeightActivity.kt : acc√®de aux poids LoRA et les g√®re, qui sont utilis√©s pour personnaliser les mod√®les de base et leur permettre de g√©n√©rer des images de concepts sp√©cifiques.
Configuration
Cette section d√©crit les √©tapes cl√©s √† suivre pour configurer votre environnement de d√©veloppement et vos projets de code sp√©cifiquement pour utiliser Image Generator. Pour obtenir des informations g√©n√©rales sur la configuration de votre environnement de d√©veloppement pour utiliser les t√¢ches MediaPipe, y compris les exigences concernant la version de la plate-forme, consultez le guide de configuration pour Android.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
D√©pendances
La t√¢che du g√©n√©rateur d'images utilise la biblioth√®que com.google.mediapipe:tasks-vision-image-generator. Ajoutez cette d√©pendance au fichier build.gradle de votre application Android:


dependencies {
    implementation 'com.google.mediapipe:tasks-vision-image-generator:latest.release'
}
Pour les appareils √©quip√©s d'Android 12 (API 31) ou version ult√©rieure, ajoutez la d√©pendance de la biblioth√®que OpenCL native. Pour en savoir plus, consultez la documentation sur la balise uses-native-library.

Ajoutez les balises uses-native-library suivantes au fichier AndroidManifest.xml:

<uses-native-library android:name="libOpenCL.so" android:required="false" />
<uses-native-library android:name="libOpenCL-car.so" android:required="false"/>
<uses-native-library android:name="libOpenCL-pixel.so" android:required="false" />
Mod√®le
La t√¢che du g√©n√©rateur d'images MediaPipe n√©cessite un mod√®le de base entra√Æn√© compatible avec cette t√¢che. Apr√®s avoir t√©l√©charg√© un mod√®le, installez les d√©pendances requises et convertissez-le dans un format appropri√©. Transmettez ensuite le mod√®le converti vers l'appareil Android.

Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour Image Generator, consultez la section Mod√®les de la pr√©sentation de la t√¢che.

Remarque :Lorsque vous t√©l√©chargez un mod√®le disponible publiquement √† partir d'un d√©p√¥t externe, vous √™tes tenu de respecter les conditions de licence applicables. Ces mod√®les Open Source ne sont pas un service Google.
T√©l√©charger le mod√®le de fondation
Le g√©n√©rateur d'images exige que le mod√®le de base corresponde au format de mod√®le runwayml/stable-diffusion-v1-5 EMA-only, bas√© sur le mod√®le suivant : runwayml/stable-diffusion-v1-5.

Installer les d√©pendances et convertir le mod√®le
$ pip install torch typing_extensions numpy Pillow requests pytorch_lightning absl-py
Ex√©cutez le script convert.py:

$ python3 convert.py --ckpt_path <ckpt_path> --output_path <output_path>
Transf√©rer le mod√®le converti sur l'appareil
Remarque :Lors du d√©veloppement, vous pouvez utiliser adb pour transf√©rer le mod√®le de fondation vers votre appareil de test pour plus de simplicit√©.
Transf√©rez le contenu du dossier <output_path> sur l'appareil Android.

$ adb shell rm -r /data/local/tmp/image_generator/ # Remove any previously loaded weights
$ adb shell mkdir -p /data/local/tmp/image_generator/
$ adb push <output_path>/. /data/local/tmp/image_generator/bins
Remarque :Pour le d√©ploiement en production, h√©bergez le mod√®le converti sur un serveur et t√©l√©chargez-le pendant l'ex√©cution. Le mod√®le est trop volumineux pour √™tre group√© dans un APK.
T√©l√©charger des mod√®les de plug-in et ajouter des poids LoRA (facultatif)
Si vous pr√©voyez d'utiliser un mod√®le de plug-in, v√©rifiez si le mod√®le doit √™tre t√©l√©charg√©. Pour les plug-ins qui n√©cessitent un mod√®le suppl√©mentaire, les mod√®les de plug-in doivent √™tre group√©s dans l'APK ou t√©l√©charg√©s √† la demande. Les mod√®les de plug-in sont l√©gers (environ 23 Mo) et peuvent √™tre group√©s directement dans l'APK. Toutefois, nous vous recommandons de t√©l√©charger les mod√®les de plug-in √† la demande.

Si vous avez personnalis√© un mod√®le avec LoRA, t√©l√©chargez-le √† la demande. Pour en savoir plus, consultez le mod√®le de plug-in des poids LoRA.

Cr√©er la t√¢che
La t√¢che du g√©n√©rateur d'images MediaPipe utilise la fonction createFromOptions() pour configurer la t√¢che. La fonction createFromOptions() accepte des valeurs pour les options de configuration. Pour en savoir plus sur les options de configuration, consultez la section Options de configuration.

Options de configuration
Cette t√¢che propose les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs
imageGeneratorModelDirectory	R√©pertoire du mod√®le de g√©n√©rateur d'images qui stocke les poids du mod√®le.	PATH
loraWeightsFilePath	D√©finit le chemin d'acc√®s au fichier de poids LoRA. Facultatif et ne s'applique que si le mod√®le a √©t√© personnalis√© avec LoRA.	PATH
errorListener	D√©finit un √©couteur d'erreur facultatif.	N/A
La t√¢che est √©galement compatible avec les mod√®les de plug-in, ce qui permet aux utilisateurs d'inclure des images de condition dans l'entr√©e de la t√¢che, que le mod√®le de fondation peut augmenter et utiliser comme r√©f√©rence pour la g√©n√©ration. Ces images de condition peuvent √™tre des rep√®res faciaux, des contours de bord et des estimations de profondeur, que le mod√®le utilise comme contexte et informations suppl√©mentaires pour g√©n√©rer des images.

Lorsque vous ajoutez un mod√®le de plug-in au mod√®le de base, configurez √©galement les options du plug-in. Le plug-in de rep√®re de visage utilise faceConditionOptions, le plug-in de bordure Canny utilise edgeConditionOptions et le plug-in de profondeur utilise depthConditionOptions.

Options de Canny
Configurez les options suivantes dans edgeConditionOptions.

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
threshold1	Premier seuil de la proc√©dure d'hyst√©r√©sis.	Float	100
threshold2	Deuxi√®me seuil pour la proc√©dure d'hyst√©r√©sis.	Float	200
apertureSize	Taille de l'ouverture pour l'op√©rateur Sobel. La plage habituelle est comprise entre 3 et 7.	Integer	3
l2Gradient	Indique si la norme L2 est utilis√©e pour calculer l'amplitude du gradient d'image au lieu de la norme L1 par d√©faut.	BOOLEAN	False
EdgePluginModelBaseOptions	Objet BaseOptions qui d√©finit le chemin d'acc√®s au mod√®le du plug-in.	Objet BaseOptions	N/A
Pour en savoir plus sur le fonctionnement de ces options de configuration, consultez la section D√©tecteur de bords Canny.

Options de points de rep√®re du visage
Configurez les options suivantes dans faceConditionOptions.

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
minFaceDetectionConfidence	Score de confiance minimal pour que la d√©tection de visage soit consid√©r√©e comme r√©ussie.	Float [0.0,1.0]	0.5
minFacePresenceConfidence	Score de confiance minimal du score de pr√©sence de visage dans la d√©tection des points de rep√®re du visage.	Float [0.0,1.0]	0.5
faceModelBaseOptions	Objet BaseOptions qui d√©finit le chemin d'acc√®s au mod√®le qui cr√©e l'image de la condition.	Objet BaseOptions	N/A
FacePluginModelBaseOptions	Objet BaseOptions qui d√©finit le chemin d'acc√®s au mod√®le du plug-in.	Objet BaseOptions	N/A
Pour en savoir plus sur le fonctionnement de ces options de configuration, consultez la section T√¢che de rep√®re facial.

Options de profondeur
Configurez les options suivantes dans depthConditionOptions.

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
depthModelBaseOptions	Objet BaseOptions qui d√©finit le chemin d'acc√®s au mod√®le qui cr√©e l'image de la condition.	Objet BaseOptions	N/A
depthPluginModelBaseOptions	Objet BaseOptions qui d√©finit le chemin d'acc√®s au mod√®le du plug-in.	Objet BaseOptions	N/A
Cr√©er uniquement avec le mod√®le de base
val options = ImageGeneratorOptions.builder()
    .setImageGeneratorModelDirectory(modelPath)
    .build()

imageGenerator = ImageGenerator.createFromOptions(context, options)
Cr√©er avec des plug-ins
Si vous appliquez un mod√®le de plug-in facultatif, d√©finissez les options de base du mod√®le de plug-in avec setPluginModelBaseOptions. Si le mod√®le de plug-in n√©cessite un mod√®le t√©l√©charg√© suppl√©mentaire pour cr√©er l'image de la condition, sp√©cifiez le chemin d'acc√®s dans BaseOptions.

Point de rep√®re du visage
Canny Edge
Profondeur
val options = ImageGeneratorOptions.builder()
    .setImageGeneratorModelDirectory(modelPath)
    .build()

val faceModelBaseOptions = BaseOptions.builder()
    .setModelAssetPath("face_landmarker.task")
    .build()

val facePluginModelBaseOptions = BaseOptions.builder()
    .setModelAssetPath("face_landmark_plugin.tflite")
    .build()

val faceConditionOptions = FaceConditionOptions.builder()
    .setFaceModelBaseOptions(faceModelBaseOptions)
    .setPluginModelBaseOptions(facePluginModelBaseOptions)
    .setMinFaceDetectionConfidence(0.3f)
    .setMinFacePresenceConfidence(0.3f)
    .build()

val conditionOptions = ConditionOptions.builder()
    .setFaceConditionOptions(faceConditionOptions)
    .build()

imageGenerator =
    ImageGenerator.createFromOptions(context, options, conditionOptions)
    
Cr√©er avec des pond√©rations LoRA
Si vous incluez des poids LoRA, utilisez le param√®tre loraWeightsFilePath pour indiquer l'emplacement du chemin.

val options = ImageGeneratorOptions.builder()
    .setLoraWeightsFilePath(weightsPath)
    .setImageGeneratorModelDirectory(modelPath)
    .build()

imageGenerator = ImageGenerator.createFromOptions(context, options)
Pr√©parer les donn√©es
Le g√©n√©rateur d'images accepte les entr√©es suivantes:

prompt (obligatoire): requ√™te textuelle d√©crivant l'image √† g√©n√©rer.
iterations (obligatoire): nombre total d'it√©rations pour g√©n√©rer l'image. Un bon point de d√©part est 20.
seed (obligatoire): graine al√©atoire utilis√©e lors de la g√©n√©ration d'images.
condition image (facultatif): image utilis√©e par le mod√®le comme r√©f√©rence pour la g√©n√©ration. Ne s'applique que lorsque vous utilisez un mod√®le de plug-in.
type de condition (facultatif): type de mod√®le de plug-in utilis√© avec la t√¢che. Ne s'applique que lorsque vous utilisez un mod√®le de plug-in.
Entr√©es avec uniquement le mod√®le de base
fun setInput(prompt: String, iteration: Int, seed: Int) {
    imageGenerator.setInputs(prompt, iteration, seed)
}
Saisies avec des plug-ins
Si vous appliquez un mod√®le de plug-in facultatif, utilisez √©galement le param√®tre conditionType pour choisir le mod√®le de plug-in et le param√®tre sourceConditionImage pour g√©n√©rer l'image de la condition.

Nom de l'option	Description	Valeur
conditionType	Mod√®le de plug-in appliqu√© au mod√®le de base.	{"FACE", "EDGE", "DEPTH"}
sourceConditionImage	Image source utilis√©e pour cr√©er l'image de condition.	Objet MPImage
Si vous utilisez un mod√®le de plug-in, utilisez createConditionImage pour cr√©er l'image de la condition:

fun createConditionImage(
    inputImage: MPImage,
    conditionType: ConditionType
): Bitmap {
    val result =
        imageGenerator.createConditionImage(inputImage, conditionType)
    return BitmapExtractor.extract(result)
}
Apr√®s avoir cr√©√© l'image de condition, incluez-la en tant qu'entr√©e avec l'invite, la valeur initiale et le nombre d'it√©rations.

imageGenerator.setInputs(
    prompt,
    conditionalImage,
    conditionType,
    iteration,
    seed
)
Entr√©es avec pond√©rations LoRA
Si vous utilisez des poids LoRA, assurez-vous que le jeton figure dans la requ√™te textuelle si vous souhaitez g√©n√©rer une image avec le concept sp√©cifique repr√©sent√© par les poids.

fun setInput(prompt: String, iteration: Int, seed: Int) {
    imageGenerator.setInputs(prompt, iteration, seed)
}
Remarque :Apr√®s avoir personnalis√© un mod√®le avec des pond√©rations LoRA, il n'utilise les nouvelles pond√©rations que si le jeton est pr√©sent dans l'invite.
Ex√©cuter la t√¢che
Utilisez la m√©thode generate() pour g√©n√©rer une image √† l'aide des entr√©es fournies dans la section pr√©c√©dente. Une seule image est g√©n√©r√©e.

G√©n√©rer uniquement avec le mod√®le de base
fun generate(prompt: String, iteration: Int, seed: Int): Bitmap {
    val result = imageGenerator.generate(prompt, iteration, seed)
    val bitmap = BitmapExtractor.extract(result?.generatedImage())
    return bitmap
}
G√©n√©rer avec des plug-ins
fun generate(
    prompt: String,
    inputImage: MPImage,
    conditionType: ConditionType,
    iteration: Int,
    seed: Int
): Bitmap {
    val result = imageGenerator.generate(
        prompt,
        inputImage,
        conditionType,
        iteration,
        seed
    )
    val bitmap = BitmapExtractor.extract(result?.generatedImage())
    return bitmap
}
G√©n√©rer avec des pond√©rations LoRA
Le processus de g√©n√©ration d'images avec un mod√®le personnalis√© avec des poids LoRA est semblable √† celui d'un mod√®le de base standard. Assurez-vous que le jeton est inclus dans l'invite et ex√©cutez le m√™me code.

fun generate(prompt: String, iteration: Int, seed: Int): Bitmap {
    val result = imageGenerator.generate(prompt, iteration, seed)
    val bitmap = BitmapExtractor.extract(result?.generatedImage())
    return bitmap
}
G√©n√©ration it√©rative
Le g√©n√©rateur d'images peut √©galement g√©n√©rer les images interm√©diaires g√©n√©r√©es √† chaque it√©ration, comme d√©fini dans le param√®tre d'entr√©e iterations. Pour afficher ces r√©sultats interm√©diaires, appelez la m√©thode setInputs, puis appelez execute() pour ex√©cuter chaque √©tape. D√©finissez le param√®tre showResult sur true pour afficher les r√©sultats interm√©diaires.

fun execute(showResult: Boolean): Bitmap {
    val result = imageGenerator.execute(showResult)

    val bitmap =
        BitmapExtractor.extract(result.generatedImage())

    return bitmap
}
Remarque :L'ex√©cution et l'affichage des g√©n√©rations interm√©diaires augmentent consid√©rablement les temps de traitement.
G√©rer et afficher les r√©sultats
Le g√©n√©rateur d'images renvoie un ImageGeneratorResult, qui inclut l'image g√©n√©r√©e, un code temporel de l'heure d'ach√®vement et l'image conditionnelle si une telle image a √©t√© fournie en entr√©e.

val bitmap = BitmapExtractor.extract(result.generatedImage())
L'image suivante a √©t√© g√©n√©r√©e √† partir des entr√©es suivantes, en utilisant uniquement un mod√®le de base.

Entr√©es :

Invite: "un raton laveur de dessin anim√© color√© portant un chapeau √† larges bords et tenant un b√¢ton en marchant dans la for√™t, anim√©, vue en trois quarts, peinture"
Graine: 312687592
It√©rations: 20
Image g√©n√©r√©e:

Image g√©n√©r√©e d&#39;un raton laveur correspondant √† la requ√™te

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide des t√¢ches de d√©tection d'objets


Un chat et un chien mis en √©vidence avec des cadres de d√©limitation correctement libell√©s

La t√¢che de d√©tection d'objets MediaPipe vous permet de d√©tecter la pr√©sence et l'emplacement de plusieurs classes d'objets dans des images ou des vid√©os. Par exemple, un d√©tecteur d'objets peut localiser des chiens dans une image. Cette t√¢che fonctionne sur des donn√©es d'image avec un mod√®le de machine learning (ML), accepte des donn√©es statiques ou un flux vid√©o continu en entr√©e et g√©n√®re une liste de r√©sultats de d√©tection. Chaque r√©sultat de d√©tection repr√©sente un objet qui appara√Æt dans l'image ou la vid√©o.

Essayez !arrow_forward

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez l'un de ces guides d'impl√©mentation pour la plate-forme sur laquelle vous travaillez:

Android ‚Äì Exemple de code ‚Äì Guide
Python ‚Äì Exemple de code ‚Äì Guide
Web ‚Äì Exemple de code ‚Äì Guide
iOS ‚Äì Exemple de code ‚Äì Guide
Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter de mani√®re basique cette t√¢che, y compris un mod√®le recommand√© et un exemple de code avec les options de configuration recommand√©es.

D√©tails de la t√¢che
Cette section d√©crit les fonctionnalit√©s, les entr√©es et les sorties de cette t√¢che.

Fonctionnalit√©s
Traitement des images d'entr√©e : le traitement comprend la rotation, le redimensionnement, la normalisation et la conversion d'espaces colorim√©triques.
Label map locale (Libell√© de la carte en param√®tres r√©gionaux) : d√©finissez la langue utilis√©e pour les noms √† afficher.
Seuil de score : filtrez les r√©sultats en fonction des scores de pr√©diction.
D√©tection des k meilleurs : filtrez les r√©sultats de d√©tection des nombres.
Liste d'autorisation et de refus des libell√©s : sp√©cifiez les cat√©gories d√©tect√©es.
Entr√©es de t√¢che	Sorties de t√¢che
L'API Object Detector accepte l'un des types de donn√©es suivants en entr√©e :
Images fixes
Images vid√©o d√©cod√©es
Flux vid√©o en direct
L'API Object Detector renvoie les r√©sultats suivants pour les objets d√©tect√©s :
Cat√©gorie d'objet
Score de probabilit√©
Coordonn√©es du cadre de d√©limitation
Options de configuration
Cette t√¢che propose les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
running_mode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
display_names	D√©finit la langue des libell√©s √† utiliser pour les noms √† afficher fournis dans les m√©tadonn√©es du mod√®le de la t√¢che, le cas √©ch√©ant. La valeur par d√©faut est en pour l'anglais. Vous pouvez ajouter des libell√©s localis√©s aux m√©tadonn√©es d'un mod√®le personnalis√© √† l'aide de l'API TensorFlow Lite Metadata Writer.	Code de param√®tres r√©gionaux	en
max_results	D√©finit le nombre maximal facultatif de r√©sultats de d√©tection les plus √©lev√©s √† renvoyer.	N'importe quel nombre positif	-1 (tous les r√©sultats sont renvoy√©s)
score_threshold	D√©finit le seuil de score de pr√©diction qui remplace celui fourni dans les m√©tadonn√©es du mod√®le (le cas √©ch√©ant). Les r√©sultats inf√©rieurs √† cette valeur sont rejet√©s.	N'importe quelle superposition	Non d√©fini
category_allowlist	D√©finit la liste facultative des noms de cat√©gories autoris√©s. Si cet ensemble n'est pas vide, les r√©sultats de d√©tection dont le nom de cat√©gorie ne figure pas dans cet ensemble seront filtr√©s. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option s'exclut mutuellement avec category_denylist. L'utilisation des deux entra√Æne une erreur.	N'importe quelle cha√Æne	Non d√©fini
category_denylist	D√©finit la liste facultative des noms de cat√©gories non autoris√©s. Si cet ensemble n'est pas vide, les r√©sultats de d√©tection dont le nom de cat√©gorie figure dans cet ensemble seront filtr√©s. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option s'exclut mutuellement avec category_allowlist. L'utilisation des deux entra√Æne une erreur.	N'importe quelle cha√Æne	Non d√©fini
Mod√®les
L'API Object Detector n√©cessite que vous t√©l√©chargiez et stockiez un mod√®le de d√©tection d'objets dans le r√©pertoire de votre projet. Si vous ne disposez pas encore d'un mod√®le, commencez par le mod√®le par d√©faut recommand√©. Les autres mod√®les pr√©sent√©s dans cette section font des compromis entre la latence et la pr√©cision.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Mod√®le EfficientDet-Lite0 (recommand√©)
Le mod√®le EfficientDet-Lite0 utilise un backbone EfficientNet-Lite0 avec une taille d'entr√©e de 320x320 et un r√©seau de caract√©ristiques BiFPN. Le mod√®le a √©t√© entra√Æn√© avec l'ensemble de donn√©es COCO, un ensemble de donn√©es de d√©tection d'objets √† grande √©chelle contenant 1,5 million d'instances d'objets et 80 √©tiquettes d'objets. Consultez la liste compl√®te des libell√©s compatibles. EfficientDet-Lite0 est disponible en tant qu'int8, float16 ou float32. Ce mod√®le est recommand√©, car il √©tablit un √©quilibre entre la latence et la pr√©cision. Il est √† la fois pr√©cis et suffisamment l√©ger pour de nombreux cas d'utilisation.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Versions
EfficientDet-Lite0 (int8)	320 x 320	int8	Nouveaut√©s
EfficientDet-Lite0 (float 16)	320 x 320	float 16	Nouveaut√©s
EfficientDet-Lite0 (float 32)	320 x 320	Aucun (float32)	Nouveaut√©s
Mod√®le EfficientDet-Lite2
Le mod√®le EfficientDet-Lite2 utilise un backbone EfficientNet-Lite2 avec une taille d'entr√©e de 448 x 448 et un r√©seau de caract√©ristiques BiFPN. Le mod√®le a √©t√© entra√Æn√© avec l'ensemble de donn√©es COCO, un ensemble de donn√©es de d√©tection d'objets √† grande √©chelle contenant 1,5 million d'instances d'objets et 80 √©tiquettes d'objets. Consultez la liste compl√®te des libell√©s compatibles. EfficientDet-Lite2 est disponible en tant que mod√®le int8, float16 ou float32. Ce mod√®le est g√©n√©ralement plus pr√©cis qu'EfficientDet-Lite0, mais il est √©galement plus lent et plus gourmand en m√©moire. Ce mod√®le est adapt√© aux cas d'utilisation o√π la pr√©cision est plus prioritaire que la vitesse et la taille.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Versions
EfficientDet-Lite2 (int8)	448 x 448	int8	Nouveaut√©s
EfficientDet-Lite2 (float 16)	448 x 448	float 16	Nouveaut√©s
EfficientDet-Lite2 (float 32)	448 x 448	Aucun (float32)	Nouveaut√©s
Mod√®le SSD MobileNetV2
Le mod√®le SSD MobileNetV2 utilise une colonne vert√©brale MobileNetV2 avec une taille d'entr√©e de 256 x 256 et un r√©seau de caract√©ristiques SSD. Le mod√®le a √©t√© entra√Æn√© avec l'ensemble de donn√©es COCO, un ensemble de donn√©es de d√©tection d'objets √† grande √©chelle contenant 1,5 million d'instances d'objets et 80 √©tiquettes d'objets. Consultez la liste compl√®te des libell√©s compatibles. SSD MobileNetV2 est disponible en tant que mod√®le int8 et float32. Ce mod√®le est plus rapide et plus l√©ger qu'EfficientDet-Lite0, mais il est √©galement g√©n√©ralement moins pr√©cis. Ce mod√®le est adapt√© aux cas d'utilisation qui n√©cessitent un mod√®le rapide et l√©ger qui sacrifie une certaine pr√©cision.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Versions
SSDMobileNet-V2 (int8)	256 x 256	int8	Nouveaut√©s
SSDMobileNet-V2 (32 d√©cimales)	256 x 256	Aucun (float32)	Nouveaut√©s
Exigences concernant les mod√®les et m√©tadonn√©es
Cette section d√©crit les exigences concernant les mod√®les personnalis√©s si vous d√©cidez d'en cr√©er un √† utiliser avec cette t√¢che. Les mod√®les personnalis√©s doivent √™tre au format TensorFlow Lite et doivent inclure des metadata d√©crivant les param√®tres de fonctionnement du mod√®le.

Exigences de conception
Entr√©e	Forme	Description
Image d'entr√©e	Tensor Float32 de forme [1, hauteur, largeur, 3]	Image d'entr√©e normalis√©e.
Sortie	Forme	Description
detection_boxes	Tensor float32 de forme [1, num_boxes, 4]	Emplacement du cadre de chaque objet d√©tect√©.
detection_classes	Tensor float32 de forme [1, num_boxes]	Indices des noms de classe pour chaque objet d√©tect√©.
detection_scores	Tensor float32 de forme [1, num_boxes]	Scores de pr√©diction pour chaque objet d√©tect√©.
num_boxes	Tensor float32 de taille 1	Nombre de cases d√©tect√©es.
Exigences concernant les m√©tadonn√©es
Param√®tre	Description	Description
input_norm_mean	Valeur moyenne utilis√©e dans la normalisation du tenseur d'entr√©e.	Image d'entr√©e normalis√©e.
input_norm_std	Norme de champ utilis√©e dans la normalisation du tenseur d'entr√©e.	Emplacement du cadre de chaque objet d√©tect√©.
label_file_paths	Chemins d'acc√®s aux fichiers de libell√©s de tenseur de cat√©gorie. Si le mod√®le ne comporte aucun fichier d'√©tiquette, transmettez une liste vide.	Indices des noms de classe pour chaque objet d√©tect√©.
score_calibration_md	Informations sur l'op√©ration de calibration du score dans le tenseur de classification. Ce param√®tre n'est pas obligatoire si le mod√®le n'utilise pas la calibration de la note.
Scores de pr√©diction pour chaque objet d√©tect√©.
num_boxes	Tensor float32 de taille 1	Nombre de cases d√©tect√©es.
Benchmarks des t√¢ches
Voici les benchmarks de t√¢che pour les mod√®les pr√©-entra√Æn√©s ci-dessus. Le r√©sultat de la latence correspond √† la latence moyenne sur le Pixel 6 √† l'aide du processeur / GPU.

Nom du mod√®le	Latence du processeur	Latence du GPU
Mod√®le float32 EfficientDet-Lite0	61,30 ms	27,83 ms
Mod√®le float16 EfficientDet-Lite0	53,97 ms	27,97 ms
Mod√®le int8 EfficientDet-Lite0	29,31 ms	-
Mod√®le float32 EfficientDet-Lite2	197,98 ms	41,15 ms
Mod√®le float16 EfficientDet-Lite2	198,77 ms	47,31 ms
Mod√®le int8 EfficientDet-Lite2	70,91 ms	-
Mod√®le SSD MobileNetV2 float32	36,30 ms	24,01 ms
Mod√®le SSD MobileNetV2 float16	37,35 ms	28,16 ms
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/01/13 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de d√©tection d'objets pour Android


La t√¢che D√©tecteur d'objets vous permet de d√©tecter la pr√©sence et l'emplacement de plusieurs classes d'objets. Par exemple, un d√©tecteur d'objets peut localiser des chiens dans une image. Ces instructions vous expliquent comment utiliser la t√¢che Object Detector sur Android. L'exemple de code d√©crit dans ces instructions est disponible sur GitHub. Pour voir cette t√¢che en action, regardez cette d√©monstration Web. Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Exemple de code
L'exemple de code MediaPipe Tasks est une impl√©mentation simple d'une application de d√©tection d'objets pour Android. L'exemple utilise l'appareil photo d'un appareil Android physique pour d√©tecter en continu des objets. Il peut √©galement utiliser des images et des vid√©os de la galerie de l'appareil pour d√©tecter des objets de mani√®re statique.

Vous pouvez utiliser l'application comme point de d√©part pour votre propre application Android ou vous y r√©f√©rer lorsque vous modifiez une application existante. L'exemple de code du d√©tecteur d'objets est h√©berg√© sur GitHub.

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple de code √† l'aide de l'outil de ligne de commande git.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Vous pouvez √©galement configurer votre instance git pour utiliser un "checkout" clairsem√© afin de n'avoir que les fichiers de l'application exemple Object Detector:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/object_detection/android
Apr√®s avoir cr√©√© une version locale de l'exemple de code, vous pouvez importer le projet dans Android Studio et ex√©cuter l'application. Pour obtenir des instructions, consultez le guide de configuration pour Android.

Composants cl√©s
Les fichiers suivants contiennent le code essentiel de l'application exemple du d√©tecteur d'objets:

ObjectDetectorHelper.kt : initialise le d√©tecteur d'objets et g√®re le mod√®le et la s√©lection du d√©l√©gu√©
MainActivity.kt : impl√©mente l'application et assemble les composants de l'interface utilisateur
OverlayView.kt : g√®re et affiche les r√©sultats
Configuration
Cette section d√©crit les √©tapes cl√©s √† suivre pour configurer votre environnement de d√©veloppement et vos projets de code afin d'utiliser Object Detector. Pour obtenir des informations g√©n√©rales sur la configuration de votre environnement de d√©veloppement pour utiliser les t√¢ches MediaPipe, y compris les exigences concernant les versions de la plate-forme, consultez le guide de configuration pour Android.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
D√©pendances
Le d√©tecteur d'objets utilise la biblioth√®que com.google.mediapipe:tasks-vision. Ajoutez cette d√©pendance au fichier build.gradle de votre projet de d√©veloppement d'application Android. Importez les d√©pendances requises avec le code suivant:

dependencies {
    implementation 'com.google.mediapipe:tasks-vision:latest.release'
}
Mod√®le
La t√¢che de d√©tection d'objets MediaPipe n√©cessite un mod√®le entra√Æn√© compatible avec cette t√¢che. Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour Object Detector, consultez la section Mod√®les de la pr√©sentation de la t√¢che.

S√©lectionnez et t√©l√©chargez le mod√®le, puis stockez-le dans le r√©pertoire de votre projet:

<dev-project-root>/src/main/assets
Remarque :Cet emplacement est recommand√©, car le syst√®me de compilation Android v√©rifie automatiquement les ressources de fichier dans ce r√©pertoire.
Utilisez la m√©thode BaseOptions.Builder.setModelAssetPath() pour sp√©cifier le chemin d'acc√®s utilis√© par le mod√®le. Pour obtenir un exemple de code, consultez la section suivante.

Cr√©er la t√¢che
Vous pouvez utiliser la fonction createFromOptions pour cr√©er la t√¢che. La fonction createFromOptions accepte des options de configuration, y compris le mode d'ex√©cution, les param√®tres r√©gionaux des noms √† afficher, le nombre maximal de r√©sultats, le seuil de confiance, la liste d'autorisation et la liste de refus des cat√©gories. Si aucune option de configuration n'est sp√©cifi√©e, la valeur par d√©faut est utilis√©e. Pour en savoir plus sur les options de configuration, consultez la section Pr√©sentation de la configuration.

La t√¢che D√©tecteur d'objets est compatible avec trois types de donn√©es d'entr√©e: les images fixes, les fichiers vid√©o et les flux vid√©o en direct. Vous devez sp√©cifier le mode d'ex√©cution correspondant √† votre type de donn√©es d'entr√©e lorsque vous cr√©ez la t√¢che. S√©lectionnez l'onglet correspondant √† votre type de donn√©es d'entr√©e pour d√©couvrir comment cr√©er la t√¢che et ex√©cuter l'inf√©rence.

Image
Vid√©o
Diffusion en direct
ObjectDetectorOptions options =
  ObjectDetectorOptions.builder()
    .setBaseOptions(BaseOptions.builder().setModelAssetPath(‚Äòmodel.tflite‚Äô).build())
    .setRunningMode(RunningMode.IMAGE)
    .setMaxResults(5)
    .build();
objectDetector = ObjectDetector.createFromOptions(context, options);
    
Remarque :Si vous utilisez le mode de diffusion en direct, veillez √† enregistrer un √©couteur de r√©sultats lorsque vous cr√©ez la t√¢che. L'√©couteur est appel√© chaque fois que la t√¢che a termin√© de traiter un frame vid√©o avec le r√©sultat de la d√©tection et l'image d'entr√©e comme param√®tres.
L'impl√©mentation de l'exemple de code du d√©tecteur d'objets permet √† l'utilisateur de basculer entre les modes de traitement. Cette approche rend le code de cr√©ation de t√¢che plus complexe et n'est peut-√™tre pas adapt√©e √† votre cas d'utilisation. Vous pouvez voir ce code dans la fonction setupObjectDetector() de la classe ObjectDetectorHelper.

Options de configuration
Cette t√¢che propose les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
runningMode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
displayNamesLocales	D√©finit la langue des libell√©s √† utiliser pour les noms √† afficher fournis dans les m√©tadonn√©es du mod√®le de la t√¢che, le cas √©ch√©ant. La valeur par d√©faut est en pour l'anglais. Vous pouvez ajouter des libell√©s localis√©s aux m√©tadonn√©es d'un mod√®le personnalis√© √† l'aide de l'API TensorFlow Lite Metadata Writer.	Code de param√®tres r√©gionaux	en
maxResults	D√©finit le nombre maximal facultatif de r√©sultats de d√©tection les plus √©lev√©s √† renvoyer.	N'importe quel nombre positif	-1 (tous les r√©sultats sont renvoy√©s)
scoreThreshold	D√©finit le seuil de score de pr√©diction qui remplace celui fourni dans les m√©tadonn√©es du mod√®le (le cas √©ch√©ant). Les r√©sultats inf√©rieurs √† cette valeur sont rejet√©s.	N'importe quelle superposition	Non d√©fini
categoryAllowlist	D√©finit la liste facultative des noms de cat√©gories autoris√©s. Si cet ensemble n'est pas vide, les r√©sultats de d√©tection dont le nom de cat√©gorie ne figure pas dans cet ensemble seront filtr√©s. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option s'exclut mutuellement avec categoryDenylist. L'utilisation des deux entra√Æne une erreur.	N'importe quelle cha√Æne	Non d√©fini
categoryDenylist	D√©finit la liste facultative des noms de cat√©gories non autoris√©s. Si cet ensemble n'est pas vide, les r√©sultats de d√©tection dont le nom de cat√©gorie figure dans cet ensemble seront filtr√©s. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option s'exclut mutuellement avec categoryAllowlist. L'utilisation des deux entra√Æne une erreur.	N'importe quelle cha√Æne	Non d√©fini
resultListener	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats de d√©tection de mani√®re asynchrone lorsque le d√©tecteur d'objets est en mode diffusion en direct. Vous ne pouvez utiliser cette option que lorsque vous d√©finissez runningMode sur LIVE_STREAM.	Non applicable	Non d√©fini
Pr√©parer les donn√©es
Vous devez convertir l'image ou le frame d'entr√©e en objet com.google.mediapipe.framework.image.MPImage avant de le transmettre au d√©tecteur d'objets.

Les exemples suivants expliquent et montrent comment pr√©parer les donn√©es pour le traitement pour chacun des types de donn√©es disponibles:

Image
Vid√©o
Diffusion en direct
import com.google.mediapipe.framework.image.BitmapImageBuilder;
import com.google.mediapipe.framework.image.MPImage;

// Load an image on the user‚Äôs device as a Bitmap object using BitmapFactory.

// Convert an Android‚Äôs Bitmap object to a MediaPipe‚Äôs Image object.
Image mpImage = new BitmapImageBuilder(bitmap).build();
    
Dans l'exemple de code du d√©tecteur d'objets, la pr√©paration des donn√©es est g√©r√©e dans la classe ObjectDetectorHelper dans les fonctions detectImage(), detectVideoFile() et detectLivestreamFrame().

Remarque :Pour les flux vid√©o en direct, veillez √† suivre la rotation vue de la cam√©ra qui fournit le flux, et tenez compte de cette rotation lors de la g√©n√©ration de la visualisation de sortie afin qu'elle corresponde au flux vu.
Ex√©cuter la t√¢che
En fonction du type de donn√©es avec lequel vous travaillez, utilisez la m√©thode ObjectDetector.detect...() sp√©cifique √† ce type de donn√©es. Utilisez detect() pour les images individuelles, detectForVideo() pour les images dans les fichiers vid√©o et detectAsync() pour les flux vid√©o. Lorsque vous effectuez des d√©tections sur un flux vid√©o, assurez-vous de les ex√©cuter sur un thread distinct pour √©viter de bloquer le thread de l'interface utilisateur.

Les exemples de code suivants montrent comment ex√©cuter le d√©tecteur d'objets dans ces diff√©rents modes de donn√©es:

Image
Vid√©o
Diffusion en direct
ObjectDetectorResult detectionResult = objectDetector.detect(image);
    
L'exemple de code du d√©tecteur d'objets montre plus en d√©tail les impl√©mentations de chacun de ces modes : detect(), detectVideoFile() et detectAsync(). L'exemple de code permet √† l'utilisateur de basculer entre les modes de traitement, ce qui n'est peut-√™tre pas n√©cessaire pour votre cas d'utilisation.

Veuillez noter les points suivants :

Lorsque vous ex√©cutez le mode vid√©o ou le mode de diffusion en direct, vous devez √©galement fournir le code temporel du frame d'entr√©e √† la t√¢che de d√©tection d'objets.
Lorsqu'elle s'ex√©cute en mode image ou vid√©o, la t√¢che du d√©tecteur d'objets bloque le thread actuel jusqu'√† ce qu'elle ait termin√© de traiter l'image ou le frame d'entr√©e. Pour √©viter de bloquer le thread actuel, ex√©cutez le traitement dans un thread en arri√®re-plan.
Lorsqu'elle s'ex√©cute en mode diffusion en direct, la t√¢che du d√©tecteur d'objets ne bloque pas le thread actuel, mais renvoie imm√©diatement. Il appelle son √©couteur de r√©sultats avec le r√©sultat de la d√©tection chaque fois qu'il a termin√© le traitement d'un frame d'entr√©e. Si la fonction de d√©tection est appel√©e lorsque la t√¢che du d√©tecteur d'objets est occup√©e √† traiter un autre frame, le nouveau frame d'entr√©e sera ignor√©.
G√©rer et afficher les r√©sultats
Lors de l'ex√©cution de l'inf√©rence, la t√¢che Object Detector renvoie un objet ObjectDetectorResult qui d√©crit les objets qu'elle a trouv√©s dans l'image d'entr√©e.

Voici un exemple des donn√©es de sortie de cette t√¢che:

ObjectDetectorResult:
 Detection #0:
  Box: (x: 355, y: 133, w: 190, h: 206)
  Categories:
   index       : 17
   score       : 0.73828
   class name  : dog
 Detection #1:
  Box: (x: 103, y: 15, w: 138, h: 369)
  Categories:
   index       : 17
   score       : 0.73047
   class name  : dog

L'image suivante pr√©sente une visualisation du r√©sultat de la t√¢che:

Deux chiens mis en surbrillance avec des cadres de d√©limitation

L'exemple de code du d√©tecteur d'objets montre comment afficher les r√©sultats de d√©tection renvoy√©s par la t√¢che. Pour en savoir plus, consultez la classe OverlayView.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide des t√¢ches de classification d'images


Un animal correctement identifi√© comme un flamant rose avec un niveau de confiance de 95 %

La t√¢che "Classificateur d'images MediaPipe" vous permet de classer des images. Vous pouvez utiliser cette t√¢che pour identifier ce qu'une image repr√©sente parmi un ensemble de cat√©gories d√©finies au moment de l'entra√Ænement. Cette t√¢che fonctionne sur des donn√©es d'image avec un mod√®le de machine learning (ML) en tant que donn√©es statiques ou flux continu, et g√©n√®re une liste de cat√©gories potentielles class√©es par score de probabilit√© d√©croissant.

Essayez !arrow_forward

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez l'un de ces guides d'impl√©mentation pour votre plate-forme cible. Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter cette t√¢che de mani√®re basique √† l'aide d'un mod√®le recommand√© et fournissent des exemples de code avec les options de configuration recommand√©es:

Android ‚Äì Exemple de code ‚Äì Guide
Python ‚Äì Exemple de code ‚Äì Guide
Web ‚Äì Exemple de code ‚Äì Guide
iOS ‚Äì Exemple de code ‚Äì Guide
Remarque :Si vous souhaitez cr√©er un classifieur d'images personnalis√© √† l'aide de votre propre ensemble de donn√©es, consultez le tutoriel sur la personnalisation du classifieur d'images.
D√©tails de la t√¢che
Cette section d√©crit les fonctionnalit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Fonctionnalit√©s
Traitement des images d'entr√©e : le traitement comprend la rotation, le redimensionnement, la normalisation et la conversion d'espaces colorim√©triques.
Zone d'int√©r√™t : effectuez la classification sur une zone de l'image plut√¥t que sur l'image enti√®re.
Localisation de la carte des libell√©s : d√©finissez la langue utilis√©e pour les noms √† afficher.
Seuil de score : filtrez les r√©sultats en fonction des scores de pr√©diction.
Classification des k premiers : limite le nombre de r√©sultats de classification.
Liste d'autorisation et de refus des libell√©s : sp√©cifiez les cat√©gories class√©es.
Entr√©es de t√¢che	Sorties de t√¢che
L'entr√©e peut √™tre l'un des types de donn√©es suivants:
Images fixes
Images vid√©o d√©cod√©es
Flux vid√©o en direct
Le classificateur d'images g√©n√®re une liste de cat√©gories contenant:
Indice de cat√©gorie: indice de la cat√©gorie dans les sorties du mod√®le
Score: score de confiance de cette cat√©gorie, g√©n√©ralement une probabilit√© comprise entre 0 et 1
Nom de la cat√©gorie (facultatif): nom de la cat√©gorie tel qu'il est sp√©cifi√© dans les m√©tadonn√©es du mod√®le TFLite, le cas √©ch√©ant
Nom √† afficher de la cat√©gorie (facultatif): nom √† afficher de la cat√©gorie, tel que sp√©cifi√© dans les m√©tadonn√©es du mod√®le TFLite, dans la langue sp√©cifi√©e via les options de param√®tres r√©gionaux des noms √† afficher, le cas √©ch√©ant
Remarque :Cette t√¢che permet de modifier les mod√®les de ML et les mod√®les personnalis√©s fournis. Pour en savoir plus sur l'utilisation de mod√®les modifi√©s ou personnalis√©s pour cette t√¢che, consultez la section Mod√®les personnalis√©s.
Options de configuration
Cette t√¢che propose les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
running_mode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
display_names_locale	D√©finit la langue des libell√©s √† utiliser pour les noms √† afficher fournis dans les m√©tadonn√©es du mod√®le de la t√¢che, le cas √©ch√©ant. La valeur par d√©faut est en pour l'anglais. Vous pouvez ajouter des libell√©s localis√©s aux m√©tadonn√©es d'un mod√®le personnalis√© √† l'aide de l'API TensorFlow Lite Metadata Writer.	Code de param√®tres r√©gionaux	en
max_results	D√©finit le nombre maximal facultatif de r√©sultats de classification les plus √©lev√©s √† renvoyer. Si la valeur est inf√©rieure √† 0, tous les r√©sultats disponibles sont renvoy√©s.	N'importe quel nombre positif	-1
score_threshold	D√©finit le seuil de score de pr√©diction qui remplace celui fourni dans les m√©tadonn√©es du mod√®le (le cas √©ch√©ant). Les r√©sultats inf√©rieurs √† cette valeur sont rejet√©s.	N'importe quelle superposition	Non d√©fini
category_allowlist	D√©finit la liste facultative des noms de cat√©gories autoris√©s. Si cet ensemble n'est pas vide, les r√©sultats de classification dont le nom de cat√©gorie ne figure pas dans cet ensemble seront filtr√©s. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option s'exclut mutuellement avec category_denylist. L'utilisation des deux entra√Æne une erreur.	Toutes les cha√Ænes	Non d√©fini
category_denylist	D√©finit la liste facultative des noms de cat√©gories non autoris√©s. Si cet ensemble n'est pas vide, les r√©sultats de classification dont le nom de cat√©gorie figure dans cet ensemble seront filtr√©s. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option s'exclut mutuellement avec category_allowlist. L'utilisation des deux entra√Æne une erreur.	N'importe quelle cha√Æne	Non d√©fini
result_callback	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats de classification de mani√®re asynchrone lorsque le classificateur d'images est en mode streaming en direct. Ne peut √™tre utilis√© que lorsque le mode d'ex√©cution est d√©fini sur LIVE_STREAM.	N/A	Non d√©fini
Remarque :resultListener d√©pend de runningMode. D√©finissez uniquement resultListener lorsque runningMode est d√©fini sur LIVE_STREAM.
Remarque :La liste d'autorisation et la liste de refus de la cat√©gorie s'excluent mutuellement.
Mod√®les
Le classificateur d'images n√©cessite que vous t√©l√©chargiez et stockiez un mod√®le de classification d'images dans le r√©pertoire de votre projet. Lorsque vous commencez √† d√©velopper avec cette t√¢che, commencez par le mod√®le par d√©faut recommand√© pour votre plate-forme cible. Les autres mod√®les disponibles font g√©n√©ralement des compromis entre les performances, la pr√©cision, la r√©solution et les exigences en termes de ressources, et incluent parfois des fonctionnalit√©s suppl√©mentaires.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Mod√®le EfficientNet-Lite0 (recommand√©)
Le mod√®le EfficientNet-Lite0 utilise une architecture EfficientNet et a √©t√© entra√Æn√© √† l'aide d'ImageNet pour reconna√Ætre 1 000 classes,telles que les arbres, les animaux, les aliments, les v√©hicules, les personnes, etc. Consultez la liste compl√®te des libell√©s compatibles. EfficientNet-Lite0 est disponible en tant que mod√®le int8 et float 32. Ce mod√®le est recommand√©, car il √©tablit un √©quilibre entre la latence et la pr√©cision. Il est √† la fois pr√©cis et suffisamment l√©ger pour de nombreux cas d'utilisation.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Versions
EfficientNet-Lite0 (int8)	224 x 224	int8	Nouveaut√©s
EfficientNet-Lite0 (float 32)	224 x 224	Aucun (float32)	Nouveaut√©s
Mod√®le EfficientNet-Lite2
Le mod√®le EfficientNet-Lite2 utilise une architecture EfficientNet et a √©t√© entra√Æn√© √† l'aide d'ImageNet pour reconna√Ætre 1 000 classes,telles que les arbres, les animaux, les aliments, les v√©hicules, les personnes, etc. Consultez la liste compl√®te des libell√©s compatibles. EfficientNet-Lite2 est disponible en tant que mod√®le int8 et float 32. Ce mod√®le est g√©n√©ralement plus pr√©cis qu'EfficientNet-Lite0, mais il est √©galement plus lent et plus gourmand en m√©moire. Ce mod√®le est adapt√© aux cas d'utilisation o√π la pr√©cision est prioritaire par rapport √† la vitesse ou √† la taille.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Versions
EfficientNet-Lite2 (int8)	224 x 224	int8	Nouveaut√©s
EfficientNet-Lite2 (float 32)	224 x 224	Aucun (float32)	Nouveaut√©s
Benchmarks des t√¢ches
Voici les benchmarks de t√¢che pour l'ensemble du pipeline bas√©s sur les mod√®les pr√©-entra√Æn√©s ci-dessus. Le r√©sultat de la latence correspond √† la latence moyenne sur le Pixel 6 √† l'aide du processeur / GPU.

Nom du mod√®le	Latence du processeur	Latence du GPU
EfficientNet-Lite0 (float 32)	23,52 ms	18,90 ms
EfficientNet-Lite0 (int8)	10,08 ms	-
EfficientNet-Lite2 (float 32)	44,17 ms	22,20 ms
EfficientNet-Lite2 (int8)	19,43 ms	-
Mod√®les personnalis√©s
Vous pouvez utiliser un mod√®le de ML personnalis√© avec cette t√¢che si vous souhaitez am√©liorer ou modifier les fonctionnalit√©s des mod√®les fournis. Vous pouvez utiliser Model Maker pour modifier les mod√®les existants ou en cr√©er un √† l'aide d'outils tels que TensorFlow. Les mod√®les personnalis√©s utilis√©s avec MediaPipe doivent √™tre au format TensorFlow Lite et doivent inclure des metadata sp√©cifiques d√©crivant les param√®tres de fonctionnement du mod√®le. Nous vous conseillons d'utiliser Model Maker pour modifier les mod√®les fournis pour cette t√¢che avant de cr√©er les v√¥tres.

Si vous souhaitez cr√©er un classifieur d'images personnalis√© √† l'aide de votre propre ensemble de donn√©es, commencez par le tutoriel sur la personnalisation du classifieur d'images.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/01/13 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de classification d'images pour Android


La t√¢che "Classificateur d'images MediaPipe" vous permet de classer des images. Vous pouvez utiliser cette t√¢che pour identifier ce qu'une image repr√©sente parmi un ensemble de cat√©gories d√©finies au moment de l'entra√Ænement. Ces instructions vous expliquent comment utiliser le classificateur d'images avec des applications Android. L'exemple de code d√©crit dans ces instructions est disponible sur GitHub.

Pour voir cette t√¢che en action, regardez la d√©monstration Web. Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Exemple de code
L'exemple de code MediaPipe Tasks est une impl√©mentation simple d'une application de classification d'images pour Android. L'exemple utilise l'appareil photo d'un appareil Android physique pour classer en continu les objets. Il peut √©galement utiliser des images et des vid√©os de la galerie de l'appareil pour classer de mani√®re statique les objets.

Vous pouvez utiliser l'application comme point de d√©part pour votre propre application Android ou vous y r√©f√©rer lorsque vous modifiez une application existante. L'exemple de code du classificateur d'images est h√©berg√© sur GitHub.

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple de code √† l'aide de l'outil de ligne de commande git.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Vous pouvez √©ventuellement configurer votre instance Git pour qu'elle utilise un "checkout sparse" afin de n'avoir que les fichiers de l'application exemple de classification des images:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/image_classification/android
Apr√®s avoir cr√©√© une version locale de l'exemple de code, vous pouvez importer le projet dans Android Studio et ex√©cuter l'application. Pour obtenir des instructions, consultez le guide de configuration pour Android.

Composants cl√©s
Les fichiers suivants contiennent le code essentiel de cet exemple d'application de classification d'images:

ImageClassifierHelper.kt : initialise le classificateur d'images et g√®re le mod√®le et la s√©lection du d√©l√©gu√©.
MainActivity.kt : impl√©mente l'application, y compris l'appel de ImageClassificationHelper et de ClassificationResultsAdapter.
ClassificationResultsAdapter.kt : g√®re et met en forme les r√©sultats.
Configuration
Cette section d√©crit les √©tapes cl√©s √† suivre pour configurer votre environnement de d√©veloppement et vos projets de code afin d'utiliser le classificateur d'images. Pour obtenir des informations g√©n√©rales sur la configuration de votre environnement de d√©veloppement pour utiliser les t√¢ches MediaPipe, y compris les exigences concernant les versions de la plate-forme, consultez le guide de configuration pour Android.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
D√©pendances
Le classificateur d'images utilise la biblioth√®que com.google.mediapipe:tasks-vision. Ajoutez cette d√©pendance au fichier build.gradle de votre projet de d√©veloppement d'application Android. Importez les d√©pendances requises avec le code suivant:

dependencies {
    ...
    implementation 'com.google.mediapipe:tasks-vision:latest.release'
}
Mod√®le
La t√¢che de classification d'images MediaPipe n√©cessite un mod√®le entra√Æn√© compatible avec cette t√¢che. Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour le classificateur d'images, consultez la section Mod√®les de la pr√©sentation de la t√¢che.

S√©lectionnez et t√©l√©chargez le mod√®le, puis stockez-le dans le r√©pertoire de votre projet:

<dev-project-root>/src/main/assets
Remarque :Cet emplacement est recommand√©, car le syst√®me de compilation Android v√©rifie automatiquement les ressources de fichier dans ce r√©pertoire.
Utilisez la m√©thode BaseOptions.Builder.setModelAssetPath() pour sp√©cifier le chemin d'acc√®s utilis√© par le mod√®le. Cette m√©thode est r√©f√©renc√©e dans l'exemple de code de la section suivante.

Dans l'exemple de code du classificateur d'images, le mod√®le est d√©fini dans le fichier ImageClassifierHelper.kt.

Cr√©er la t√¢che
Vous pouvez utiliser la fonction createFromOptions pour cr√©er la t√¢che. La fonction createFromOptions accepte des options de configuration, y compris le mode d'ex√©cution, les param√®tres r√©gionaux des noms √† afficher, le nombre maximal de r√©sultats, le seuil de confiance et une liste d'autorisation ou de refus de cat√©gorie. Pour en savoir plus sur les options de configuration, consultez la section Pr√©sentation de la configuration.

La t√¢che de classification d'images accepte trois types de donn√©es d'entr√©e: les images fixes, les fichiers vid√©o et les flux vid√©o en direct. Vous devez sp√©cifier le mode d'ex√©cution correspondant √† votre type de donn√©es d'entr√©e lorsque vous cr√©ez la t√¢che. S√©lectionnez l'onglet correspondant √† votre type de donn√©es d'entr√©e pour d√©couvrir comment cr√©er la t√¢che et ex√©cuter l'inf√©rence.

Image
Vid√©o
Diffusion en direct
ImageClassifierOptions options =
  ImageClassifierOptions.builder()
    .setBaseOptions(
      BaseOptions.builder().setModelAssetPath("model.tflite").build())
    .setRunningMode(RunningMode.IMAGE)
    .setMaxResults(5)
    .build();
imageClassifier = ImageClassifier.createFromOptions(context, options);
    
Remarque :Si vous utilisez le mode de diffusion en direct, veillez √† enregistrer un √©couteur de r√©sultats lorsque vous cr√©ez la t√¢che. L'√©couteur est appel√© chaque fois que la t√¢che a termin√© de traiter un frame vid√©o avec le r√©sultat de classification et l'image d'entr√©e comme param√®tres.
L'impl√©mentation de l'exemple de code du classificateur d'images permet √† l'utilisateur de basculer entre les modes de traitement. Cette approche rend le code de cr√©ation de t√¢che plus complexe et n'est peut-√™tre pas adapt√©e √† votre cas d'utilisation. Vous pouvez voir ce code dans la fonction setupImageClassifier() du fichier ImageClassifierHelper.kt.

Options de configuration
Cette t√¢che propose les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
runningMode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
displayNamesLocale	D√©finit la langue des libell√©s √† utiliser pour les noms √† afficher fournis dans les m√©tadonn√©es du mod√®le de la t√¢che, le cas √©ch√©ant. La valeur par d√©faut est en pour l'anglais. Vous pouvez ajouter des libell√©s localis√©s aux m√©tadonn√©es d'un mod√®le personnalis√© √† l'aide de l'API TensorFlow Lite Metadata Writer.	Code de param√®tres r√©gionaux	en
maxResults	D√©finit le nombre maximal facultatif de r√©sultats de classification les plus √©lev√©s √† renvoyer. Si la valeur est inf√©rieure √† 0, tous les r√©sultats disponibles sont renvoy√©s.	N'importe quel nombre positif	-1
scoreThreshold	D√©finit le seuil de score de pr√©diction qui remplace celui fourni dans les m√©tadonn√©es du mod√®le (le cas √©ch√©ant). Les r√©sultats inf√©rieurs √† cette valeur sont rejet√©s.	N'importe quelle superposition	Non d√©fini
categoryAllowlist	D√©finit la liste facultative des noms de cat√©gories autoris√©s. Si cet ensemble n'est pas vide, les r√©sultats de classification dont le nom de cat√©gorie ne figure pas dans cet ensemble seront filtr√©s. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option s'exclut mutuellement avec categoryDenylist. L'utilisation des deux entra√Æne une erreur.	Toutes les cha√Ænes	Non d√©fini
categoryDenylist	D√©finit la liste facultative des noms de cat√©gories non autoris√©s. Si cet ensemble n'est pas vide, les r√©sultats de classification dont le nom de cat√©gorie figure dans cet ensemble seront filtr√©s. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option s'exclut mutuellement avec categoryAllowlist. L'utilisation des deux entra√Æne une erreur.	Toutes les cha√Ænes	Non d√©fini
resultListener	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats de classification de mani√®re asynchrone lorsque le classificateur d'images est en mode streaming en direct. Ne peut √™tre utilis√© que lorsque le mode d'ex√©cution est d√©fini sur LIVE_STREAM.	N/A	Non d√©fini
errorListener	D√©finit un √©couteur d'erreur facultatif.	N/A	Non d√©fini
Pr√©parer les donn√©es
Le classificateur d'images fonctionne avec les images, les fichiers vid√©o et les vid√©os en direct. La t√¢che g√®re le pr√©traitement de l'entr√©e des donn√©es, y compris le redimensionnement, la rotation et la normalisation des valeurs.

Vous devez convertir l'image ou le frame d'entr√©e en objet com.google.mediapipe.framework.image.MPImage avant de le transmettre au classificateur d'images.

Image
Vid√©o
Diffusion en direct
import com.google.mediapipe.framework.image.BitmapImageBuilder;
import com.google.mediapipe.framework.image.MPImage;

// Load an image on the user‚Äôs device as a Bitmap object using BitmapFactory.

// Convert an Android‚Äôs Bitmap object to a MediaPipe‚Äôs Image object.
Image mpImage = new BitmapImageBuilder(bitmap).build();
    
Dans l'exemple de code du classificateur d'images, la pr√©paration des donn√©es est g√©r√©e dans le fichier ImageClassifierHelper.kt.

Ex√©cuter la t√¢che
Vous pouvez appeler la fonction classify correspondant √† votre mode d'ex√©cution pour d√©clencher des inf√©rences. L'API Image Classifier renvoie les cat√©gories possibles pour l'objet dans l'image ou le frame d'entr√©e.

Image
Vid√©o
Diffusion en direct
ImageClassifierResult classifierResult = imageClassifier.classify(image);
    
Veuillez noter les points suivants :

Lorsque vous ex√©cutez le mode vid√©o ou le mode de diffusion en direct, vous devez √©galement fournir le code temporel du frame d'entr√©e √† la t√¢che de classification des images.
Lorsqu'elle s'ex√©cute en mode image ou vid√©o, la t√¢che de classification d'images bloque le thread actuel jusqu'√† ce qu'elle ait termin√© de traiter l'image ou le frame d'entr√©e. Pour √©viter de bloquer l'interface utilisateur, ex√©cutez le traitement dans un thread en arri√®re-plan.
Lorsqu'elle s'ex√©cute en mode diffusion en direct, la t√¢che de classification des images ne bloque pas le thread actuel, mais renvoie imm√©diatement. Il appelle son √©couteur de r√©sultats avec le r√©sultat de la d√©tection chaque fois qu'il a termin√© le traitement d'un frame d'entr√©e. Si la fonction classifyAsync est appel√©e lorsque la t√¢che de classification des images est occup√©e √† traiter un autre frame, la t√¢che ignore le nouveau frame d'entr√©e.
Dans l'exemple de code du classificateur d'images, les fonctions classify sont d√©finies dans le fichier ImageClassifierHelper.kt.

G√©rer et afficher les r√©sultats
Lors de l'ex√©cution de l'inf√©rence, la t√¢che de classification des images renvoie un objet ImageClassifierResult contenant la liste des cat√©gories possibles pour les objets de l'image ou du frame d'entr√©e.

Voici un exemple des donn√©es de sortie de cette t√¢che:

ImageClassifierResult:
 Classifications #0 (single classification head):
  head index: 0
  category #0:
   category name: "/m/01bwb9"
   display name: "Passer domesticus"
   score: 0.91406
   index: 671
  category #1:
   category name: "/m/01bwbt"
   display name: "Passer montanus"
   score: 0.00391
   index: 670
Ce r√©sultat a √©t√© obtenu en ex√©cutant le classificateur d'oiseaux sur:

Photographie en gros plan d&#39;un moineau domestique

Dans l'exemple de code du classificateur d'images, la classe ClassificationResultsAdapter du fichier ClassificationResultsAdapter.kt g√®re les r√©sultats:

fun updateResults(imageClassifierResult: ImageClassifierResult? = null) {
    categories = MutableList(adapterSize) { null }
    if (imageClassifierResult != null) {
        val sortedCategories = imageClassifierResult.classificationResult()
            .classifications()[0].categories().sortedBy { it.index() }
        val min = kotlin.math.min(sortedCategories.size, categories.size)
        for (i in 0 until min) {
            categories[i] = sortedCategories[i]
        }
    }
}
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de segmentation d'image


Exemples c√¥te √† c√¥te d&#39;une photo en gros plan d&#39;un homme √† c√¥t√© du masque d&#39;image qui dessine sa silhouette

La t√¢che de segmentation d'image MediaPipe vous permet de diviser les images en r√©gions en fonction de cat√©gories pr√©d√©finies. Vous pouvez utiliser cette fonctionnalit√© pour identifier des objets ou des textures sp√©cifiques, puis appliquer des effets visuels tels que le floutage de l'arri√®re-plan. Cette t√¢che inclut plusieurs mod√®les sp√©cialement entra√Æn√©s pour segmenter les personnes et leurs caract√©ristiques dans les donn√©es d'image, y compris:

Personne et arri√®re-plan
Cheveux de la personne uniquement
Cheveux, visage, peau, v√™tements et accessoires d'une personne
Cette t√¢che fonctionne sur des donn√©es d'image avec un mod√®le de machine learning (ML) avec des images uniques ou un flux vid√©o continu. Il g√©n√®re une liste de r√©gions segment√©es, repr√©sentant des objets ou des zones d'une image, en fonction du mod√®le que vous choisissez.

Essayez !arrow_forward

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez l'un de ces guides d'impl√©mentation pour votre plate-forme cible. Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter de mani√®re basique cette t√¢che, y compris un mod√®le recommand√© et un exemple de code avec les options de configuration recommand√©es:

Android ‚Äì Exemple de code ‚Äì Guide
Python ‚Äì Guide d'exemples de code
Web ‚Äì Exemple de code ‚Äì Guide
D√©tails de la t√¢che
Cette section d√©crit les fonctionnalit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Fonctionnalit√©s
Traitement des images d'entr√©e : le traitement comprend la rotation, le redimensionnement, la normalisation et la conversion d'espaces colorim√©triques des images.
Entr√©es de t√¢che	Sorties de t√¢che
L'entr√©e peut √™tre l'un des types de donn√©es suivants:
Images fixes
Images vid√©o d√©cod√©es
Flux vid√©o en direct
Le segmenteur d'images g√©n√®re des donn√©es d'image segment√©es, qui peuvent inclure l'un ou les deux des √©l√©ments suivants, en fonction des options de configuration que vous d√©finissez :
CATEGORY_MASK: liste contenant un masque segment√© au format uint8. Chaque valeur de pixel indique s'il fait partie d'une cat√©gorie de segment sp√©cifique compatible avec le mod√®le.
CONFIDENCE_MASK: liste de canaux contenant un masque segment√© avec des valeurs de pixel au format float32. Chaque valeur de pixel indique le niveau de confiance qu'il fait partie d'une cat√©gorie sp√©cifique prise en charge par le mod√®le.
Options de configuration
Cette t√¢che propose les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
running_mode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
output_category_mask	Si la valeur est d√©finie sur True, la sortie inclut un masque de segmentation sous la forme d'une image uint8, o√π chaque valeur de pixel indique la valeur de la cat√©gorie gagnante.	{True, False}	False
output_confidence_masks	Si la valeur est d√©finie sur True, la sortie inclut un masque de segmentation sous la forme d'une image de valeur flottante, o√π chaque valeur flottante repr√©sente la carte de score de confiance de la cat√©gorie.	{True, False}	True
display_names_locale	D√©finit la langue des libell√©s √† utiliser pour les noms √† afficher fournis dans les m√©tadonn√©es du mod√®le de la t√¢che, le cas √©ch√©ant. La valeur par d√©faut est en pour l'anglais. Vous pouvez ajouter des libell√©s localis√©s aux m√©tadonn√©es d'un mod√®le personnalis√© √† l'aide de l'API TensorFlow Lite Metadata Writer.	Code de param√®tres r√©gionaux	en
result_callback	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats de segmentation de mani√®re asynchrone lorsque le segmenteur d'images est en mode LIVE_STREAM. Ne peut √™tre utilis√© que lorsque le mode d'ex√©cution est d√©fini sur LIVE_STREAM.	N/A	N/A
Mod√®les
Le segmenteur d'images peut √™tre utilis√© avec plusieurs mod√®les de ML. La plupart des mod√®les de segmentation suivants sont cr√©√©s et entra√Æn√©s pour effectuer une segmentation avec des images de personnes. Toutefois, le mod√®le DeepLab-v3 est con√ßu comme un segmenteur d'images √† usage g√©n√©ral. S√©lectionnez le mod√®le qui convient le mieux √† votre application.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Mod√®le de segmentation des selfies
Ce mod√®le peut segmenter le portrait d'une personne et peut √™tre utilis√© pour remplacer ou modifier l'arri√®re-plan d'une image. Le mod√®le produit deux cat√©gories : l'arri√®re-plan √† l'indice 0 et la personne √† l'indice 1. Ce mod√®le comporte des versions avec diff√©rentes formes d'entr√©e, y compris une version carr√©e et une version paysage, qui peuvent √™tre plus efficaces pour les applications o√π l'entr√©e est toujours de cette forme, comme les appels vid√©o.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Fiche de mod√®le	Versions
SelfieSegmenter (carr√©)	256 x 256	float 16	info	Nouveaut√©s
SelfieSegmenter (paysage)	144 x 256	float 16	info	Nouveaut√©s
Mod√®le de segmentation des cheveux
Ce mod√®le prend une image d'une personne, localise les cheveux sur sa t√™te et g√©n√®re une carte de segmentation d'image pour ses cheveux. Vous pouvez utiliser ce mod√®le pour recolorer les cheveux ou appliquer d'autres effets capillaires. Le mod√®le g√©n√®re les cat√©gories de segmentation suivantes:


0 - background
1 - hair
Nom du mod√®le	Forme d'entr√©e	Type de quantification	Fiche de mod√®le	Versions
HairSegmenter	512 x 512	Aucun (float32)	info	Nouveaut√©s
Mod√®le de segmentation des selfies √† classes multiples
Ce mod√®le prend une image d'une personne, identifie diff√©rentes zones telles que les cheveux, la peau et les v√™tements, puis g√©n√®re une carte de segmentation d'image pour ces √©l√©ments. Vous pouvez utiliser ce mod√®le pour appliquer diff√©rents effets aux personnes sur des images ou des vid√©os. Le mod√®le g√©n√®re les cat√©gories de segmentation suivantes:


0 - background
1 - hair
2 - body-skin
3 - face-skin
4 - clothes
5 - others (accessories)
Nom du mod√®le	Forme d'entr√©e	Type de quantification	Fiche de mod√®le	Versions
SelfieMulticlass (256 x 256)	256 x 256	Aucun (float32)	info	Nouveaut√©s
Mod√®le DeepLab-v3
Ce mod√®le identifie des segments pour un certain nombre de cat√©gories, y compris l'arri√®re-plan, la personne, le chat, le chien et la plante en pot. Le mod√®le utilise la pooling atrouse des pyramides spatiales pour capturer des informations √† plus longue port√©e. Pour en savoir plus, consultez DeepLab-v3.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Versions
DeepLab-V3	257 x 257	Aucun (float32)	Nouveaut√©s
Benchmarks des t√¢ches
Voici les benchmarks de t√¢che pour l'ensemble du pipeline bas√©s sur les mod√®les pr√©-entra√Æn√©s ci-dessus. Le r√©sultat de la latence correspond √† la latence moyenne sur le Pixel 6 √† l'aide du processeur / GPU.

Nom du mod√®le	Latence du processeur	Latence du GPU
SelfieSegmenter (carr√©)	33,46 ms	35,15 ms
SelfieSegmenter (paysage)	34,19 ms	33,55 ms
HairSegmenter	57,90 ms	52,14 ms
SelfieMulticlass (256 x 256)	217,76 ms	71,24 ms
DeepLab-V3	123,93 ms	103,30 ms
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/01/13 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
Copi√© dans le presse-papiers
La nouvelle page a √©t√© charg√©e.


Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de segmentation d'image pour Android


La t√¢che de segmentation d'images MediaPipe vous permet de diviser les images en r√©gions en fonction de cat√©gories pr√©d√©finies pour appliquer des effets visuels tels que le floutage de l'arri√®re-plan. Ces instructions vous expliquent comment utiliser le segmenteur d'images avec les applications Android. L'exemple de code d√©crit dans ces instructions est disponible sur GitHub. Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Exemple de code
L'exemple de code MediaPipe Tasks contient deux impl√©mentations simples d'une application de segmentation d'images pour Android:

Segmenteur d'images avec un masque de cat√©gorie
Segmenteur d'images avec un masque de confiance
Les exemples utilisent l'appareil photo d'un appareil Android physique pour effectuer une segmentation d'image sur un flux d'appareil photo en direct. Vous pouvez √©galement choisir des images et des vid√©os dans la galerie de l'appareil. Vous pouvez utiliser ces applications comme point de d√©part pour votre propre application Android ou vous y r√©f√©rer lorsque vous modifiez une application existante. L'exemple de code du segmenteur d'images est h√©berg√© sur GitHub.

Les sections suivantes font r√©f√©rence √† l'application Segmenteur d'images avec un masque de cat√©gorie.

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple de code √† l'aide de l'outil de ligne de commande git.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Vous pouvez √©galement configurer votre instance Git pour utiliser un "checkout sparse" afin de n'avoir que les fichiers de l'application exemple Image Segmenter:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/image_segmentation/android
Apr√®s avoir cr√©√© une version locale de l'exemple de code, vous pouvez importer le projet dans Android Studio et ex√©cuter l'application. Pour obtenir des instructions, consultez le guide de configuration pour Android.

Composants cl√©s
Les fichiers suivants contiennent le code essentiel de cet exemple d'application de segmentation d'images:

ImageSegmenterHelper.kt : initialise la t√¢che Image Segmenter et g√®re le mod√®le et la s√©lection du d√©l√©gu√©.
CameraFragment.kt : fournit l'interface utilisateur et le code de contr√¥le d'une cam√©ra.
GalleryFragment.kt : fournit l'interface utilisateur et le code de contr√¥le pour s√©lectionner des fichiers image et vid√©o.
OverlayView.kt : g√®re et met en forme les r√©sultats de segmentation.
Configuration
Cette section d√©crit les √©tapes cl√©s √† suivre pour configurer votre environnement de d√©veloppement et vos projets de code afin d'utiliser Image Segmenter. Pour obtenir des informations g√©n√©rales sur la configuration de votre environnement de d√©veloppement pour utiliser les t√¢ches MediaPipe, y compris les exigences concernant les versions de la plate-forme, consultez le guide de configuration pour Android.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
D√©pendances
Image Segmenter utilise la biblioth√®que com.google.mediapipe:tasks-vision. Ajoutez cette d√©pendance au fichier build.gradle de votre projet de d√©veloppement d'application Android. Importez les d√©pendances requises avec le code suivant:

dependencies {
    ...
    implementation 'com.google.mediapipe:tasks-vision:latest.release'
}
Mod√®le
La t√¢che de segmentation d'images MediaPipe n√©cessite un mod√®le entra√Æn√© compatible avec cette t√¢che. Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour Image Segmenter, consultez la section Mod√®les de la pr√©sentation de la t√¢che.

S√©lectionnez et t√©l√©chargez le mod√®le, puis stockez-le dans le r√©pertoire de votre projet:

<dev-project-root>/src/main/assets
Remarque :Cet emplacement est recommand√©, car le syst√®me de compilation Android v√©rifie automatiquement les ressources de fichier dans ce r√©pertoire.
Utilisez la m√©thode BaseOptions.Builder.setModelAssetPath() pour sp√©cifier le chemin d'acc√®s utilis√© par le mod√®le. Cette m√©thode est r√©f√©renc√©e dans l'exemple de code de la section suivante.

Dans l'exemple de code du segmenteur d'images, le mod√®le est d√©fini dans la classe ImageSegmenterHelper.kt dans la fonction setupImageSegmenter().

Cr√©er la t√¢che
Vous pouvez utiliser la fonction createFromOptions pour cr√©er la t√¢che. La fonction createFromOptions accepte des options de configuration, y compris les types de sortie de masque. Pour en savoir plus sur la configuration des t√¢ches, consultez la section Options de configuration.

La t√¢che de segmentation d'images accepte les types de donn√©es d'entr√©e suivants: images fixes, fichiers vid√©o et flux vid√©o en direct. Vous devez sp√©cifier le mode d'ex√©cution correspondant √† votre type de donn√©es d'entr√©e lorsque vous cr√©ez la t√¢che. S√©lectionnez l'onglet correspondant √† votre type de donn√©es d'entr√©e pour d√©couvrir comment cr√©er cette t√¢che.

Image
Vid√©o
Diffusion en direct
ImageSegmenterOptions options =
  ImageSegmenterOptions.builder()
    .setBaseOptions(
      BaseOptions.builder().setModelAssetPath("model.tflite").build())
    .setRunningMode(RunningMode.IMAGE)
    .setOutputCategoryMask(true)
    .setOutputConfidenceMasks(false)
    .build();
imagesegmenter = ImageSegmenter.createFromOptions(context, options);
    
Remarque :Si vous utilisez le mode de diffusion en direct, veillez √† enregistrer un √©couteur de r√©sultats lorsque vous cr√©ez la t√¢che. L'√©couteur est appel√© chaque fois que la t√¢che a termin√© de traiter un frame vid√©o avec le r√©sultat de segmentation et l'image d'entr√©e comme param√®tres.
L'impl√©mentation de l'exemple de code du segmenteur d'images permet √† l'utilisateur de basculer entre les modes de traitement. Cette approche rend le code de cr√©ation de t√¢che plus complexe et n'est peut-√™tre pas adapt√©e √† votre cas d'utilisation. Vous pouvez voir ce code dans la classe ImageSegmenterHelper par la fonction setupImageSegmenter().

Options de configuration
Cette t√¢che propose les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
runningMode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
outputCategoryMask	Si la valeur est d√©finie sur True, la sortie inclut un masque de segmentation sous la forme d'une image uint8, o√π chaque valeur de pixel indique la valeur de la cat√©gorie gagnante.	{True, False}	False
outputConfidenceMasks	Si la valeur est d√©finie sur True, la sortie inclut un masque de segmentation sous la forme d'une image de valeur flottante, o√π chaque valeur flottante repr√©sente la carte de score de confiance de la cat√©gorie.	{True, False}	True
displayNamesLocale	D√©finit la langue des libell√©s √† utiliser pour les noms √† afficher fournis dans les m√©tadonn√©es du mod√®le de la t√¢che, le cas √©ch√©ant. La valeur par d√©faut est en pour l'anglais. Vous pouvez ajouter des libell√©s localis√©s aux m√©tadonn√©es d'un mod√®le personnalis√© √† l'aide de l'API TensorFlow Lite Metadata Writer.	Code de param√®tres r√©gionaux	en
resultListener	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats de segmentation de mani√®re asynchrone lorsque le segmenteur d'images est en mode LIVE_STREAM. Ne peut √™tre utilis√© que lorsque le mode d'ex√©cution est d√©fini sur LIVE_STREAM.	N/A	N/A
errorListener	D√©finit un √©couteur d'erreur facultatif.	N/A	Non d√©fini
Pr√©parer les donn√©es
Le segmenteur d'images fonctionne avec les images, les fichiers vid√©o et les vid√©os en direct. La t√¢che g√®re le pr√©traitement de l'entr√©e des donn√©es, y compris le redimensionnement, la rotation et la normalisation des valeurs.

Vous devez convertir l'image ou le frame d'entr√©e en objet com.google.mediapipe.framework.image.MPImage avant de le transmettre au segmenteur d'images.

Image
Vid√©o
Diffusion en direct
import com.google.mediapipe.framework.image.BitmapImageBuilder;
import com.google.mediapipe.framework.image.MPImage;

// Load an image on the user‚Äôs device as a Bitmap object using BitmapFactory.

// Convert an Android‚Äôs Bitmap object to a MediaPipe‚Äôs Image object.
Image mpImage = new BitmapImageBuilder(bitmap).build();
    
Dans l'exemple de code de l'outil de segmentation d'images, la pr√©paration des donn√©es est g√©r√©e dans la classe ImageSegmenterHelper par la fonction segmentLiveStreamFrame().

Ex√©cuter la t√¢che
Vous appelez une fonction segment diff√©rente en fonction du mode d'ex√©cution que vous utilisez. La fonction Image Segmenter renvoie les r√©gions de segment identifi√©es dans l'image ou le frame d'entr√©e.

Image
Vid√©o
Diffusion en direct
ImageSegmenterResult segmenterResult = imagesegmenter.segment(image);
    
Veuillez noter les points suivants :

Lorsque vous ex√©cutez le mode vid√©o ou le mode de diffusion en direct, vous devez √©galement fournir le code temporel du frame d'entr√©e √† la t√¢che de segmentation d'images.
Lorsqu'elle s'ex√©cute en mode image ou vid√©o, la t√¢che de segmentation d'image bloque le thread actuel jusqu'√† ce qu'elle ait termin√© de traiter l'image ou le frame d'entr√©e. Pour √©viter de bloquer l'interface utilisateur, ex√©cutez le traitement dans un thread en arri√®re-plan.
Lorsqu'elle s'ex√©cute en mode diffusion en direct, la t√¢che de segmentation d'image ne bloque pas le thread en cours, mais renvoie imm√©diatement. Il appelle son √©couteur de r√©sultats avec le r√©sultat de la d√©tection chaque fois qu'il a termin√© le traitement d'un frame d'entr√©e. Si la fonction segmentAsync est appel√©e lorsque la t√¢che de segmentation d'image est en train de traiter un autre frame, la t√¢che ignore le nouveau frame d'entr√©e.
Dans l'exemple de code de l'outil de segmentation d'images, les fonctions segment sont d√©finies dans le fichier ImageSegmenterHelper.kt.

G√©rer et afficher les r√©sultats
Lors de l'ex√©cution de l'inf√©rence, la t√¢che de segmentation d'image renvoie un objet ImageSegmenterResult contenant les r√©sultats de la t√¢che de segmentation. Le contenu de la sortie d√©pend de l'outputType que vous avez d√©fini lorsque vous avez configur√© la t√¢che.

Les sections suivantes pr√©sentent des exemples des donn√©es de sortie de cette t√¢che:

Confiance de la cat√©gorie
Les images suivantes montrent une visualisation de la sortie de la t√¢che pour un masque de confiance de cat√©gorie. La sortie du masque de confiance contient des valeurs flottantes comprises entre [0, 1].

Deux filles √† cheval et une fille √† c√¥t√© du cheval Masque d&#39;image qui d√©limite la forme des filles et du cheval de la photo pr√©c√©dente. La moiti√© gauche du contour de l&#39;image est captur√©e, mais pas la moiti√© droite

Image d'origine et sortie du masque de confiance de la cat√©gorie. Image source de l'ensemble de donn√©es Pascal VOC 2012.

Remarque :Dans la deuxi√®me image de sortie, les valeurs de sortie √† virgule flottante sont mises √† l'√©chelle en valeurs de la plage [0, 255] et converties en type de donn√©es uint8.
Valeur de la cat√©gorie
Les images suivantes montrent une visualisation de la sortie de la t√¢che pour un masque de valeur de cat√©gorie. La plage du masque de cat√©gorie est [0, 255], et chaque valeur de pixel repr√©sente l'indice de cat√©gorie gagnante de la sortie du mod√®le. L'indice de cat√©gorie gagnante affiche le score le plus √©lev√© parmi les cat√©gories que le mod√®le peut reconna√Ætre.

Deux filles √† cheval et une fille √† c√¥t√© du cheval Masque d&#39;image qui d√©limite la forme des filles et du cheval de l&#39;image pr√©c√©dente. La forme des trois filles et du cheval est masqu√©e avec pr√©cision

Image d'origine et sortie du masque de cat√©gorie. Image source de l'ensemble de donn√©es Pascal VOC 2012.

Remarque :Dans la deuxi√®me image de sortie, les valeurs de pixel sont multipli√©es par 10 pour augmenter le contraste et am√©liorer la visualisation.
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide des t√¢ches interactives de segmentation d'image
Images c√¥te √† c√¥te montrant une photo d&#39;une chaise sur une image, puis la m√™me image avec la chaise mise en surbrillance pour indiquer que le mod√®le l&#39;a d√©tect√©e

La t√¢che de segmentation d'image interactive MediaPipe vous permet de diviser une image en deux r√©gions: un objet s√©lectionn√© et tout le reste. La t√¢che prend un emplacement dans une image, estime les limites d'un objet √† cet emplacement et renvoie des donn√©es d'image d√©finissant la zone de l'objet. Vous pouvez utiliser cette t√¢che pour s√©lectionner interactivement un objet dans une image et utiliser la sortie pour appliquer des effets √† l'image, tels que des superpositions de couleurs mettant en √©vidence l'objet ou floutant l'arri√®re-plan autour de celui-ci. Cette t√¢che fonctionne sur des donn√©es d'image avec un mod√®le de machine learning (ML). Vous pouvez l'utiliser sur des images individuelles, des fichiers vid√©o ou un flux vid√©o continu.

Essayez !arrow_forward

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez l'un de ces guides d'impl√©mentation pour votre plate-forme cible. Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter de mani√®re basique cette t√¢che, y compris un mod√®le recommand√© et un exemple de code avec les options de configuration recommand√©es:

Android ‚Äì Exemple de code ‚Äì Guide
Python - Exemple de code - Guide
Web - Exemple de code - Guide
D√©tails de la t√¢che
Cette section d√©crit les fonctionnalit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Fonctionnalit√©s
Traitement des images d'entr√©e : le traitement comprend la rotation, le redimensionnement, la normalisation et la conversion d'espaces colorim√©triques.
Entr√©es de t√¢che	Sorties de t√¢che
Coordonn√©es du point d'int√©r√™t d'un objet dans une image
Fichier image √† traiter
Le segmenteur d'images interactif produit des donn√©es d'image segment√©es, qui peuvent inclure l'un ou les deux des √©l√©ments suivants, en fonction des options de configuration que vous d√©finissez :
CATEGORY_MASK: liste contenant un masque segment√© au format uint8. Chaque valeur de pixel indique s'il fait partie de l'objet situ√© dans la zone d'int√©r√™t.
CONFIDENCE_MASK: liste de canaux contenant un masque segment√© avec des valeurs de pixel au format float32. Chaque valeur de pixel indique le niveau de confiance qu'il fait partie de l'objet situ√© dans la zone d'int√©r√™t.
Options de configuration
Cette t√¢che propose les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
output_category_mask	Si la valeur est True, la sortie inclut un masque de segmentation sous la forme d'une image uint8, o√π chaque valeur de pixel indique si le pixel fait partie de l'objet situ√© dans la zone d'int√©r√™t.	{True, False}	False
output_confidence_masks	Si la valeur est True, la sortie inclut un masque de segmentation sous la forme d'une image de valeur flottante, o√π chaque valeur flottante repr√©sente la confiance que le pixel fait partie de l'objet situ√© dans la zone d'int√©r√™t.	{True, False}	True
display_names_locale	D√©finit la langue des libell√©s √† utiliser pour les noms √† afficher fournis dans les m√©tadonn√©es du mod√®le de la t√¢che, le cas √©ch√©ant. La valeur par d√©faut est en pour l'anglais. Vous pouvez ajouter des libell√©s localis√©s aux m√©tadonn√©es d'un mod√®le personnalis√© √† l'aide de l'API TensorFlow Lite Metadata Writer.	Code de param√®tres r√©gionaux	en
Mod√®les
Le segmenteur d'images interactif peut √™tre utilis√© avec plusieurs mod√®les de ML. Lorsque vous commencez √† d√©velopper avec cette t√¢che, commencez par le mod√®le par d√©faut recommand√© pour votre plate-forme cible. Les autres mod√®les disponibles font g√©n√©ralement des compromis entre les performances, la pr√©cision, la r√©solution et les exigences en termes de ressources, et incluent parfois des fonctionnalit√©s suppl√©mentaires.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Mod√®le MagicTouch (recommand√©)
Ce mod√®le identifie des segments en fonction des coordonn√©es d'une zone d'int√©r√™t dans une image. Le mod√®le utilise un r√©seau de neurones convolutifs, semblable √† une architecture MobileNetV3, avec un d√©codeur personnalis√©.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Fiche de mod√®le	Versions
MagicTouch	512 x 512 x 4	Aucun (float32)	info	Nouveaut√©s
Benchmarks des t√¢ches
Voici les benchmarks de t√¢che pour l'ensemble du pipeline bas√©s sur les mod√®les pr√©-entra√Æn√©s ci-dessus. Le r√©sultat de la latence correspond √† la latence moyenne sur le Pixel 6 √† l'aide du processeur / GPU.

Nom du mod√®le	Latence du processeur	Latence du GPU
MagicTouch	130,11 ms	67,25 ms
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/01/13 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide interactif de segmentation d'image pour Android


La t√¢che de segmentation d'image interactive MediaPipe prend un emplacement dans une image, estime les limites d'un objet √† cet emplacement et renvoie la segmentation de l'objet sous forme de donn√©es d'image. Ces instructions vous expliquent comment utiliser le segmenteur d'images interactif avec les applications Android. L'exemple de code d√©crit dans ces instructions est disponible sur GitHub. Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Exemple de code
L'exemple de code MediaPipe Tasks est une impl√©mentation simple d'une application de segmentation d'images interactive pour Android. L'exemple fonctionne avec les images s√©lectionn√©es dans la galerie de l'appareil.

Vous pouvez utiliser l'application comme point de d√©part pour votre propre application Android ou vous y r√©f√©rer lorsque vous modifiez une application existante. L'exemple de code du segmenteur d'images interactif est h√©berg√© sur GitHub.

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple de code √† l'aide de l'outil de ligne de commande git.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Vous pouvez √©galement configurer votre instance git pour utiliser un "checkout" clairsem√© afin de n'avoir que les fichiers de l'exemple d'application Interactive Image Segmenter:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/interactive_segmentation/android
Apr√®s avoir cr√©√© une version locale de l'exemple de code, vous pouvez importer le projet dans Android Studio et ex√©cuter l'application. Pour obtenir des instructions, consultez le guide de configuration pour Android.

Composants cl√©s
Les fichiers suivants contiennent le code essentiel de cet exemple d'application de segmentation d'images:

InteractiveSegmentationHelper.kt : initialise la t√¢che de segmentation d'image interactive et g√®re le mod√®le et la s√©lection du d√©l√©gu√©.
OverlayView.kt : g√®re et met en forme les r√©sultats de segmentation.
Configuration
Cette section d√©crit les √©tapes cl√©s √† suivre pour configurer votre environnement de d√©veloppement et vos projets de code afin d'utiliser le segmenteur d'images interactif. Pour obtenir des informations g√©n√©rales sur la configuration de votre environnement de d√©veloppement pour utiliser les t√¢ches MediaPipe, y compris les exigences concernant les versions de la plate-forme, consultez le guide de configuration pour Android.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
D√©pendances
Le segmenteur d'images interactif utilise la biblioth√®que com.google.mediapipe:tasks-vision. Ajoutez cette d√©pendance au fichier build.gradle de votre projet de d√©veloppement d'application Android. Importez les d√©pendances requises avec le code suivant:

dependencies {
    ...
    implementation 'com.google.mediapipe:tasks-vision:latest.release'
}
Mod√®le
La t√¢che de segmentation d'images interactive MediaPipe n√©cessite un mod√®le entra√Æn√© compatible avec cette t√¢che. Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour le segmenteur d'images interactif, consultez la section Mod√®les de la pr√©sentation de la t√¢che.

S√©lectionnez et t√©l√©chargez le mod√®le, puis stockez-le dans le r√©pertoire de votre projet:

<dev-project-root>/src/main/assets
Remarque :Cet emplacement est recommand√©, car le syst√®me de compilation Android v√©rifie automatiquement les ressources de fichier dans ce r√©pertoire.
Utilisez la m√©thode BaseOptions.Builder.setModelAssetPath() pour sp√©cifier le chemin d'acc√®s utilis√© par le mod√®le. Cette m√©thode est illustr√©e dans l'exemple de code de la section suivante.

Dans l'exemple de code de l'outil Interactive Image Segmenter, le mod√®le est d√©fini dans la classe InteractiveSegmenterHelper.kt dans la fonction setupInteractiveSegmenter().

Cr√©er la t√¢che
Vous pouvez utiliser la fonction createFromOptions pour cr√©er la t√¢che. La fonction createFromOptions accepte des options de configuration, y compris des types de sortie de masque. Pour en savoir plus sur les options de configuration, consultez la section Pr√©sentation de la configuration.

InteractiveSegmenterOptions options =
  InteractiveSegmenterOptions.builder()
    .setBaseOptions(
      BaseOptions.builder().setModelAssetPath("model.tflite").build())
    .setOutputCategoryMask(true)
    .setOutputConfidenceMasks(false)
    .setResultListener((result, inputImage) -> {
         // Process the segmentation result here.
    })
    .setErrorListener(exception -> {
         // Process the segmentation errors here.
    })    
    .build();
interactivesegmenter = InteractiveSegmenter.createFromOptions(context, options);
Pour obtenir un exemple plus d√©taill√© de configuration de cette t√¢che, consultez la fonction setupInteractiveSegmenter() de la classe InteractiveSegmenterHelper.

Options de configuration
Cette t√¢che propose les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
outputCategoryMask	Si la valeur est True, la sortie inclut un masque de segmentation sous la forme d'une image uint8, o√π chaque valeur de pixel indique si le pixel fait partie de l'objet situ√© dans la zone d'int√©r√™t.	{True, False}	False
outputConfidenceMasks	Si la valeur est True, la sortie inclut un masque de segmentation sous la forme d'une image de valeur flottante, o√π chaque valeur flottante repr√©sente la confiance que le pixel fait partie de l'objet situ√© dans la zone d'int√©r√™t.	{True, False}	True
displayNamesLocale	D√©finit la langue des libell√©s √† utiliser pour les noms √† afficher fournis dans les m√©tadonn√©es du mod√®le de la t√¢che, le cas √©ch√©ant. La valeur par d√©faut est en pour l'anglais. Vous pouvez ajouter des libell√©s localis√©s aux m√©tadonn√©es d'un mod√®le personnalis√© √† l'aide de l'API TensorFlow Lite Metadata Writer.	Code de param√®tres r√©gionaux	en
errorListener	D√©finit un √©couteur d'erreur facultatif.	N/A	Non d√©fini
Pr√©parer les donn√©es
Le segmenteur d'images interactif fonctionne avec des images. La t√¢che g√®re le pr√©traitement de l'entr√©e de donn√©es, y compris le redimensionnement, la rotation et la normalisation des valeurs. Vous devez convertir l'image d'entr√©e en objet com.google.mediapipe.framework.image.MPImage avant de la transmettre √† la t√¢che.

import com.google.mediapipe.framework.image.BitmapImageBuilder;
import com.google.mediapipe.framework.image.MPImage;

// Load an image on the user‚Äôs device as a Bitmap object using BitmapFactory.

// Convert an Android‚Äôs Bitmap object to a MediaPipe‚Äôs Image object.
MPImage mpImage = new BitmapImageBuilder(bitmap).build();
Dans l'exemple de code du segmenteur d'images interactif, la pr√©paration des donn√©es est g√©r√©e dans la classe InteractiveSegmenterHelper par la fonction segment().

Ex√©cuter la t√¢che
Appelez la fonction segment pour ex√©cuter la pr√©diction et g√©n√©rer des segments. La t√¢che "Segmenteur d'images interactif" renvoie les r√©gions de segment identifi√©es dans l'image d'entr√©e.

RegionOfInterest roi = RegionOfInterest.create(
    NormalizedKeypoint.create(
        normX * it.width,
        normY * it.height
    )
);

ImageSegmenterResult segmenterResult = interactivesegmenter.segment(image, roi);
Dans l'exemple de code du segmenteur d'images interactif, les fonctions segment sont d√©finies dans le fichier InteractiveSegmenterHelper.kt.

G√©rer et afficher les r√©sultats
Lors de l'ex√©cution de l'inf√©rence, la t√¢che Interactive Image Segmenter renvoie un objet ImageSegmenterResult contenant les r√©sultats de la t√¢che de segmentation. Le contenu de la sortie peut inclure un masque de cat√©gorie, un masque de confiance ou les deux, en fonction de ce que vous avez d√©fini lorsque vous avez configur√© la t√¢che.

Les sections suivantes expliquent plus en d√©tail les donn√©es de sortie de cette t√¢che:

Masque de cat√©gorie
Les images suivantes montrent une visualisation de la sortie de la t√¢che pour un masque de valeur de cat√©gorie avec une zone d'int√©r√™t indiqu√©e. Chaque pixel est une valeur uint8 indiquant si le pixel fait partie de l'objet situ√© dans la zone d'int√©r√™t. Le cercle noir et blanc de la deuxi√®me image indique la zone d'int√©r√™t s√©lectionn√©e.

Chien debout au milieu d&#39;une pile de feuilles Forme du chien de l&#39;image pr√©c√©dente

Image d'origine et sortie du masque de cat√©gorie. Image source de l'ensemble de donn√©es Pascal VOC 2012.

Masque de confiance
La sortie d'un masque de confiance contient des valeurs flottantes comprises entre [0, 1] pour chaque canal d'entr√©e d'image. Des valeurs plus √©lev√©es indiquent une plus grande confiance que le pixel de l'image fait partie de l'objet situ√© dans la zone d'int√©r√™t.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide des t√¢ches de reconnaissance des gestes


Main faisant un geste de pouce lev√©, identifi√© comme tel par le mod√®le avec un niveau de confiance de 63 %

La t√¢che de reconnaissance des gestes MediaPipe vous permet de reconna√Ætre les gestes de la main en temps r√©el et fournit les r√©sultats de la reconnaissance des gestes de la main, ainsi que les rep√®res des mains d√©tect√©es. Vous pouvez utiliser cette t√¢che pour reconna√Ætre des gestes de la main sp√©cifiques d'un utilisateur et appeler les fonctionnalit√©s de l'application qui correspondent √† ces gestes.

Cette t√¢che fonctionne sur des donn√©es d'image avec un mod√®le de machine learning (ML) et accepte des donn√©es statiques ou un flux continu. La t√¢che produit des rep√®res de main en coordonn√©es d'image, des rep√®res de main en coordonn√©es mondiales, une lat√©ralit√© (main gauche/droite) et les cat√©gories de gestes de plusieurs mains.

Essayez !arrow_forward

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez l'un de ces guides d'impl√©mentation pour votre plate-forme cible. Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter de mani√®re basique cette t√¢che √† l'aide d'un mod√®le recommand√© et fournissent des exemples de code avec les options de configuration recommand√©es:

Android ‚Äì Exemple de code
Guide
Python ‚Äì Exemple de code
Guide
Web ‚Äì Exemple de code ‚Äì Guide
Remarque :Si vous souhaitez cr√©er un d√©tecteur de gestes personnalis√© √† l'aide de votre propre ensemble de donn√©es, consultez le tutoriel Personnalisation du d√©tecteur de gestes.
D√©tails de la t√¢che
Cette section d√©crit les fonctionnalit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Fonctionnalit√©s
Traitement des images d'entr√©e : le traitement comprend la rotation, le redimensionnement, la normalisation et la conversion d'espaces colorim√©triques des images.
Seuil de score : filtrez les r√©sultats en fonction des scores de pr√©diction.
√âtiquetter la liste d'autorisation et la liste de blocage : sp√©cifiez les cat√©gories de gestes reconnues par le mod√®le.
Entr√©es de t√¢che	Sorties de t√¢che
Le d√©tecteur de gestes accepte l'un des types de donn√©es suivants:
Images fixes
Images vid√©o d√©cod√©es
Flux vid√©o en direct
Le d√©tecteur de gestes g√©n√®re les r√©sultats suivants:
Cat√©gories de gestes
Main dominante des mains d√©tect√©es
Points de rep√®re des mains d√©tect√©es en coordonn√©es d'image
Points de rep√®re des mains d√©tect√©es en coordonn√©es mondiales
Remarque :Cette t√¢che permet de modifier les mod√®les ML et les mod√®les personnalis√©s fournis. Pour en savoir plus sur l'utilisation de mod√®les modifi√©s ou personnalis√©s pour cette t√¢che, consultez la section Mod√®les personnalis√©s.
Options de configuration
Cette t√¢che propose les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
running_mode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
num_hands	Le nombre maximal de mains peut √™tre d√©tect√© par le GestureRecognizer.	Any integer > 0	1
min_hand_detection_confidence	Score de confiance minimal pour que la d√©tection de la main soit consid√©r√©e comme r√©ussie dans le mod√®le de d√©tection de la paume de la main.	0.0 - 1.0	0.5
min_hand_presence_confidence	Score de confiance minimal du score de pr√©sence de la main dans le mod√®le de d√©tection des rep√®res de la main. En mode Vid√©o et en mode Diffusion en direct du service de reconnaissance des gestes, si le score de confiance de la pr√©sence de la main du mod√®le de rep√®re de la main est inf√©rieur √† ce seuil, le mod√®le de d√©tection de la paume est d√©clench√©. Sinon, un algorithme de suivi des mains l√©ger est utilis√© pour d√©terminer l'emplacement de la ou des mains afin de d√©tecter les rep√®res par la suite.	0.0 - 1.0	0.5
min_tracking_confidence	Score de confiance minimal pour que le suivi des mains soit consid√©r√© comme r√©ussi. Il s'agit du seuil d'IoU du cadre de d√©limitation entre les mains dans le frame actuel et le dernier frame. En mode Vid√©o et en mode Flux du service de reconnaissance des gestes, si le suivi √©choue, le service de reconnaissance des gestes d√©clenche la d√©tection des mains. Sinon, la d√©tection de la main est ignor√©e.	0.0 - 1.0	0.5
canned_gestures_classifier_options	Options de configuration du comportement du classificateur de gestes pr√©d√©finis. Les gestes pr√©d√©finis sont ["None", "Closed_Fist", "Open_Palm", "Pointing_Up", "Thumb_Down", "Thumb_Up", "Victory", "ILoveYou"]
.
Locales des noms √† afficher: √©ventuelles langues √† utiliser pour les noms √† afficher sp√©cifi√©s dans les m√©tadonn√©es du mod√®le TFLite.
Nombre maximal de r√©sultats: nombre maximal de r√©sultats de classification les plus √©lev√©s √† renvoyer. Si la valeur est inf√©rieure √† 0, tous les r√©sultats disponibles sont renvoy√©s.
Seuil de score: score en dessous duquel les r√©sultats sont rejet√©s. Si cette valeur est d√©finie sur 0, tous les r√©sultats disponibles sont renvoy√©s.
Liste d'autorisation des cat√©gories: liste d'autorisation des noms de cat√©gories. Si cet ensemble n'est pas vide, les r√©sultats de classification dont la cat√©gorie ne figure pas dans cet ensemble seront filtr√©s. S'exclut mutuellement avec la liste de blocage.
Liste de blocage des cat√©gories: liste de blocage des noms de cat√©gories. Si cet ensemble n'est pas vide, les r√©sultats de classification dont la cat√©gorie figure dans cet ensemble seront filtr√©s. S'exclut mutuellement avec la liste d'autorisation.
Param√®tre r√©gional des noms √† afficher: any string
R√©sultats max. : any integer
Seuil de score: 0.0-1.0
Liste d'autorisation de la cat√©gorie: vector of strings
Liste de blocage des cat√©gories: vector of strings
Param√®tre r√©gional des noms √† afficher: "en"
R√©sultats max. : -1
Seuil de score: 0
Liste d'autorisation des cat√©gories: vide
Liste de blocage des cat√©gories: vide
custom_gestures_classifier_options	Options permettant de configurer le comportement du classificateur de gestes personnalis√©s.
Locales des noms √† afficher: √©ventuelles langues √† utiliser pour les noms √† afficher sp√©cifi√©s dans les m√©tadonn√©es du mod√®le TFLite.
Nombre maximal de r√©sultats: nombre maximal de r√©sultats de classification les plus √©lev√©s √† renvoyer. Si la valeur est inf√©rieure √† 0, tous les r√©sultats disponibles sont renvoy√©s.
Seuil de score: score en dessous duquel les r√©sultats sont rejet√©s. Si cette valeur est d√©finie sur 0, tous les r√©sultats disponibles sont renvoy√©s.
Liste d'autorisation des cat√©gories: liste d'autorisation des noms de cat√©gories. Si cet ensemble n'est pas vide, les r√©sultats de classification dont la cat√©gorie ne figure pas dans cet ensemble seront filtr√©s. S'exclut mutuellement avec la liste de blocage.
Liste de blocage des cat√©gories: liste de blocage des noms de cat√©gories. Si cet ensemble n'est pas vide, les r√©sultats de classification dont la cat√©gorie figure dans cet ensemble seront filtr√©s. S'exclut mutuellement avec la liste d'autorisation.
Param√®tre r√©gional des noms √† afficher: any string
R√©sultats max. : any integer
Seuil de score: 0.0-1.0
Liste d'autorisation de la cat√©gorie: vector of strings
Liste de blocage des cat√©gories: vector of strings
Param√®tre r√©gional des noms √† afficher: "en"
R√©sultats max. : -1
Seuil de score: 0
Liste d'autorisation des cat√©gories: vide
Liste de blocage des cat√©gories: vide
result_callback	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats de classification de mani√®re asynchrone lorsque le d√©tecteur de gestes est en mode flux en direct. Ne peut √™tre utilis√© que lorsque le mode d'ex√©cution est d√©fini sur LIVE_STREAM.	ResultListener	N/A	N/A
Mod√®les
Le d√©tecteur de gestes utilise un bundle de mod√®les avec deux bundles de mod√®les pr√©emball√©s: un bundle de mod√®les de rep√®res manuels et un bundle de mod√®les de classification des gestes. Le mod√®le de rep√®re d√©tecte la pr√©sence de mains et la g√©om√©trie des mains, et le mod√®le de reconnaissance des gestes reconna√Æt les gestes en fonction de la g√©om√©trie des mains.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Nom du mod√®le	Forme d'entr√©e	Type de quantification	Fiche de mod√®le	Versions
HandGestureClassifier	192 x 192, 224 x 224	float 16	info	Nouveaut√©s
Cette t√¢che permet √©galement de modifier le bundle de mod√®les √† l'aide de Model Maker. Pour en savoir plus sur l'utilisation de Model Maker pour personnaliser des mod√®les pour cette t√¢che, consultez la page Personnaliser des mod√®les pour le d√©tecteur de gestes.

Lot de mod√®les de rep√®res manuels
Le bundle de mod√®le de rep√®re de la main d√©tecte la localisation des points cl√©s de 21 coordonn√©es de jointures de la main dans les r√©gions de la main d√©tect√©es. Le mod√®le a √©t√© entra√Æn√© sur environ 30 000 images r√©elles, ainsi que sur plusieurs mod√®les d'images synth√©tiques de mains superpos√©s √† diff√©rents arri√®re-plans. Consultez la d√©finition des 21 points de rep√®re ci-dessous:

Codes pour des points de rep√®re sp√©cifiques de la main

Le bundle de mod√®les de rep√®res de la main contient le mod√®le de d√©tection de la paume et le mod√®le de d√©tection des rep√®res de la main. Le mod√®le de d√©tection de la paume localise la r√©gion des mains dans l'ensemble de l'image d'entr√©e, et le mod√®le de d√©tection des rep√®res de la main trouve les rep√®res sur l'image recadr√©e de la main d√©finie par le mod√®le de d√©tection de la paume.

√âtant donn√© que le mod√®le de d√©tection de la paume de la main est beaucoup plus long, en mode Vid√©o ou en mode flux en direct, le service de reconnaissance des gestes utilise la zone de d√©limitation d√©finie par les rep√®res de la main d√©tect√©s dans le frame actuel pour localiser la r√©gion des mains dans le frame suivant. Cela r√©duit le nombre de fois o√π le d√©tecteur de gestes d√©clenche le mod√®le de d√©tection de la paume de la main. Le mod√®le de d√©tection de la paume de la main n'est appel√© que lorsque le mod√®le de rep√®res de la main ne parvient plus √† identifier suffisamment de pr√©sences de mains ou que le suivi des mains √©choue.

Remarque :Si votre application ne n√©cessite que la d√©tection de rep√®res manuels, utilisez la t√¢che Rep√®re manuel.
Bundle de mod√®les de classification des gestes
Le bundle de mod√®les de classification des gestes peut reconna√Ætre les gestes de la main courants suivants:

0 - Unrecognized gesture, label: Unknown
1 - Closed fist, label: Closed_Fist
2 - Open palm, label: Open_Palm
3 - Pointing up, label: Pointing_Up
4 - Thumbs down, label: Thumb_Down
5 - Thumbs up, label: Thumb_Up
6 - Victory, label: Victory
7 - Love, label: ILoveYou
Si le mod√®le d√©tecte des mains, mais ne reconna√Æt pas de geste, le syst√®me de reconnaissance gestuelle renvoie le r√©sultat "None". Si le mod√®le ne d√©tecte pas de mains, le syst√®me de reconnaissance des gestes renvoie une valeur vide.

Le bundle de mod√®les de classification des gestes contient un pipeline de r√©seau de neurones en deux √©tapes avec un mod√®le d'encapsulation des gestes suivi d'un mod√®le de classification des gestes. Pour en savoir plus, consultez la fiche du mod√®le de classification des gestes.

Le mod√®le d'encapsulation de gestes encode les caract√©ristiques de l'image dans un vecteur de caract√©ristiques, et le mod√®le de classification est un classificateur de gestes l√©ger qui utilise le vecteur de caract√©ristiques comme entr√©e. Le bundle de mod√®le de classification des gestes fourni contient le classificateur de gestes pr√©d√©finis, qui d√©tecte les sept gestes de la main courants pr√©sent√©s ci-dessus. Vous pouvez √©tendre le bundle de mod√®les pour reconna√Ætre davantage de gestes en entra√Ænant votre propre classificateur de gestes personnalis√©. Pour en savoir plus, consultez la section Mod√®les personnalis√©s suivante.

Le d√©tecteur de gestes avec un classificateur de gestes pr√©d√©fini et un classificateur de gestes personnalis√© pr√©f√®re le geste personnalis√© si les deux classificateurs reconnaissent le m√™me geste dans leurs cat√©gories. Si un seul classificateur de gestes reconna√Æt le geste, le service de reconnaissance des gestes g√©n√®re directement le geste reconnu.

Benchmarks des t√¢ches
Voici les benchmarks de t√¢che pour l'ensemble du pipeline bas√©s sur les mod√®les pr√©-entra√Æn√©s ci-dessus. Le r√©sultat de la latence correspond √† la latence moyenne sur le Pixel 6 √† l'aide du processeur / GPU.

Nom du mod√®le	Latence du processeur	Latence du GPU
GestureRecognizer	16,76 ms	20,87 ms
Mod√®les personnalis√©s
Si vous souhaitez am√©liorer ou modifier les fonctionnalit√©s des mod√®les fournis dans cette t√¢che, vous pouvez utiliser Model Maker pour modifier les mod√®les existants. Les mod√®les personnalis√©s utilis√©s avec MediaPipe doivent √™tre au format .task, qui est un fichier de bundle de mod√®le. Nous vous recommandons d'utiliser Model Maker pour modifier les mod√®les fournis pour cette t√¢che avant de cr√©er les v√¥tres.

Pour en savoir plus sur la personnalisation d'un mod√®le pour cette t√¢che, consultez la section Personnaliser des mod√®les pour le d√©tecteur de gestes.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/01/13 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de reconnaissance des gestes pour Android


La t√¢che de reconnaissance des gestes MediaPipe vous permet de reconna√Ætre les gestes de la main en temps r√©el et fournit les r√©sultats de la reconnaissance des gestes de la main et les rep√®res de la main des mains d√©tect√©es. Ces instructions vous expliquent comment utiliser le d√©tecteur de gestes avec les applications Android. L'exemple de code d√©crit dans ces instructions est disponible sur GitHub.

Pour voir cette t√¢che en action, regardez la d√©monstration Web. Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Exemple de code
L'exemple de code MediaPipe Tasks est une impl√©mentation simple d'une application de reconnaissance gestuelle pour Android. L'exemple utilise l'appareil photo d'un appareil Android physique pour d√©tecter en continu les gestes des mains. Il peut √©galement utiliser des images et des vid√©os de la galerie de l'appareil pour d√©tecter de mani√®re statique les gestes.

Vous pouvez utiliser l'application comme point de d√©part pour votre propre application Android ou vous y r√©f√©rer lorsque vous modifiez une application existante. L'exemple de code du d√©tecteur de gestes est h√©berg√© sur GitHub.

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple de code √† l'aide de l'outil de ligne de commande git.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Vous pouvez √©galement configurer votre instance git pour utiliser un "checkout" clairsem√© afin de n'avoir que les fichiers de l'application exemple du d√©tecteur de gestes:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/gesture_recognizer/android
Apr√®s avoir cr√©√© une version locale de l'exemple de code, vous pouvez importer le projet dans Android Studio et ex√©cuter l'application. Pour obtenir des instructions, consultez le guide de configuration pour Android.

Composants cl√©s
Les fichiers suivants contiennent le code essentiel de cet exemple d'application de reconnaissance des gestes manuels:

GestureRecognizerHelper.kt : initialise le d√©tecteur de gestes et g√®re le mod√®le et la s√©lection du d√©l√©gu√©.
MainActivity.kt : impl√©mente l'application, y compris l'appel de GestureRecognizerHelper et de GestureRecognizerResultsAdapter.
GestureRecognizerResultsAdapter.kt : g√®re et met en forme les r√©sultats.
Configuration
Cette section d√©crit les √©tapes cl√©s √† suivre pour configurer votre environnement de d√©veloppement et vos projets de code sp√©cifiquement pour utiliser le d√©tecteur de gestes. Pour obtenir des informations g√©n√©rales sur la configuration de votre environnement de d√©veloppement pour utiliser les t√¢ches MediaPipe, y compris les exigences concernant les versions de la plate-forme, consultez le guide de configuration pour Android.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
D√©pendances
La t√¢che de l'outil de reconnaissance des gestes utilise la biblioth√®que com.google.mediapipe:tasks-vision. Ajoutez cette d√©pendance au fichier build.gradle de votre application Android:

dependencies {
    implementation 'com.google.mediapipe:tasks-vision:latest.release'
}
Mod√®le
La t√¢che de reconnaissance des gestes MediaPipe n√©cessite un bundle de mod√®le entra√Æn√© compatible avec cette t√¢che. Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour le d√©tecteur de gestes, consultez la section Mod√®les de la pr√©sentation de la t√¢che.

S√©lectionnez et t√©l√©chargez le mod√®le, puis stockez-le dans le r√©pertoire de votre projet:

<dev-project-root>/src/main/assets
Sp√©cifiez le chemin d'acc√®s au mod√®le dans le param√®tre ModelAssetPath. Dans l'exemple de code, le mod√®le est d√©fini dans le fichier GestureRecognizerHelper.kt:

baseOptionBuilder.setModelAssetPath(MP_RECOGNIZER_TASK)
Cr√©er la t√¢che
La t√¢che de reconnaissance des gestes MediaPipe utilise la fonction createFromOptions() pour configurer la t√¢che. La fonction createFromOptions() accepte des valeurs pour les options de configuration. Pour en savoir plus sur les options de configuration, consultez la section Options de configuration.

Le d√©tecteur de gestes est compatible avec trois types de donn√©es d'entr√©e: les images fixes, les fichiers vid√©o et les flux vid√©o en direct. Vous devez sp√©cifier le mode d'ex√©cution correspondant √† votre type de donn√©es d'entr√©e lorsque vous cr√©ez la t√¢che. S√©lectionnez l'onglet correspondant √† votre type de donn√©es d'entr√©e pour d√©couvrir comment cr√©er la t√¢che et ex√©cuter l'inf√©rence.

Image
Vid√©o
Diffusion en direct
val baseOptionsBuilder = BaseOptions.builder().setModelAssetPath(MP_RECOGNIZER_TASK)
val baseOptions = baseOptionBuilder.build()

val optionsBuilder =
    GestureRecognizer.GestureRecognizerOptions.builder()
        .setBaseOptions(baseOptions)
        .setMinHandDetectionConfidence(minHandDetectionConfidence)
        .setMinTrackingConfidence(minHandTrackingConfidence)
        .setMinHandPresenceConfidence(minHandPresenceConfidence)
        .setRunningMode(RunningMode.IMAGE)

val options = optionsBuilder.build()
gestureRecognizer =
    GestureRecognizer.createFromOptions(context, options)
    
Remarque :Si vous utilisez le mode de diffusion en direct, vous devez enregistrer un √©couteur de r√©sultats lorsque vous cr√©ez la t√¢che. L'√©couteur est appel√© chaque fois que la t√¢che a termin√© le traitement d'un frame vid√©o avec le r√©sultat de la d√©tection et l'image d'entr√©e comme param√®tres.
Remarque :Si vous utilisez le mode vid√©o ou le mode de diffusion en direct, le d√©tecteur de gestes utilise le suivi pour √©viter de d√©clencher le mod√®le de d√©tection de la paume de la main sur chaque frame, ce qui permet de r√©duire la latence du d√©tecteur de gestes.
L'impl√©mentation de l'exemple de code du d√©tecteur de gestes permet √† l'utilisateur de basculer entre les modes de traitement. Cette approche rend le code de cr√©ation de t√¢che plus complexe et n'est peut-√™tre pas adapt√©e √† votre cas d'utilisation. Vous pouvez voir ce code dans la fonction setupGestureRecognizer() du fichier GestureRecognizerHelper.kt.

Options de configuration
Cette t√¢che propose les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
runningMode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
numHands	Le nombre maximal de mains peut √™tre d√©tect√© par le GestureRecognizer.	Any integer > 0	1
minHandDetectionConfidence	Score de confiance minimal pour que la d√©tection de la main soit consid√©r√©e comme r√©ussie dans le mod√®le de d√©tection de la paume de la main.	0.0 - 1.0	0.5
minHandPresenceConfidence	Score de confiance minimal du score de pr√©sence de la main dans le mod√®le de d√©tection des rep√®res de la main. En mode Vid√©o et en mode Diffusion en direct du service de reconnaissance des gestes, si le score de confiance de la pr√©sence de la main du mod√®le de rep√®re de la main est inf√©rieur √† ce seuil, le mod√®le de d√©tection de la paume est d√©clench√©. Sinon, un algorithme de suivi des mains l√©ger est utilis√© pour d√©terminer l'emplacement de la ou des mains afin de d√©tecter les rep√®res par la suite.	0.0 - 1.0	0.5
minTrackingConfidence	Score de confiance minimal pour que le suivi des mains soit consid√©r√© comme r√©ussi. Il s'agit du seuil d'IoU du cadre de d√©limitation entre les mains dans le frame actuel et le dernier frame. En mode Vid√©o et en mode Flux du service de reconnaissance des gestes, si le suivi √©choue, le service de reconnaissance des gestes d√©clenche la d√©tection des mains. Sinon, la d√©tection de la main est ignor√©e.	0.0 - 1.0	0.5
cannedGesturesClassifierOptions	Options de configuration du comportement du classificateur de gestes pr√©d√©finis. Les gestes pr√©d√©finis sont ["None", "Closed_Fist", "Open_Palm", "Pointing_Up", "Thumb_Down", "Thumb_Up", "Victory", "ILoveYou"]
.
Locales des noms √† afficher: √©ventuelles langues √† utiliser pour les noms √† afficher sp√©cifi√©s via les m√©tadonn√©es du mod√®le TFLite.
Nombre maximal de r√©sultats: nombre maximal de r√©sultats de classification les plus √©lev√©s √† renvoyer. Si la valeur est inf√©rieure √† 0, tous les r√©sultats disponibles sont renvoy√©s.
Seuil de score: score en dessous duquel les r√©sultats sont rejet√©s. Si cette valeur est d√©finie sur 0, tous les r√©sultats disponibles sont renvoy√©s.
Liste d'autorisation des cat√©gories: liste d'autorisation des noms de cat√©gories. Si cet ensemble n'est pas vide, les r√©sultats de classification dont la cat√©gorie ne figure pas dans cet ensemble seront filtr√©s. S'exclut mutuellement avec la liste de blocage.
Liste de blocage des cat√©gories: liste de blocage des noms de cat√©gories. Si cet ensemble n'est pas vide, les r√©sultats de classification dont la cat√©gorie figure dans cet ensemble seront filtr√©s. S'exclut mutuellement avec la liste d'autorisation.
Param√®tre r√©gional des noms √† afficher: any string
R√©sultats max. : any integer
Seuil de score: 0.0-1.0
Liste d'autorisation de la cat√©gorie: vector of strings
Liste de blocage des cat√©gories: vector of strings
Param√®tre r√©gional des noms √† afficher: "en"
R√©sultats max. : -1
Seuil de score: 0
Liste d'autorisation des cat√©gories: vide
Liste de blocage des cat√©gories: vide
customGesturesClassifierOptions	Options permettant de configurer le comportement du classificateur de gestes personnalis√©s.
Locales des noms √† afficher: √©ventuelles langues √† utiliser pour les noms √† afficher sp√©cifi√©s dans les m√©tadonn√©es du mod√®le TFLite.
Nombre maximal de r√©sultats: nombre maximal de r√©sultats de classification les plus √©lev√©s √† renvoyer. Si la valeur est inf√©rieure √† 0, tous les r√©sultats disponibles sont renvoy√©s.
Seuil de score: score en dessous duquel les r√©sultats sont rejet√©s. Si cette valeur est d√©finie sur 0, tous les r√©sultats disponibles sont renvoy√©s.
Liste d'autorisation des cat√©gories: liste d'autorisation des noms de cat√©gories. Si cet ensemble n'est pas vide, les r√©sultats de classification dont la cat√©gorie ne figure pas dans cet ensemble seront filtr√©s. S'exclut mutuellement avec la liste de blocage.
Liste de blocage des cat√©gories: liste de blocage des noms de cat√©gories. Si cet ensemble n'est pas vide, les r√©sultats de classification dont la cat√©gorie figure dans cet ensemble seront filtr√©s. S'exclut mutuellement avec la liste d'autorisation.
Param√®tre r√©gional des noms √† afficher: any string
R√©sultats max. : any integer
Seuil de score: 0.0-1.0
Liste d'autorisation de la cat√©gorie: vector of strings
Liste de blocage des cat√©gories: vector of strings
Param√®tre r√©gional des noms √† afficher: "en"
R√©sultats max. : -1
Seuil de score: 0
Liste d'autorisation des cat√©gories: vide
Liste de blocage des cat√©gories: vide
resultListener	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats de classification de mani√®re asynchrone lorsque le d√©tecteur de gestes est en mode flux en direct. Ne peut √™tre utilis√© que lorsque le mode d'ex√©cution est d√©fini sur LIVE_STREAM.	ResultListener	N/A	N/A
errorListener	D√©finit un √©couteur d'erreur facultatif.	ErrorListener	N/A	N/A
Pr√©parer les donn√©es
Le d√©tecteur de gestes fonctionne avec les images, les fichiers vid√©o et les vid√©os en direct. La t√¢che g√®re le pr√©traitement de l'entr√©e des donn√©es, y compris le redimensionnement, la rotation et la normalisation des valeurs.

Le code suivant montre comment transmettre des donn√©es pour traitement. Ces exemples incluent des informations sur la gestion des donn√©es issues d'images, de fichiers vid√©o et de flux vid√©o en direct.

Image
Vid√©o
Diffusion en direct
import com.google.mediapipe.framework.image.BitmapImageBuilder
import com.google.mediapipe.framework.image.MPImage

// Convert the input Bitmap object to an MPImage object to run inference
val mpImage = BitmapImageBuilder(image).build()
    
Dans l'exemple de code du d√©tecteur de gestes, la pr√©paration des donn√©es est g√©r√©e dans le fichier GestureRecognizerHelper.kt.

Ex√©cuter la t√¢che
Le d√©tecteur de gestes utilise les fonctions recognize, recognizeForVideo et recognizeAsync pour d√©clencher des inf√©rences. Pour la reconnaissance des gestes, cela implique de pr√©traiter les donn√©es d'entr√©e, de d√©tecter les mains dans l'image, de d√©tecter les points de rep√®re des mains et de reconna√Ætre les gestes des mains √† partir des points de rep√®re.

Le code suivant montre comment ex√©cuter le traitement avec le mod√®le de t√¢che. Ces exemples incluent des informations sur la gestion des donn√©es issues d'images, de fichiers vid√©o et de flux vid√©o en direct.

Image
Vid√©o
Diffusion en direct
val result = gestureRecognizer?.recognize(mpImage)
    
Veuillez noter les points suivants :

Lorsque vous ex√©cutez le mode vid√©o ou le mode diffusion en direct, vous devez √©galement fournir le code temporel du frame d'entr√©e √† la t√¢che de reconnaissance des gestes.
Lorsqu'elle s'ex√©cute en mode image ou vid√©o, la t√¢che du d√©tecteur de gestes bloque le thread actuel jusqu'√† ce qu'elle ait termin√© de traiter l'image ou le frame d'entr√©e. Pour √©viter de bloquer l'interface utilisateur, ex√©cutez le traitement dans un thread en arri√®re-plan.
Lorsqu'elle s'ex√©cute en mode diffusion en direct, la t√¢che du d√©tecteur de gestes ne bloque pas le thread en cours, mais renvoie imm√©diatement. Il appelle son √©couteur de r√©sultats avec le r√©sultat de la reconnaissance chaque fois qu'il a termin√© le traitement d'un frame d'entr√©e. Si la fonction de reconnaissance est appel√©e lorsque la t√¢che du d√©tecteur de gestes est occup√©e √† traiter un autre frame, la t√¢che ignore le nouveau frame d'entr√©e.
Dans l'exemple de code du d√©tecteur de gestes, les fonctions recognize, recognizeForVideo et recognizeAsync sont d√©finies dans le fichier GestureRecognizerHelper.kt.

G√©rer et afficher les r√©sultats
Le d√©tecteur de gestes g√©n√®re un objet de r√©sultat de d√©tection de gestes pour chaque ex√©cution de la reconnaissance. L'objet r√©sultat contient des rep√®res de main en coordonn√©es d'image, des rep√®res de main en coordonn√©es mondiales, une lat√©ralit√©(main gauche/droite) et des cat√©gories de gestes de la main des mains d√©tect√©es.

Voici un exemple des donn√©es de sortie de cette t√¢che:

Le GestureRecognizerResult g√©n√©r√© contient quatre composants, chacun √©tant un tableau, o√π chaque √©l√©ment contient le r√©sultat d√©tect√© d'une seule main d√©tect√©e.

Main dominante

La maniabilit√© indique si les mains d√©tect√©es sont des mains gauches ou droites.

Gestes

Cat√©gories de gestes reconnues pour les mains d√©tect√©es.

Points de rep√®re

Il existe 21 rep√®res de la main, chacun compos√© de coordonn√©es x, y et z. Les coordonn√©es x et y sont normalis√©es √† [0,0, 1,0] par la largeur et la hauteur de l'image, respectivement. La coordonn√©e z repr√©sente la profondeur du rep√®re, la profondeur au poignet √©tant l'origine. Plus la valeur est faible, plus le rep√®re est proche de la cam√©ra. L'ampleur de z utilise √† peu pr√®s la m√™me √©chelle que x.

Monuments du monde

Les 21 rep√®res de la main sont √©galement pr√©sent√©s en coordonn√©es mondiales. Chaque rep√®re est compos√© de x, y et z, qui repr√©sentent des coordonn√©es 3D r√©elles en m√®tres, avec l'origine au centre g√©om√©trique de la main.

GestureRecognizerResult:
  Handedness:
    Categories #0:
      index        : 0
      score        : 0.98396
      categoryName : Left
  Gestures:
    Categories #0:
      score        : 0.76893
      categoryName : Thumb_Up
  Landmarks:
    Landmark #0:
      x            : 0.638852
      y            : 0.671197
      z            : -3.41E-7
    Landmark #1:
      x            : 0.634599
      y            : 0.536441
      z            : -0.06984
    ... (21 landmarks for a hand)
  WorldLandmarks:
    Landmark #0:
      x            : 0.067485
      y            : 0.031084
      z            : 0.055223
    Landmark #1:
      x            : 0.063209
      y            : -0.00382
      z            : 0.020920
    ... (21 world landmarks for a hand)
Les images suivantes pr√©sentent une visualisation de la sortie de la t√¢che:

Main levant le pouce avec la structure squelettique de la main mapp√©e

Dans l'exemple de code du d√©tecteur de gestes, la classe GestureRecognizerResultsAdapter du fichier GestureRecognizerResultsAdapter.kt g√®re les r√©sultats.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de d√©tection des points de rep√®re de la main
Main tenant un ≈ìuf. La forme de la main est marqu√©e d&#39;une maquette fonctionnelle qui indique la structure identifi√©e.

La t√¢che MediaPipe Hand Landmarker vous permet de d√©tecter les points de rep√®re des mains dans une image. Vous pouvez utiliser cette t√¢che pour localiser les points cl√©s des mains et y appliquer des effets visuels. Cette t√¢che fonctionne sur des donn√©es d'image avec un mod√®le de machine learning (ML) en tant que donn√©es statiques ou flux continu, et produit des rep√®res de la main en coordonn√©es d'image, des rep√®res de la main en coordonn√©es mondiales et la lat√©ralit√©(main gauche/droite) de plusieurs mains d√©tect√©es.

Essayez !arrow_forward

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez l'un de ces guides d'impl√©mentation pour votre plate-forme cible. Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter de mani√®re basique cette t√¢che, y compris un mod√®le recommand√© et un exemple de code avec les options de configuration recommand√©es:

Android ‚Äì Exemple de code
Guide
Python ‚Äì Exemple de code
Guide
Web ‚Äì Exemple de code ‚Äì Guide
D√©tails de la t√¢che
Cette section d√©crit les fonctionnalit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Fonctionnalit√©s
Traitement des images d'entr√©e : le traitement comprend la rotation, le redimensionnement, la normalisation et la conversion d'espaces colorim√©triques des images.
Seuil de score : filtrez les r√©sultats en fonction des scores de pr√©diction.
Entr√©es de t√¢che	Sorties de t√¢che
Le rep√®re de la main accepte l'un des types de donn√©es suivants:
Images fixes
Images vid√©o d√©cod√©es
Flux vid√©o en direct
Le rep√®re de la main produit les r√©sultats suivants:
Main dominante des mains d√©tect√©es
Points de rep√®re des mains d√©tect√©es en coordonn√©es d'image
Points de rep√®re des mains d√©tect√©es en coordonn√©es mondiales
Options de configuration
Cette t√¢che propose les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
running_mode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
num_hands	Nombre maximal de mains d√©tect√©es par le d√©tecteur de rep√®res de main.	Any integer > 0	1
min_hand_detection_confidence	Score de confiance minimal pour que la d√©tection de la main soit consid√©r√©e comme r√©ussie dans le mod√®le de d√©tection de la paume de la main.	0.0 - 1.0	0.5
min_hand_presence_confidence	Score de confiance minimal pour le score de pr√©sence de la main dans le mod√®le de d√©tection des rep√®res de la main. En mode Vid√©o et en mode Diffusion en direct, si le score de confiance de la pr√©sence de la main du mod√®le de rep√®res de la main est inf√©rieur √† ce seuil, le rep√®re de la main d√©clenche le mod√®le de d√©tection de la paume. Sinon, un algorithme de suivi des mains l√©ger d√©termine l'emplacement de la ou des mains pour les d√©tections de rep√®res ult√©rieures.	0.0 - 1.0	0.5
min_tracking_confidence	Score de confiance minimal pour que le suivi des mains soit consid√©r√© comme r√©ussi. Il s'agit du seuil d'IoU du cadre de d√©limitation entre les mains dans le frame actuel et le dernier frame. En mode Vid√©o et en mode Flux de Hand Landmarker, si le suivi √©choue, Hand Landmarker d√©clenche la d√©tection de la main. Dans le cas contraire, la d√©tection de la main est ignor√©e.	0.0 - 1.0	0.5
result_callback	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats de d√©tection de mani√®re asynchrone lorsque le rep√®re de la main est en mode diffusion en direct. Uniquement applicable lorsque le mode d'ex√©cution est d√©fini sur LIVE_STREAM	N/A	N/A
Mod√®les
Le d√©tecteur de rep√®res de la main utilise un lot de mod√®les avec deux mod√®les empaquet√©s: un mod√®le de d√©tection de paume et un mod√®le de d√©tection de rep√®res de la main. Pour ex√©cuter cette t√¢che, vous avez besoin d'un bundle de mod√®les contenant ces deux mod√®les.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Nom du mod√®le	Forme d'entr√©e	Type de quantification	Fiche de mod√®le	Versions
HandLandmarker (full)	192 x 192, 224 x 224	float 16	info	Nouveaut√©s
Le bundle de mod√®le de rep√®re de la main d√©tecte la localisation des points cl√©s de 21 coordonn√©es de jointures de la main dans les r√©gions de la main d√©tect√©es. Le mod√®le a √©t√© entra√Æn√© sur environ 30 000 images r√©elles, ainsi que sur plusieurs mod√®les d'images synth√©tiques de mains superpos√©s √† diff√©rents arri√®re-plans.



Le bundle de mod√®les de rep√®res de la main contient un mod√®le de d√©tection de paume et un mod√®le de d√©tection de rep√®res de la main. Le mod√®le de d√©tection de la paume de la main localise les mains dans l'image d'entr√©e, et le mod√®le de d√©tection des points de rep√®re de la main identifie des points de rep√®re sp√©cifiques sur l'image de la main recadr√©e d√©finie par le mod√®le de d√©tection de la paume de la main.

√âtant donn√© que l'ex√©cution du mod√®le de d√©tection de la paume de la main prend du temps, en mode d'ex√©cution de la vid√©o ou du streaming en direct, Hand Landmarker utilise la zone de d√©limitation d√©finie par le mod√®le de rep√®res de la main dans un seul frame pour localiser la r√©gion des mains pour les frames suivants. Le rep√®re de la main ne r√©active le mod√®le de d√©tection de la paume que si le mod√®le de rep√®re de la main n'identifie plus la pr√©sence de mains ou ne parvient pas √† suivre les mains dans le cadre. Cela r√©duit le nombre de fois o√π Hand Landmarker d√©clenche le mod√®le de d√©tection de la paume de la main.

Benchmarks des t√¢ches
Voici les benchmarks de t√¢che pour l'ensemble du pipeline bas√©s sur les mod√®les pr√©-entra√Æn√©s ci-dessus. Le r√©sultat de la latence correspond √† la latence moyenne sur le Pixel 6 √† l'aide du processeur / GPU.

Nom du mod√®le	Latence du processeur	Latence du GPU
HandLandmarker (complet)	17,12 ms	12,27 ms
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/01/13 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de d√©tection des points de rep√®re de la main pour Android


La t√¢che MediaPipe Hand Landmarker vous permet de d√©tecter les points de rep√®re des mains dans une image. Ces instructions vous expliquent comment utiliser le rep√®re de la main avec les applications Android. L'exemple de code d√©crit dans ces instructions est disponible sur GitHub.

Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Exemple de code
L'exemple de code MediaPipe Tasks est une impl√©mentation simple d'une application de rep√®re de main pour Android. L'exemple utilise l'appareil photo d'un appareil Android physique pour d√©tecter en continu les rep√®res de la main. Il peut √©galement utiliser des images et des vid√©os de la galerie de l'appareil pour d√©tecter de mani√®re statique les rep√®res de la main.

Vous pouvez utiliser l'application comme point de d√©part pour votre propre application Android ou vous y r√©f√©rer lorsque vous modifiez une application existante. L'exemple de code Hand Landmarker est h√©berg√© sur GitHub.

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple de code √† l'aide de l'outil de ligne de commande git.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Vous pouvez √©ventuellement configurer votre instance git pour utiliser un "checkout" clairsem√© afin de n'avoir que les fichiers de l'application exemple Hand Landmarker:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/hand_landmarker/android
Apr√®s avoir cr√©√© une version locale de l'exemple de code, vous pouvez importer le projet dans Android Studio et ex√©cuter l'application. Pour obtenir des instructions, consultez le guide de configuration pour Android.

Composants cl√©s
Les fichiers suivants contiennent le code essentiel de cet exemple d'application de d√©tection de rep√®res manuels:

HandLandmarkerHelper.kt : initialise le d√©tecteur de points de rep√®re de la main et g√®re le mod√®le et la s√©lection du d√©l√©gu√©.
MainActivity.kt : impl√©mente l'application, y compris l'appel de HandLandmarkerHelper.
Configuration
Cette section d√©crit les √©tapes cl√©s √† suivre pour configurer votre environnement de d√©veloppement et vos projets de code sp√©cifiquement pour utiliser Hand Landmarker. Pour obtenir des informations g√©n√©rales sur la configuration de votre environnement de d√©veloppement pour utiliser les t√¢ches MediaPipe, y compris les exigences concernant les versions de la plate-forme, consultez le guide de configuration pour Android.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
D√©pendances
La t√¢che de rep√®re de la main utilise la biblioth√®que com.google.mediapipe:tasks-vision. Ajoutez cette d√©pendance au fichier build.gradle de votre application Android:

dependencies {
    implementation 'com.google.mediapipe:tasks-vision:latest.release'
}
Mod√®le
La t√¢che de rep√®re de la main MediaPipe n√©cessite un bundle de mod√®le entra√Æn√© compatible avec cette t√¢che. Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour Hand Landmarker, consultez la section Mod√®les de la pr√©sentation de la t√¢che.

S√©lectionnez et t√©l√©chargez le mod√®le, puis stockez-le dans le r√©pertoire de votre projet:

<dev-project-root>/src/main/assets
Sp√©cifiez le chemin d'acc√®s au mod√®le dans le param√®tre ModelAssetPath. Dans l'exemple de code, le mod√®le est d√©fini dans le fichier HandLandmarkerHelper.kt:

baseOptionBuilder.setModelAssetPath(MP_HAND_LANDMARKER_TASK)
Cr√©er la t√¢che
La t√¢che MediaPipe Hand Landmarker utilise la fonction createFromOptions() pour configurer la t√¢che. La fonction createFromOptions() accepte des valeurs pour les options de configuration. Pour en savoir plus sur les options de configuration, consultez la section Options de configuration.

Le rep√®re de main est compatible avec trois types de donn√©es d'entr√©e: les images fixes, les fichiers vid√©o et le streaming en direct. Vous devez sp√©cifier le mode d'ex√©cution correspondant au type de donn√©es d'entr√©e lorsque vous cr√©ez la t√¢che. S√©lectionnez l'onglet correspondant √† votre type de donn√©es d'entr√©e pour d√©couvrir comment cr√©er la t√¢che et ex√©cuter l'inf√©rence.

Image
Vid√©o
Diffusion en direct
val baseOptionsBuilder = BaseOptions.builder().setModelAssetPath(MP_HAND_LANDMARKER_TASK)
val baseOptions = baseOptionBuilder.build()

val optionsBuilder =
    HandLandmarker.HandLandmarkerOptions.builder()
        .setBaseOptions(baseOptions)
        .setMinHandDetectionConfidence(minHandDetectionConfidence)
        .setMinTrackingConfidence(minHandTrackingConfidence)
        .setMinHandPresenceConfidence(minHandPresenceConfidence)
        .setNumHands(maxNumHands)
        .setRunningMode(RunningMode.IMAGE)

val options = optionsBuilder.build()

handLandmarker =
    HandLandmarker.createFromOptions(context, options)
    
Remarque :Si vous utilisez le mode de diffusion en direct, vous devez enregistrer un √©couteur de r√©sultats lorsque vous cr√©ez la t√¢che. L'√©couteur est appel√© chaque fois que la t√¢che a termin√© de traiter un frame vid√©o avec le r√©sultat de la d√©tection et l'image d'entr√©e comme param√®tres.
Remarque :Si vous utilisez le mode vid√©o ou le mode streaming en direct, Hand Landmarker utilise le suivi pour √©viter de d√©clencher le mod√®le de d√©tection de la paume de la main sur chaque frame, ce qui permet de r√©duire la latence.
L'impl√©mentation de l'exemple de code Hand Landmarker permet √† l'utilisateur de basculer entre les modes de traitement. Cette approche rend le code de cr√©ation de t√¢che plus complexe et n'est peut-√™tre pas adapt√©e √† votre cas d'utilisation. Vous pouvez voir ce code dans la fonction setupHandLandmarker() du fichier HandLandmarkerHelper.kt.

Options de configuration
Cette t√¢che propose les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
runningMode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
numHands	Nombre maximal de mains d√©tect√©es par le d√©tecteur de rep√®res de main.	Any integer > 0	1
minHandDetectionConfidence	Score de confiance minimal pour que la d√©tection de la main soit consid√©r√©e comme r√©ussie dans le mod√®le de d√©tection de la paume de la main.	0.0 - 1.0	0.5
minHandPresenceConfidence	Score de confiance minimal pour le score de pr√©sence de la main dans le mod√®le de d√©tection des rep√®res de la main. En mode Vid√©o et en mode Diffusion en direct, si le score de confiance de la pr√©sence de la main du mod√®le de rep√®res de la main est inf√©rieur √† ce seuil, le rep√®re de la main d√©clenche le mod√®le de d√©tection de la paume. Sinon, un algorithme de suivi des mains l√©ger d√©termine l'emplacement de la ou des mains pour les d√©tections de rep√®res ult√©rieures.	0.0 - 1.0	0.5
minTrackingConfidence	Score de confiance minimal pour que le suivi des mains soit consid√©r√© comme r√©ussi. Il s'agit du seuil d'IoU du cadre de d√©limitation entre les mains dans le frame actuel et le dernier frame. En mode Vid√©o et en mode Flux de Hand Landmarker, si le suivi √©choue, Hand Landmarker d√©clenche la d√©tection de la main. Dans le cas contraire, la d√©tection de la main est ignor√©e.	0.0 - 1.0	0.5
resultListener	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats de d√©tection de mani√®re asynchrone lorsque le rep√®re de la main est en mode diffusion en direct. Uniquement applicable lorsque le mode d'ex√©cution est d√©fini sur LIVE_STREAM	N/A	N/A
errorListener	D√©finit un √©couteur d'erreur facultatif.	N/A	N/A
Pr√©parer les donn√©es
Le rep√®re de main fonctionne avec les images, les fichiers vid√©o et les diffusions en direct. La t√¢che g√®re le pr√©traitement de l'entr√©e des donn√©es, y compris le redimensionnement, la rotation et la normalisation des valeurs.

Le code suivant montre comment transmettre des donn√©es pour traitement. Ces exemples incluent des informations sur la gestion des donn√©es issues d'images, de fichiers vid√©o et de flux vid√©o en direct.

Image
Vid√©o
Diffusion en direct
import com.google.mediapipe.framework.image.BitmapImageBuilder
import com.google.mediapipe.framework.image.MPImage

// Convert the input Bitmap object to an MPImage object to run inference
val mpImage = BitmapImageBuilder(image).build()
    
Dans l'exemple de code du rep√®re manuel, la pr√©paration des donn√©es est g√©r√©e dans le fichier HandLandmarkerHelper.kt.

Ex√©cuter la t√¢che
En fonction du type de donn√©es avec lequel vous travaillez, utilisez la m√©thode HandLandmarker.detect...() sp√©cifique √† ce type de donn√©es. Utilisez detect() pour les images individuelles, detectForVideo() pour les images dans les fichiers vid√©o et detectAsync() pour les flux vid√©o. Lorsque vous effectuez des d√©tections sur un flux vid√©o, assurez-vous de les ex√©cuter sur un thread distinct pour √©viter de bloquer le thread de l'interface utilisateur.

Les exemples de code suivants montrent comment ex√©cuter le rep√®re de la main dans ces diff√©rents modes de donn√©es:

Image
Vid√©o
Diffusion en direct
val result = handLandmarker?.detect(mpImage)
    
Veuillez noter les points suivants :

Lorsque vous ex√©cutez le mode vid√©o ou le mode diffusion en direct, vous devez √©galement fournir le code temporel du frame d'entr√©e √† la t√¢che de rep√®re manuel.
Lors de l'ex√©cution en mode image ou vid√©o, la t√¢che de rep√®re manuel bloque le thread actuel jusqu'√† ce qu'elle ait termin√© le traitement de l'image ou du frame d'entr√©e. Pour √©viter de bloquer l'interface utilisateur, ex√©cutez le traitement dans un thread en arri√®re-plan.
Lorsqu'elle s'ex√©cute en mode diffusion en direct, la t√¢che de rep√®re de la main ne bloque pas le thread actuel, mais renvoie imm√©diatement. Il appelle son √©couteur de r√©sultats avec le r√©sultat de la d√©tection chaque fois qu'il a termin√© le traitement d'un frame d'entr√©e. Si la fonction de d√©tection est appel√©e lorsque la t√¢che de rep√®re de la main est occup√©e √† traiter un autre frame, la t√¢che ignore le nouveau frame d'entr√©e.
Dans l'exemple de code du rep√®re manuel, les fonctions detect, detectForVideo et detectAsync sont d√©finies dans le fichier HandLandmarkerHelper.kt.

G√©rer et afficher les r√©sultats
Le rep√®re de la main g√©n√®re un objet de r√©sultat de rep√®re de la main pour chaque ex√©cution de d√©tection. L'objet r√©sultat contient des rep√®res de main en coordonn√©es d'image, des rep√®res de main en coordonn√©es mondiales et la lat√©ralit√©(main gauche/droite) des mains d√©tect√©es.

Voici un exemple des donn√©es de sortie de cette t√¢che:

La sortie HandLandmarkerResult contient trois composants. Chaque composant est un tableau, o√π chaque √©l√©ment contient les r√©sultats suivants pour une seule main d√©tect√©e:

Main dominante

La maniabilit√© indique si les mains d√©tect√©es sont des mains gauches ou droites.

Points de rep√®re

Il existe 21 rep√®res de la main, chacun compos√© de coordonn√©es x, y et z. Les coordonn√©es x et y sont normalis√©es √† [0,0, 1,0] par la largeur et la hauteur de l'image, respectivement. La coordonn√©e z repr√©sente la profondeur du rep√®re, la profondeur au poignet √©tant l'origine. Plus la valeur est faible, plus le rep√®re est proche de la cam√©ra. L'ampleur de z utilise √† peu pr√®s la m√™me √©chelle que x.

Monuments du monde

Les 21 rep√®res de la main sont √©galement pr√©sent√©s en coordonn√©es mondiales. Chaque rep√®re est compos√© de x, y et z, qui repr√©sentent des coordonn√©es 3D r√©elles en m√®tres, avec l'origine au centre g√©om√©trique de la main.

HandLandmarkerResult:
  Handedness:
    Categories #0:
      index        : 0
      score        : 0.98396
      categoryName : Left
  Landmarks:
    Landmark #0:
      x            : 0.638852
      y            : 0.671197
      z            : -3.41E-7
    Landmark #1:
      x            : 0.634599
      y            : 0.536441
      z            : -0.06984
    ... (21 landmarks for a hand)
  WorldLandmarks:
    Landmark #0:
      x            : 0.067485
      y            : 0.031084
      z            : 0.055223
    Landmark #1:
      x            : 0.063209
      y            : -0.00382
      z            : 0.020920
    ... (21 world landmarks for a hand)
L'image suivante pr√©sente une visualisation du r√©sultat de la t√¢che:

Main avec le pouce lev√© et structure squelettique de la main cartographi√©e

L'exemple de code Hand Landmarker montre comment afficher les r√©sultats renvoy√©s par la t√¢che. Pour en savoir plus, consultez la classe OverlayView.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide des t√¢ches d'int√©gration d'images
Deux exemples d&#39;images de c√¥nes glac√©s incluant les repr√©sentations vectorielles continues num√©riques des images sous forme de tableaux

La t√¢che MediaPipe Image Embedder vous permet de cr√©er une repr√©sentation num√©rique d'une image, ce qui est utile pour effectuer diverses t√¢ches d'image bas√©es sur le ML. Cette fonctionnalit√© est fr√©quemment utilis√©e pour comparer la similarit√© de deux images √† l'aide de techniques de comparaison math√©matique telles que la similarit√© cosinus. Cette t√¢che fonctionne sur des donn√©es d'image avec un mod√®le de machine learning (ML) en tant que donn√©es statiques ou flux continu, et produit une repr√©sentation num√©rique des donn√©es d'image sous la forme d'une liste de vecteurs de caract√©ristiques haute dimension, √©galement appel√©s vecteurs d'embedding, sous forme √† virgule flottante ou quantique.

Essayez !arrow_forward

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez l'un de ces guides d'impl√©mentation pour votre plate-forme cible. Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter de mani√®re basique cette t√¢che √† l'aide d'un mod√®le recommand√© et fournissent des exemples de code avec les options de configuration recommand√©es:

Android ‚Äì Exemple de code ‚Äì Guide
Python ‚Äì Exemple de code ‚Äì Guide
Web ‚Äì Exemple de code ‚Äì Guide
D√©tails de la t√¢che
Cette section d√©crit les fonctionnalit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Fonctionnalit√©s
Traitement des images d'entr√©e : le traitement comprend la rotation, le redimensionnement, la normalisation et la conversion d'espaces colorim√©triques des images.
Zone d'int√©r√™t : effectue l'embedding sur une zone de l'image plut√¥t que sur l'image enti√®re.
Calcul de la similarit√© d'embedding : fonction utilitaire int√©gr√©e permettant de calculer la similarit√© cosinus entre deux vecteurs de caract√©ristiques
Quantification : compatible avec la quantification scalaire des vecteurs de caract√©ristiques.
Entr√©es de t√¢che	Sorties de t√¢che
L'entr√©e peut √™tre l'un des types de donn√©es suivants:
Images fixes
Images vid√©o d√©cod√©es
Flux vid√©o en direct
L'outil Image Embedder g√©n√®re une liste d'embeddings compos√©e des √©l√©ments suivants:
Embedding: vecteur de caract√©ristiques lui-m√™me, sous forme √† virgule flottante ou quantifi√© de mani√®re scalaire.
Indice de t√™te: indice de la t√™te qui a g√©n√©r√© cet embedding.
Nom de la t√™te (facultatif): nom de la t√™te qui a produit cet entra√Ænement.
Options de configuration
Cette t√¢che propose les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
running_mode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
l2_normalize	Indique si le vecteur de caract√©ristiques renvoy√© doit √™tre normalis√© avec la norme L2. N'utilisez cette option que si le mod√®le ne contient pas d√©j√† une op√©ration TFLite L2_NORMALIZATION native. Dans la plupart des cas, c'est d√©j√† le cas, et la normalisation L2 est donc obtenue via l'inf√©rence TFLite sans avoir besoin de cette option.	Boolean	False
quantize	Indique si l'embedding renvoy√© doit √™tre quantifi√© en octets via une quantification scalaire. Les repr√©sentations vectorielles continues sont implicitement suppos√©es avoir une norme unitaire. Par cons√©quent, toute dimension a une valeur comprise dans la plage [-1,0, 1,0]. Utilisez l'option l2_normalize si ce n'est pas le cas.	Boolean	False
result_callback	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats d'int√©gration de mani√®re asynchrone lorsque l'outil d'int√©gration d'images est en mode diffusion en direct. Ne peut √™tre utilis√© que lorsque le mode d'ex√©cution est d√©fini sur LIVE_STREAM.	N/A	Non d√©fini
Remarque :resultListener d√©pend de runningMode. Ne d√©finissez resultListener que lorsque runningMode est d√©fini sur LIVE_STREAM.
Mod√®les
L'outil Image Embedder n√©cessite que vous t√©l√©chargiez et stockiez un mod√®le d'embedding d'image dans le r√©pertoire de votre projet. Lorsque vous commencez √† d√©velopper avec cette t√¢che, commencez par le mod√®le par d√©faut recommand√© pour votre plate-forme cible. Les autres mod√®les disponibles font g√©n√©ralement des compromis entre les performances, la pr√©cision, la r√©solution et les exigences en termes de ressources, et incluent parfois des fonctionnalit√©s suppl√©mentaires.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Mod√®le MobileNetV3
Cette famille de mod√®les utilise une architecture MobileNet V3 et a √©t√© entra√Æn√©e √† l'aide de donn√©es ImageNet. Ce mod√®le utilise un multiplicateur de 0,75 pour la profondeur (nombre de caract√©ristiques) dans les couches convolutives afin d'ajuster le compromis pr√©cision-latence. De plus, MobileNet V3 se d√©cline en deux tailles diff√©rentes, petite et grande, pour adapter le r√©seau aux cas d'utilisation des ressources faibles ou √©lev√©es.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Versions
MobileNet-V3 (petit)	224 x 224	Aucun (float32)	Nouveaut√©s
MobileNet-V3 (large)	224 x 224	Aucun (float32)	Nouveaut√©s
Benchmarks des t√¢ches
Voici les benchmarks de t√¢che pour l'ensemble du pipeline bas√©s sur les mod√®les pr√©-entra√Æn√©s ci-dessus. Le r√©sultat de la latence correspond √† la latence moyenne sur le Pixel 6 √† l'aide du processeur / GPU.

Nom du mod√®le	Latence du processeur	Latence du GPU
MobileNet-V3 (petit)	3,94 ms	7,83 ms
MobileNet-V3 (large)	9,75 ms	9,08 ms
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/01/13 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide d'int√©gration d'images pour Android


La t√¢che MediaPipe Image Embedder vous permet de convertir les donn√©es d'image en repr√©sentation num√©rique pour effectuer des t√¢ches de traitement d'image li√©es au ML, telles que la comparaison de la similarit√© de deux images. Ces instructions vous expliquent comment utiliser l'outil d'int√©gration d'images avec les applications Android.

Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Exemple de code
L'exemple de code MediaPipe Tasks est une impl√©mentation simple d'une application d'int√©gration d'images pour Android. L'exemple utilise l'appareil photo d'un appareil Android physique pour int√©grer des images en continu et peut √©galement ex√©cuter l'outil d'int√©gration sur les fichiers image stock√©s sur l'appareil.

Vous pouvez utiliser l'application comme point de d√©part pour votre propre application Android ou vous y r√©f√©rer lorsque vous modifiez une application existante. L'exemple de code de l'outil d'int√©gration d'images est h√©berg√© sur GitHub.

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple de code √† l'aide de l'outil de ligne de commande git.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Vous pouvez √©galement configurer votre instance git pour utiliser un "checkout sparse" afin de n'avoir que les fichiers de l'exemple d'application Image Embedder:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/image_embedder/android
Apr√®s avoir cr√©√© une version locale de l'exemple de code, vous pouvez importer le projet dans Android Studio et ex√©cuter l'application. Pour obtenir des instructions, consultez le guide de configuration pour Android.

Composants cl√©s
Les fichiers suivants contiennent le code essentiel de cet exemple d'application d'int√©gration d'images:

ImageEmbedderHelper.kt: initialise l'outil d'int√©gration d'images et g√®re le mod√®le et la s√©lection du d√©l√©gu√©.
MainActivity.kt: impl√©mente l'application et assemble les composants de l'interface utilisateur.
Configuration
Cette section d√©crit les √©tapes cl√©s √† suivre pour configurer votre environnement de d√©veloppement et vos projets de code afin d'utiliser Image Embedder. Pour obtenir des informations g√©n√©rales sur la configuration de votre environnement de d√©veloppement pour utiliser les t√¢ches MediaPipe, y compris les exigences concernant la version de la plate-forme, consultez le guide de configuration pour Android.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
D√©pendances
Image Embedder utilise la biblioth√®que com.google.mediapipe:tasks-vision. Ajoutez cette d√©pendance au fichier build.gradle de votre projet de d√©veloppement d'application Android. Importez les d√©pendances requises avec le code suivant:

dependencies {
    ...
    implementation 'com.google.mediapipe:tasks-vision:latest.release'
}
Mod√®le
La t√¢che d'encapsulation d'image MediaPipe n√©cessite un mod√®le entra√Æn√© compatible avec cette t√¢che. Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour Image Embedder, consultez la section Mod√®les de la pr√©sentation de la t√¢che.

S√©lectionnez et t√©l√©chargez le mod√®le, puis stockez-le dans le r√©pertoire de votre projet:

<dev-project-root>/src/main/assets
Remarque :Cet emplacement est recommand√©, car le syst√®me de compilation Android v√©rifie automatiquement les ressources de fichier dans ce r√©pertoire.
Sp√©cifiez le chemin d'acc√®s au mod√®le dans le param√®tre ModelAssetPath. Dans l'exemple de code, le mod√®le est d√©fini dans la fonction setupImageEmbedder() du fichier ImageEmbedderHelper.kt:

Utilisez la m√©thode BaseOptions.Builder.setModelAssetPath() pour sp√©cifier le chemin d'acc√®s utilis√© par le mod√®le. Cette m√©thode est r√©f√©renc√©e dans l'exemple de code de la section suivante.

Cr√©er la t√¢che
Vous pouvez utiliser la fonction createFromOptions pour cr√©er la t√¢che. La fonction createFromOptions accepte des options de configuration pour d√©finir les options d'int√©gration. Pour en savoir plus sur les options de configuration, consultez la section Pr√©sentation de la configuration.

La t√¢che d'insertion d'images accepte trois types de donn√©es d'entr√©e: les images fixes, les fichiers vid√©o et les flux vid√©o en direct. Vous devez sp√©cifier le mode d'ex√©cution correspondant √† votre type de donn√©es d'entr√©e lorsque vous cr√©ez la t√¢che. S√©lectionnez l'onglet correspondant √† votre type de donn√©es d'entr√©e pour d√©couvrir comment cr√©er la t√¢che et ex√©cuter l'inf√©rence.

Image
Vid√©o
Diffusion en direct
ImageEmbedderOptions options =
  ImageEmbedderOptions.builder()
    .setBaseOptions(
      BaseOptions.builder().setModelAssetPath("model.tflite").build())
    .setQuantize(true)
    .setRunningMode(RunningMode.IMAGE)
    .build();
imageEmbedder = ImageEmbedder.createFromOptions(context, options);
    
Remarque :Si vous utilisez le mode de diffusion en direct, vous devez enregistrer un √©couteur de r√©sultats lorsque vous cr√©ez la t√¢che. L'√©couteur est appel√© chaque fois que la t√¢che a termin√© de traiter un frame vid√©o avec le r√©sultat de l'int√©gration et l'image d'entr√©e comme param√®tres.
L'impl√©mentation de l'exemple de code permet √† l'utilisateur de basculer entre les modes de traitement. Cette approche rend le code de cr√©ation de t√¢ches plus complexe et n'est peut-√™tre pas adapt√©e √† votre cas d'utilisation. Vous pouvez voir ce code dans la fonction setupImageEmbedder() du fichier ImageEmbedderHelper.kt.

Options de configuration
Cette t√¢che propose les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
runningMode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
l2_normalize	Indique si le vecteur de caract√©ristiques renvoy√© doit √™tre normalis√© avec la norme L2. N'utilisez cette option que si le mod√®le ne contient pas d√©j√† une op√©ration TFLite L2_NORMALIZATION native. Dans la plupart des cas, c'est d√©j√† le cas, et la normalisation L2 est donc obtenue via l'inf√©rence TFLite sans avoir besoin de cette option.	Boolean	False
quantize	Indique si l'embedding renvoy√© doit √™tre quantifi√© en octets via une quantification scalaire. Les repr√©sentations vectorielles continues sont implicitement suppos√©es avoir une norme unitaire. Par cons√©quent, toute dimension a une valeur comprise dans la plage [-1,0, 1,0]. Utilisez l'option l2_normalize si ce n'est pas le cas.	Boolean	False
resultListener	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats d'int√©gration de mani√®re asynchrone lorsque l'outil d'int√©gration d'images est en mode diffusion en direct. Ne peut √™tre utilis√© que lorsque le mode d'ex√©cution est d√©fini sur LIVE_STREAM.	N/A	Non d√©fini
errorListener	D√©finit un √©couteur d'erreur facultatif.	N/A	Non d√©fini
Pr√©parer les donn√©es
L'outil d'int√©gration d'images fonctionne avec les images, les fichiers vid√©o et les vid√©os en direct. La t√¢che g√®re le pr√©traitement de l'entr√©e des donn√©es, y compris le redimensionnement, la rotation et la normalisation des valeurs.

Vous devez convertir l'image ou le frame d'entr√©e en objet com.google.mediapipe.framework.image.MPImage avant de le transmettre √† la t√¢che d'int√©gration d'images.

Image
Vid√©o
Diffusion en direct
import com.google.mediapipe.framework.image.BitmapImageBuilder;
import com.google.mediapipe.framework.image.MPImage;

// Load an image on the user‚Äôs device as a Bitmap object using BitmapFactory.

// Convert an Android‚Äôs Bitmap object to a MediaPipe‚Äôs Image object.
Image mpImage = new BitmapImageBuilder(bitmap).build();
    
Dans l'exemple de code, la pr√©paration des donn√©es est g√©r√©e dans le fichier ImageEmbedderHelper.kt.

Ex√©cuter la t√¢che
Vous pouvez appeler la fonction embed correspondant √† votre mode d'ex√©cution pour d√©clencher des inf√©rences. L'API Image Embedder renvoie les vecteurs d'embedding pour l'image ou le frame d'entr√©e.

Image
Vid√©o
Diffusion en direct
ImageEmbedderResult embedderResult = imageEmbedder.embed(image);
    
Veuillez noter les points suivants :

Lorsque vous ex√©cutez le mode vid√©o ou le mode diffusion en direct, vous devez √©galement fournir le code temporel du frame d'entr√©e √† la t√¢che d'int√©gration d'images.
Lorsqu'elle s'ex√©cute en mode image ou vid√©o, la t√¢che d'int√©gration d'images bloque le thread actuel jusqu'√† ce qu'elle ait termin√© de traiter l'image ou le frame d'entr√©e. Pour √©viter de bloquer le thread actuel, ex√©cutez le traitement dans un thread en arri√®re-plan.
Lorsqu'elle s'ex√©cute en mode diffusion en direct, la t√¢che d'int√©gration d'images ne bloque pas le thread en cours, mais renvoie imm√©diatement. Il appelle son √©couteur de r√©sultats avec le r√©sultat de la d√©tection chaque fois qu'il a termin√© le traitement d'un frame d'entr√©e. Si la fonction embedAsync est appel√©e lorsque la t√¢che d'int√©gration d'images est occup√©e √† traiter un autre frame, la t√¢che ignore le nouveau frame d'entr√©e.
Dans l'exemple de code, la fonction embed est d√©finie dans le fichier ImageEmbedderHelper.kt.

G√©rer et afficher les r√©sultats
Lors de l'ex√©cution de l'inf√©rence, la t√¢che Image Embedder renvoie un objet ImageEmbedderResult contenant une liste d'embeddings (√† virgule flottante ou √† quantification scalaire) pour l'image d'entr√©e.

Voici un exemple des donn√©es de sortie de cette t√¢che:

ImageEmbedderResult:
  Embedding #0 (sole embedding head):
    float_embedding: {0.0, 0.0, ..., 0.0, 1.0, 0.0, 0.0, 2.0}
    head_index: 0
Ce r√©sultat a √©t√© obtenu en encapsulant l'image suivante:

Plan moyen d&#39;un chat exotique

Vous pouvez comparer la similarit√© de deux repr√©sentations vectorielles continues √† l'aide de la fonction ImageEmbedder.cosineSimilarity. Pour en savoir plus, consultez le code suivant.

// Compute cosine similarity.
double similarity = ImageEmbedder.cosineSimilarity(
  result.embeddingResult().embeddings().get(0),
  otherResult.embeddingResult().embeddings().get(0));
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de d√©tection de visages



Trois personnes dont le visage est mis en √©vidence par des cadres de d√©limitation

La t√¢che de d√©tection des visages MediaPipe vous permet de d√©tecter des visages dans une image ou une vid√©o. Vous pouvez utiliser cette t√¢che pour localiser des visages et des caract√©ristiques faciales dans un cadre. Cette t√¢che utilise un mod√®le de machine learning (ML) qui fonctionne avec des images uniques ou un flux d'images continu. La t√¢che renvoie les emplacements des visages, ainsi que les points cl√©s faciaux suivants: ≈ìil gauche, ≈ìil droit, pointe du nez, bouche, tragus de l'≈ìil gauche et tragus de l'≈ìil droit.

Essayez !arrow_forward

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez l'un de ces guides d'impl√©mentation pour votre plate-forme cible. Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter de mani√®re basique cette t√¢che, y compris un mod√®le recommand√© et un exemple de code avec les options de configuration recommand√©es:

Android ‚Äì Exemple de code ‚Äì Guide
Python ‚Äì Exemple de code ‚Äì Guide
Web ‚Äì Exemple de code ‚Äì Guide
iOS ‚Äì Exemple de code ‚Äì Guide
D√©tails de la t√¢che
Cette section d√©crit les fonctionnalit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Fonctionnalit√©s
Traitement des images d'entr√©e : le traitement comprend la rotation, le redimensionnement, la normalisation et la conversion d'espaces colorim√©triques des images.
Seuil de score : filtrez les r√©sultats en fonction des scores de pr√©diction.
Entr√©es de t√¢che	Sorties de t√¢che
Le d√©tecteur de visage accepte l'un des types de donn√©es suivants:
Images fixes
Images vid√©o d√©cod√©es
Flux vid√©o en direct
Le d√©tecteur de visage renvoie les r√©sultats suivants:
Cadres de d√©limitation des visages d√©tect√©s dans un cadre d'image.
Coordonn√©es de six points de rep√®re du visage pour chaque visage d√©tect√©.
Options de configuration
Cette t√¢che propose les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
running_mode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
min_detection_confidence	Score de confiance minimal pour que la d√©tection de visage soit consid√©r√©e comme r√©ussie.	Float [0,1]	0.5
min_suppression_threshold	Seuil minimal de suppression non maximale pour que la d√©tection de visage soit consid√©r√©e comme chevauch√©e.	Float [0,1]	0.3
result_callback	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats de d√©tection de mani√®re asynchrone lorsque le d√©tecteur de visage est en mode flux en direct. Ne peut √™tre utilis√© que lorsque le mode d'ex√©cution est d√©fini sur LIVE_STREAM.	N/A	Not set
Mod√®les
Les mod√®les de d√©tection des visages peuvent varier en fonction des cas d'utilisation pr√©vus, tels que la d√©tection √† courte et longue port√©e. Les mod√®les font √©galement g√©n√©ralement des compromis entre les performances, la pr√©cision, la r√©solution et les exigences en termes de ressources, et incluent parfois des fonctionnalit√©s suppl√©mentaires.

Les mod√®les list√©s dans cette section sont des variantes de BlazeFace, un d√©tecteur de visage l√©ger et pr√©cis optimis√© pour l'inf√©rence GPU mobile. Les mod√®les BlazeFace sont adapt√©s √† des applications telles que l'estimation des points cl√©s du visage en 3D, la classification des expressions et la segmentation des r√©gions du visage. BlazeFace utilise un r√©seau d'extraction de caract√©ristiques l√©ger semblable √† MobileNetV1/V2.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
BlazeFace (plage courte)
Mod√®le l√©ger permettant de d√©tecter un ou plusieurs visages dans des images de type selfie √† partir d'une cam√©ra de smartphone ou d'une webcam. Le mod√®le est optimis√© pour les images de la cam√©ra avant du t√©l√©phone √† courte port√©e. L'architecture du mod√®le utilise une technique de r√©seau convolutif Single Shot Detector (SSD) avec un encodeur personnalis√©. Pour en savoir plus, consultez l'article de recherche sur le d√©tecteur multicaisse √† une seule image.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Fiche de mod√®le	Versions
BlazeFace (√† courte port√©e)	128 x 128	float 16	info	Nouveaut√©s
BlazeFace (plage compl√®te)
Mod√®le relativement l√©ger permettant de d√©tecter un ou plusieurs visages dans les images d'une cam√©ra de smartphone ou d'une webcam. Le mod√®le est optimis√© pour les images √† plage compl√®te, comme celles prises avec un appareil photo de t√©l√©phone orient√© vers l'arri√®re. L'architecture du mod√®le utilise une technique semblable √† un r√©seau de convolution CenterNet avec un encodeur personnalis√©.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Fiche de mod√®le	Versions
BlazeFace (plage compl√®te)	128 x 128	float 16	infos	Bient√¥t disponible
BlazeFace Sparse (plage compl√®te)
Version all√©g√©e du mod√®le BlazeFace complet standard, d'environ 60% plus petite. Le mod√®le est optimis√© pour les images √† plage compl√®te, comme celles prises avec un appareil photo de t√©l√©phone √† l'arri√®re. L'architecture du mod√®le utilise une technique semblable √† celle d'un r√©seau de convolution CenterNet avec un encodeur personnalis√©.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Fiche de mod√®le	Versions
BlazeFace Sparse (plage compl√®te)	128 x 128	float 16	infos	Bient√¥t disponible
Benchmarks des t√¢ches
Voici les benchmarks de t√¢che pour l'ensemble du pipeline bas√©s sur les mod√®les pr√©-entra√Æn√©s ci-dessus. Le r√©sultat de la latence correspond √† la latence moyenne sur le Pixel 6 √† l'aide du processeur / GPU.

Nom du mod√®le	Latence du processeur	Latence du GPU
BlazeFace (plage courte)	2,94 ms	7,41 ms
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/01/13 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de d√©tection de visages pour Android



La t√¢che de d√©tection des visages MediaPipe vous permet de d√©tecter des visages dans une image ou une vid√©o. Vous pouvez utiliser cette t√¢che pour localiser des visages et des caract√©ristiques faciales dans un cadre. Cette t√¢che utilise un mod√®le de machine learning (ML) qui fonctionne avec des images uniques ou un flux d'images continu. La t√¢che renvoie les emplacements des visages, ainsi que les points cl√©s faciaux suivants: ≈ìil gauche, ≈ìil droit, pointe du nez, bouche, tragus de l'≈ìil gauche et tragus de l'≈ìil droit.

L'exemple de code d√©crit dans ces instructions est disponible sur GitHub. Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Exemple de code
L'exemple de code MediaPipe Tasks est une impl√©mentation simple d'une application de d√©tection des visages pour Android. L'exemple utilise la cam√©ra d'un appareil Android physique pour d√©tecter des visages dans un flux vid√©o continu. L'application peut √©galement d√©tecter des visages dans les images et les vid√©os de la galerie de l'appareil.

Vous pouvez utiliser l'application comme point de d√©part pour votre propre application Android ou vous y r√©f√©rer lorsque vous modifiez une application existante. L'exemple de code du d√©tecteur de visage est h√©berg√© sur GitHub.

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple de code √† l'aide de l'outil de ligne de commande git.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Vous pouvez √©galement configurer votre instance Git pour utiliser un "checkout" clairsem√© afin de n'avoir que les fichiers de l'exemple d'application du d√©tecteur de visage:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/face_detector/android
Apr√®s avoir cr√©√© une version locale de l'exemple de code, vous pouvez importer le projet dans Android Studio et ex√©cuter l'application. Pour obtenir des instructions, consultez le guide de configuration pour Android.

Composants cl√©s
Les fichiers suivants contiennent le code essentiel de cet exemple d'application de d√©tection de visage:

FaceDetectorHelper.kt : initialise le d√©tecteur de visages et g√®re le mod√®le et la s√©lection du d√©l√©gu√©.
CameraFragment.kt : g√®re l'appareil photo de l'appareil et traite les donn√©es d'entr√©e d'image et de vid√©o.
GalleryFragment.kt : interagit avec OverlayView pour afficher l'image ou la vid√©o de sortie.
OverlayView.kt : impl√©mente l'affichage avec des cadres de d√©limitation pour les visages d√©tect√©s.
Configuration
Cette section d√©crit les √©tapes cl√©s √† suivre pour configurer votre environnement de d√©veloppement et vos projets de code sp√©cifiquement pour utiliser le d√©tecteur de visage. Pour obtenir des informations g√©n√©rales sur la configuration de votre environnement de d√©veloppement pour utiliser les t√¢ches MediaPipe, y compris les exigences concernant les versions de la plate-forme, consultez le guide de configuration pour Android.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
D√©pendances
La t√¢che D√©tecteur de visage utilise la biblioth√®que com.google.mediapipe:tasks-vision. Ajoutez cette d√©pendance au fichier build.gradle de votre application Android:

dependencies {
    implementation 'com.google.mediapipe:tasks-vision:latest.release'
}
Mod√®le
La t√¢che de d√©tection de visage MediaPipe n√©cessite un bundle de mod√®le entra√Æn√© compatible avec cette t√¢che. Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour le d√©tecteur de visage, consultez la section Mod√®les de la pr√©sentation de la t√¢che.

S√©lectionnez et t√©l√©chargez le mod√®le, puis stockez-le dans le r√©pertoire de votre projet:

<dev-project-root>/src/main/assets
Sp√©cifiez le chemin d'acc√®s au mod√®le dans le param√®tre ModelAssetPath. Dans l'exemple de code, le mod√®le est d√©fini dans le fichier FaceDetectorHelper.kt:

val modelName = "face_detection_short_range.tflite"
baseOptionsBuilder.setModelAssetPath(modelName)
Cr√©er la t√¢che
La t√¢che du d√©tecteur de visage MediaPipe utilise la fonction createFromOptions() pour configurer la t√¢che. La fonction createFromOptions() accepte des valeurs pour les options de configuration. Pour en savoir plus sur les options de configuration, consultez la section Options de configuration.

Le d√©tecteur de visage est compatible avec les types de donn√©es d'entr√©e suivants: images fixes, fichiers vid√©o et flux vid√©o en direct. Vous devez sp√©cifier le mode d'ex√©cution correspondant au type de donn√©es d'entr√©e lorsque vous cr√©ez la t√¢che. S√©lectionnez l'onglet correspondant √† votre type de donn√©es d'entr√©e pour d√©couvrir comment cr√©er la t√¢che et ex√©cuter l'inf√©rence.

Image
Vid√©o
Diffusion en direct
val baseOptionsBuilder = BaseOptions.builder().setModelAssetPath(modelName)
val baseOptions = baseOptionBuilder.build()

val optionsBuilder =
    FaceDetector.FaceDetectorOptions.builder()
        .setBaseOptions(baseOptionsBuilder.build())
        .setMinDetectionConfidence(threshold)
        .setRunningMode(RunningMode.IMAGE)

val options = optionsBuilder.build()

FaceDetector =
    FaceDetector.createFromOptions(context, options)
    
Remarque :Si vous utilisez le mode de diffusion en direct, vous devez enregistrer un √©couteur de r√©sultats lorsque vous cr√©ez la t√¢che. La t√¢che appelle l'√©couteur lorsqu'elle a termin√© de traiter un frame vid√©o, avec le r√©sultat de la d√©tection et l'image d'entr√©e comme param√®tres.
Remarque :Si vous utilisez le mode vid√©o ou le mode diffusion en direct, le d√©tecteur de visage utilise le suivi pour √©viter de d√©clencher le mod√®le de d√©tection de la paume de la main sur chaque frame, ce qui permet de r√©duire la latence.
L'impl√©mentation de l'exemple de code du d√©tecteur de visage permet √† l'utilisateur de basculer entre les modes de traitement. Cette approche rend le code de cr√©ation de t√¢che plus complexe et n'est peut-√™tre pas adapt√©e √† votre cas d'utilisation. Vous pouvez voir ce code dans la fonction setupFaceDetector() du fichier FaceDetectorHelper.kt.

Options de configuration
Cette t√¢che propose les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
runningMode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
minDetectionConfidence	Score de confiance minimal pour que la d√©tection de visage soit consid√©r√©e comme r√©ussie.	Float [0,1]	0.5
minSuppressionThreshold	Seuil minimal de suppression non maximale pour que la d√©tection de visage soit consid√©r√©e comme chevauch√©e.	Float [0,1]	0.3
resultListener	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats de d√©tection de mani√®re asynchrone lorsque le d√©tecteur de visage est en mode flux en direct. Ne peut √™tre utilis√© que lorsque le mode d'ex√©cution est d√©fini sur LIVE_STREAM.	N/A	Not set
errorListener	D√©finit un √©couteur d'erreur facultatif.	N/A	Not set
Pr√©parer les donn√©es
Le d√©tecteur de visage fonctionne avec les images, les fichiers vid√©o et les diffusions vid√©o en direct. La t√¢che g√®re le pr√©traitement de l'entr√©e des donn√©es, y compris le redimensionnement, la rotation et la normalisation des valeurs.

Le code suivant montre comment transmettre des donn√©es pour traitement. Ces exemples incluent des informations sur la gestion des donn√©es issues d'images, de fichiers vid√©o et de flux vid√©o en direct.

Image
Vid√©o
Diffusion en direct
import com.google.mediapipe.framework.image.BitmapImageBuilder
import com.google.mediapipe.framework.image.MPImage

// Convert the input Bitmap object to an MPImage object to run inference
val mpImage = BitmapImageBuilder(image).build()
    
Dans l'exemple de code du d√©tecteur de visage, la pr√©paration des donn√©es est g√©r√©e dans le fichier FaceDetectorHelper.kt.

Ex√©cuter la t√¢che
En fonction du type de donn√©es avec lequel vous travaillez, utilisez la m√©thode faceDetector.detect...() sp√©cifique √† ce type de donn√©es. Utilisez detect() pour les images individuelles, detectForVideo() pour les images dans les fichiers vid√©o et detectAsync() pour les flux vid√©o. Lorsque vous effectuez des d√©tections sur un flux vid√©o, assurez-vous de les ex√©cuter sur un thread distinct pour √©viter de bloquer le thread de l'interface utilisateur.

Les exemples de code suivants montrent comment ex√©cuter le d√©tecteur de visage dans ces diff√©rents modes de donn√©es:

Image
Vid√©o
Diffusion en direct
val result = faceDetector.detect(mpImage)
    
Veuillez noter les points suivants :

Lorsque vous ex√©cutez le mode vid√©o ou le mode diffusion en direct, vous devez fournir le code temporel du frame d'entr√©e √† la t√¢che de d√©tection des visages.
Lorsqu'elle s'ex√©cute en mode image ou vid√©o, la t√¢che du d√©tecteur de visage bloque le thread actuel jusqu'√† ce qu'elle ait termin√© de traiter l'image ou le frame d'entr√©e. Pour √©viter de bloquer l'interface utilisateur, ex√©cutez le traitement dans un thread en arri√®re-plan.
Lorsqu'elle s'ex√©cute en mode diffusion en direct, la t√¢che du d√©tecteur de visage renvoie imm√©diatement et ne bloque pas le thread en cours. Il appelle l'√©couteur de r√©sultats avec le r√©sultat de la d√©tection chaque fois qu'il a termin√© le traitement d'un frame d'entr√©e. Si la fonction de d√©tection est appel√©e lorsque la t√¢che du d√©tecteur de visage est occup√©e √† traiter un autre frame, la t√¢che ignore le nouveau frame d'entr√©e.
Dans l'exemple de code du d√©tecteur de visage, les fonctions detect, detectForVideo et detectAsync sont d√©finies dans le fichier FaceDetectorHelper.kt.

G√©rer et afficher les r√©sultats
Le d√©tecteur de visages renvoie un objet FaceDetectorResult pour chaque ex√©cution de d√©tection. L'objet de r√©sultat contient des cadres de d√©limitation pour les visages d√©tect√©s et un score de confiance pour chaque visage d√©tect√©.

Voici un exemple des donn√©es de sortie de cette t√¢che:

FaceDetectionResult:
  Detections:
    Detection #0:
      BoundingBox:
        origin_x: 126
        origin_y: 100
        width: 463
        height: 463
      Categories:
        Category #0:
          index: 0
          score: 0.9729152917861938
      NormalizedKeypoints:
        NormalizedKeypoint #0:
          x: 0.18298381567001343
          y: 0.2961040139198303
        NormalizedKeypoint #1:
          x: 0.3302789330482483
          y: 0.29289937019348145
        ... (6 keypoints for each face)
    Detection #1:
      BoundingBox:
        origin_x: 616
        origin_y: 193
        width: 430
        height: 430
      Categories:
        Category #0:
          index: 0
          score: 0.9251380562782288
      NormalizedKeypoints:
        NormalizedKeypoint #0:
          x: 0.6151331663131714
          y: 0.3713381886482239
        NormalizedKeypoint #1:
          x: 0.7460576295852661
          y: 0.38825345039367676
        ... (6 keypoints for each face)
L'image suivante pr√©sente une visualisation du r√©sultat de la t√¢che:

Deux enfants avec des rectangles autour de leur visage

Pour voir l'image sans cadres de d√©limitation, consultez l'image d'origine.

L'exemple de code du d√©tecteur de visage montre comment afficher les r√©sultats renvoy√©s par la t√¢che. Pour en savoir plus, consultez la classe OverlayView.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de d√©tection des points de rep√®re

T√¢che de reconnaissance faciale

La t√¢che MediaPipe Face Marker vous permet de d√©tecter des points de rep√®re et des expressions faciales dans des images et des vid√©os. Vous pouvez utiliser cette t√¢che pour identifier des expressions du visage, appliquer des filtres et des effets de visage, et cr√©er des avatars virtuels. Cette t√¢che utilise des mod√®les de machine learning (ML) pouvant fonctionner avec des images uniques ou un flux continu d'images. La t√¢che g√©n√®re des points de rep√®re de visage tridimensionnels, des scores Blendshape (coefficients repr√©sentant l'expression faciale) pour d√©duire les surfaces faciales d√©taill√©es en temps r√©el, ainsi que des matrices de transformation pour effectuer les transformations requises pour le rendu des effets.

Essayerarrow_forward

Commencer
Commencez √† utiliser cette t√¢che en suivant l'un des guides d'impl√©mentation correspondant √† votre plate-forme cible. Ces guides sp√©cifiques √† la plate-forme vous guident tout au long d'une impl√©mentation de base de cette t√¢che, y compris un mod√®le et un exemple de code recommand√©s, ainsi que les options de configuration recommand√©es:

Android ‚Äì Exemple de code ‚Äì Guide
Python ‚Äì Exemple de code ‚Äì Guide
Web ‚Äì Exemple de code ‚Äì Guide
D√©tails de la t√¢che
Cette section d√©crit les capacit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Caract√©ristiques
Traitement de l'image d'entr√©e : le traitement comprend la rotation, le redimensionnement, la normalisation et la conversion de l'espace colorim√©trique de l'image.
Seuil de score : filtre les r√©sultats en fonction des scores de pr√©diction.
Entr√©es des t√¢ches	Sorties de t√¢ches
Le service de rep√®re de visage accepte l'un des types de donn√©es suivants:
Images fixes
Images vid√©o d√©cod√©es
Flux vid√©o en direct
L'outil de reconnaissance de visage g√©n√®re les r√©sultats suivants:
Cadres de d√©limitation pour les visages d√©tect√©s dans un cadre d'image.
Un maillage complet pour chaque visage d√©tect√©, avec des scores de fusion qui indiquent les expressions faciales et les coordonn√©es des rep√®res faciaux
Options de configuration
Cette t√¢che comporte les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
running_mode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode pour les images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct de donn√©es d'entr√©e, issues par exemple d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
num_faces	Nombre maximal de visages pouvant √™tre d√©tect√©s par la propri√©t√© FaceLandmarker. Le lissage n'est appliqu√© que lorsque num_faces est d√©fini sur 1.	Integer > 0	1
min_face_detection_confidence	Score de confiance minimal pour que la d√©tection de visages soit consid√©r√©e comme r√©ussie.	Float [0.0,1.0]	0.5
min_face_presence_confidence	Score de confiance minimal du score de pr√©sence de visage dans la d√©tection des points de rep√®re de visage.	Float [0.0,1.0]	0.5
min_tracking_confidence	Score de confiance minimal pour que le suivi du visage soit consid√©r√© comme r√©ussi.	Float [0.0,1.0]	0.5
output_face_blendshapes	Indique si l'outil de rep√®re de visage g√©n√®re des combinaisons de visages. Les combinaisons de visage sont utilis√©es pour afficher le mod√®le de visage 3D.	Boolean	False
output_facial_transformation_matrixes	Indique si FaceLandMarker g√©n√®re la matrice de transformation du visage. FaceLandMarker utilise la matrice pour transformer les points de rep√®re de visage d'une empreinte faciale canonique en visage d√©tect√©, afin que les utilisateurs puissent appliquer des effets sur ces points de rep√®re.	Boolean	False
result_callback	D√©finit l'√©couteur de r√©sultats pour recevoir les r√©sultats du point de rep√®re de mani√®re asynchrone lorsque FaceLandMarker est en mode de diffusion en direct. Ne peut √™tre utilis√© que lorsque le mode En cours d'ex√©cution est d√©fini sur LIVE_STREAM	ResultListener	N/A
Mod√®les
Cet outil utilise une s√©rie de mod√®les pour pr√©dire les points de rep√®re de visage. Le premier mod√®le d√©tecte les visages, un deuxi√®me mod√®le localise les points de rep√®re sur les visages d√©tect√©s, et un troisi√®me mod√®le les utilise pour identifier les caract√©ristiques et les expressions du visage.

Les mod√®les suivants sont empaquet√©s dans un lot de mod√®les t√©l√©chargeable:

Mod√®le de d√©tection de visages: d√©tecte la pr√©sence de visages √† l'aide de quelques points de rep√®re faciales cl√©s.
Mod√®le du maillage de visages: ajoute un mappage complet du visage. Le mod√®le g√©n√®re une estimation de 478 points de rep√®re de face tridimensionnels.
Mod√®le de pr√©diction Blendshape: re√ßoit les r√©sultats du mod√®le de maillage de visages. Il pr√©dit 52 scores Blendshape, qui sont des coefficients repr√©sentant diff√©rentes expressions faciales.
Le mod√®le de d√©tection de visages est le mod√®le BlazeFace √† courte port√©e, un d√©tecteur de visages l√©ger et pr√©cis optimis√© pour l'inf√©rence GPU sur mobile. Pour en savoir plus, consultez la section D√©tecteur de visages.

L'image ci-dessous montre une cartographie compl√®te des points de rep√®re faciaux √† partir de la sortie du groupe de mod√®les.

Faire face √† des points-cl√©s du rep√®re

Pour une vue plus d√©taill√©e des points de rep√®re de visage, consultez l'image en taille r√©elle.

Attention:Cette preview des solutions MediaPipe est une version pr√©liminaire. En savoir plus
Lot de mod√®les	Forme de saisie	Type de donn√©es	Fiches de mod√®le	Versions
FaceLandmarker	FaceDetector: 192 x 192
FaceMesh-V2: 256 x 256
Blendshape: 1 x 146 x 2	nombre d√©cimal 16	FaceDetector
FaceMesh-V2
Blendshape
Les plus r√©cents
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2024/05/21 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..


Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de d√©tection des points de rep√®re de visage pour Android



La t√¢che de d√©tection des points de rep√®re sur le visage MediaPipe vous permet de d√©tecter les points de rep√®re sur le visage et les expressions faciales dans les images et les vid√©os. Vous pouvez utiliser cette t√¢che pour identifier les expressions faciales humaines, appliquer des filtres et des effets sur le visage, et cr√©er des avatars virtuels. Cette t√¢che utilise des mod√®les de machine learning (ML) qui peuvent fonctionner avec des images uniques ou un flux d'images continu. La t√¢che produit des rep√®res faciaux tridimensionnels, des scores de blendshape (coefficients repr√©sentant l'expression faciale) pour inf√©rer des surfaces faciales d√©taill√©es en temps r√©el et des matrices de transformation pour effectuer les transformations requises pour le rendu des effets.

L'exemple de code d√©crit dans ces instructions est disponible sur GitHub. Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Exemple de code
L'exemple de code MediaPipe Tasks est une impl√©mentation simple d'une application de rep√®re facial pour Android. L'exemple utilise la cam√©ra d'un appareil Android physique pour d√©tecter des visages dans un flux vid√©o continu. L'application peut √©galement d√©tecter des visages dans les images et les vid√©os de la galerie de l'appareil.

Vous pouvez utiliser l'application comme point de d√©part pour votre propre application Android ou vous y r√©f√©rer lorsque vous modifiez une application existante. L'exemple de code de Face Landmarker est h√©berg√© sur GitHub.

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple de code √† l'aide de l'outil de ligne de commande git.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Vous pouvez √©ventuellement configurer votre instance git pour qu'elle utilise un "checkout sparse", de sorte que vous n'ayez que les fichiers de l'application exemple Face Landmarker:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/face_landmarker/android
Apr√®s avoir cr√©√© une version locale de l'exemple de code, vous pouvez importer le projet dans Android Studio et ex√©cuter l'application. Pour obtenir des instructions, consultez le guide de configuration pour Android.

Composants cl√©s
Les fichiers suivants contiennent le code essentiel de cet exemple d'application de rep√®re de visage:

FaceLandmarkerHelper.kt : initialise le point de rep√®re du visage et g√®re le mod√®le et la s√©lection du d√©l√©gu√©.
CameraFragment.kt : g√®re l'appareil photo de l'appareil et traite les donn√©es d'entr√©e d'image et de vid√©o.
GalleryFragment.kt : interagit avec OverlayView pour afficher l'image ou la vid√©o de sortie.
OverlayView.kt : impl√©mente l'affichage avec un maillage de visage pour les visages d√©tect√©s.
Configuration
Cette section d√©crit les √©tapes cl√©s pour configurer votre environnement de d√©veloppement et vos projets de code sp√©cifiquement pour utiliser Face Landmarker. Pour obtenir des informations g√©n√©rales sur la configuration de votre environnement de d√©veloppement pour utiliser les t√¢ches MediaPipe, y compris les exigences concernant la version de la plate-forme, consultez le guide de configuration pour Android.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
D√©pendances
La t√¢che Face Landmarker utilise la biblioth√®que com.google.mediapipe:tasks-vision. Ajoutez cette d√©pendance au fichier build.gradle de votre application Android:

dependencies {
    implementation 'com.google.mediapipe:tasks-vision:latest.release'
}
Mod√®le
La t√¢che de rep√®re facial MediaPipe n√©cessite un bundle de mod√®le entra√Æn√© compatible avec cette t√¢che. Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour Face Landmarker, consultez la section Mod√®les de la pr√©sentation de la t√¢che.

S√©lectionnez et t√©l√©chargez le mod√®le, puis stockez-le dans le r√©pertoire de votre projet:

<dev-project-root>/src/main/assets
Sp√©cifiez le chemin d'acc√®s au mod√®le dans le param√®tre ModelAssetPath. Dans l'exemple de code, le mod√®le est d√©fini dans le fichier FaceLandmarkerHelper.kt:

baseOptionsBuilder.setModelAssetPath(MP_FACE_LANDMARKER_TASK)
Cr√©er la t√¢che
La t√¢che de rep√®re facial MediaPipe utilise la fonction createFromOptions() pour configurer la t√¢che. La fonction createFromOptions() accepte des valeurs pour les options de configuration. Pour en savoir plus sur les options de configuration, consultez la section Options de configuration.

Le d√©tecteur de points de rep√®re du visage accepte les types de donn√©es d'entr√©e suivants: images fixes, fichiers vid√©o et flux vid√©o en direct. Vous devez sp√©cifier le mode d'ex√©cution correspondant au type de donn√©es d'entr√©e lorsque vous cr√©ez la t√¢che. S√©lectionnez l'onglet correspondant √† votre type de donn√©es d'entr√©e pour d√©couvrir comment cr√©er la t√¢che et ex√©cuter l'inf√©rence.

Image
Vid√©o
Diffusion en direct
val baseOptionsBuilder = BaseOptions.builder().setModelAssetPath(MP_FACE_LANDMARKER_TASK)
val baseOptions = baseOptionBuilder.build()

val optionsBuilder = 
    FaceLandmarker.FaceLandmarkerOptions.builder()
        .setBaseOptions(baseOptionsBuilder.build())
        .setMinFaceDetectionConfidence(minFaceDetectionConfidence)
        .setMinTrackingConfidence(minFaceTrackingConfidence)
        .setMinFacePresenceConfidence(minFacePresenceConfidence)
        .setNumFaces(maxNumFaces)
        .setRunningMode(RunningMode.IMAGE)

val options = optionsBuilder.build()
FaceLandmarker = FaceLandmarker.createFromOptions(context, options)
    
Remarque :Si vous utilisez le mode de diffusion en direct, vous devez enregistrer un √©couteur de r√©sultats lorsque vous cr√©ez la t√¢che. La t√¢che appelle l'√©couteur une fois qu'elle a termin√© le traitement d'un frame de la cam√©ra, avec le r√©sultat de la d√©tection et l'image d'entr√©e comme param√®tres.
Remarque :Si vous utilisez le mode vid√©o ou le mode de diffusion en direct, Face Landmarker utilise le suivi pour √©viter de d√©clencher le mod√®le de d√©tection de la paume de la main sur chaque frame, ce qui permet de r√©duire la latence.
L'impl√©mentation de l'exemple de code de Face Landmarker permet √† l'utilisateur de basculer entre les modes de traitement. Cette approche rend le code de cr√©ation de t√¢che plus complexe et n'est peut-√™tre pas adapt√©e √† votre cas d'utilisation. Vous pouvez voir ce code dans la fonction setupFaceLandmarker() du fichier FaceLandmarkerHelper.kt.

Options de configuration
Cette t√¢che propose les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
runningMode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
numFaces	Nombre maximal de visages pouvant √™tre d√©tect√©s par FaceLandmarker. Le lissage n'est appliqu√© que lorsque num_faces est d√©fini sur 1.	Integer > 0	1
minFaceDetectionConfidence	Score de confiance minimal pour que la d√©tection de visage soit consid√©r√©e comme r√©ussie.	Float [0.0,1.0]	0.5
minFacePresenceConfidence	Score de confiance minimal du score de pr√©sence de visage dans la d√©tection des points de rep√®re du visage.	Float [0.0,1.0]	0.5
minTrackingConfidence	Score de confiance minimal pour que le suivi du visage soit consid√©r√© comme r√©ussi.	Float [0.0,1.0]	0.5
outputFaceBlendshapes	Indique si le pointeur de rep√®re du visage g√©n√®re des blendshapes de visage. Les blendshapes de visage sont utilis√©es pour afficher le mod√®le de visage 3D.	Boolean	False
outputFacialTransformationMatrixes	Indique si FaceLandmarker g√©n√®re la matrice de transformation du visage. FaceLandmarker utilise la matrice pour transformer les points de rep√®re du visage d'un mod√®le de visage canonique en visage d√©tect√© afin que les utilisateurs puissent appliquer des effets sur les points de rep√®re d√©tect√©s.	Boolean	False
resultListener	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats du rep√®re de mani√®re asynchrone lorsque FaceLandmarker est en mode diffusion en direct. Ne peut √™tre utilis√© que lorsque le mode d'ex√©cution est d√©fini sur LIVE_STREAM.	ResultListener	N/A
errorListener	D√©finit un √©couteur d'erreur facultatif.	ErrorListener	N/A
Pr√©parer les donn√©es
Face Landmarker fonctionne avec les images, les fichiers vid√©o et les diffusions vid√©o en direct. La t√¢che g√®re le pr√©traitement de l'entr√©e des donn√©es, y compris le redimensionnement, la rotation et la normalisation des valeurs.

Le code suivant montre comment transmettre des donn√©es pour traitement. Ces exemples incluent des informations sur la gestion des donn√©es issues d'images, de fichiers vid√©o et de flux vid√©o en direct.

Image
Vid√©o
Diffusion en direct
import com.google.mediapipe.framework.image.BitmapImageBuilder
import com.google.mediapipe.framework.image.MPImage

// Convert the input Bitmap object to an MPImage object to run inference
val mpImage = BitmapImageBuilder(image).build()
    
Dans l'exemple de code de Face Landmarker, la pr√©paration des donn√©es est g√©r√©e dans le fichier FaceLandmarkerHelper.kt.

Ex√©cuter la t√¢che
En fonction du type de donn√©es avec lequel vous travaillez, utilisez la m√©thode FaceLandmarker.detect...() sp√©cifique √† ce type de donn√©es. Utilisez detect() pour les images individuelles, detectForVideo() pour les images dans les fichiers vid√©o et detectAsync() pour les flux vid√©o. Lorsque vous effectuez des d√©tections sur un flux vid√©o, assurez-vous de les ex√©cuter sur un thread distinct pour √©viter de bloquer le thread de l'interface utilisateur.

Les exemples de code suivants montrent comment ex√©cuter Face Landmarker dans ces diff√©rents modes de donn√©es:

Image
Vid√©o
Diffusion en direct
val result = FaceLandmarker.detect(mpImage)
    
Veuillez noter les points suivants :

Lorsque vous ex√©cutez le mode vid√©o ou le mode diffusion en direct, vous devez fournir le code temporel du frame d'entr√©e √† la t√¢che de rep√®re facial.
Lorsqu'elle s'ex√©cute en mode image ou vid√©o, la t√¢che de rep√®re du visage bloque le thread actuel jusqu'√† ce qu'elle ait termin√© de traiter l'image ou le frame d'entr√©e. Pour √©viter de bloquer l'interface utilisateur, ex√©cutez le traitement dans un thread en arri√®re-plan.
Lorsqu'elle s'ex√©cute en mode diffusion en direct, la t√¢che de rep√®re du visage renvoie imm√©diatement et ne bloque pas le thread en cours. Il appelle l'√©couteur de r√©sultats avec le r√©sultat de la d√©tection chaque fois qu'il a termin√© le traitement d'un frame d'entr√©e.
Dans l'exemple de code de Face Landmarker, les fonctions detect, detectForVideo et detectAsync sont d√©finies dans le fichier FaceLandmarkerHelper.kt.

G√©rer et afficher les r√©sultats
Le d√©tecteur de points de rep√®re du visage renvoie un objet FaceLandmarkerResult pour chaque ex√©cution de d√©tection. L'objet de r√©sultat contient un maillage de visage pour chaque visage d√©tect√©, avec les coordonn√©es de chaque rep√®re de visage. L'objet de r√©sultat peut √©galement contenir des blendshapes, qui d√©signent les expressions faciales, et des matrices de transformation du visage pour appliquer des effets sur le visage aux points de rep√®re d√©tect√©s.

Voici un exemple des donn√©es de sortie de cette t√¢che:

FaceLandmarkerResult:
  face_landmarks:
    NormalizedLandmark #0:
      x: 0.5971359014511108
      y: 0.485361784696579
      z: -0.038440968841314316
    NormalizedLandmark #1:
      x: 0.3302789330482483
      y: 0.29289937019348145
      z: -0.09489090740680695
    ... (478 landmarks for each face)
  face_blendshapes:
    browDownLeft: 0.8296722769737244
    browDownRight: 0.8096957206726074
    browInnerUp: 0.00035583582939580083
    browOuterUpLeft: 0.00035752105759456754
    ... (52 blendshapes for each face)
  facial_transformation_matrixes:
    [9.99158978e-01, -1.23036895e-02, 3.91213447e-02, -3.70770246e-01]
    [1.66496094e-02,  9.93480563e-01, -1.12779640e-01, 2.27719707e+01]
    ...
L'image suivante pr√©sente une visualisation du r√©sultat de la t√¢che:

Homme dont les r√©gions du visage sont cartographi√©es g√©om√©triquement pour indiquer sa forme et ses dimensions

L'exemple de code de la fonctionnalit√© Face Landmarker montre comment afficher les r√©sultats renvoy√©s par la t√¢che. Pour en savoir plus, consultez la classe OverlayView.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de d√©tection des points de rep√®re des postures

Femme dans une position m√©ditative. Sa pose est mise en √©vidence par une maquette fonctionnelle qui indique le positionnement de ses membres et de son torse.

La t√¢che MediaPipe Pose Landmarker vous permet de d√©tecter les points de rep√®re des corps humains dans une image ou une vid√©o. Vous pouvez utiliser cette t√¢che pour identifier les principaux points du corps, analyser la posture et cat√©goriser les mouvements. Cette t√¢che utilise des mod√®les de machine learning (ML) qui fonctionnent avec des images ou des vid√©os uniques. La t√¢che g√©n√®re des rep√®res de position du corps en coordonn√©es d'image et en coordonn√©es mondiales tridimensionnelles.

Essayez !arrow_forward

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez le guide d'impl√©mentation de votre plate-forme cible. Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter de mani√®re basique cette t√¢che, y compris un mod√®le recommand√© et un exemple de code avec les options de configuration recommand√©es:

Android ‚Äì Exemple de code ‚Äì Guide
Python ‚Äì Exemple de code ‚Äì Guide
Web ‚Äì Exemple de code ‚Äì Guide
D√©tails de la t√¢che
Cette section d√©crit les fonctionnalit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Fonctionnalit√©s
Traitement des images d'entr√©e : le traitement comprend la rotation, le redimensionnement, la normalisation et la conversion d'espaces colorim√©triques.
Seuil de score : filtrez les r√©sultats en fonction des scores de pr√©diction.
Entr√©es de t√¢che	Sorties de t√¢che
Le rep√®re de position accepte l'un des types de donn√©es suivants:
Images fixes
Images vid√©o d√©cod√©es
Flux vid√©o en direct
Le Pose Landmarker g√©n√®re les r√©sultats suivants:
Points de rep√®re de la pose en coordonn√©es d'image normalis√©es
Placer des rep√®res en coordonn√©es mondiales
Facultatif: un masque de segmentation pour la pose.
Options de configuration
Cette t√¢che propose les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
running_mode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
num_poses	Nombre maximal de poses pouvant √™tre d√©tect√©es par le rep√®re de pose.	Integer > 0	1
min_pose_detection_confidence	Score de confiance minimal pour que la d√©tection de la position soit consid√©r√©e comme r√©ussie.	Float [0.0,1.0]	0.5
min_pose_presence_confidence	Score de confiance minimal de la pr√©sence de la pose dans la d√©tection des rep√®res de la pose.	Float [0.0,1.0]	0.5
min_tracking_confidence	Score de confiance minimal pour que le suivi de la position soit consid√©r√© comme r√©ussi.	Float [0.0,1.0]	0.5
output_segmentation_masks	Indique si Pose Landmarker g√©n√®re un masque de segmentation pour la position d√©tect√©e.	Boolean	False
result_callback	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats du rep√®re de mani√®re asynchrone lorsque le rep√®re de pose est en mode diffusion en direct. Ne peut √™tre utilis√© que lorsque le mode d'ex√©cution est d√©fini sur LIVE_STREAM.	ResultListener	N/A
Mod√®les
Le d√©tecteur de rep√®res de position utilise une s√©rie de mod√®les pour pr√©dire les rep√®res de position. Le premier mod√®le d√©tecte la pr√©sence de corps humains dans un cadre d'image, et le second mod√®le localise des rep√®res sur les corps.

Les mod√®les suivants sont regroup√©s dans un lot de mod√®les t√©l√©chargeable:

Mod√®le de d√©tection des postures: d√©tecte la pr√©sence de corps avec quelques rep√®res cl√©s de la posture.
Mod√®le de rep√®re de pose: ajoute une cartographie compl√®te de la pose. Le mod√®le g√©n√®re une estimation de 33 rep√®res de position tridimensionnels.
Ce bundle utilise un r√©seau de neurones convolutifs semblable √† MobileNetV2 et est optimis√© pour les applications de fitness en temps r√©el sur l'appareil. Cette variante du mod√®le BlazePose utilise GHUM, un pipeline de mod√©lisation de la forme humaine en 3D, pour estimer la position compl√®te du corps d'un individu dans des images ou des vid√©os.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Groupe de mod√®les	Forme d'entr√©e	Type de donn√©es	Fiches de mod√®le	Versions
Pos Marker (lite)	D√©tecteur de position: 224 x 224 x 3
D√©limiteur de position: 256 x 256 x 3	float 16	info	Nouveaut√©s
Pos (Full)	D√©tecteur de position: 224 x 224 x 3
D√©limiteur de position: 256 x 256 x 3	float 16	info	Nouveaut√©s
Pos Marker (Heavy)	D√©tecteur de position: 224 x 224 x 3
D√©limiteur de position: 256 x 256 x 3	float 16	info	Nouveaut√©s
Mod√®le de rep√®re de posture
Le mod√®le de rep√®res de position suit 33 rep√®res corporels, qui repr√©sentent l'emplacement approximatif des parties du corps suivantes:



0 - nose
1 - left eye (inner)
2 - left eye
3 - left eye (outer)
4 - right eye (inner)
5 - right eye
6 - right eye (outer)
7 - left ear
8 - right ear
9 - mouth (left)
10 - mouth (right)
11 - left shoulder
12 - right shoulder
13 - left elbow
14 - right elbow
15 - left wrist
16 - right wrist
17 - left pinky
18 - right pinky
19 - left index
20 - right index
21 - left thumb
22 - right thumb
23 - left hip
24 - right hip
25 - left knee
26 - right knee
27 - left ankle
28 - right ankle
29 - left heel
30 - right heel
31 - left foot index
32 - right foot index
La sortie du mod√®le contient √† la fois des coordonn√©es normalis√©es (Landmarks) et des coordonn√©es mondiales (WorldLandmarks) pour chaque rep√®re.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/01/13 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de d√©tection des points de rep√®re de position pour Android



La t√¢che MediaPipe Pose Landmarker vous permet de d√©tecter les points de rep√®re des corps humains dans une image ou une vid√©o. Vous pouvez utiliser cette t√¢che pour identifier les principaux points du corps, analyser la posture et cat√©goriser les mouvements. Cette t√¢che utilise des mod√®les de machine learning (ML) qui fonctionnent avec des images ou des vid√©os uniques. La t√¢che g√©n√®re des rep√®res de position du corps en coordonn√©es d'image et en coordonn√©es mondiales tridimensionnelles.

L'exemple de code d√©crit dans ces instructions est disponible sur GitHub. Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Exemple de code
L'exemple de code MediaPipe Tasks est une impl√©mentation simple d'une application de rep√®re de pose pour Android. L'exemple utilise la cam√©ra d'un appareil Android physique pour d√©tecter les poses dans un flux vid√©o continu. L'application peut √©galement d√©tecter les poses dans les images et les vid√©os de la galerie de l'appareil.

Vous pouvez utiliser l'application comme point de d√©part pour votre propre application Android ou vous y r√©f√©rer lorsque vous modifiez une application existante. L'exemple de code de Pose Landmarker est h√©berg√© sur GitHub.

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple de code √† l'aide de l'outil de ligne de commande git.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Vous pouvez √©ventuellement configurer votre instance git pour utiliser un "checkout sparse" afin de n'avoir que les fichiers de l'application exemple Pose Landmarker:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/pose_landmarker/android
Apr√®s avoir cr√©√© une version locale de l'exemple de code, vous pouvez importer le projet dans Android Studio et ex√©cuter l'application. Pour obtenir des instructions, consultez le guide de configuration pour Android.

Composants cl√©s
Les fichiers suivants contiennent le code essentiel de cet exemple d'application de rep√®re de pose:

PoseLandmarkerHelper.kt : initialise le rep√®re de position et g√®re le mod√®le et la s√©lection du d√©l√©gu√©.
CameraFragment.kt : g√®re l'appareil photo de l'appareil et traite les donn√©es d'entr√©e d'image et de vid√©o.
GalleryFragment.kt : interagit avec OverlayView pour afficher l'image ou la vid√©o de sortie.
OverlayView.kt : impl√©mente l'affichage des poses d√©tect√©es.
Configuration
Cette section d√©crit les √©tapes cl√©s √† suivre pour configurer votre environnement de d√©veloppement et vos projets de code sp√©cifiquement pour utiliser Pose Landmarker. Pour obtenir des informations g√©n√©rales sur la configuration de votre environnement de d√©veloppement pour utiliser les t√¢ches MediaPipe, y compris les exigences concernant la version de la plate-forme, consultez le guide de configuration pour Android.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
D√©pendances
La t√¢che Pose Landmarker utilise la biblioth√®que com.google.mediapipe:tasks-vision. Ajoutez cette d√©pendance au fichier build.gradle de votre application Android:

dependencies {
    implementation 'com.google.mediapipe:tasks-vision:latest.release'
}
Mod√®le
La t√¢che de rep√®re de pose MediaPipe n√©cessite un bundle de mod√®le entra√Æn√© compatible avec cette t√¢che. Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour Pose Landmarker, consultez la section Mod√®les de la pr√©sentation de la t√¢che.

S√©lectionnez et t√©l√©chargez le mod√®le, puis stockez-le dans le r√©pertoire de votre projet:

<dev-project-root>/src/main/assets
Sp√©cifiez le chemin d'acc√®s au mod√®le dans le param√®tre ModelAssetPath. Dans l'exemple de code, le mod√®le est d√©fini dans le fichier PoseLandmarkerHelper.kt:

val modelName = "pose_landmarker_lite.task"
baseOptionsBuilder.setModelAssetPath(modelName)
Cr√©er la t√¢che
La t√¢che MediaPipe Pose Landmarker utilise la fonction createFromOptions() pour configurer la t√¢che. La fonction createFromOptions() accepte des valeurs pour les options de configuration. Pour en savoir plus sur les options de configuration, consultez la section Options de configuration.

Le rep√®re de pose est compatible avec les types de donn√©es d'entr√©e suivants: images fixes, fichiers vid√©o et flux vid√©o en direct. Vous devez sp√©cifier le mode d'ex√©cution correspondant au type de donn√©es d'entr√©e lorsque vous cr√©ez la t√¢che. S√©lectionnez l'onglet correspondant √† votre type de donn√©es d'entr√©e pour d√©couvrir comment cr√©er la t√¢che.

Image
Vid√©o
Diffusion en direct
val baseOptionsBuilder = BaseOptions.builder().setModelAssetPath(modelName)
val baseOptions = baseOptionBuilder.build()

val optionsBuilder = 
    poseLandmarker.poseLandmarkerOptions.builder()
        .setBaseOptions(baseOptionsBuilder.build())
        .setMinPoseDetectionConfidence(minPoseDetectionConfidence)
        .setMinTrackingConfidence(minPoseTrackingConfidence)
        .setMinPosePresenceConfidence(minposePresenceConfidence)
        .setNumPoses(maxNumPoses)
        .setRunningMode(RunningMode.IMAGE)

val options = optionsBuilder.build()
poseLandmarker = poseLandmarker.createFromOptions(context, options)
    
Remarque :Si vous utilisez le mode de diffusion en direct, vous devez enregistrer un √©couteur de r√©sultats lorsque vous cr√©ez la t√¢che. La t√¢che appelle l'√©couteur une fois qu'elle a termin√© le traitement d'un frame de la cam√©ra, avec le r√©sultat de la d√©tection et l'image d'entr√©e comme param√®tres.
Remarque :Si vous utilisez le mode vid√©o ou le mode de diffusion en direct, Pose Landmarker utilise le suivi pour √©viter de d√©clencher le mod√®le de d√©tection de la paume de la main sur chaque frame, ce qui permet de r√©duire la latence.
L'impl√©mentation de l'exemple de code du rep√®re de position permet √† l'utilisateur de basculer entre les modes de traitement. Cette approche rend le code de cr√©ation de t√¢che plus complexe et n'est peut-√™tre pas adapt√©e √† votre cas d'utilisation. Vous pouvez voir ce code dans la fonction setupPoseLandmarker() du fichier PoseLandmarkerHelper.kt.

Options de configuration
Cette t√¢che propose les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
runningMode	D√©finit le mode d'ex√©cution de la t√¢che. Il existe trois modes:

IMAGE: mode pour les entr√©es d'une seule image.

VIDEO: mode des images d√©cod√©es d'une vid√©o.

LIVE_STREAM: mode de diffusion en direct des donn√©es d'entr√©e, par exemple √† partir d'une cam√©ra. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de mani√®re asynchrone.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
numposes	Nombre maximal de poses pouvant √™tre d√©tect√©es par le rep√®re de pose.	Integer > 0	1
minPoseDetectionConfidence	Score de confiance minimal pour que la d√©tection de la position soit consid√©r√©e comme r√©ussie.	Float [0.0,1.0]	0.5
minPosePresenceConfidence	Score de confiance minimal de la pr√©sence de la pose dans la d√©tection des rep√®res de la pose.	Float [0.0,1.0]	0.5
minTrackingConfidence	Score de confiance minimal pour que le suivi de la position soit consid√©r√© comme r√©ussi.	Float [0.0,1.0]	0.5
outputSegmentationMasks	Indique si Pose Landmarker g√©n√®re un masque de segmentation pour la position d√©tect√©e.	Boolean	False
resultListener	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats du rep√®re de mani√®re asynchrone lorsque le rep√®re de pose est en mode diffusion en direct. Ne peut √™tre utilis√© que lorsque le mode d'ex√©cution est d√©fini sur LIVE_STREAM.	ResultListener	N/A
errorListener	D√©finit un √©couteur d'erreur facultatif.	ErrorListener	N/A
Pr√©parer les donn√©es
Pose Landmarker fonctionne avec les images, les fichiers vid√©o et les diffusions vid√©o en direct. La t√¢che g√®re le pr√©traitement de l'entr√©e des donn√©es, y compris le redimensionnement, la rotation et la normalisation des valeurs.

Le code suivant montre comment transmettre des donn√©es pour traitement. Ces exemples incluent des informations sur la gestion des donn√©es issues d'images, de fichiers vid√©o et de flux vid√©o en direct.

Image
Vid√©o
Diffusion en direct
import com.google.mediapipe.framework.image.BitmapImageBuilder
import com.google.mediapipe.framework.image.MPImage

// Convert the input Bitmap object to an MPImage object to run inference
val mpImage = BitmapImageBuilder(image).build()
    
Dans l'exemple de code du rep√®re de position, la pr√©paration des donn√©es est g√©r√©e dans le fichier PoseLandmarkerHelper.kt.

Ex√©cuter la t√¢che
En fonction du type de donn√©es avec lequel vous travaillez, utilisez la m√©thode poseLandmarker.detect...() sp√©cifique √† ce type de donn√©es. Utilisez detect() pour les images individuelles, detectForVideo() pour les images dans les fichiers vid√©o et detectAsync() pour les flux vid√©o. Lorsque vous effectuez des d√©tections sur un flux vid√©o, assurez-vous d'ex√©cuter les d√©tections sur un thread distinct pour √©viter de bloquer le thread d'interposition de l'utilisateur.

Les exemples de code suivants montrent comment ex√©cuter Pose Landmarker dans ces diff√©rents modes de donn√©es:

Image
Vid√©o
Diffusion en direct
val result = poseLandmarker.detect(mpImage)
    
Veuillez noter les points suivants :

Lorsque vous ex√©cutez le mode vid√©o ou le mode diffusion en direct, vous devez fournir le code temporel du frame d'entr√©e √† la t√¢che de rep√®re de pose.
Lorsqu'elle s'ex√©cute en mode image ou vid√©o, la t√¢che de rep√®re de pose bloque le thread actuel jusqu'√† ce qu'elle ait termin√© de traiter l'image ou le frame d'entr√©e. Pour √©viter de bloquer l'interposition de l'utilisateur, ex√©cutez le traitement dans un thread en arri√®re-plan.
Lorsqu'elle s'ex√©cute en mode diffusion en direct, la t√¢che Pose Landmarker renvoie imm√©diatement et ne bloque pas le thread en cours. Il appelle l'√©couteur de r√©sultats avec le r√©sultat de la d√©tection chaque fois qu'il a termin√© le traitement d'un frame d'entr√©e.
Dans l'exemple de code de Pose Landmarker, les fonctions detect, detectForVideo et detectAsync sont d√©finies dans le fichier PoseLandmarkerHelper.kt.

G√©rer et afficher les r√©sultats
Le rep√®re de position renvoie un objet poseLandmarkerResult pour chaque ex√©cution de d√©tection. L'objet r√©sultat contient les coordonn√©es de chaque rep√®re de la pose.

Voici un exemple des donn√©es de sortie de cette t√¢che:

PoseLandmarkerResult:
  Landmarks:
    Landmark #0:
      x            : 0.638852
      y            : 0.671197
      z            : 0.129959
      visibility   : 0.9999997615814209
      presence     : 0.9999984502792358
    Landmark #1:
      x            : 0.634599
      y            : 0.536441
      z            : -0.06984
      visibility   : 0.999909
      presence     : 0.999958
    ... (33 landmarks per pose)
  WorldLandmarks:
    Landmark #0:
      x            : 0.067485
      y            : 0.031084
      z            : 0.055223
      visibility   : 0.9999997615814209
      presence     : 0.9999984502792358
    Landmark #1:
      x            : 0.063209
      y            : -0.00382
      z            : 0.020920
      visibility   : 0.999976
      presence     : 0.999998
    ... (33 world landmarks per pose)
  SegmentationMasks:
    ... (pictured below)
La sortie contient √† la fois des coordonn√©es normalis√©es (Landmarks) et des coordonn√©es mondiales (WorldLandmarks) pour chaque rep√®re.

La sortie contient les coordonn√©es normalis√©es (Landmarks) suivantes :

x et y: coordonn√©es des rep√®res normalis√©es entre 0,0 et 1,0 en fonction de la largeur (x) et de la hauteur (y) de l'image.

z: profondeur du rep√®re, avec la profondeur au milieu des hanches comme origine. Plus la valeur est faible, plus le rep√®re est proche de la cam√©ra. L'ampleur de z utilise √† peu pr√®s la m√™me √©chelle que x.

visibility: probabilit√© que le rep√®re soit visible dans l'image.

La sortie contient les coordonn√©es mondiales (WorldLandmarks) suivantes :

x, y et z: coordonn√©es tridimensionnelles r√©elles en m√®tres, avec le milieu des hanches comme origine.

visibility: probabilit√© que le rep√®re soit visible dans l'image.

L'image suivante pr√©sente une visualisation du r√©sultat de la t√¢che:

Femme dans une position m√©ditative. Sa pose est mise en √©vidence par une maquette fonctionnelle qui indique le positionnement de ses membres et de son torse.

Le masque de segmentation facultatif repr√©sente la probabilit√© que chaque pixel appartienne √† une personne d√©tect√©e. L'image suivante est un masque de segmentation de la sortie de la t√¢che:

Masque de segmentation de l&#39;image pr√©c√©dente qui d√©limite la forme de la femme

L'exemple de code du rep√®re de position montre comment afficher les r√©sultats renvoy√©s par la t√¢che. Pour en savoir plus, consultez la classe OverlayView.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de stylisation des visages



Attention:La t√¢che MediaPipe Face Stylizer est exp√©rimentale et en cours de d√©veloppement.
Images c√¥te √† c√¥te d&#39;un gros plan d&#39;un homme et d&#39;une version g√©n√©r√©e de la photo qui utilise un style de dessin anim√©.

La t√¢che "Styleur de visage MediaPipe" vous permet d'appliquer des stylisations aux visages d'une image. Vous pouvez utiliser cette t√¢che pour cr√©er des avatars virtuels de diff√©rents styles.

La t√¢che utilise le mod√®le BlazeFaceStylizer, qui se compose d'un g√©n√©rateur de visages et d'un encodeur de visages. Le g√©n√©rateur de visages BlazeStyleGAN, qui est une impl√©mentation l√©g√®re de la famille de mod√®les StyleGAN, g√©n√®re et modifie des visages en fonction d'un style sp√©cifi√©. L'encodeur de visage, qui utilise une colonne vert√©brale MobileNet V2, met en correspondance les images d'entr√©e avec les visages g√©n√©r√©s par le g√©n√©rateur de visages.

Essayez !arrow_forward

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez l'un de ces guides d'impl√©mentation pour votre plate-forme cible. Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter cette t√¢che de mani√®re basique, √† l'aide d'un mod√®le recommand√© et d'exemples de code avec les options de configuration recommand√©es:

Android ‚Äì Exemple de code ‚Äì Guide
Python ‚Äì Exemple de code ‚Äì Guide
Web ‚Äì Exemple de code ‚Äì Guide
D√©tails de la t√¢che
Cette section d√©crit les fonctionnalit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Fonctionnalit√©s
Traitement des images d'entr√©e : le traitement comprend la rotation, le redimensionnement, la normalisation et la conversion d'espaces colorim√©triques des images.
Entr√©es de t√¢che	Sorties de t√¢che
Images fixes	Le styliseur de visage g√©n√®re une image stylis√©e du visage le plus pro√©minent de l'image d'entr√©e.
Remarque :Cette t√¢che permet de modifier les mod√®les de ML et les mod√®les personnalis√©s fournis. Pour en savoir plus sur l'utilisation de mod√®les modifi√©s ou personnalis√©s pour cette t√¢che, consultez le guide de personnalisation.
Options de configuration
Cette t√¢che propose les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
errorListener	D√©finit un √©couteur d'erreur facultatif.	N/A	Not set
Mod√®les
Le styliseur de visage n√©cessite que vous t√©l√©chargiez et stockiez un mod√®le de stylisation de visage dans le r√©pertoire de votre projet. Les mod√®les de stylisation de visage de cette section sont bas√©s sur l'architecture BlazeStyleGAN. Chaque mod√®le a √©t√© entra√Æn√© pour appliquer un style sp√©cifique aux visages des images d'entr√©e.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Esquisse couleur
Le mod√®le transforme les visages en une image qui imite un croquis avec des traits de crayon et de pinceau color√©s. Le style utilis√© pour entra√Æner ce mod√®le est illustr√© ci-dessous:

Sortie du croquis en couleur

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Versions
—ç—Å–∫–∏–∑ –≤ —Ü–≤–µ—Ç–µ	256 x 256 x 3	Float32	Nouveaut√©s
Encre de couleur
Le mod√®le transforme les visages en une image qui imite une peinture √† l'aquarelle. Le style utilis√© pour entra√Æner ce mod√®le est illustr√© ci-dessous:

Impression couleur

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Versions
Ink couleur	256 x 256 x 3	Float32	Nouveaut√©s
Peinture √† l'huile
Le mod√®le transforme les visages en une image qui imite une peinture √† l'huile. Le style utilis√© pour entra√Æner ce mod√®le est illustr√© ci-dessous:

sortie de peinture √† l&#39;huile

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Versions
Peinture √† l'huile	256 x 256 x 3	Float32	Nouveaut√©s
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/01/13 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de stylisation des visages pour Android



Attention:La t√¢che MediaPipe Face Stylizer est exp√©rimentale et en cours de d√©veloppement.
La t√¢che "Styleur de visage MediaPipe" vous permet d'appliquer des stylisations aux visages d'une image. Vous pouvez utiliser cette t√¢che pour cr√©er des avatars virtuels de diff√©rents styles.

L'exemple de code d√©crit dans ces instructions est disponible sur GitHub. Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Exemple de code
L'exemple de code MediaPipe Tasks est une impl√©mentation de base d'une application de stylisation du visage pour Android. L'exemple applique la stylisation des visages aux images fournies √† l'application.

Vous pouvez utiliser l'application comme point de d√©part pour votre propre application Android ou vous y r√©f√©rer lorsque vous modifiez une application existante. L'exemple de code de Face Stylister est h√©berg√© sur GitHub.

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple de code √† l'aide de l'outil de ligne de commande git.

Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Vous pouvez √©ventuellement configurer votre instance git pour qu'elle utilise un "checkout sparse", de sorte que vous n'ayez que les fichiers de l'exemple d'application Face Stylister:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/face_stylization/android
Apr√®s avoir cr√©√© une version locale de l'exemple de code, vous pouvez importer le projet dans Android Studio et ex√©cuter l'application. Pour obtenir des instructions, consultez le guide de configuration pour Android.

Composants cl√©s
Les fichiers suivants contiennent le code essentiel de cet exemple d'application de stylisation de visage:

FaceStylizationHelper.kt : initialise le styliseur de visage et g√®re le mod√®le et la s√©lection du d√©l√©gu√©.
MainActivity.kt : fournit des r√©sultats et des sorties, et g√®re les erreurs.
Configuration
Cette section d√©crit les √©tapes cl√©s √† suivre pour configurer votre environnement de d√©veloppement et vos projets de code sp√©cifiquement pour utiliser Face Stylister. Pour obtenir des informations g√©n√©rales sur la configuration de votre environnement de d√©veloppement pour utiliser les t√¢ches MediaPipe, y compris les exigences concernant la version de la plate-forme, consultez le guide de configuration pour Android.

D√©pendances
La t√¢che de stylisation des visages utilise la biblioth√®que com.google.mediapipe:tasks-vision. Ajoutez cette d√©pendance au fichier build.gradle de votre application Android:

dependencies {
    implementation 'com.google.mediapipe:tasks-vision:latest.release'
}
Mod√®le
La t√¢che de stylisation du visage MediaPipe n√©cessite un bundle de mod√®le entra√Æn√© compatible avec cette t√¢che. Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour Face Stylist, consultez la section Mod√®les de la pr√©sentation de la t√¢che.

S√©lectionnez et t√©l√©chargez le mod√®le, puis stockez-le dans le r√©pertoire de votre projet:

<dev-project-root>/src/main/assets
Sp√©cifiez le chemin d'acc√®s au mod√®le dans le param√®tre ModelAssetPath.

val modelName = "https://storage.googleapis.com/mediapipe-models/face_stylizer/blaze_face_stylizer/float32/latest/face_stylizer_color_sketch.task"
baseOptionsBuilder.setModelAssetPath(modelName)
Cr√©er la t√¢che
La t√¢che de stylisation du visage MediaPipe utilise la fonction createFromOptions() pour configurer la t√¢che. La fonction createFromOptions() accepte des valeurs pour les options de configuration. Pour en savoir plus sur les options de configuration, consultez la section Options de configuration.

val baseOptionsBuilder = BaseOptions.builder().setModelAssetPath(modelName)
val baseOptions = baseOptionBuilder.build()

val optionsBuilder =
    FaceStylizer.FaceStylizerOptions.builder()
        .setBaseOptions(baseOptionsBuilder.build())

val options = optionsBuilder.build()

FaceStylizer =
    FaceStylizer.createFromOptions(context, options)
Options de configuration
Cette t√¢che propose les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
errorListener	D√©finit un √©couteur d'erreur facultatif.	N/A	Not set
Pr√©parer les donn√©es
Le styliseur de visage fonctionne avec des images fixes. La t√¢che g√®re le pr√©traitement de l'entr√©e de donn√©es, y compris le redimensionnement, la rotation et la normalisation des valeurs. Le code suivant montre comment transmettre des donn√©es pour traitement.

import com.google.mediapipe.framework.image.BitmapImageBuilder
import com.google.mediapipe.framework.image.MPImage

// Convert the input Bitmap object to an MPImage object to run inference
val mpImage = BitmapImageBuilder(image).build()
Ex√©cuter la t√¢che
Utilisez la m√©thode FaceStylizer.stylize() sur l'image d'entr√©e pour ex√©cuter le styliseur:

val result = FaceStylizer.stylize(mpImage)
G√©rer et afficher les r√©sultats
Le styliseur de visage renvoie un objet FaceStylizerResult, qui contient un objet MPImage avec une stylisation du visage le plus pro√©minent dans l'image d'entr√©e.

Voici un exemple des donn√©es de sortie de cette t√¢che:

Gros plan g√©n√©r√© sur une femme dessin√©e au crayon et au marqueur.

Le r√©sultat ci-dessus a √©t√© cr√©√© en appliquant le mod√®le Croquis de couleur √† l'image d'entr√©e suivante:

Photographie de la femme dont l&#39;image a √©t√© utilis√©e pour g√©n√©rer le r√©sultat pr√©c√©dent

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide des t√¢ches pour la d√©tection globale des points de rep√®re
La t√¢che de rep√®re holistique MediaPipe vous permet de combiner des composants des rep√®res de pose, de visage et de main afin de cr√©er un rep√®re complet pour le corps humain. Vous pouvez utiliser cette t√¢che pour analyser les gestes, les postures et les actions de l'ensemble du corps. Cette t√¢che utilise un mod√®le de machine learning (ML) sur un flux d'images continu. La t√¢che g√©n√®re un total de 543 points de rep√®re (33 points de rep√®re de pose, 468 points de rep√®re de visage et 21 points de rep√®re pour la main par main) en temps r√©el.

Une version mise √† niveau de cette solution MediaPipe sera bient√¥t disponible. L'ancienne solution MediaPipe pour cette t√¢che est disponible sur GitHub.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2024/05/14 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide des t√¢ches de classification de texte


Exemple d&#39;UI affichant une critique de film positive comme entr√©e et cinq √©toiles et un pouce lev√© comme sortie

La t√¢che de classification du texte MediaPipe vous permet de classer le texte dans un ensemble de cat√©gories d√©finies, telles que le sentiment positif ou n√©gatif. Les cat√©gories sont d√©finies lors de l'entra√Ænement du mod√®le. Cette t√¢che fonctionne sur des donn√©es textuelles avec un mod√®le de machine learning (ML) en tant que donn√©es statiques, et produit une liste de cat√©gories et leurs scores de probabilit√©.

Essayez !arrow_forward

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez l'un de ces guides d'impl√©mentation pour la plate-forme sur laquelle vous travaillez:

Android ‚Äì Exemple de code
Guide
Python ‚Äì Exemple de code
Guide
Web ‚Äì Exemple de code ‚Äì Guide
iOS ‚Äì Exemple de code
Guide
Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter de mani√®re basique cette t√¢che, y compris un mod√®le recommand√© et un exemple de code avec les options de configuration recommand√©es.

D√©tails de la t√¢che
Cette section d√©crit les fonctionnalit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Fonctionnalit√©s
Traitement du texte d'entr√©e : prise en charge de la tokenisation hors graphique pour les mod√®les sans tokenisation dans le graphique
Plusieurs titres de classification : chaque titre peut utiliser son propre ensemble de cat√©gories.
Label map locale (Libell√© de la carte en param√®tres r√©gionaux) : d√©finissez la langue utilis√©e pour les noms √† afficher.
Seuil de score : filtrez les r√©sultats en fonction des scores de pr√©diction.
R√©sultats de classification des k meilleurs : filtrez le nombre de r√©sultats de d√©tection.
Liste d'autorisation et de blocage des libell√©s : sp√©cifiez les cat√©gories d√©tect√©es.
Entr√©es de t√¢che	Sorties de t√¢che
Le classificateur de texte accepte le type de donn√©es d'entr√©e suivant:
Cha√Æne
Le classificateur de texte g√©n√®re une liste de cat√©gories contenant les √©l√©ments suivants:
Indice de cat√©gorie: indice de la cat√©gorie dans les sorties du mod√®le
Score: score de confiance de cette cat√©gorie, exprim√© sous la forme d'une probabilit√© comprise entre z√©ro et un sous la forme d'une valeur √† virgule flottante.
Nom de la cat√©gorie (facultatif): nom de la cat√©gorie tel qu'il est sp√©cifi√© dans les m√©tadonn√©es du mod√®le TensorFlow Lite, le cas √©ch√©ant.
Nom √† afficher de la cat√©gorie (facultatif): nom √† afficher de la cat√©gorie, tel que sp√©cifi√© dans les m√©tadonn√©es du mod√®le TensorFlow Lite, dans la langue sp√©cifi√©e via les options de param√®tres r√©gionaux des noms √† afficher, le cas √©ch√©ant.
Remarque :Cette t√¢che permet de modifier les mod√®les ML et les mod√®les personnalis√©s fournis. Pour en savoir plus sur l'utilisation de mod√®les modifi√©s ou personnalis√©s pour cette t√¢che, consultez la section Mod√®les personnalis√©s.
Options de configuration
Cette t√¢che propose les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
displayNamesLocale	D√©finit la langue des libell√©s √† utiliser pour les noms √† afficher fournis dans les m√©tadonn√©es du mod√®le de la t√¢che, le cas √©ch√©ant. La valeur par d√©faut est en pour l'anglais. Vous pouvez ajouter des libell√©s localis√©s aux m√©tadonn√©es d'un mod√®le personnalis√© √† l'aide de l'API TensorFlow Lite Metadata Writer.	Code de param√®tres r√©gionaux	en
maxResults	D√©finit le nombre maximal facultatif de r√©sultats de classification les plus √©lev√©s √† renvoyer. Si la valeur est inf√©rieure √† 0, tous les r√©sultats disponibles sont renvoy√©s.	N'importe quel nombre positif	-1
scoreThreshold	D√©finit le seuil de score de pr√©diction qui remplace celui fourni dans les m√©tadonn√©es du mod√®le (le cas √©ch√©ant). Les r√©sultats inf√©rieurs √† cette valeur sont rejet√©s.	N'importe quelle superposition	Non d√©fini
categoryAllowlist	D√©finit la liste facultative des noms de cat√©gories autoris√©s. Si cet ensemble n'est pas vide, les r√©sultats de classification dont le nom de cat√©gorie ne figure pas dans cet ensemble seront filtr√©s. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option s'exclut mutuellement avec categoryDenylist. L'utilisation des deux entra√Æne une erreur.	N'importe quelle cha√Æne	Non d√©fini
categoryDenylist	D√©finit la liste facultative des noms de cat√©gories non autoris√©s. Si cet ensemble n'est pas vide, les r√©sultats de classification dont le nom de cat√©gorie figure dans cet ensemble seront filtr√©s. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option s'exclut mutuellement avec categoryAllowlist. L'utilisation des deux entra√Æne une erreur.	N'importe quelle cha√Æne	Non d√©fini
Mod√®les
Le classificateur de texte peut √™tre utilis√© avec plusieurs mod√®les de ML. Lorsque vous commencez √† d√©velopper avec cette t√¢che, commencez par le mod√®le par d√©faut recommand√© pour votre plate-forme cible. Les autres mod√®les disponibles font g√©n√©ralement des compromis entre les performances, la pr√©cision, la r√©solution et les exigences en termes de ressources, et incluent parfois des fonctionnalit√©s suppl√©mentaires.

Les mod√®les pr√©-entra√Æn√©s sont entra√Æn√©s pour l'analyse des sentiments et pr√©disent si le sentiment du texte d'entr√©e est positif ou n√©gatif. Les mod√®les ont √©t√© entra√Æn√©s sur l'ensemble de donn√©es SST-2 (Stanford Sentiment Treebank), qui se compose d'avis sur les films class√©s comme positifs ou n√©gatifs. Notez que les mod√®les ne sont disponibles qu'en anglais. √âtant donn√© qu'ils ont √©t√© entra√Æn√©s sur un ensemble de donn√©es de critiques de films, la qualit√© du texte couvrant d'autres sujets peut √™tre r√©duite.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Mod√®le de classification BERT (recommand√©)
Ce mod√®le utilise une architecture bas√©e sur BERT (plus pr√©cis√©ment, le mod√®le MobileBERT) et est recommand√© en raison de sa grande pr√©cision. Il contient des m√©tadonn√©es qui permettent √† la t√¢che d'effectuer une tokenisation BERT hors graphique.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Versions
Classifier BERT	[1x128],[1x128],[1x128]	Dynamic range	Nouveaut√©s
Mod√®le d'embedding lexical moyen
Ce mod√®le utilise une architecture d'embedding lexical moyen. Ce mod√®le offre une taille de mod√®le plus petite et une latence plus faible, mais au d√©triment d'une pr√©cision de pr√©diction inf√©rieure √† celle du classificateur BERT. La personnalisation de ce mod√®le via un entra√Ænement suppl√©mentaire est √©galement plus rapide que l'entra√Ænement du classificateur bas√© sur BERT. Ce mod√®le contient des m√©tadonn√©es qui permettent √† la t√¢che d'effectuer une tokenisation hors graphique avec expression r√©guli√®re.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Versions
Embedding lexical moyen	1 x 256	Aucun (float32)	Nouveaut√©s
Benchmarks des t√¢ches
Voici les benchmarks de t√¢che pour l'ensemble du pipeline bas√©s sur les mod√®les pr√©-entra√Æn√©s ci-dessus. Le r√©sultat de la latence correspond √† la latence moyenne sur le Pixel 6 √† l'aide du processeur / GPU.

Nom du mod√®le	Latence du processeur	Latence du GPU
Embedding lexical moyen	0,14 ms	-
BERT-classifier	57,68 ms	-
Mod√®les personnalis√©s
Vous pouvez utiliser un mod√®le de ML personnalis√© avec cette t√¢che si vous souhaitez am√©liorer ou modifier les fonctionnalit√©s des mod√®les fournis. Vous pouvez utiliser Model Maker pour modifier les mod√®les existants ou en cr√©er un √† l'aide d'outils tels que TensorFlow. Les mod√®les personnalis√©s utilis√©s avec MediaPipe doivent √™tre au format TensorFlow Lite et doivent inclure des metadata sp√©cifiques d√©crivant les param√®tres de fonctionnement du mod√®le. Nous vous conseillons d'utiliser Model Maker pour modifier les mod√®les fournis pour cette t√¢che avant de cr√©er les v√¥tres.

Remarque :Si vous souhaitez cr√©er un classificateur de texte personnalis√© √† l'aide de votre propre ensemble de donn√©es, consultez le tutoriel Personnalisation du classificateur de texte.
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/01/13 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de classification de texte pour Android


La t√¢che de classification de texte MediaPipe vous permet de classer du texte en un ensemble de cat√©gories d√©finies, comme un sentiment positif ou n√©gatif. Les cat√©gories d√©terminent le mod√®le que vous utilisez et comment ce mod√®le a √©t√© entra√Æn√©. Ces instructions vous expliquent comment utiliser le classificateur de texte avec les applications Android.

Pour voir concr√®tement en quoi consiste cette t√¢che, consultez le demo. Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et de configuration de cette t√¢che, consultez la Pr√©sentation.

Exemple de code
L'exemple de code pour le classificateur de texte fournit une impl√©mentation simple de ce pour r√©f√©rence. Ce code vous aide √† tester cette t√¢che et √† commencer cr√©er votre propre application de classification de texte. Vous pouvez parcourir les Exemple de code pour le classificateur de texte sur GitHub.

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple √† l'aide de l'outil de ligne de commande de contr√¥le des versions git.

<ph type="x-smartling-placeholder">
</ph> Attention:Cet aper√ßu de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Vous pouvez √©ventuellement configurer votre instance Git pour utiliser le paiement creuse. Vous n'avez donc que les fichiers de l'application exemple de classificateur de texte:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/text_classification/android
Pour savoir comment configurer et ex√©cuter un exemple avec Android Studio, consultez les exemples d'instructions de configuration du code dans le Guide de configuration pour Android

Composants cl√©s
Les fichiers suivants contiennent le code crucial pour la classification de texte exemple d'application:

TextClassifierHelper.kt : Initialise le classificateur de texte et g√®re la s√©lection du mod√®le.
MainActivity.kt : Elle impl√©mente l'application, y compris en appelant TextClassifierHelper et ResultsAdapter
ResultsAdapter.kt : Il g√®re et met en forme les r√©sultats.
Configuration
Cette section d√©crit les √©tapes cl√©s √† suivre pour configurer votre environnement de d√©veloppement projets de code sp√©cifiquement pour utiliser le classificateur de texte. Pour obtenir des informations g√©n√©rales sur configurer votre environnement de d√©veloppement pour utiliser MediaPipe Tasks, y compris versions de la plate-forme requises, consultez la Guide de configuration pour Android

<ph type="x-smartling-placeholder">
</ph> Attention:Cet aper√ßu de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
D√©pendances
Le classificateur de texte utilise les biblioth√®ques com.google.mediapipe:tasks-text. Ajouter au fichier build.gradle de votre projet de d√©veloppement d'applications Android. Vous pouvez importer les d√©pendances requises avec le code suivant:

dependencies {
    implementation 'com.google.mediapipe:tasks-text:latest.release'
}
Mod√®le
La t√¢che de classification de texte MediaPipe n√©cessite un mod√®le entra√Æn√© compatible avec t√¢che. Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour le classificateur de texte, consultez la page la section Mod√®les de la pr√©sentation des t√¢ches.

S√©lectionnez et t√©l√©chargez un mod√®le, puis stockez-le dans votre projet assets. r√©pertoire:

<dev-project-root>/src/main/assets
Remarque :Cet emplacement est recommand√©, car le syst√®me de compilation Android recherche automatiquement les ressources de fichiers dans ce r√©pertoire.
Utiliser la m√©thode BaseOptions.Builder.setModelAssetPath() pour sp√©cifier le chemin d'acc√®s du mod√®le √† utiliser. Pour obtenir un exemple de code, consultez la section suivante.

Cr√©er la t√¢che
Utilisez l'une des fonctions TextClassifier.createFrom...() de l'outil de classification de texte pour : pr√©parer la t√¢che √† ex√©cuter des inf√©rences. Vous pouvez utiliser l'createFromFile() avec un chemin d'acc√®s relatif ou absolu vers le fichier de mod√®le entra√Æn√©. Le code L'exemple ci-dessous illustre l'utilisation de TextClassifier.createFromOptions(). . Pour en savoir plus sur les options de configuration disponibles, consultez Options de configuration.

Le code suivant montre comment compiler et configurer cette t√¢che.

// no directory path required if model file is in src/main/assets:
String currentModel = "text_classifier_model.tflite";

fun initClassifier() {
    val baseOptionsBuilder = BaseOptions.builder()
        .setModelAssetPath(currentModel)
    try {
        val baseOptions = baseOptionsBuilder.build()
        val optionsBuilder = TextClassifier.TextClassifierOptions.builder()
            .setBaseOptions(baseOptions)
        val options = optionsBuilder.build()
        textClassifier = TextClassifier.createFromOptions(context, options)
    } catch (e: IllegalStateException) { // exception handling
    }
}
Vous trouverez un exemple de cr√©ation d'une t√¢che dans l'exemple de code TextClassifierHelper fonction initClassifier() de la classe.

Options de configuration
Cette t√¢che comporte les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
displayNamesLocale	D√©finit la langue des libell√©s √† utiliser pour les noms √† afficher fournis dans les les m√©tadonn√©es du mod√®le de la t√¢che, le cas √©ch√©ant. La valeur par d√©faut est en pour anglais. Vous pouvez ajouter des libell√©s localis√©s aux m√©tadonn√©es d'un mod√®le personnalis√© √† l'aide de l'API TensorFlow Lite Metadata Writer ;	Code des param√®tres r√©gionaux	en
maxResults	D√©finit le nombre maximal facultatif de r√©sultats de classification les mieux not√©s sur retour. Si < 0, tous les r√©sultats disponibles sont renvoy√©s.	Tous les nombres positifs	-1
scoreThreshold	D√©finit le seuil de score de pr√©diction qui remplace celui indiqu√© dans les m√©tadonn√©es du mod√®le (le cas √©ch√©ant). Les r√©sultats inf√©rieurs √† cette valeur sont refus√©s.	N'importe quel nombre d√©cimal	Non d√©fini
categoryAllowlist	D√©finit la liste facultative des noms de cat√©gories autoris√©s. Si ce champ n'est pas vide, les r√©sultats de classification dont le nom de cat√©gorie ne fait pas partie de cet ensemble seront filtr√©es. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option s'exclue mutuellement avec categoryDenylist et utilise g√©n√®rent une erreur.	Toutes les cha√Ænes	Non d√©fini
categoryDenylist	D√©finit la liste facultative des noms de cat√©gories non autoris√©s. Si non vide, les r√©sultats de classification dont le nom de cat√©gorie se trouve dans cet ensemble seront filtr√©s s'affiche. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option est mutuellement exclusive avec categoryAllowlist et l'utilisation des deux entra√Æne une erreur.	Toutes les cha√Ænes	Non d√©fini
Pr√©parer les donn√©es
L'outil de classification de texte fonctionne avec des donn√©es textuelles (String). La t√¢che g√®re la saisie des donn√©es le pr√©traitement, y compris la tokenisation et le pr√©traitement des Tensors.

L'ensemble du pr√©traitement est g√©r√© dans la fonction classify(). Il n'est pas n√©cessaire pour pr√©traiter davantage le texte d'entr√©e.

String inputText = "The input text to be classified.";
Ex√©cuter la t√¢che
Le classificateur de texte utilise la fonction TextClassifier.classify() pour ex√©cuter les inf√©rences. Utiliser un thread d'ex√©cution distinct pour ex√©cuter la classification pour √©viter de bloquer le thread de l'interface utilisateur Android avec votre application.

Le code suivant montre comment ex√©cuter le traitement avec la t√¢che √† l'aide d'un thread d'ex√©cution distinct.

    fun classify(text: String) {
        executor = ScheduledThreadPoolExecutor(1)

        executor.execute {
            val results = textClassifier.classify(text)
            listener.onResult(results)
        }
    }
Vous pouvez voir un exemple d'ex√©cution d'une t√¢che dans l'exemple de code TextClassifierHelper fonction classify() de la classe.

G√©rer et afficher les r√©sultats
Le classificateur de texte g√©n√®re un TextClassifierResult qui contient la liste de cat√©gories possibles pour le texte d'entr√©e. Ces cat√©gories sont d√©finies par Le mod√®le que vous utilisez. Si vous souhaitez utiliser des cat√©gories diff√©rentes, ou √† r√©entra√Æner un mod√®le existant.

Voici un exemple de donn√©es de sortie de cette t√¢che:

TextClassificationResult:
  Classification #0 (single classification head):
    ClassificationEntry #0:
      Category #0:
        category name: "positive"
        score: 0.8904
        index: 0
      Category #1:
        category name: "negative"
        score: 0.1096
        index: 1

Ce r√©sultat a √©t√© obtenu en ex√©cutant le classificateur BERT sur le texte d'entr√©e: "an imperfect but overall entertaining mystery"

Vous pouvez voir un exemple d'affichage des r√©sultats dans l'exemple de code ResultsAdapter et ViewHolder en classe interne.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide sur l'int√©gration de texte
Deux exemples de phrases qui montrent les repr√©sentations vectorielles continues correspondantes pour chaque mot des phrases sous forme de tableau.

La t√¢che MediaPipe Text Embedder vous permet de cr√©er une repr√©sentation num√©rique des donn√©es textuelles pour en capturer la signification s√©mantique. Cette fonctionnalit√© est fr√©quemment utilis√©e pour comparer la similarit√© s√©mantique de deux textes √† l'aide de techniques de comparaison math√©matique telles que la similarit√© cosinus. Cette t√¢che fonctionne sur des donn√©es textuelles avec un mod√®le de machine learning (ML) et g√©n√®re une repr√©sentation num√©rique des donn√©es textuelles sous la forme d'une liste de vecteurs de caract√©ristiques haute dimension, √©galement appel√©s vecteurs d'encapsulation, au format √† virgule flottante ou quantifi√©.

Essayez !arrow_forward

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez l'un de ces guides d'impl√©mentation pour votre plate-forme cible. Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter de mani√®re basique cette t√¢che, y compris un mod√®le recommand√© et un exemple de code avec les options de configuration recommand√©es:

Android ‚Äì Exemple de code ‚Äì Guide
Python ‚Äì Exemple de code ‚Äì Guide
Web ‚Äì Exemple de code ‚Äì Guide
D√©tails de la t√¢che
Cette section d√©crit les fonctionnalit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Fonctionnalit√©s
Traitement du texte d'entr√©e : compatible avec la tokenisation hors graphique pour les mod√®les sans tokenisation dans le graphique.
Calcul de la similarit√© d'embedding : fonction utilitaire int√©gr√©e permettant de calculer la similarit√© cosinus entre deux vecteurs de caract√©ristiques.
Quantification : compatible avec la quantification scalaire des vecteurs de caract√©ristiques.
Entr√©es de t√¢che	Sorties de t√¢che
L'outil d'int√©gration de texte accepte le type de donn√©es d'entr√©e suivant:
Cha√Æne
L'outil d'embedding de texte g√©n√®re une liste d'embeddings compos√©e des √©l√©ments suivants:
Embedding: vecteur de caract√©ristiques lui-m√™me, sous forme √† virgule flottante ou quantifi√© de mani√®re scalaire.
Indice de t√™te: indice de la t√™te qui a g√©n√©r√© cet embedding.
Nom de la t√™te (facultatif): nom de la t√™te qui a produit cet entra√Ænement.
Options de configuration
Cette t√¢che propose les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
l2_normalize	Indique si le vecteur de caract√©ristiques renvoy√© doit √™tre normalis√© avec la norme L2. N'utilisez cette option que si le mod√®le ne contient pas d√©j√† une op√©ration TFLite L2_NORMALIZATION native. Dans la plupart des cas, c'est d√©j√† le cas, et la normalisation L2 est donc obtenue via l'inf√©rence TFLite sans avoir besoin de cette option.	Boolean	False
quantize	Indique si l'embedding renvoy√© doit √™tre quantifi√© en octets via une quantification scalaire. Les repr√©sentations vectorielles continues sont implicitement suppos√©es avoir une norme unitaire. Par cons√©quent, toute dimension a une valeur comprise dans la plage [-1,0, 1,0]. Utilisez l'option l2_normalize si ce n'est pas le cas.	Boolean	False
Mod√®les
Nous proposons un mod√®le par d√©faut recommand√© lorsque vous commencez √† d√©velopper avec cette t√¢che.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Mod√®le Universal Sentence Encoder (recommand√©)
Ce mod√®le utilise une architecture √† double encodeur et a √©t√© entra√Æn√© sur divers ensembles de donn√©es de questions/r√©ponses.

Consid√©rons les paires de phrases suivantes:

("c'est un voyage charmant et souvent touchant", "quel voyage fantastique et g√©nial")
("I like my phone", "I hate my phone")
("Ce restaurant a un bon concept", "Nous devons v√©rifier les d√©tails de notre plan")
Les repr√©sentations vectorielles continues de texte des deux premi√®res paires auront une similarit√© cosinus plus √©lev√©e que celles de la troisi√®me paire, car les deux premi√®res paires de phrases partagent un th√®me commun, respectivement "sentiment de voyage" et "opinion sur le t√©l√©phone", tandis que la troisi√®me paire de phrases ne partage pas de th√®me commun.

Notez que m√™me si les deux phrases de la deuxi√®me paire ont des sentiments oppos√©s, elles ont un score de similarit√© √©lev√©, car elles partagent un th√®me commun.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Versions
Universal Sentence Encoder	cha√Æne, cha√Æne, cha√Æne	Aucun (float32)	Nouveaut√©s
Benchmarks des t√¢ches
Voici les benchmarks de t√¢che pour l'ensemble du pipeline bas√©s sur les mod√®les pr√©-entra√Æn√©s ci-dessus. Le r√©sultat de la latence correspond √† la latence moyenne sur le Pixel 6 √† l'aide du processeur / GPU.

Nom du mod√®le	Latence du processeur	Latence du GPU
Universal Sentence Encoder	18,21 ms	-
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/01/13 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide d'int√©gration de texte pour Android


La t√¢che de l'int√©grateur de texte MediaPipe vous permet de cr√©er une repr√©sentation num√©rique de donn√©es textuelles pour saisir sa signification s√©mantique. Ces instructions vous expliquent comment utiliser Outil d'int√©gration de texte avec des applications Android

Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Exemple de code
L'exemple de code MediaPipe Tasks est une impl√©mentation simple d'un int√©grateur de texte pour Android. L'exemple √©value les similitudes s√©mantiques entre deux de texte, et n√©cessite un appareil Android physique √©mulateur d'application.

Vous pouvez utiliser l'application comme point de d√©part pour votre propre application Android ou vous y r√©f√©rer. lorsque vous modifiez une application existante. L'exemple de code de l'outil d'int√©gration de texte est h√©berg√© GitHub

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple √† l'aide de l'outil de ligne de commande git.

<ph type="x-smartling-placeholder">
</ph> Attention:Cet aper√ßu de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Si vous le souhaitez, vous pouvez configurer votre instance Git pour utiliser le paiement creuse. uniquement les fichiers de l'application exemple de l'outil d'int√©gration de texte:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/text_embedder/android
Apr√®s avoir cr√©√© une version locale de l'exemple de code, vous pouvez importer le projet dans Android Studio et ex√©cuter l'application. Pour obtenir des instructions, consultez le Guide de configuration Android

Composants cl√©s
Les fichiers suivants contiennent le code essentiel pour cet exemple d'outil d'int√©gration de texte application:

TextEmbedderHelper.kt: Initialise l'outil d'int√©gration de texte, et g√®re la s√©lection du mod√®le et du d√©l√©gu√©.
MainActivity.kt: Met en ≈ìuvre l'application et assemble les composants de l'interface utilisateur.
Configuration
Cette section d√©crit les √©tapes cl√©s √† suivre pour configurer votre environnement de d√©veloppement projets de code sp√©cifiques pour utiliser l'outil d'int√©gration de texte. Pour obtenir des informations g√©n√©rales sur configurer votre environnement de d√©veloppement pour utiliser les t√¢ches MediaPipe, y compris version de la plate-forme requise, consultez le guide de configuration Android

<ph type="x-smartling-placeholder">
</ph> Attention:Cet aper√ßu de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
D√©pendances
L'outil d'int√©gration de texte utilise les biblioth√®ques com.google.mediapipe:tasks-text. Ajouter au fichier build.gradle de votre projet de d√©veloppement d'applications Android. Vous pouvez importer les d√©pendances requises avec le code suivant:

dependencies {
    implementation 'com.google.mediapipe:tasks-text:latest.release'
}
Mod√®le
La t√¢che de l'outil d'int√©gration de texte MediaPipe n√©cessite un mod√®le entra√Æn√© compatible avec t√¢che. Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour l'outil d'int√©gration de texte, consultez la section Mod√®les de la pr√©sentation des t√¢ches.

S√©lectionnez et t√©l√©chargez le mod√®le, puis stockez-le dans le r√©pertoire de votre projet:

<dev-project-root>/src/main/assets
Remarque :Le syst√®me de compilation Android recherche automatiquement dans le r√©pertoire du projet ressources de fichiers.
Sp√©cifiez le chemin d'acc√®s du mod√®le dans le param√®tre ModelAssetPath. Dans exemple de code, le mod√®le est d√©fini dans la fonction setupTextEmbedder() TextEmbedderHelper.kt :

Utiliser la fonction BaseOptions.Builder.setModelAssetPath() pour sp√©cifier le chemin d'acc√®s utilis√©es par le mod√®le. Cette m√©thode est mentionn√©e dans l'exemple de code de la .

Cr√©er la t√¢che
Vous pouvez utiliser l'une des fonctions createFrom...() pour cr√©er la t√¢che. La La fonction createFromOptions() accepte les options de configuration pour d√©finir l'int√©grateur options. Vous pouvez √©galement initialiser la t√¢che √† l'aide de la fabrique createFromFile(). . La fonction createFromFile() accepte un chemin d'acc√®s relatif ou absolu vers le fichier du mod√®le entra√Æn√©. Pour en savoir plus sur les options de configuration, consultez Options de configuration.

Le code suivant montre comment compiler et configurer cette t√¢che.

val baseOptions = baseOptionsBuilder.build()
val optionsBuilder =
    TextEmbedderOptions.builder().setBaseOptions(baseOptions)
val options = optionsBuilder.build()
textEmbedder = TextEmbedder.createFromOptions(context, options)
L'exemple d'impl√©mentation de code d√©finit les options d'int√©gration de texte dans la section fonction setupTextEmbedder() dans TextEmbedderHelper.kt .

Options de configuration
Cette t√¢che comporte les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
l2_normalize	Indique s'il faut normaliser le vecteur de caract√©ristiques renvoy√© avec la norme L2. N'utilisez cette option que si le mod√®le ne contient pas encore d'√©l√©ment natif L2_NORMALIZATION TFLite Op. Dans la plupart des cas, c'est d√©j√† le cas et La normalisation L2 est ainsi obtenue via l'inf√©rence TFLite sans avoir besoin pour cette option.	Boolean	False
quantize	Indique si la repr√©sentation vectorielle continue renvoy√©e doit √™tre quantifi√©e en octets via la quantification scalaire. Les repr√©sentations vectorielles continues sont implicitement consid√©r√©es comme √©tant une norme unitaire Par cons√©quent, toute dimension aura une valeur comprise dans [-1.0, 1.0]. Utilisez l'option l2_normalize si ce n'est pas le cas.	Boolean	False
Pr√©parer les donn√©es
L'outil d'int√©gration de texte fonctionne avec des donn√©es textuelles (String). La t√¢che g√®re la saisie des donn√©es le pr√©traitement, y compris la tokenisation et le pr√©traitement des Tensors. Tout le pr√©traitement est g√©r√© dans la fonction embed(). Il n'est pas n√©cessaire un pr√©traitement suppl√©mentaire du texte d'entr√©e au pr√©alable.

val inputText = "The input text to be embedded."
Ex√©cuter la t√¢che
L'outil d'int√©gration de texte utilise la fonction embed pour d√©clencher des inf√©rences. Texte cela signifie renvoyer les vecteurs de repr√©sentation vectorielle continue pour le texte d'entr√©e.

Le code suivant montre comment ex√©cuter le traitement avec le mod√®le de t√¢che.

textEmbedder?.let {
    val firstEmbed =
        it.embed(firstText).embeddingResult().embeddings().first()
    val secondEmbed =
        it.embed(secondText).embeddingResult().embeddings().first()
    ...
}
Dans l'exemple de code, la fonction embed est appel√©e dans le TextEmbedderHelper.kt .

G√©rer et afficher les r√©sultats
Lors de l'ex√©cution de l'inf√©rence, la t√¢che de l'int√©grateur de texte renvoie un TextEmbedderResult contenant une liste de repr√©sentations vectorielles continues (√† virgule flottante ou quantifi√©e scalaire) pour le texte d'entr√©e.

Voici un exemple de donn√©es de sortie de cette t√¢che:

TextEmbedderResult:
  Embedding #0 (sole embedding head):
    float_embedding: {0.2345f, 0.1234f, ..., 0.6789f}
    head_index: 0
Vous pouvez comparer la similarit√© s√©mantique de deux repr√©sentations vectorielles continues √† l'aide de la fonction fonction TextEmbedder.cosineSimilarity. Consultez le code suivant pour obtenir un exemple.

val similarity = TextEmbedder.cosineSimilarity(firstEmbed, secondEmbed)
Dans l'exemple de code, la fonction TextEmbedder.cosineSimilarity() est appel√©e dans le TextEmbedderHelper.kt .

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de d√©tection de la langue
Exemple d&#39;UI affichant une phrase d&#39;entr√©e en fran√ßais correctement identifi√©e comme telle dans la sortie.

La t√¢che MediaPipe Language Detector vous permet d'identifier la langue d'un texte. Cette t√¢che fonctionne sur des donn√©es textuelles avec un mod√®le de machine learning (ML) et produit une liste de pr√©dictions, o√π chaque pr√©diction consiste en un code de langue ISO 639-1 et une probabilit√©.

Essayez !arrow_forward

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez l'un de ces guides d'impl√©mentation pour votre plate-forme cible. Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter de mani√®re basique cette t√¢che, y compris un mod√®le recommand√© et un exemple de code avec les options de configuration recommand√©es:

Android ‚Äì Exemple de code ‚Äì Guide
Python ‚Äì Exemple de code ‚Äì Guide
Web ‚Äì Exemple de code ‚Äì Guide
D√©tails de la t√¢che
Cette section d√©crit les fonctionnalit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Fonctionnalit√©s
Seuil de score : filtrez les r√©sultats en fonction des scores de pr√©diction.
Liste d'autorisation et de blocage des libell√©s : sp√©cifiez les cat√©gories d√©tect√©es.
Entr√©es de t√¢che	Sorties de t√¢che
Le d√©tecteur de langue accepte le type de donn√©es d'entr√©e suivant:
Cha√Æne
Le d√©tecteur de langue g√©n√®re une liste de pr√©dictions contenant les √©l√©ments suivants:
Code de langue : code de langue/locale ISO 639-1 (par exemple, "en" pour l'anglais, "uz" pour l'ouzbek, "ja-Latn" pour le japonais (romaji)) sous la forme d'une cha√Æne.
Probabilit√©: score de confiance de cette pr√©diction, exprim√© sous la forme d'une probabilit√© comprise entre z√©ro et un sous la forme d'une valeur √† virgule flottante.
Options de configuration
Cette t√¢che propose les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
max_results	D√©finit le nombre maximal facultatif de pr√©dictions de langue les plus √©lev√©es √† renvoyer. Si cette valeur est inf√©rieure √† z√©ro, tous les r√©sultats disponibles sont renvoy√©s.	N'importe quel nombre positif	-1
score_threshold	D√©finit le seuil de score de pr√©diction qui remplace celui fourni dans les m√©tadonn√©es du mod√®le (le cas √©ch√©ant). Les r√©sultats inf√©rieurs √† cette valeur sont rejet√©s.	N'importe quelle superposition	Non d√©fini
category_allowlist	D√©finit la liste facultative des codes de langue autoris√©s. Si cet ensemble n'est pas vide, les pr√©dictions de langue dont le code de langue ne figure pas dans cet ensemble sont filtr√©es. Cette option s'exclut mutuellement avec category_denylist. L'utilisation des deux entra√Æne une erreur.	N'importe quelle cha√Æne	Non d√©fini
category_denylist	D√©finit la liste facultative des codes de langue non autoris√©s. Si cet ensemble n'est pas vide, les pr√©dictions de langue dont le code de langue figure dans cet ensemble seront filtr√©es. Cette option s'exclut mutuellement avec category_allowlist. L'utilisation des deux entra√Æne une erreur.	N'importe quelle cha√Æne	Non d√©fini
Mod√®les
Nous proposons un mod√®le par d√©faut recommand√© lorsque vous commencez √† d√©velopper avec cette t√¢che.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Mod√®le de d√©tecteur de langage (recommand√©)
Ce mod√®le est con√ßu pour √™tre l√©ger (315 ko) et utilise une architecture de classification de r√©seau de neurones bas√©e sur l'imbrication. Le mod√®le identifie la langue √† l'aide d'un code de langue ISO 639-1 et peut identifier 110 langues. Pour obtenir la liste des langues accept√©es par le mod√®le, consultez le fichier de libell√©s, qui liste les langues par code ISO 639-1.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Fiche de mod√®le	Versions
D√©tecteur de langue	cha√Æne UTF-8	none (float32)	info	Nouveaut√©s
Benchmarks des t√¢ches
Voici les benchmarks de t√¢che pour l'ensemble du pipeline bas√©s sur les mod√®les pr√©-entra√Æn√©s ci-dessus. Le r√©sultat de la latence correspond √† la latence moyenne sur le Pixel 6 √† l'aide du processeur / GPU.

Nom du mod√®le	Latence du processeur	Latence du GPU
D√©tecteur de langue	0,31 ms	-
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/01/13 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.


Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide sur la d√©tection de la langue pour Android


La t√¢che "D√©tecteur de langue MediaPipe" vous permet d'identifier la langue d'un texte. Ces vous explique comment utiliser le d√©tecteur de langue avec les applications Android. Le code l'exemple d√©crit dans ces instructions est disponible sur GitHub

Pour voir concr√®tement en quoi consiste cette t√¢che, consultez le demo. Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Exemple de code
L'exemple de code du d√©tecteur de langue fournit une impl√©mentation simple pour r√©f√©rence. Ce code vous aide √† tester cette t√¢che et √† commencer cr√©er votre propre fonctionnalit√© de d√©tection de la langue. Vous pouvez parcourir les Exemple de code du d√©tecteur de langue sur GitHub.

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple √† l'aide de l'outil de ligne de commande de contr√¥le des versions git.

<ph type="x-smartling-placeholder">
</ph> Attention:Cet aper√ßu de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Vous pouvez √©ventuellement configurer votre instance Git pour utiliser le paiement creuse. Vous n'avez donc que les fichiers de l'application exemple D√©tecteur de langue:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/languagedetector/android
Pour savoir comment configurer et ex√©cuter un exemple avec Android Studio, consultez les exemples d'instructions de configuration du code dans le Guide de configuration pour Android

Composants cl√©s
Les fichiers suivants contiennent le code crucial pour la classification de texte exemple d'application:

LanguageDetectorHelper.kt : Initialise le d√©tecteur de langue et g√®re la s√©lection du mod√®le.
ResultsAdapter.kt : G√®re et formate les r√©sultats de la d√©tection.
MainActivity.kt : Elle impl√©mente l'application, y compris en appelant LanguageDetectorHelper et ResultsAdapter
Configuration
Cette section d√©crit les √©tapes cl√©s √† suivre pour configurer votre environnement de d√©veloppement sp√©cifiquement pour utiliser le d√©tecteur de langage. Pour obtenir des informations g√©n√©rales sur configurer votre environnement de d√©veloppement pour utiliser les t√¢ches MediaPipe, y compris versions de la plate-forme requises, consultez la Guide de configuration pour Android

<ph type="x-smartling-placeholder">
</ph> Attention:Cet aper√ßu de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
D√©pendances
Le d√©tecteur de langue utilise les biblioth√®ques com.google.mediapipe:tasks-text. Ajouter au fichier build.gradle de votre projet de d√©veloppement d'applications Android. Vous pouvez importer les d√©pendances requises avec le code suivant:

dependencies {
    implementation 'com.google.mediapipe:tasks-text:latest.release'
}
Mod√®le
La t√¢che de d√©tection de la langue MediaPipe n√©cessite un mod√®le entra√Æn√© et compatible avec cette t√¢che. Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour le d√©tecteur de langue, consultez la section Mod√®les de la pr√©sentation des t√¢ches.

S√©lectionnez et t√©l√©chargez le mod√®le, puis stockez-le dans le r√©pertoire de votre projet:

<dev-project-root>/src/main/assets
Sp√©cifiez le chemin d'acc√®s du mod√®le dans le param√®tre ModelName.

Cr√©er la t√¢che
Vous pouvez utiliser l'une des fonctions createFrom...() pour cr√©er la t√¢che. La La fonction createFromOptions() accepte les options de configuration pour le langage d√©tecteur de fum√©e et de monoxyde de carbone. Vous pouvez √©galement initialiser la t√¢che √† l'aide de la fabrique createFromFile(). . La fonction createFromFile() accepte un chemin d'acc√®s relatif ou absolu vers le fichier du mod√®le entra√Æn√©. Pour plus d'informations sur la configuration des t√¢ches, consultez Options de configuration.

Le code suivant montre comment cr√©er et configurer cette t√¢che.

// For creating a language detector instance:
LanguageDetectorOptions options =
       LanguageDetectorOptions.builder()
       .setBaseOptions(
          BaseOptions.builder()
            .setModelAssetPath(modelPath)
            .build()
          )
       .build();
LanguageDetector languageDetector = LanguageDetector.createFromOptions(context, options);
Vous trouverez un exemple de cr√©ation d'une t√¢che dans l'exemple de code LanguageDetectorHelper fonction initDetector() de la classe.

Options de configuration
Cette t√¢che comporte les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
maxResults	D√©finit le nombre maximal facultatif de pr√©dictions de langues les mieux not√©es sur retour. Si cette valeur est inf√©rieure √† z√©ro, tous les r√©sultats disponibles sont renvoy√©s.	Tous les nombres positifs	-1
scoreThreshold	D√©finit le seuil de score de pr√©diction qui remplace celui indiqu√© dans les m√©tadonn√©es du mod√®le (le cas √©ch√©ant). Les r√©sultats inf√©rieurs √† cette valeur sont refus√©s.	N'importe quel nombre d√©cimal	Non d√©fini
categoryAllowlist	D√©finit la liste facultative des codes de langue autoris√©s. Si ce champ n'est pas vide, les pr√©dictions de langue dont le code de langue n'est pas dans cet ensemble seront filtr√©es. Cette option s'exclue mutuellement avec categoryDenylist et l'utilisation des deux entra√Ænent une erreur.	Toutes les cha√Ænes	Non d√©fini
categoryDenylist	D√©finit la liste facultative des codes de langue qui ne sont pas autoris√©s. Si non vide, les pr√©dictions de langue dont le code de langue fait partie de cet ensemble seront filtr√©es s'affiche. Cette option s'exclue mutuellement avec categoryAllowlist et l'utilisation des deux entra√Æne une erreur.	Toutes les cha√Ænes	Non d√©fini
Pr√©parer les donn√©es
Le d√©tecteur de langue fonctionne avec des donn√©es textuelles (String). La t√¢che g√®re la saisie des donn√©es le pr√©traitement, y compris la tokenisation et le pr√©traitement des Tensors. Tout le pr√©traitement est g√©r√© dans la fonction detect(). Il n'est pas n√©cessaire un pr√©traitement suppl√©mentaire du texte d'entr√©e au pr√©alable.

String inputText = "Some input text for the language detector";
Ex√©cuter la t√¢che
Le d√©tecteur de langue utilise la m√©thode LanguageDetector.detect() pour traiter l'entr√©e du texte et pr√©dire sa langue. Vous devez utiliser une m√©thode d'ex√©cution distincte thread pour ex√©cuter la d√©tection afin d'√©viter de bloquer l'utilisateur Android thread d'interface utilisateur avec votre application.

Le code suivant montre comment ex√©cuter le traitement avec la t√¢che √† l'aide d'un thread d'ex√©cution distinct.

// Predict the language of the input text.
fun classify(text: String) {
    executor = ScheduledThreadPoolExecutor(1)

    executor.execute {
        val results = languageDetector.detect(text)
        listener.onResult(results)
    }
}
Vous pouvez voir un exemple d'ex√©cution d'une t√¢che dans l'exemple de code LanguageDetectorHelper fonction detect() de la classe.

G√©rer et afficher les r√©sultats
Le d√©tecteur de langue g√©n√®re un LanguageDetectorResult compos√© d'une liste de des pr√©dictions en langage et les probabilit√©s de ces pr√©dictions. La des cat√©gories de langage d√©finies dans le mod√®le, consultez la pr√©sentation des t√¢ches Mod√®les pour en savoir plus sur le mod√®le que vous utilisez.

Voici un exemple de donn√©es de sortie de cette t√¢che:

LanguageDetectorResult:
  LanguagePrediction #0:
    language_code: "fr"
    probability: 0.999781

Ce r√©sultat a √©t√© obtenu en ex√©cutant le mod√®le sur le texte d'entr√©e: "Il y a beaucoup de bouches qui parlent et fort peu de t√™tes qui pensent."

Vous pouvez voir un exemple d'affichage des r√©sultats dans l'exemple de code ResultsAdapter et ViewHolder en classe interne.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de classification audio
Forme d&#39;onde d&#39;un cri d&#39;oiseau superpos√©e √† une photo de l&#39;oiseau dont le cri correspond √† la forme d&#39;onde.

La t√¢che de classification audio MediaPipe vous permet de classer des extraits audio dans un ensemble de cat√©gories d√©finies, telles que la musique de guitare, le sifflement d'un train ou le chant d'un oiseau. Les cat√©gories sont d√©finies lors de l'entra√Ænement du mod√®le. Cette t√¢che fonctionne sur des donn√©es audio avec un mod√®le de machine learning (ML) sous forme de clips audio ind√©pendants ou de flux continus, et renvoie une liste de cat√©gories potentielles class√©es par score de probabilit√© d√©croissant.

Essayez !arrow_forward

Premiers pas
Pour commencer √† utiliser cette t√¢che, suivez l'un de ces guides d'impl√©mentation pour votre plate-forme cible. Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter de mani√®re basique cette t√¢che, y compris un mod√®le recommand√© et un exemple de code avec les options de configuration recommand√©es:

Android ‚Äì Exemple de code ‚Äì Guide
Python : guide d'exemple de code
Web ‚Äì Exemple de code ‚Äì Guide
Ces guides sp√©cifiques √† la plate-forme vous expliquent comment impl√©menter de mani√®re basique cette t√¢che, y compris un mod√®le recommand√© et un exemple de code avec les options de configuration recommand√©es.

D√©tails de la t√¢che
Cette section d√©crit les fonctionnalit√©s, les entr√©es, les sorties et les options de configuration de cette t√¢che.

Fonctionnalit√©s
Traitement audio de l'entr√©e : le traitement comprend le re√©chantillonnage audio, le tamponnage, le cadrage et la transformation de Fourier.
Label map locale (Libell√© de la carte en param√®tres r√©gionaux) : d√©finissez la langue utilis√©e pour les noms √† afficher.
Seuil de score : filtrez les r√©sultats en fonction des scores de pr√©diction.
D√©tection des k meilleurs : filtrez les r√©sultats de d√©tection des nombres.
Liste d'autorisation et de refus des libell√©s : sp√©cifiez les cat√©gories d√©tect√©es.
Entr√©es de t√¢che	Sorties de t√¢che
L'entr√©e peut √™tre l'un des types de donn√©es suivants:
Clips audio
Flux audio
Le classificateur audio g√©n√®re une liste de cat√©gories contenant les √©l√©ments suivants:
Indice de cat√©gorie: indice de la cat√©gorie dans les sorties du mod√®le
Score: score de confiance de cette cat√©gorie, g√©n√©ralement une probabilit√© comprise entre 0 et 1
Nom de la cat√©gorie (facultatif): nom de la cat√©gorie tel qu'il est sp√©cifi√© dans les m√©tadonn√©es du mod√®le TFLite, le cas √©ch√©ant
Nom √† afficher de la cat√©gorie (facultatif): nom √† afficher de la cat√©gorie, tel que sp√©cifi√© dans les m√©tadonn√©es du mod√®le TFLite, dans la langue sp√©cifi√©e via les options de param√®tres r√©gionaux des noms √† afficher, le cas √©ch√©ant
Options de configuration
Cette t√¢che propose les options de configuration suivantes:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
running_mode	D√©finit le mode d'ex√©cution de la t√¢che. Le classificateur audio propose deux modes:

AUDIO_CLIPS: mode d'ex√©cution de la t√¢che audio sur des extraits audio ind√©pendants.

AUDIO_STREAM: mode d'ex√©cution de la t√¢che audio sur un flux audio, par exemple √† partir d'un micro. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de classification de mani√®re asynchrone.	{AUDIO_CLIPS, AUDIO_STREAM}	AUDIO_CLIPS
display_names_locale	D√©finit la langue des libell√©s √† utiliser pour les noms √† afficher fournis dans les m√©tadonn√©es du mod√®le de la t√¢che, le cas √©ch√©ant. La valeur par d√©faut est en pour l'anglais. Vous pouvez ajouter des libell√©s localis√©s aux m√©tadonn√©es d'un mod√®le personnalis√© √† l'aide de l'API TensorFlow Lite Metadata Writer.	Code de param√®tres r√©gionaux	en
max_results	D√©finit le nombre maximal facultatif de r√©sultats de classification les plus √©lev√©s √† renvoyer. Si la valeur est inf√©rieure √† 0, tous les r√©sultats disponibles sont renvoy√©s.	N'importe quel nombre positif	-1
score_threshold	D√©finit le seuil de score de pr√©diction qui remplace celui fourni dans les m√©tadonn√©es du mod√®le (le cas √©ch√©ant). Les r√©sultats inf√©rieurs √† cette valeur sont rejet√©s.	[0.0, 1.0]	Non d√©fini
category_allowlist	D√©finit la liste facultative des noms de cat√©gories autoris√©s. Si cet ensemble n'est pas vide, les r√©sultats de classification dont le nom de cat√©gorie ne figure pas dans cet ensemble seront filtr√©s. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option s'exclut mutuellement avec category_denylist. L'utilisation des deux entra√Æne une erreur.	N'importe quelle cha√Æne	Non d√©fini
category_denylist	D√©finit la liste facultative des noms de cat√©gories non autoris√©s. Si cet ensemble n'est pas vide, les r√©sultats de classification dont le nom de cat√©gorie figure dans cet ensemble seront filtr√©s. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option s'exclut mutuellement avec category_allowlist. L'utilisation des deux entra√Æne une erreur.	N'importe quelle cha√Æne	Non d√©fini
result_callback	D√©finit l'√©couteur de r√©sultats pour qu'il re√ßoive les r√©sultats de classification de mani√®re asynchrone lorsque le classificateur audio est en mode flux audio. Ne peut √™tre utilis√© que lorsque le mode d'ex√©cution est d√©fini sur AUDIO_STREAM.	N/A	Non d√©fini
Remarque :resultListener d√©pend de runningMode. D√©finissez uniquement resultListener lorsque runningMode est d√©fini sur AUDIO_STREAM.
Remarque :La liste d'autorisation et la liste de refus de la cat√©gorie s'excluent mutuellement.
Mod√®les
Le classificateur audio n√©cessite que vous t√©l√©chargiez et stockiez un mod√®le de classification audio dans le r√©pertoire de votre projet. Lorsque vous commencez √† d√©velopper avec cette t√¢che, commencez par le mod√®le par d√©faut recommand√© pour votre plate-forme cible. Les autres mod√®les disponibles font g√©n√©ralement des compromis entre les performances, la pr√©cision, la r√©solution et les exigences en termes de ressources, et incluent parfois des fonctionnalit√©s suppl√©mentaires.

Attention:Cette version preview de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Mod√®le Yamnet (recommand√©)
Le mod√®le Yamnet est un classificateur d'√©v√©nements audio entra√Æn√© sur l'ensemble de donn√©es AudioSet pour pr√©dire les √©v√©nements audio d√©finis dans les donn√©es AudioSet. Pour en savoir plus sur les √©v√©nements audio reconnus par ce mod√®le, consultez la liste des libell√©s du mod√®le.

Nom du mod√®le	Forme d'entr√©e	Type de quantification	Versions
YamNet	1 x 15 600	Aucun (float32)	Nouveaut√©s
Benchmarks des t√¢ches
Voici les benchmarks de t√¢che pour l'ensemble du pipeline bas√©s sur les mod√®les pr√©-entra√Æn√©s ci-dessus. Le r√©sultat de la latence correspond √† la latence moyenne sur le Pixel 6 √† l'aide du processeur / GPU.

Nom du mod√®le	Latence du processeur	Latence du GPU
YamNet	12,29 ms	-
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/01/13 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.

Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de classification audio pour Android


La t√¢che de classification audio MediaPipe vous permet d'effectuer une classification sur des donn√©es audio. Vous pouvez utilisez cette t√¢che pour identifier les √©v√©nements sonores √† partir d'un ensemble de cat√©gories entra√Æn√©es. Ces vous expliquent comment utiliser le classificateur audio avec des applications Android.

Pour en savoir plus sur les fonctionnalit√©s, les mod√®les et les options de configuration de cette t√¢che, consultez la section Pr√©sentation.

Exemple de code
L'exemple de code MediaPipe Tasks est une impl√©mentation simple d'un classificateur audio pour Android. Cet exemple utilise le micro d'un appareil Android physique pour classer les sons en continu et ex√©cuter le classificateur sur les fichiers audio stock√©es sur l'appareil.

Vous pouvez utiliser l'application comme point de d√©part pour votre propre application Android ou vous y r√©f√©rer. lorsque vous modifiez une application existante. L'exemple de code pour le classificateur audio est h√©berg√© GitHub

T√©l√©charger le code
Les instructions suivantes vous expliquent comment cr√©er une copie locale de l'exemple √† l'aide de l'outil de ligne de commande git.

<ph type="x-smartling-placeholder">
</ph> Attention:Cet aper√ßu de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
Pour t√©l√©charger l'exemple de code:

Clonez le d√©p√¥t Git √† l'aide de la commande suivante:

git clone https://github.com/google-ai-edge/mediapipe-samples
Vous pouvez √©ventuellement configurer votre instance Git pour utiliser le paiement creuse. Vous n'avez donc que les fichiers de l'application exemple de classificateur audio:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/audio_classifier/android
Apr√®s avoir cr√©√© une version locale de l'exemple de code, vous pouvez importer le projet dans Android Studio et ex√©cuter l'application. Pour obtenir des instructions, consultez les Guide de configuration pour Android

Composants cl√©s
Les fichiers suivants contiennent le code essentiel pour ce contenu audio Exemple d'application de classification:

AudioClassifierHelper.kt : Il initialise le classificateur audio, et g√®re le mod√®le et la d√©l√©gation de votre choix.
RecorderFragment.kt : Cr√©e l'interface utilisateur et le code de contr√¥le pour l'enregistrement audio en direct.
LibraryFragment.kt : Cr√©e l'interface utilisateur et le code de contr√¥le pour la s√©lection de fichiers audio.
ProbabilitiesAdapter.kt : G√®re et met en forme les r√©sultats de pr√©diction du classificateur.
Configuration
Cette section d√©crit les √©tapes cl√©s √† suivre pour configurer votre environnement de d√©veloppement sp√©cifiquement pour utiliser le classificateur audio. Pour obtenir des informations g√©n√©rales sur configurer votre environnement de d√©veloppement pour utiliser les t√¢ches MediaPipe, y compris versions de la plate-forme requises, consultez la Guide de configuration pour Android

<ph type="x-smartling-placeholder">
</ph> Attention:Cet aper√ßu de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
D√©pendances
Le classificateur audio utilise la biblioth√®que com.google.mediapipe:tasks-audio. Ajouter au fichier build.gradle de votre Projet de d√©veloppement d'applications Android. Importez les d√©pendances requises avec le code suivant:

dependencies {
    ...
    implementation 'com.google.mediapipe:tasks-audio:latest.release'
}
Mod√®le
La t√¢che de classification audio MediaPipe n√©cessite un mod√®le entra√Æn√© et compatible avec t√¢che. Pour en savoir plus sur les mod√®les entra√Æn√©s disponibles pour le classificateur audio, consultez la section Mod√®les de la pr√©sentation des t√¢ches.

S√©lectionnez et t√©l√©chargez le mod√®le, puis stockez-le dans le r√©pertoire de votre projet:

<dev-project-root>/src/main/assets
Remarque :Cet emplacement est recommand√©, car le syst√®me de compilation Android recherche automatiquement les ressources de fichiers dans ce r√©pertoire.
Utiliser la m√©thode BaseOptions.Builder.setModelAssetPath() pour sp√©cifier le chemin d'acc√®s utilis√©es par le mod√®le. Cette m√©thode est mentionn√©e dans l'exemple de code de la .

Dans Exemple de code pour le classificateur audio le mod√®le est d√©fini dans l'√©l√©ment AudioClassifierHelper.kt. .

Cr√©er la t√¢che
Vous pouvez utiliser la fonction createFromOptions pour cr√©er la t√¢che. La La fonction createFromOptions accepte les options de configuration, y compris l'ex√©cution mode, param√®tres r√©gionaux des noms √† afficher, nombre maximal de r√©sultats, seuil de confiance, et une liste d'autorisation ou de refus de cat√©gories. Pour en savoir plus sur la configuration consultez la page Pr√©sentation de la configuration.

La t√¢che "Outil de classification audio" accepte les types de donn√©es d'entr√©e suivants: clips audio et les flux audio. Vous devez sp√©cifier le mode d'ex√©cution correspondant votre type de donn√©es d'entr√©e lorsque vous cr√©ez une t√¢che. Choisissez l'onglet correspondant votre type de donn√©es d'entr√©e pour d√©couvrir comment cr√©er la t√¢che et ex√©cuter l'inf√©rence.

Clips audio
Flux audio
AudioClassifierOptions options =
    AudioClassifierOptions.builder()
        .setBaseOptions(
            BaseOptions.builder().setModelAssetPath("model.tflite").build())
        .setRunningMode(RunningMode.AUDIO_CLIPS)
        .setMaxResults(5)
        .build();
audioClassifier = AudioClassifier.createFromOptions(context, options);
    
L'exemple d'impl√©mentation de code pour le classificateur audio permet √† l'utilisateur de basculer entre diff√©rents modes de traitement. L'approche rend le code de cr√©ation de la t√¢che plus compliqu√© et peuvent ne pas √™tre adapt√©s √† votre cas d'utilisation. Vous pouvez voir le code de changement de mode dans la fonction initClassifier() du AudioClassifierHelper

Options de configuration
Cette t√¢che comporte les options de configuration suivantes pour les applications Android:

Nom de l'option	Description	Plage de valeurs	Valeur par d√©faut
runningMode	D√©finit le mode d'ex√©cution de la t√¢che. Le classificateur audio propose deux modes:

AUDIO_CLIPS: mode d'ex√©cution de la t√¢che audio sur des clips audio ind√©pendants

AUDIO_STREAM: mode permettant d'ex√©cuter la t√¢che audio sur un flux audio, par exemple √† partir d'un micro. Dans ce mode, resultListener doit √™tre appel√© pour configurer un √©couteur afin de recevoir les r√©sultats de la classification. de mani√®re asynchrone.	{AUDIO_CLIPS, AUDIO_STREAM}	AUDIO_CLIPS
displayNamesLocale	D√©finit la langue des libell√©s √† utiliser pour les noms √† afficher fournis dans les les m√©tadonn√©es du mod√®le de la t√¢che, le cas √©ch√©ant. La valeur par d√©faut est en pour anglais. Vous pouvez ajouter des libell√©s localis√©s aux m√©tadonn√©es d'un mod√®le personnalis√© √† l'aide de l'API TensorFlow Lite Metadata Writer ;	Code des param√®tres r√©gionaux	en
maxResults	D√©finit le nombre maximal facultatif de r√©sultats de classification les mieux not√©s sur retour. Si < 0, tous les r√©sultats disponibles sont renvoy√©s.	Tous les nombres positifs	-1
scoreThreshold	D√©finit le seuil de score de pr√©diction qui remplace celui indiqu√© dans les m√©tadonn√©es du mod√®le (le cas √©ch√©ant). Les r√©sultats inf√©rieurs √† cette valeur sont refus√©s.	[0,0, 1,0]	Non d√©fini
categoryAllowlist	D√©finit la liste facultative des noms de cat√©gories autoris√©s. Si ce champ n'est pas vide, les r√©sultats de classification dont le nom de cat√©gorie ne fait pas partie de cet ensemble seront filtr√©es. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option s'exclue mutuellement avec categoryDenylist et utilise g√©n√®rent une erreur.	Toutes les cha√Ænes	Non d√©fini
categoryDenylist	D√©finit la liste facultative des noms de cat√©gories non autoris√©s. Si non vide, les r√©sultats de classification dont le nom de cat√©gorie se trouve dans cet ensemble seront filtr√©s s'affiche. Les noms de cat√©gories en double ou inconnus sont ignor√©s. Cette option est mutuellement exclusive avec categoryAllowlist et l'utilisation des deux entra√Æne une erreur.	Toutes les cha√Ænes	Non d√©fini
resultListener	D√©finit l'√©couteur des r√©sultats pour qu'il re√ßoive les r√©sultats de la classification. de mani√®re asynchrone lorsque le classificateur audio se trouve dans le flux audio. . Ne peut √™tre utilis√© que lorsque le mode d'ex√©cution est d√©fini sur AUDIO_STREAM	N/A	Non d√©fini
errorListener	D√©finit un √©couteur d'erreurs facultatif.	N/A	Non d√©fini
Pr√©parer les donn√©es
Le classificateur audio fonctionne avec les clips audio et les flux audio. La t√¢che g√®re le pr√©traitement de l'entr√©e des donn√©es, y compris le r√©√©chantillonnage, la mise en m√©moire tampon et le cadrage. Cependant, vous devez convertir les donn√©es audio d'entr√©e com.google.mediapipe.tasks.components.containers.AudioData avant de le transmettre √† la t√¢che de classificateur audio.

Clips audio
Flux audio
import com.google.mediapipe.tasks.components.containers.AudioData;

// Load an audio on the user‚Äôs device as a float array.

// Convert a float array to a MediaPipe‚Äôs AudioData object.
AudioData audioData =
    AudioData.create(
        AudioData.AudioDataFormat.builder()
            .setNumOfChannels(numOfChannels)
            .setSampleRate(sampleRate)
            .build(),
        floatData.length);
audioData.load(floatData);
    
Ex√©cuter la t√¢che
Vous pouvez appeler la fonction classify correspondant √† votre mode de course pour : pour d√©clencher des inf√©rences. L'API Audio Classifier renvoie les cat√©gories possibles pour les √©v√©nements audio reconnus dans les donn√©es audio d'entr√©e.

Clips audio
Flux audio
AudioClassifierResult classifierResult = audioClassifier.classify(audioData);
    
Veuillez noter les points suivants :

Lors de l'ex√©cution en mode de flux audio, vous devez √©galement fournir le param√®tre T√¢che de classificateur audio avec un code temporel pour suivre les donn√©es audio qu'elle contient le flux a √©t√© utilis√© pour l'inf√©rence.
Lors de l'ex√©cution dans le mod√®le de clips audio, la t√¢che de classificateur audio bloque jusqu'√† ce qu'il ait fini de traiter l'audio d'entr√©e. Pour √©viter de bloquer les r√©ponses de l'interface utilisateur, ex√©cutez le traitement dans un thread d'arri√®re-plan.
Pour voir un exemple d'ex√©cution d'un classificateur audio avec des clips audio, consultez la Classe AudioClassifierHelper dans exemple de code.

G√©rer et afficher les r√©sultats
Apr√®s l'ex√©cution d'une inf√©rence, la t√¢che "Classificateur audio" renvoie une liste pour les √©v√©nements audio dans l'audio d'entr√©e. La liste suivante montre un exemple de donn√©es de sortie de cette t√¢che:

AudioClassifierResult:
  Timestamp in microseconds: 100
  ClassificationResult #0:
    Timestamp in microseconds: 100  
    Classifications #0 (single classification head):
      head index: 0
      category #0:
        category name: "Speech"
        score: 0.6
        index: 0
      category #1:
        category name: "Music"
        score: 0.2
        index: 1
Dans une application Android, la t√¢che renvoie un ClassificationResult contenant un liste d'objets AudioClassifierResult repr√©sentant les pr√©dictions pour une √©v√©nement audio, y compris l'√©tiquette de la cat√©gorie et le score de confiance.

Clips audio
Flux audio
// In the audio clips mode, the classification results are for the entire audio
// clip. The results are timestamped AudioClassifierResult objects, each
// classifying an interval of the entire audio clip that starts at
// ClassificationResult.timestampMs().get().

for (ClassificationResult result : audioClassifierResult.classificationResults()) {
  // Audio interval start timestamp:
  result.timestampMs().get();
  // Classification result of the audio interval.
  result.classifications();
}
    
Vous pouvez voir un exemple d'affichage de la classification r√©sultats renvoy√©s par cette t√¢che dans la classe ProbabilitiesAdapter du exemple de code.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e..


Passer au contenu principal
Google AI for Developers
Solutions

Plus
Recherche
/


Fran√ßais
Connexion
Google AI Edge
MediaPipe
LiteRT
Explorateur de mod√®les
AI Edge Portal
Documentation de r√©f√©rence de l'API
Filtrer

Pr√©sentation de Google AI Edge Portal: √©valuez l'IA Edge √† grande √©chelle. Inscrivez-vous pour demander l'acc√®s pendant l'aper√ßu priv√©.

Cette page a √©t√© traduite par l'API Cloud Translation.
Switch to English
Accueil
Google AI Edge
Solutions
Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentairesGuide de configuration pour Android


Cette page vous explique comment configurer votre environnement de d√©veloppement pour utiliser MediaPipe. T√¢ches dans vos applications Android.

Appareils et plates-formes compatibles
Pour cr√©er des applications Android avec MediaPipe Tasks, votre environnement de d√©veloppement n√©cessite les √©l√©ments suivants:

Android Studio avec un version recommand√©e d'au moins 2021.1.1 (Bumblebee) ou une autre version compatible IDE.
SDK Android 24 ou version ult√©rieure
Appareil Android disposant au moins de la version minimale du SDK. Un √©mulateur Android peut ne pas fonctionner pour toutes les t√¢ches.
Configuration de l'environnement de d√©veloppement
Avant d'ex√©cuter une t√¢che MediaPipe sur une application Android, vous devez une application existante ou cr√©ez un projet Android Studio sur votre ordinateur local. MediaPipe s'int√®gre dans l'environnement d'une couche de votre application, qui contient les donn√©es d'application et la logique m√©tier. Pour en savoir plus sur Architecture des applications Android, consultez le Guide des applications architecture.

Configuration des appareils Android
Vous devez activer les options pour les d√©veloppeurs et le d√©bogage USB sur un appareil Android physique avant de l'utiliser pour tester votre application. Pour savoir comment configurer votre appareil avec les options pour les d√©veloppeurs, consultez Configurer sur l'appareil les d√©veloppeurs options.

Pour les t√¢ches qui ne n√©cessitent pas de cam√©ra ni de micro, vous pouvez utiliser un √âmulateur d'appareil Android au lieu d'un appareil Android physique. Instructions sur la configuration d'Android Emulator, consultez l'article Ex√©cuter des applications sur le √† l'aide de l'√©mulateur.

Exemple de configuration de code
Le MediaPipe D√©p√¥t d'exemples contient des exemples d'applications Android pour chaque t√¢che MediaPipe. Vous pouvez cr√©er un √† partir de l'exemple de code, cr√©ez le projet, puis ex√©cutez-le.

Pour importer et cr√©er l'exemple de projet de code:

Lancez Android Studio.
Dans Android Studio, s√©lectionnez File > Nouveau > Importer un projet.
Acc√©dez au r√©pertoire de l'exemple de code contenant le fichier build.gradle. et s√©lectionnez ce r√©pertoire, par exemple: .../mediapipe/examples/text_classification/android/build.gradle
Si Android Studio demande une synchronisation Gradle, s√©lectionnez OK.
Assurez-vous que votre appareil Android est connect√© √† votre ordinateur et votre d√©veloppeur est activ√©. Cliquez sur la fl√®che verte Run.
Si vous s√©lectionnez le r√©pertoire appropri√©, Android Studio cr√©e un projet et le cr√©e. Cette op√©ration peut prendre quelques minutes, en fonction de la vitesse ordinateur et si vous avez utilis√© Android Studio pour d'autres projets. Lorsque la compilation est termin√©e, Android Studio affiche le message BUILD SUCCESSFUL dans Panneau d'√©tat Build Output (Sortie de compilation)

Pour ex√©cuter le projet:

Dans Android Studio, ex√©cutez le projet en s√©lectionnant Run > Ex√©cuter...
S√©lectionnez un appareil Android (ou un √©mulateur) connect√© pour tester l'application.
D√©pendances MediaPipe Tasks
MediaPipe Tasks fournit trois biblioth√®ques pr√©d√©finies pour la vision, le texte et l'audio. La Le fichier de mod√®le .tflite doit se trouver dans le r√©pertoire d'√©l√©ments d'Android qui utilise le mod√®le. Selon la t√¢che MediaPipe utilis√©e par l'application, ajoutez la biblioth√®que d'images, de texte ou audio √† la liste des d√©pendances build.gradle.

<ph type="x-smartling-placeholder">
</ph> Attention:Cet aper√ßu de MediaPipe Solutions est une version pr√©liminaire. En savoir plus
T√¢ches d'IA g√©n√©rative
Les biblioth√®ques d'IA g√©n√©rative de MediaPipe Tasks contiennent des t√¢ches qui traitent la g√©n√©ration de texte. Pour importer les biblioth√®ques d'IA g√©n√©rative MediaPipe Tasks dans Android Studio : ajoutez les d√©pendances √† votre fichier build.gradle.

G√©n√©rateur d'images
La t√¢che de g√©n√©ration d'images MediaPipe se trouve dans la section biblioth√®que tasks-vision-image-generator. Ajoutez la d√©pendance √† votre Fichier build.gradle:

dependencies {
    implementation 'com.google.mediapipe:tasks-vision-image-generator:latest.release'
}
API LLM Inference
La t√¢che d'inf√©rence LLM MediaPipe est contenue dans tasks-genai biblioth√®que. Ajoutez la d√©pendance √† votre fichier build.gradle:

dependencies {
    implementation 'com.google.mediapipe:tasks-genai:latest.release'
}
T√¢ches visuelles
La biblioth√®que MediaPipe Tasks Vision contient des t√¢ches qui g√®rent des images ou des vid√©os d'entr√©e. Pour importer la biblioth√®que MediaPipe Tasks Vision dans Android Studio, ajoutez le les d√©pendances suivantes √† votre fichier build.gradle:

dependencies {
    implementation 'com.google.mediapipe:tasks-vision:latest.release'
}
T√¢ches li√©es au texte
La biblioth√®que de texte MediaPipe Tasks contient des t√¢ches qui traitent des donn√©es de langue dans format texte. Pour importer la biblioth√®que de texte MediaPipe Tasks dans Android Studio, ajoutez les d√©pendances suivantes √† votre fichier build.gradle:

dependencies {
    implementation 'com.google.mediapipe:tasks-text:latest.release'
}
T√¢ches audio
La biblioth√®que audio MediaPipe Tasks contient des t√¢ches qui g√®rent les entr√©es audio. √Ä importez la biblioth√®que audio MediaPipe Tasks dans Android Studio, ajoutez ce qui suit : d√©pendances √† votre fichier build.gradle:

dependencies {
    implementation 'com.google.mediapipe:tasks-audio:latest.release'
}
Configuration de BaseOptions
Le fichier BaseOptions permet une configuration g√©n√©rale des API MediaPipe Tasks.

Nom de l'option	Description	Valeurs accept√©es
modelAssetBuffer	Contenu du fichier d'√©l√©ments du mod√®le sous forme de ByteBuffer ou de MappedByteBuffer direct.	ByteBuffer ou MappedByteBuffer sous forme de cha√Æne
modelAssetPath	Chemin d'acc√®s du mod√®le √† un fichier d'√©l√©ments de mod√®le dans le dossier d'√©l√©ments d'application Android.	Chemin d'acc√®s au fichier sous forme de cha√Æne
modelAssetFileDescriptor	Entier du descripteur de fichier natif d'un fichier d'√©l√©ments de mod√®le.	Entier sp√©cifiant le descripteur de fichier
Delegate	Active l'acc√©l√©ration mat√©rielle via un d√©l√©gu√© d'appareil afin d'ex√©cuter le pipeline MediaPipe. Valeur par d√©faut : CPU	[CPU,
GPU]
Acc√©l√©ration mat√©rielle
MediaPipe Tasks prend en charge l'utilisation de processeurs graphiques (GPU) pour ex√©cuter des mod√®les de machine learning. Sur les appareils Android, vous pouvez activer l'acc√®s L'ex√©cution de vos mod√®les est acc√©l√©r√©e par le GPU √† l'aide d'un d√©l√©gu√©. Les d√©l√©gu√©s agissent en tant que pilotes mat√©riels pour MediaPipe, qui vous permettent d'ex√©cuter vos mod√®les sur des GPU au lieu des processeurs standards.

Configurez le d√©l√©gu√© de GPU dans les options de t√¢che via BaseOptions:

BaseOptions baseOptions = BaseOptions.builder().useGpu().build();
D√©pannage
Pour obtenir de l'aide sur des questions techniques concernant MediaPipe, consultez la discussion group ou Stack D√©passement pour obtenir de l'aide la communaut√©. Pour signaler des bugs ou demander des fonctionnalit√©s, signalez un probl√®me sur GitHub

Pour obtenir de l'aide pour la configuration de votre environnement de d√©veloppement Android, consultez le documentation destin√©e aux d√©veloppeurs.

Ce contenu vous a-t-il √©t√© utile ?

Envoyer des commentaires
Sauf indication contraire, le contenu de cette page est r√©gi par une licence Creative Commons Attribution 4.0, et les √©chantillons de code sont r√©gis par une licence Apache 2.0. Pour en savoir plus, consultez les R√®gles du site Google Developers. Java est une marque d√©pos√©e d'Oracle et/ou de ses soci√©t√©s affili√©es.

Derni√®re mise √† jour le 2025/07/24 (UTC).

Conditions d'utilisation
R√®gles de confidentialit√©

Fran√ßais
La nouvelle page a √©t√© charg√©e.