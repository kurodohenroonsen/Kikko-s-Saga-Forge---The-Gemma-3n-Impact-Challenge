

--- START OF FILE app/build.gradle ---

plugins {
    id 'com.android.application'
    id 'org.jetbrains.kotlin.android'
}

android {
    namespace "com.google.mlkit.vision.demo"
    compileSdkVersion 34

    defaultConfig {
        applicationId "com.google.mlkit.vision.demo"
        minSdkVersion 21
        targetSdkVersion 34
        multiDexEnabled true

        versionCode 11
        versionName "1.11"
        vectorDrawables.useSupportLibrary = true
        setProperty("archivesBaseName", "vision-quickstart")
    }

    // ADDED: buildFeatures to explicitly enable BuildConfig generation
    buildFeatures {
        buildConfig true
    }

    buildTypes {
        release {
            minifyEnabled true
            shrinkResources true
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
        }
        debug {
            // debug is the default build type
        }
    }

    // CHANGED: Updated to Java 17, the modern standard for Android
    compileOptions {
        sourceCompatibility JavaVersion.VERSION_17
        targetCompatibility JavaVersion.VERSION_17
    }
    kotlinOptions {
        jvmTarget = '17'
    }

    // Do NOT compress tflite model files
    aaptOptions {
        noCompress "tflite"
    }
}

// REMOVED: The snapshot repository is unstable and not needed with updated libraries.

dependencies {
    // Standard Kotlin library
    implementation "org.jetbrains.kotlin:kotlin-stdlib:$kotlin_version"
    implementation 'androidx.multidex:multidex:2.0.1'

// Barcode model
    implementation 'com.google.mlkit:barcode-scanning:17.3.0'
    // Or comment the dependency above and uncomment the dependency below to
    // use unbundled model that depends on Google Play Services
    // implementation 'com.google.android.gms:play-services-mlkit-barcode-scanning:18.3.1'

    // Object detection feature with bundled default classifier
    implementation 'com.google.mlkit:object-detection:17.0.2'

    // Object detection feature with custom classifier support
    implementation 'com.google.mlkit:object-detection-custom:17.0.2'

    // Face features
    implementation 'com.google.mlkit:face-detection:16.1.7'
    // Or comment the dependency above and uncomment the dependency below to
    // use unbundled model that depends on Google Play Services
    // implementation 'com.google.android.gms:play-services-mlkit-face-detection:17.1.0'

    // Text features
    implementation 'com.google.mlkit:text-recognition:16.0.1'
    // Or comment the dependency above and uncomment the dependency below to
    // use unbundled model that depends on Google Play Services
    // implementation 'com.google.android.gms:play-services-mlkit-text-recognition:19.0.1'
    implementation 'com.google.mlkit:text-recognition-chinese:16.0.1'
    // Or comment the dependency above and uncomment the dependency below to
    // use unbundled model that depends on Google Play Services
    // implementation 'com.google.android.gms:play-services-mlkit-text-recognition-chinese:16.0.1'
    implementation 'com.google.mlkit:text-recognition-devanagari:16.0.1'
    // Or comment the dependency above and uncomment the dependency below to
    // use unbundled model that depends on Google Play Services
    // implementation 'com.google.android.gms:play-services-mlkit-text-recognition-devanagari:16.0.1'
    implementation 'com.google.mlkit:text-recognition-japanese:16.0.1'
    // Or comment the dependency above and uncomment the dependency below to
    // use unbundled model that depends on Google Play Services
    // implementation 'com.google.android.gms:play-services-mlkit-text-recognition-japanese:16.0.1'
    implementation 'com.google.mlkit:text-recognition-korean:16.0.1'
    // Or comment the dependency above and uncomment the dependency below to
    // use unbundled model that depends on Google Play Services
    // implementation 'com.google.android.gms:play-services-mlkit-text-recognition-korean:16.0.1'

    // Image labeling
    implementation 'com.google.mlkit:image-labeling:17.0.9'
    // Or comment the dependency above and uncomment the dependency below to
    // use unbundled model that depends on Google Play Services
    // implementation 'com.google.android.gms:play-services-mlkit-image-labeling:16.0.8'

    // Image labeling custom
    implementation 'com.google.mlkit:image-labeling-custom:17.0.3'
    // Or comment the dependency above and uncomment the dependency below to
    // use unbundled model that depends on Google Play Services
    // implementation 'com.google.android.gms:play-services-mlkit-image-labeling-custom:16.0.0-beta5'

    // Pose detection with default models
    implementation 'com.google.mlkit:pose-detection:18.0.0-beta5'
    // Pose detection with accurate models
    implementation 'com.google.mlkit:pose-detection-accurate:18.0.0-beta5'

    // Selfie segmentation
    implementation 'com.google.mlkit:segmentation-selfie:16.0.0-beta6'

    implementation 'com.google.mlkit:camera:16.0.0-beta3'

    // Face Mesh Detection
    implementation 'com.google.mlkit:face-mesh-detection:16.0.0-beta3'

    // Subject Segmentation
    implementation 'com.google.android.gms:play-services-mlkit-subject-segmentation:16.0.0-beta1'

    // ML Kit Camera helper
    implementation 'com.google.mlkit:camera:16.0.0-beta4'

    // UPDATED Core Libraries to latest stable versions
    implementation 'com.google.code.gson:gson:2.10.1'
    implementation 'com.google.guava:guava:33.2.1-android'

    // UPDATED AndroidX & Lifecycle Libraries
    implementation 'androidx.appcompat:appcompat:1.7.0'
    implementation 'androidx.annotation:annotation:1.8.0'
    implementation 'androidx.constraintlayout:constraintlayout:2.1.4'
    implementation "androidx.lifecycle:lifecycle-livedata-ktx:2.8.3"
    implementation "androidx.lifecycle:lifecycle-viewmodel-ktx:2.8.3"

    // UPDATED CameraX Libraries to latest stable versions
    def camerax_version = "1.3.4"
    implementation "androidx.camera:camera-core:$camerax_version"
    implementation "androidx.camera:camera-camera2:$camerax_version"
    implementation "androidx.camera:camera-lifecycle:$camerax_version"
    implementation "androidx.camera:camera-view:$camerax_version"

    // On Device Machine Learnings
    implementation "com.google.android.odml:image:1.0.0-beta1"

    // UPDATED Testing Libraries
    androidTestImplementation 'androidx.test:core:1.6.1'
    androidTestImplementation 'androidx.test:runner:1.6.1'
    androidTestImplementation 'androidx.test:rules:1.6.1'
    androidTestImplementation 'androidx.test.ext:junit:1.2.1'
}

configurations {
    all*.exclude group: 'com.google.guava', module: 'listenablefuture'
}

--- END OF FILE app/build.gradle ---


--- START OF FILE app/src/main/AndroidManifest.xml ---

<?xml version="1.0" encoding="utf-8"?>
<manifest
    xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:tools="http://schemas.android.com/tools"
    package="com.google.mlkit.vision.demo"
    android:installLocation="auto">
    <!-- CameraX libraries require minSdkVersion 21, while this quickstart app
    supports low to 19. Needs to use overrideLibrary to make the merger tool
    ignore this conflict and import the libraries while keeping the app's lower
    minSdkVersion value. In code, will check SDK version, before calling CameraX
    APIs. -->
    <uses-sdk
        tools:overrideLibrary="
          androidx.camera.camera2, androidx.camera.core,
          androidx.camera.view, androidx.camera.lifecycle,
          com.google.mlkit.vision.segmentation.subject" />

    <uses-feature android:name="android.hardware.camera"/>

    <uses-permission android:name="android.permission.INTERNET"/>
    <uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE"/>
    <uses-permission android:name="android.permission.READ_EXTERNAL_STORAGE"/>
    <uses-permission android:name="android.permission.CAMERA"/>

    <application
        android:name="androidx.multidex.MultiDexApplication"
        android:icon="@drawable/logo_mlkit"
        android:label="@string/app_name"
        android:theme="@style/Theme.AppCompat">

        <meta-data
            android:name="com.google.android.gms.version"
            android:value="@integer/google_play_services_version"/>

        <!-- Optional: Add it to automatically download ML model to device after
          your app is installed.-->
        <meta-data
            android:name="com.google.mlkit.vision.DEPENDENCIES"
            android:value="barcode,face,ocr,ocr_chinese,ocr_devanagari,ocr_japanese,ocr_korean,ica,custom_ica,subject_segment"/>

        <activity
            android:name=".EntryChoiceActivity"
            android:exported="true"
            android:theme="@style/AppTheme">
            <intent-filter>
                <action android:name="android.intent.action.MAIN"/>
                <category android:name="android.intent.category.LAUNCHER"/>
            </intent-filter>
        </activity>

        <activity
            android:name=".java.ChooserActivity"
            android:exported="false">
        </activity>

        <activity
            android:name=".java.LivePreviewActivity"
            android:exported="false"
            android:theme="@style/AppTheme">
        </activity>

        <activity
            android:name=".java.CameraXLivePreviewActivity"
            android:exported="false"
            android:theme="@style/AppTheme">
        </activity>

        <activity
            android:name=".java.CameraXSourceDemoActivity"
            android:exported="false"
            android:theme="@style/AppTheme">
        </activity>

        <activity
            android:name=".java.StillImageActivity"
            android:exported="false"
            android:theme="@style/AppTheme">
        </activity>

        <activity
            android:name=".kotlin.ChooserActivity"
            android:exported="false">
        </activity>

        <activity
            android:name=".kotlin.LivePreviewActivity"
            android:exported="false"
            android:theme="@style/AppTheme">
        </activity>

        <activity
            android:name=".kotlin.CameraXLivePreviewActivity"
            android:exported="false"
            android:theme="@style/AppTheme">
        </activity>

        <activity
            android:name=".kotlin.CameraXSourceDemoActivity"
            android:exported="false"
            android:theme="@style/AppTheme">
        </activity>

        <activity
            android:name=".kotlin.StillImageActivity"
            android:exported="false"
            android:theme="@style/AppTheme">
        </activity>

        <activity
            android:name=".preference.SettingsActivity"
            android:exported="false"/>

    </application>
    <queries>
        <intent>
            <action android:name="android.media.action.IMAGE_CAPTURE" />
        </intent>
    </queries>

</manifest>


--- END OF FILE app/src/main/AndroidManifest.xml ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/EntryChoiceActivity.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo

import android.Manifest
import android.content.Context
import android.content.Intent
import android.content.pm.PackageManager
import android.os.Bundle
import androidx.appcompat.app.AppCompatActivity
import android.util.Log
import android.widget.TextView
import androidx.core.app.ActivityCompat
import androidx.core.content.ContextCompat
import com.google.mlkit.vision.demo.java.ChooserActivity
import java.util.ArrayList

class EntryChoiceActivity : AppCompatActivity(), ActivityCompat.OnRequestPermissionsResultCallback {

  override fun onCreate(savedInstanceState: Bundle?) {
    super.onCreate(savedInstanceState)
    setContentView(R.layout.activity_vision_entry_choice)

    findViewById<TextView>(R.id.java_entry_point).setOnClickListener {
      val intent = Intent(this@EntryChoiceActivity, ChooserActivity::class.java)
      startActivity(intent)
    }

    findViewById<TextView>(R.id.kotlin_entry_point).setOnClickListener {
      val intent =
        Intent(
          this@EntryChoiceActivity,
          com.google.mlkit.vision.demo.kotlin.ChooserActivity::class.java
        )
      startActivity(intent)
    }

    if (!allRuntimePermissionsGranted()) {
      getRuntimePermissions()
    }
  }

  private fun allRuntimePermissionsGranted(): Boolean {
    for (permission in REQUIRED_RUNTIME_PERMISSIONS) {
      permission?.let {
        if (!isPermissionGranted(this, it)) {
          return false
        }
      }
    }
    return true
  }

  private fun getRuntimePermissions() {
    val permissionsToRequest = ArrayList<String>()
    for (permission in REQUIRED_RUNTIME_PERMISSIONS) {
      permission?.let {
        if (!isPermissionGranted(this, it)) {
          permissionsToRequest.add(permission)
        }
      }
    }

    if (permissionsToRequest.isNotEmpty()) {
      ActivityCompat.requestPermissions(
        this,
        permissionsToRequest.toTypedArray(),
        PERMISSION_REQUESTS
      )
    }
  }

  private fun isPermissionGranted(context: Context, permission: String): Boolean {
    if (ContextCompat.checkSelfPermission(context, permission) == PackageManager.PERMISSION_GRANTED
    ) {
      Log.i(TAG, "Permission granted: $permission")
      return true
    }
    Log.i(TAG, "Permission NOT granted: $permission")
    return false
  }

  companion object {
    private const val TAG = "EntryChoiceActivity"
    private const val PERMISSION_REQUESTS = 1

    private val REQUIRED_RUNTIME_PERMISSIONS =
      arrayOf(
        Manifest.permission.CAMERA,
        Manifest.permission.WRITE_EXTERNAL_STORAGE,
        Manifest.permission.READ_EXTERNAL_STORAGE
      )
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/EntryChoiceActivity.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/CameraXLivePreviewActivity.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin

import android.content.Intent
import android.os.Build.VERSION_CODES
import android.os.Bundle
import androidx.appcompat.app.AppCompatActivity
import android.util.Log
import android.view.View
import android.widget.AdapterView
import android.widget.AdapterView.OnItemSelectedListener
import android.widget.ArrayAdapter
import android.widget.CompoundButton
import android.widget.ImageView
import android.widget.Spinner
import android.widget.Toast
import android.widget.ToggleButton
import androidx.annotation.RequiresApi
import androidx.camera.core.Camera
import androidx.camera.core.CameraInfoUnavailableException
import androidx.camera.core.CameraSelector
import androidx.camera.core.ImageAnalysis
import androidx.camera.core.ImageProxy
import androidx.camera.core.Preview
import androidx.camera.lifecycle.ProcessCameraProvider
import androidx.camera.view.PreviewView
import androidx.core.content.ContextCompat
import androidx.lifecycle.Observer
import androidx.lifecycle.ViewModelProvider
import com.google.android.gms.common.annotation.KeepName
import com.google.mlkit.common.MlKitException
import com.google.mlkit.common.model.LocalModel
import com.google.mlkit.vision.barcode.ZoomSuggestionOptions.ZoomCallback
import com.google.mlkit.vision.demo.CameraXViewModel
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.R
import com.google.mlkit.vision.demo.VisionImageProcessor
import com.google.mlkit.vision.demo.kotlin.barcodescanner.BarcodeScannerProcessor
import com.google.mlkit.vision.demo.kotlin.facedetector.FaceDetectorProcessor
import com.google.mlkit.vision.demo.kotlin.facemeshdetector.FaceMeshDetectorProcessor
import com.google.mlkit.vision.demo.kotlin.labeldetector.LabelDetectorProcessor
import com.google.mlkit.vision.demo.kotlin.objectdetector.ObjectDetectorProcessor
import com.google.mlkit.vision.demo.kotlin.posedetector.PoseDetectorProcessor
import com.google.mlkit.vision.demo.kotlin.segmenter.SegmenterProcessor
import com.google.mlkit.vision.demo.kotlin.textdetector.TextRecognitionProcessor
import com.google.mlkit.vision.demo.preference.PreferenceUtils
import com.google.mlkit.vision.demo.preference.SettingsActivity
import com.google.mlkit.vision.demo.preference.SettingsActivity.LaunchSource
import com.google.mlkit.vision.label.custom.CustomImageLabelerOptions
import com.google.mlkit.vision.label.defaults.ImageLabelerOptions
import com.google.mlkit.vision.text.chinese.ChineseTextRecognizerOptions
import com.google.mlkit.vision.text.devanagari.DevanagariTextRecognizerOptions
import com.google.mlkit.vision.text.japanese.JapaneseTextRecognizerOptions
import com.google.mlkit.vision.text.korean.KoreanTextRecognizerOptions
import com.google.mlkit.vision.text.latin.TextRecognizerOptions

/** Live preview demo app for ML Kit APIs using CameraX. */
@KeepName
@RequiresApi(VERSION_CODES.LOLLIPOP)
class CameraXLivePreviewActivity :
  AppCompatActivity(), OnItemSelectedListener, CompoundButton.OnCheckedChangeListener {

  private var previewView: PreviewView? = null
  private var graphicOverlay: GraphicOverlay? = null
  private var cameraProvider: ProcessCameraProvider? = null
  private var camera: Camera? = null
  private var previewUseCase: Preview? = null
  private var analysisUseCase: ImageAnalysis? = null
  private var imageProcessor: VisionImageProcessor? = null
  private var needUpdateGraphicOverlayImageSourceInfo = false
  private var selectedModel = OBJECT_DETECTION
  private var lensFacing = CameraSelector.LENS_FACING_BACK
  private var cameraSelector: CameraSelector? = null

  override fun onCreate(savedInstanceState: Bundle?) {
    super.onCreate(savedInstanceState)
    Log.d(TAG, "onCreate")
    if (savedInstanceState != null) {
      selectedModel = savedInstanceState.getString(STATE_SELECTED_MODEL, OBJECT_DETECTION)
    }
    cameraSelector = CameraSelector.Builder().requireLensFacing(lensFacing).build()
    setContentView(R.layout.activity_vision_camerax_live_preview)
    previewView = findViewById(R.id.preview_view)
    if (previewView == null) {
      Log.d(TAG, "previewView is null")
    }
    graphicOverlay = findViewById(R.id.graphic_overlay)
    if (graphicOverlay == null) {
      Log.d(TAG, "graphicOverlay is null")
    }
    val spinner = findViewById<Spinner>(R.id.spinner)
    val options: MutableList<String> = ArrayList()
    options.add(OBJECT_DETECTION)
    options.add(OBJECT_DETECTION_CUSTOM)
    options.add(CUSTOM_AUTOML_OBJECT_DETECTION)
    options.add(FACE_DETECTION)
    options.add(BARCODE_SCANNING)
    options.add(IMAGE_LABELING)
    options.add(IMAGE_LABELING_CUSTOM)
    options.add(CUSTOM_AUTOML_LABELING)
    options.add(POSE_DETECTION)
    options.add(SELFIE_SEGMENTATION)
    options.add(TEXT_RECOGNITION_LATIN)
    options.add(TEXT_RECOGNITION_CHINESE)
    options.add(TEXT_RECOGNITION_DEVANAGARI)
    options.add(TEXT_RECOGNITION_JAPANESE)
    options.add(TEXT_RECOGNITION_KOREAN)
    options.add(FACE_MESH_DETECTION)

    // Creating adapter for spinner
    val dataAdapter = ArrayAdapter(this, R.layout.spinner_style, options)
    // Drop down layout style - list view with radio button
    dataAdapter.setDropDownViewResource(android.R.layout.simple_spinner_dropdown_item)
    // attaching data adapter to spinner
    spinner.adapter = dataAdapter
    spinner.onItemSelectedListener = this
    val facingSwitch = findViewById<ToggleButton>(R.id.facing_switch)
    facingSwitch.setOnCheckedChangeListener(this)
    ViewModelProvider(this, ViewModelProvider.AndroidViewModelFactory.getInstance(application))
      .get(CameraXViewModel::class.java)
      .processCameraProvider
      .observe(
        this,
        Observer { provider: ProcessCameraProvider? ->
          cameraProvider = provider
          bindAllCameraUseCases()
        },
      )

    val settingsButton = findViewById<ImageView>(R.id.settings_button)
    settingsButton.setOnClickListener {
      val intent = Intent(applicationContext, SettingsActivity::class.java)
      intent.putExtra(SettingsActivity.EXTRA_LAUNCH_SOURCE, LaunchSource.CAMERAX_LIVE_PREVIEW)
      startActivity(intent)
    }
  }

  override fun onSaveInstanceState(bundle: Bundle) {
    super.onSaveInstanceState(bundle)
    bundle.putString(STATE_SELECTED_MODEL, selectedModel)
  }

  @Synchronized
  override fun onItemSelected(parent: AdapterView<*>?, view: View?, pos: Int, id: Long) {
    // An item was selected. You can retrieve the selected item using
    // parent.getItemAtPosition(pos)
    selectedModel = parent?.getItemAtPosition(pos).toString()
    Log.d(TAG, "Selected model: $selectedModel")
    bindAnalysisUseCase()
  }

  override fun onNothingSelected(parent: AdapterView<*>?) {
    // Do nothing.
  }

  override fun onCheckedChanged(buttonView: CompoundButton, isChecked: Boolean) {
    if (cameraProvider == null) {
      return
    }
    val newLensFacing =
      if (lensFacing == CameraSelector.LENS_FACING_FRONT) {
        CameraSelector.LENS_FACING_BACK
      } else {
        CameraSelector.LENS_FACING_FRONT
      }
    val newCameraSelector = CameraSelector.Builder().requireLensFacing(newLensFacing).build()
    try {
      if (cameraProvider!!.hasCamera(newCameraSelector)) {
        Log.d(TAG, "Set facing to " + newLensFacing)
        lensFacing = newLensFacing
        cameraSelector = newCameraSelector
        bindAllCameraUseCases()
        return
      }
    } catch (e: CameraInfoUnavailableException) {
      // Falls through
    }
    Toast.makeText(
        applicationContext,
        "This device does not have lens with facing: $newLensFacing",
        Toast.LENGTH_SHORT,
      )
      .show()
  }

  public override fun onResume() {
    super.onResume()
    bindAllCameraUseCases()
  }

  override fun onPause() {
    super.onPause()

    imageProcessor?.run { this.stop() }
  }

  public override fun onDestroy() {
    super.onDestroy()
    imageProcessor?.run { this.stop() }
  }

  private fun bindAllCameraUseCases() {
    if (cameraProvider != null) {
      // As required by CameraX API, unbinds all use cases before trying to re-bind any of them.
      cameraProvider!!.unbindAll()
      bindPreviewUseCase()
      bindAnalysisUseCase()
    }
  }

  private fun bindPreviewUseCase() {
    if (!PreferenceUtils.isCameraLiveViewportEnabled(this)) {
      return
    }
    if (cameraProvider == null) {
      return
    }
    if (previewUseCase != null) {
      cameraProvider!!.unbind(previewUseCase)
    }

    val builder = Preview.Builder()
    val targetResolution = PreferenceUtils.getCameraXTargetResolution(this, lensFacing)
    if (targetResolution != null) {
      builder.setTargetResolution(targetResolution)
    }
    previewUseCase = builder.build()
    previewUseCase!!.setSurfaceProvider(previewView!!.getSurfaceProvider())
    camera = cameraProvider!!.bindToLifecycle(this, cameraSelector!!, previewUseCase)
  }

  private fun bindAnalysisUseCase() {
    if (cameraProvider == null) {
      return
    }
    if (analysisUseCase != null) {
      cameraProvider!!.unbind(analysisUseCase)
    }
    if (imageProcessor != null) {
      imageProcessor!!.stop()
    }
    imageProcessor =
      try {
        when (selectedModel) {
          OBJECT_DETECTION -> {
            Log.i(TAG, "Using Object Detector Processor")
            val objectDetectorOptions = PreferenceUtils.getObjectDetectorOptionsForLivePreview(this)
            ObjectDetectorProcessor(this, objectDetectorOptions)
          }
          OBJECT_DETECTION_CUSTOM -> {
            Log.i(TAG, "Using Custom Object Detector (with object labeler) Processor")
            val localModel =
              LocalModel.Builder().setAssetFilePath("custom_models/object_labeler.tflite").build()
            val customObjectDetectorOptions =
              PreferenceUtils.getCustomObjectDetectorOptionsForLivePreview(this, localModel)
            ObjectDetectorProcessor(this, customObjectDetectorOptions)
          }
          CUSTOM_AUTOML_OBJECT_DETECTION -> {
            Log.i(TAG, "Using Custom AutoML Object Detector Processor")
            val customAutoMLODTLocalModel =
              LocalModel.Builder().setAssetManifestFilePath("automl/manifest.json").build()
            val customAutoMLODTOptions =
              PreferenceUtils.getCustomObjectDetectorOptionsForLivePreview(
                this,
                customAutoMLODTLocalModel,
              )
            ObjectDetectorProcessor(this, customAutoMLODTOptions)
          }
          TEXT_RECOGNITION_LATIN -> {
            Log.i(TAG, "Using on-device Text recognition Processor for Latin")
            TextRecognitionProcessor(this, TextRecognizerOptions.Builder().build())
          }
          TEXT_RECOGNITION_CHINESE -> {
            Log.i(TAG, "Using on-device Text recognition Processor for Latin and Chinese")
            TextRecognitionProcessor(this, ChineseTextRecognizerOptions.Builder().build())
          }
          TEXT_RECOGNITION_DEVANAGARI -> {
            Log.i(TAG, "Using on-device Text recognition Processor for Latin and Devanagari")
            TextRecognitionProcessor(this, DevanagariTextRecognizerOptions.Builder().build())
          }
          TEXT_RECOGNITION_JAPANESE -> {
            Log.i(TAG, "Using on-device Text recognition Processor for Latin and Japanese")
            TextRecognitionProcessor(this, JapaneseTextRecognizerOptions.Builder().build())
          }
          TEXT_RECOGNITION_KOREAN -> {
            Log.i(TAG, "Using on-device Text recognition Processor for Latin and Korean")
            TextRecognitionProcessor(this, KoreanTextRecognizerOptions.Builder().build())
          }
          FACE_DETECTION -> {
            Log.i(TAG, "Using Face Detector Processor")
            val faceDetectorOptions = PreferenceUtils.getFaceDetectorOptions(this)
            FaceDetectorProcessor(this, faceDetectorOptions)
          }
          BARCODE_SCANNING -> {
            Log.i(TAG, "Using Barcode Detector Processor")
            var zoomCallback: ZoomCallback? = null
            if (PreferenceUtils.shouldEnableAutoZoom(this)) {
              zoomCallback = ZoomCallback { zoomLevel: Float ->
                Log.i(TAG, "Set zoom ratio $zoomLevel")
                val ignored = camera!!.cameraControl.setZoomRatio(zoomLevel)
                true
              }
            }
            BarcodeScannerProcessor(this, zoomCallback)
          }
          IMAGE_LABELING -> {
            Log.i(TAG, "Using Image Label Detector Processor")
            LabelDetectorProcessor(this, ImageLabelerOptions.DEFAULT_OPTIONS)
          }
          IMAGE_LABELING_CUSTOM -> {
            Log.i(TAG, "Using Custom Image Label (Birds) Detector Processor")
            val localClassifier =
              LocalModel.Builder().setAssetFilePath("custom_models/bird_classifier.tflite").build()
            val customImageLabelerOptions =
              CustomImageLabelerOptions.Builder(localClassifier).build()
            LabelDetectorProcessor(this, customImageLabelerOptions)
          }
          CUSTOM_AUTOML_LABELING -> {
            Log.i(TAG, "Using Custom AutoML Image Label Detector Processor")
            val customAutoMLLabelLocalModel =
              LocalModel.Builder().setAssetManifestFilePath("automl/manifest.json").build()
            val customAutoMLLabelOptions =
              CustomImageLabelerOptions.Builder(customAutoMLLabelLocalModel)
                .setConfidenceThreshold(0f)
                .build()
            LabelDetectorProcessor(this, customAutoMLLabelOptions)
          }
          POSE_DETECTION -> {
            val poseDetectorOptions = PreferenceUtils.getPoseDetectorOptionsForLivePreview(this)
            val shouldShowInFrameLikelihood =
              PreferenceUtils.shouldShowPoseDetectionInFrameLikelihoodLivePreview(this)
            val visualizeZ = PreferenceUtils.shouldPoseDetectionVisualizeZ(this)
            val rescaleZ = PreferenceUtils.shouldPoseDetectionRescaleZForVisualization(this)
            val runClassification = PreferenceUtils.shouldPoseDetectionRunClassification(this)
            PoseDetectorProcessor(
              this,
              poseDetectorOptions,
              shouldShowInFrameLikelihood,
              visualizeZ,
              rescaleZ,
              runClassification,
              /* isStreamMode = */ true,
            )
          }
          SELFIE_SEGMENTATION -> SegmenterProcessor(this)
          FACE_MESH_DETECTION -> FaceMeshDetectorProcessor(this)
          else -> throw IllegalStateException("Invalid model name")
        }
      } catch (e: Exception) {
        Log.e(TAG, "Can not create image processor: $selectedModel", e)
        Toast.makeText(
            applicationContext,
            "Can not create image processor: " + e.localizedMessage,
            Toast.LENGTH_LONG,
          )
          .show()
        return
      }

    val builder = ImageAnalysis.Builder()
    val targetResolution = PreferenceUtils.getCameraXTargetResolution(this, lensFacing)
    if (targetResolution != null) {
      builder.setTargetResolution(targetResolution)
    }
    analysisUseCase = builder.build()

    needUpdateGraphicOverlayImageSourceInfo = true

    analysisUseCase?.setAnalyzer(
      // imageProcessor.processImageProxy will use another thread to run the detection underneath,
      // thus we can just runs the analyzer itself on main thread.
      ContextCompat.getMainExecutor(this),
      ImageAnalysis.Analyzer { imageProxy: ImageProxy ->
        if (needUpdateGraphicOverlayImageSourceInfo) {
          val isImageFlipped = lensFacing == CameraSelector.LENS_FACING_FRONT
          val rotationDegrees = imageProxy.imageInfo.rotationDegrees
          if (rotationDegrees == 0 || rotationDegrees == 180) {
            graphicOverlay!!.setImageSourceInfo(imageProxy.width, imageProxy.height, isImageFlipped)
          } else {
            graphicOverlay!!.setImageSourceInfo(imageProxy.height, imageProxy.width, isImageFlipped)
          }
          needUpdateGraphicOverlayImageSourceInfo = false
        }
        try {
          imageProcessor!!.processImageProxy(imageProxy, graphicOverlay)
        } catch (e: MlKitException) {
          Log.e(TAG, "Failed to process image. Error: " + e.localizedMessage)
          Toast.makeText(applicationContext, e.localizedMessage, Toast.LENGTH_SHORT).show()
        }
      },
    )
    cameraProvider!!.bindToLifecycle(this, cameraSelector!!, analysisUseCase)
  }

  companion object {
    private const val TAG = "CameraXLivePreview"
    private const val OBJECT_DETECTION = "Object Detection"
    private const val OBJECT_DETECTION_CUSTOM = "Custom Object Detection"
    private const val CUSTOM_AUTOML_OBJECT_DETECTION = "Custom AutoML Object Detection (Flower)"
    private const val FACE_DETECTION = "Face Detection"
    private const val TEXT_RECOGNITION_LATIN = "Text Recognition Latin"
    private const val TEXT_RECOGNITION_CHINESE = "Text Recognition Chinese"
    private const val TEXT_RECOGNITION_DEVANAGARI = "Text Recognition Devanagari"
    private const val TEXT_RECOGNITION_JAPANESE = "Text Recognition Japanese"
    private const val TEXT_RECOGNITION_KOREAN = "Text Recognition Korean"
    private const val BARCODE_SCANNING = "Barcode Scanning"
    private const val IMAGE_LABELING = "Image Labeling"
    private const val IMAGE_LABELING_CUSTOM = "Custom Image Labeling (Birds)"
    private const val CUSTOM_AUTOML_LABELING = "Custom AutoML Image Labeling (Flower)"
    private const val POSE_DETECTION = "Pose Detection"
    private const val SELFIE_SEGMENTATION = "Selfie Segmentation"
    private const val FACE_MESH_DETECTION = "Face Mesh Detection (Beta)"

    private const val STATE_SELECTED_MODEL = "selected_model"
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/CameraXLivePreviewActivity.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/CameraXSourceDemoActivity.kt ---

/*
 * Copyright 2021 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin

import android.content.Intent
import android.content.res.Configuration
import android.os.Build.VERSION_CODES
import android.os.Bundle
import androidx.appcompat.app.AppCompatActivity
import android.util.Log
import android.util.Size
import android.widget.CompoundButton
import android.widget.ImageView
import android.widget.Toast
import android.widget.ToggleButton
import androidx.annotation.RequiresApi
import androidx.camera.view.PreviewView
import com.google.android.gms.common.annotation.KeepName
import com.google.mlkit.common.model.LocalModel
import com.google.mlkit.vision.camera.CameraSourceConfig
import com.google.mlkit.vision.camera.CameraXSource
import com.google.mlkit.vision.camera.DetectionTaskCallback
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.InferenceInfoGraphic
import com.google.mlkit.vision.demo.R
import com.google.mlkit.vision.demo.kotlin.objectdetector.ObjectGraphic
import com.google.mlkit.vision.demo.preference.PreferenceUtils
import com.google.mlkit.vision.demo.preference.SettingsActivity
import com.google.mlkit.vision.demo.preference.SettingsActivity.LaunchSource
import com.google.mlkit.vision.objects.DetectedObject
import com.google.mlkit.vision.objects.ObjectDetection
import com.google.mlkit.vision.objects.ObjectDetector
import com.google.mlkit.vision.objects.custom.CustomObjectDetectorOptions
import java.util.Objects
import kotlin.collections.List

/** Live preview demo app for ML Kit APIs using CameraXSource API. */
@KeepName
@RequiresApi(VERSION_CODES.LOLLIPOP)
class CameraXSourceDemoActivity : AppCompatActivity(), CompoundButton.OnCheckedChangeListener {
  private var previewView: PreviewView? = null
  private var graphicOverlay: GraphicOverlay? = null
  private var needUpdateGraphicOverlayImageSourceInfo = false
  private var lensFacing: Int = CameraSourceConfig.CAMERA_FACING_BACK
  private var cameraXSource: CameraXSource? = null
  private var customObjectDetectorOptions: CustomObjectDetectorOptions? = null
  private var targetResolution: Size? = null

  override fun onCreate(savedInstanceState: Bundle?) {
    super.onCreate(savedInstanceState)
    Log.d(TAG, "onCreate")
    setContentView(R.layout.activity_vision_cameraxsource_demo)
    previewView = findViewById(R.id.preview_view)
    if (previewView == null) {
      Log.d(TAG, "previewView is null")
    }
    graphicOverlay = findViewById(R.id.graphic_overlay)
    if (graphicOverlay == null) {
      Log.d(TAG, "graphicOverlay is null")
    }
    val facingSwitch = findViewById<ToggleButton>(R.id.facing_switch)
    facingSwitch.setOnCheckedChangeListener(this)
    val settingsButton = findViewById<ImageView>(R.id.settings_button)
    settingsButton.setOnClickListener {
      val intent = Intent(applicationContext, SettingsActivity::class.java)
      intent.putExtra(SettingsActivity.EXTRA_LAUNCH_SOURCE, LaunchSource.CAMERAXSOURCE_DEMO)
      startActivity(intent)
    }
  }

  override fun onCheckedChanged(buttonView: CompoundButton, isChecked: Boolean) {
    if (lensFacing == CameraSourceConfig.CAMERA_FACING_FRONT) {
      lensFacing = CameraSourceConfig.CAMERA_FACING_BACK
    } else {
      lensFacing = CameraSourceConfig.CAMERA_FACING_FRONT
    }
    createThenStartCameraXSource()
  }

  public override fun onResume() {
    super.onResume()
    if (cameraXSource != null &&
        PreferenceUtils.getCustomObjectDetectorOptionsForLivePreview(this, localModel)
          .equals(customObjectDetectorOptions) &&
        PreferenceUtils.getCameraXTargetResolution(getApplicationContext(), lensFacing) != null &&
        (Objects.requireNonNull(
          PreferenceUtils.getCameraXTargetResolution(getApplicationContext(), lensFacing)
        ) == targetResolution)
    ) {
      cameraXSource!!.start()
    } else {
      createThenStartCameraXSource()
    }
  }

  override fun onPause() {
    super.onPause()
    if (cameraXSource != null) {
      cameraXSource!!.stop()
    }
  }

  override fun onDestroy() {
    super.onDestroy()
    if (cameraXSource != null) {
      cameraXSource!!.stop()
    }
  }

  private fun createThenStartCameraXSource() {
    if (cameraXSource != null) {
      cameraXSource!!.close()
    }
    customObjectDetectorOptions =
      PreferenceUtils.getCustomObjectDetectorOptionsForLivePreview(
        getApplicationContext(),
        localModel
      )
    val objectDetector: ObjectDetector = ObjectDetection.getClient(customObjectDetectorOptions!!)
    var detectionTaskCallback: DetectionTaskCallback<List<DetectedObject>> =
      DetectionTaskCallback<List<DetectedObject>> { detectionTask ->
        detectionTask
          .addOnSuccessListener { results -> onDetectionTaskSuccess(results) }
          .addOnFailureListener { e -> onDetectionTaskFailure(e) }
      }
    val builder: CameraSourceConfig.Builder =
      CameraSourceConfig.Builder(getApplicationContext(), objectDetector!!, detectionTaskCallback)
        .setFacing(lensFacing)
    targetResolution =
      PreferenceUtils.getCameraXTargetResolution(getApplicationContext(), lensFacing)
    if (targetResolution != null) {
      builder.setRequestedPreviewSize(targetResolution!!.width, targetResolution!!.height)
    }
    cameraXSource = CameraXSource(builder.build(), previewView!!)
    needUpdateGraphicOverlayImageSourceInfo = true
    cameraXSource!!.start()
  }

  private fun onDetectionTaskSuccess(results: List<DetectedObject>) {
    graphicOverlay!!.clear()
    if (needUpdateGraphicOverlayImageSourceInfo) {
      val size: Size = cameraXSource!!.getPreviewSize()!!
      if (size != null) {
        Log.d(TAG, "preview width: " + size.width)
        Log.d(TAG, "preview height: " + size.height)
        val isImageFlipped =
          cameraXSource!!.getCameraFacing() == CameraSourceConfig.CAMERA_FACING_FRONT
        if (isPortraitMode) {
          // Swap width and height sizes when in portrait, since it will be rotated by
          // 90 degrees. The camera preview and the image being processed have the same size.
          graphicOverlay!!.setImageSourceInfo(size.height, size.width, isImageFlipped)
        } else {
          graphicOverlay!!.setImageSourceInfo(size.width, size.height, isImageFlipped)
        }
        needUpdateGraphicOverlayImageSourceInfo = false
      } else {
        Log.d(TAG, "previewsize is null")
      }
    }
    Log.v(TAG, "Number of object been detected: " + results.size)
    for (`object` in results) {
      graphicOverlay!!.add(ObjectGraphic(graphicOverlay!!, `object`))
    }
    graphicOverlay!!.add(InferenceInfoGraphic(graphicOverlay!!))
    graphicOverlay!!.postInvalidate()
  }

  private fun onDetectionTaskFailure(e: Exception) {
    graphicOverlay!!.clear()
    graphicOverlay!!.postInvalidate()
    val error = "Failed to process. Error: " + e.localizedMessage
    Toast.makeText(
        graphicOverlay!!.getContext(),
        """
   $error
   Cause: ${e.cause}
      """.trimIndent(),
        Toast.LENGTH_SHORT
      )
      .show()
    Log.d(TAG, error)
  }

  private val isPortraitMode: Boolean
    private get() =
      (getApplicationContext().getResources().getConfiguration().orientation !==
        Configuration.ORIENTATION_LANDSCAPE)

  companion object {
    private const val TAG = "CameraXSourcePreview"
    private val localModel: LocalModel =
      LocalModel.Builder().setAssetFilePath("custom_models/object_labeler.tflite").build()
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/CameraXSourceDemoActivity.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/ChooserActivity.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin

import android.content.Context
import android.content.Intent
import android.os.Build.VERSION
import android.os.Build.VERSION_CODES
import android.os.Bundle
import androidx.appcompat.app.AppCompatActivity
import android.util.Log
import android.view.LayoutInflater
import android.view.View
import android.view.ViewGroup
import android.widget.AdapterView
import android.widget.AdapterView.OnItemClickListener
import android.widget.ArrayAdapter
import android.widget.ListView
import android.widget.TextView
import androidx.core.app.ActivityCompat
import com.google.mlkit.vision.demo.R

/** Demo app chooser which allows you pick from all available testing Activities. */
class ChooserActivity :
  AppCompatActivity(), ActivityCompat.OnRequestPermissionsResultCallback, OnItemClickListener {
  override fun onCreate(savedInstanceState: Bundle?) {
    super.onCreate(savedInstanceState)
    Log.d(TAG, "onCreate")
    setContentView(R.layout.activity_chooser)

    // Set up ListView and Adapter
    val listView = findViewById<ListView>(R.id.test_activity_list_view)
    val adapter = MyArrayAdapter(this, android.R.layout.simple_list_item_2, CLASSES)
    adapter.setDescriptionIds(DESCRIPTION_IDS)
    listView.adapter = adapter
    listView.onItemClickListener = this
  }

  override fun onItemClick(parent: AdapterView<*>?, view: View, position: Int, id: Long) {
    val clicked = CLASSES[position]
    startActivity(Intent(this, clicked))
  }

  private class MyArrayAdapter(
    private val ctx: Context,
    resource: Int,
    private val classes: Array<Class<*>>
  ) : ArrayAdapter<Class<*>>(ctx, resource, classes) {
    private var descriptionIds: IntArray? = null

    override fun getView(position: Int, convertView: View?, parent: ViewGroup): View {
      var view = convertView

      if (convertView == null) {
        val inflater = ctx.getSystemService(Context.LAYOUT_INFLATER_SERVICE) as LayoutInflater
        view = inflater.inflate(android.R.layout.simple_list_item_2, null)
      }

      (view!!.findViewById<View>(android.R.id.text1) as TextView).text =
        classes[position].simpleName
      descriptionIds?.let {
        (view.findViewById<View>(android.R.id.text2) as TextView).setText(it[position])
      }

      return view
    }

    fun setDescriptionIds(descriptionIds: IntArray) {
      this.descriptionIds = descriptionIds
    }
  }

  companion object {
    private const val TAG = "ChooserActivity"
    private val CLASSES =
      if (VERSION.SDK_INT < VERSION_CODES.LOLLIPOP)
        arrayOf<Class<*>>(
          LivePreviewActivity::class.java,
          StillImageActivity::class.java,
        )
      else
        arrayOf<Class<*>>(
          LivePreviewActivity::class.java,
          StillImageActivity::class.java,
          CameraXLivePreviewActivity::class.java,
          CameraXSourceDemoActivity::class.java
        )
    private val DESCRIPTION_IDS =
      if (VERSION.SDK_INT < VERSION_CODES.LOLLIPOP)
        intArrayOf(
          R.string.desc_camera_source_activity,
          R.string.desc_still_image_activity,
        )
      else
        intArrayOf(
          R.string.desc_camera_source_activity,
          R.string.desc_still_image_activity,
          R.string.desc_camerax_live_preview_activity,
          R.string.desc_cameraxsource_demo_activity
        )
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/ChooserActivity.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/LivePreviewActivity.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin

import android.content.Intent
import android.os.Bundle
import androidx.appcompat.app.AppCompatActivity
import android.util.Log
import android.view.View
import android.widget.AdapterView
import android.widget.AdapterView.OnItemSelectedListener
import android.widget.ArrayAdapter
import android.widget.CompoundButton
import android.widget.ImageView
import android.widget.Spinner
import android.widget.Toast
import android.widget.ToggleButton
import com.google.android.gms.common.annotation.KeepName
import com.google.mlkit.common.model.LocalModel
import com.google.mlkit.vision.barcode.ZoomSuggestionOptions.ZoomCallback
import com.google.mlkit.vision.demo.CameraSource
import com.google.mlkit.vision.demo.CameraSourcePreview
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.R
import com.google.mlkit.vision.demo.kotlin.barcodescanner.BarcodeScannerProcessor
import com.google.mlkit.vision.demo.kotlin.facedetector.FaceDetectorProcessor
import com.google.mlkit.vision.demo.kotlin.facemeshdetector.FaceMeshDetectorProcessor
import com.google.mlkit.vision.demo.kotlin.labeldetector.LabelDetectorProcessor
import com.google.mlkit.vision.demo.kotlin.objectdetector.ObjectDetectorProcessor
import com.google.mlkit.vision.demo.kotlin.posedetector.PoseDetectorProcessor
import com.google.mlkit.vision.demo.kotlin.segmenter.SegmenterProcessor
import com.google.mlkit.vision.demo.kotlin.textdetector.TextRecognitionProcessor
import com.google.mlkit.vision.demo.preference.PreferenceUtils
import com.google.mlkit.vision.demo.preference.SettingsActivity
import com.google.mlkit.vision.demo.preference.SettingsActivity.LaunchSource
import com.google.mlkit.vision.label.custom.CustomImageLabelerOptions
import com.google.mlkit.vision.label.defaults.ImageLabelerOptions
import com.google.mlkit.vision.text.chinese.ChineseTextRecognizerOptions
import com.google.mlkit.vision.text.devanagari.DevanagariTextRecognizerOptions
import com.google.mlkit.vision.text.japanese.JapaneseTextRecognizerOptions
import com.google.mlkit.vision.text.korean.KoreanTextRecognizerOptions
import com.google.mlkit.vision.text.latin.TextRecognizerOptions
import java.io.IOException

/** Live preview demo for ML Kit APIs. */
@KeepName
class LivePreviewActivity :
  AppCompatActivity(), OnItemSelectedListener, CompoundButton.OnCheckedChangeListener {

  private var cameraSource: CameraSource? = null
  private var preview: CameraSourcePreview? = null
  private var graphicOverlay: GraphicOverlay? = null
  private var selectedModel = OBJECT_DETECTION

  override fun onCreate(savedInstanceState: Bundle?) {
    super.onCreate(savedInstanceState)
    Log.d(TAG, "onCreate")
    setContentView(R.layout.activity_vision_live_preview)

    preview = findViewById(R.id.preview_view)
    if (preview == null) {
      Log.d(TAG, "Preview is null")
    }

    graphicOverlay = findViewById(R.id.graphic_overlay)
    if (graphicOverlay == null) {
      Log.d(TAG, "graphicOverlay is null")
    }

    val spinner = findViewById<Spinner>(R.id.spinner)
    val options: MutableList<String> = ArrayList()
    options.add(OBJECT_DETECTION)
    options.add(OBJECT_DETECTION_CUSTOM)
    options.add(CUSTOM_AUTOML_OBJECT_DETECTION)
    options.add(FACE_DETECTION)
    options.add(BARCODE_SCANNING)
    options.add(IMAGE_LABELING)
    options.add(IMAGE_LABELING_CUSTOM)
    options.add(CUSTOM_AUTOML_LABELING)
    options.add(POSE_DETECTION)
    options.add(SELFIE_SEGMENTATION)
    options.add(TEXT_RECOGNITION_LATIN)
    options.add(TEXT_RECOGNITION_CHINESE)
    options.add(TEXT_RECOGNITION_DEVANAGARI)
    options.add(TEXT_RECOGNITION_JAPANESE)
    options.add(TEXT_RECOGNITION_KOREAN)
    options.add(FACE_MESH_DETECTION)

    // Creating adapter for spinner
    val dataAdapter = ArrayAdapter(this, R.layout.spinner_style, options)

    // Drop down layout style - list view with radio button
    dataAdapter.setDropDownViewResource(android.R.layout.simple_spinner_dropdown_item)
    // attaching data adapter to spinner
    spinner.adapter = dataAdapter
    spinner.onItemSelectedListener = this

    val facingSwitch = findViewById<ToggleButton>(R.id.facing_switch)
    facingSwitch.setOnCheckedChangeListener(this)

    val settingsButton = findViewById<ImageView>(R.id.settings_button)
    settingsButton.setOnClickListener {
      val intent = Intent(applicationContext, SettingsActivity::class.java)
      intent.putExtra(SettingsActivity.EXTRA_LAUNCH_SOURCE, LaunchSource.LIVE_PREVIEW)
      startActivity(intent)
    }

    createCameraSource(selectedModel)
  }

  @Synchronized
  override fun onItemSelected(parent: AdapterView<*>?, view: View?, pos: Int, id: Long) {
    // An item was selected. You can retrieve the selected item using
    // parent.getItemAtPosition(pos)
    selectedModel = parent?.getItemAtPosition(pos).toString()
    Log.d(TAG, "Selected model: $selectedModel")
    preview?.stop()
    createCameraSource(selectedModel)
    startCameraSource()
  }

  override fun onNothingSelected(parent: AdapterView<*>?) {
    // Do nothing.
  }

  override fun onCheckedChanged(buttonView: CompoundButton, isChecked: Boolean) {
    Log.d(TAG, "Set facing")
    if (cameraSource != null) {
      if (isChecked) {
        cameraSource?.setFacing(CameraSource.CAMERA_FACING_FRONT)
      } else {
        cameraSource?.setFacing(CameraSource.CAMERA_FACING_BACK)
      }
    }
    preview?.stop()
    startCameraSource()
  }

  private fun createCameraSource(model: String) {
    // If there's no existing cameraSource, create one.
    if (cameraSource == null) {
      cameraSource = CameraSource(this, graphicOverlay)
    }
    try {
      when (model) {
        OBJECT_DETECTION -> {
          Log.i(TAG, "Using Object Detector Processor")
          val objectDetectorOptions = PreferenceUtils.getObjectDetectorOptionsForLivePreview(this)
          cameraSource!!.setMachineLearningFrameProcessor(
            ObjectDetectorProcessor(this, objectDetectorOptions)
          )
        }
        OBJECT_DETECTION_CUSTOM -> {
          Log.i(TAG, "Using Custom Object Detector Processor")
          val localModel =
            LocalModel.Builder().setAssetFilePath("custom_models/object_labeler.tflite").build()
          val customObjectDetectorOptions =
            PreferenceUtils.getCustomObjectDetectorOptionsForLivePreview(this, localModel)
          cameraSource!!.setMachineLearningFrameProcessor(
            ObjectDetectorProcessor(this, customObjectDetectorOptions)
          )
        }
        CUSTOM_AUTOML_OBJECT_DETECTION -> {
          Log.i(TAG, "Using Custom AutoML Object Detector Processor")
          val customAutoMLODTLocalModel =
            LocalModel.Builder().setAssetManifestFilePath("automl/manifest.json").build()
          val customAutoMLODTOptions =
            PreferenceUtils.getCustomObjectDetectorOptionsForLivePreview(
              this,
              customAutoMLODTLocalModel
            )
          cameraSource!!.setMachineLearningFrameProcessor(
            ObjectDetectorProcessor(this, customAutoMLODTOptions)
          )
        }
        TEXT_RECOGNITION_LATIN -> {
          Log.i(TAG, "Using on-device Text recognition Processor for Latin and Latin")
          cameraSource!!.setMachineLearningFrameProcessor(
            TextRecognitionProcessor(this, TextRecognizerOptions.Builder().build())
          )
        }
        TEXT_RECOGNITION_CHINESE -> {
          Log.i(TAG, "Using on-device Text recognition Processor for Latin and Chinese")
          cameraSource!!.setMachineLearningFrameProcessor(
            TextRecognitionProcessor(this, ChineseTextRecognizerOptions.Builder().build())
          )
        }
        TEXT_RECOGNITION_DEVANAGARI -> {
          Log.i(TAG, "Using on-device Text recognition Processor for Latin and Devanagari")
          cameraSource!!.setMachineLearningFrameProcessor(
            TextRecognitionProcessor(this, DevanagariTextRecognizerOptions.Builder().build())
          )
        }
        TEXT_RECOGNITION_JAPANESE -> {
          Log.i(TAG, "Using on-device Text recognition Processor for Latin and Japanese")
          cameraSource!!.setMachineLearningFrameProcessor(
            TextRecognitionProcessor(this, JapaneseTextRecognizerOptions.Builder().build())
          )
        }
        TEXT_RECOGNITION_KOREAN -> {
          Log.i(TAG, "Using on-device Text recognition Processor for Latin and Korean")
          cameraSource!!.setMachineLearningFrameProcessor(
            TextRecognitionProcessor(this, KoreanTextRecognizerOptions.Builder().build())
          )
        }
        FACE_DETECTION -> {
          Log.i(TAG, "Using Face Detector Processor")
          val faceDetectorOptions = PreferenceUtils.getFaceDetectorOptions(this)
          cameraSource!!.setMachineLearningFrameProcessor(
            FaceDetectorProcessor(this, faceDetectorOptions)
          )
        }
        BARCODE_SCANNING -> {
          Log.i(TAG, "Using Barcode Detector Processor")
          var zoomCallback: ZoomCallback? = null
          if (PreferenceUtils.shouldEnableAutoZoom(this)) {
            zoomCallback = ZoomCallback { zoomLevel: Float -> cameraSource!!.setZoom(zoomLevel) }
          }
          cameraSource!!.setMachineLearningFrameProcessor(
            BarcodeScannerProcessor(this, zoomCallback)
          )
        }
        IMAGE_LABELING -> {
          Log.i(TAG, "Using Image Label Detector Processor")
          cameraSource!!.setMachineLearningFrameProcessor(
            LabelDetectorProcessor(this, ImageLabelerOptions.DEFAULT_OPTIONS)
          )
        }
        IMAGE_LABELING_CUSTOM -> {
          Log.i(TAG, "Using Custom Image Label Detector Processor")
          val localClassifier =
            LocalModel.Builder().setAssetFilePath("custom_models/bird_classifier.tflite").build()
          val customImageLabelerOptions = CustomImageLabelerOptions.Builder(localClassifier).build()
          cameraSource!!.setMachineLearningFrameProcessor(
            LabelDetectorProcessor(this, customImageLabelerOptions)
          )
        }
        CUSTOM_AUTOML_LABELING -> {
          Log.i(TAG, "Using Custom AutoML Image Label Detector Processor")
          val customAutoMLLabelLocalModel =
            LocalModel.Builder().setAssetManifestFilePath("automl/manifest.json").build()
          val customAutoMLLabelOptions =
            CustomImageLabelerOptions.Builder(customAutoMLLabelLocalModel)
              .setConfidenceThreshold(0f)
              .build()
          cameraSource!!.setMachineLearningFrameProcessor(
            LabelDetectorProcessor(this, customAutoMLLabelOptions)
          )
        }
        POSE_DETECTION -> {
          val poseDetectorOptions = PreferenceUtils.getPoseDetectorOptionsForLivePreview(this)
          Log.i(TAG, "Using Pose Detector with options $poseDetectorOptions")
          val shouldShowInFrameLikelihood =
            PreferenceUtils.shouldShowPoseDetectionInFrameLikelihoodLivePreview(this)
          val visualizeZ = PreferenceUtils.shouldPoseDetectionVisualizeZ(this)
          val rescaleZ = PreferenceUtils.shouldPoseDetectionRescaleZForVisualization(this)
          val runClassification = PreferenceUtils.shouldPoseDetectionRunClassification(this)
          cameraSource!!.setMachineLearningFrameProcessor(
            PoseDetectorProcessor(
              this,
              poseDetectorOptions,
              shouldShowInFrameLikelihood,
              visualizeZ,
              rescaleZ,
              runClassification,
              /* isStreamMode = */ true
            )
          )
        }
        SELFIE_SEGMENTATION -> {
          cameraSource!!.setMachineLearningFrameProcessor(SegmenterProcessor(this))
        }
        FACE_MESH_DETECTION -> {
          cameraSource!!.setMachineLearningFrameProcessor(FaceMeshDetectorProcessor(this))
        }
        else -> Log.e(TAG, "Unknown model: $model")
      }
    } catch (e: Exception) {
      Log.e(TAG, "Can not create image processor: $model", e)
      Toast.makeText(
          applicationContext,
          "Can not create image processor: " + e.message,
          Toast.LENGTH_LONG
        )
        .show()
    }
  }

  /**
   * Starts or restarts the camera source, if it exists. If the camera source doesn't exist yet
   * (e.g., because onResume was called before the camera source was created), this will be called
   * again when the camera source is created.
   */
  private fun startCameraSource() {
    if (cameraSource != null) {
      try {
        if (preview == null) {
          Log.d(TAG, "resume: Preview is null")
        }
        if (graphicOverlay == null) {
          Log.d(TAG, "resume: graphOverlay is null")
        }
        preview!!.start(cameraSource, graphicOverlay)
      } catch (e: IOException) {
        Log.e(TAG, "Unable to start camera source.", e)
        cameraSource!!.release()
        cameraSource = null
      }
    }
  }

  public override fun onResume() {
    super.onResume()
    Log.d(TAG, "onResume")
    createCameraSource(selectedModel)
    startCameraSource()
  }

  /** Stops the camera. */
  override fun onPause() {
    super.onPause()
    preview?.stop()
  }

  public override fun onDestroy() {
    super.onDestroy()
    if (cameraSource != null) {
      cameraSource?.release()
    }
  }

  companion object {
    private const val OBJECT_DETECTION = "Object Detection"
    private const val OBJECT_DETECTION_CUSTOM = "Custom Object Detection"
    private const val CUSTOM_AUTOML_OBJECT_DETECTION = "Custom AutoML Object Detection (Flower)"
    private const val FACE_DETECTION = "Face Detection"
    private const val TEXT_RECOGNITION_LATIN = "Text Recognition Latin"
    private const val TEXT_RECOGNITION_CHINESE = "Text Recognition Chinese"
    private const val TEXT_RECOGNITION_DEVANAGARI = "Text Recognition Devanagari"
    private const val TEXT_RECOGNITION_JAPANESE = "Text Recognition Japanese"
    private const val TEXT_RECOGNITION_KOREAN = "Text Recognition Korean"
    private const val BARCODE_SCANNING = "Barcode Scanning"
    private const val IMAGE_LABELING = "Image Labeling"
    private const val IMAGE_LABELING_CUSTOM = "Custom Image Labeling (Birds)"
    private const val CUSTOM_AUTOML_LABELING = "Custom AutoML Image Labeling (Flower)"
    private const val POSE_DETECTION = "Pose Detection"
    private const val SELFIE_SEGMENTATION = "Selfie Segmentation"
    private const val FACE_MESH_DETECTION = "Face Mesh Detection (Beta)"

    private const val TAG = "LivePreviewActivity"
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/LivePreviewActivity.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/StillImageActivity.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin

import android.app.Activity
import android.content.ContentValues
import android.content.Intent
import android.content.res.Configuration
import android.graphics.Bitmap
import android.net.Uri
import android.os.Build
import android.os.Build.VERSION
import android.os.Bundle
import android.provider.MediaStore
import androidx.appcompat.app.AppCompatActivity
import android.util.Log
import android.util.Pair
import android.view.MenuItem
import android.view.View
import android.view.ViewTreeObserver
import android.widget.AdapterView
import android.widget.AdapterView.OnItemSelectedListener
import android.widget.ArrayAdapter
import android.widget.ImageView
import android.widget.PopupMenu
import android.widget.Spinner
import android.widget.Toast
import com.google.android.gms.common.annotation.KeepName
import com.google.mlkit.common.model.LocalModel
import com.google.mlkit.vision.demo.BitmapUtils
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.R
import com.google.mlkit.vision.demo.VisionImageProcessor
import com.google.mlkit.vision.demo.kotlin.barcodescanner.BarcodeScannerProcessor
import com.google.mlkit.vision.demo.kotlin.facedetector.FaceDetectorProcessor
import com.google.mlkit.vision.demo.kotlin.facemeshdetector.FaceMeshDetectorProcessor
import com.google.mlkit.vision.demo.kotlin.labeldetector.LabelDetectorProcessor
import com.google.mlkit.vision.demo.kotlin.objectdetector.ObjectDetectorProcessor
import com.google.mlkit.vision.demo.kotlin.posedetector.PoseDetectorProcessor
import com.google.mlkit.vision.demo.kotlin.segmenter.SegmenterProcessor
import com.google.mlkit.vision.demo.kotlin.subjectsegmenter.SubjectSegmenterProcessor
import com.google.mlkit.vision.demo.kotlin.textdetector.TextRecognitionProcessor
import com.google.mlkit.vision.demo.preference.PreferenceUtils
import com.google.mlkit.vision.demo.preference.SettingsActivity
import com.google.mlkit.vision.demo.preference.SettingsActivity.LaunchSource
import com.google.mlkit.vision.label.custom.CustomImageLabelerOptions
import com.google.mlkit.vision.label.defaults.ImageLabelerOptions
import com.google.mlkit.vision.text.chinese.ChineseTextRecognizerOptions
import com.google.mlkit.vision.text.devanagari.DevanagariTextRecognizerOptions
import com.google.mlkit.vision.text.japanese.JapaneseTextRecognizerOptions
import com.google.mlkit.vision.text.korean.KoreanTextRecognizerOptions
import com.google.mlkit.vision.text.latin.TextRecognizerOptions
import java.io.IOException
import java.util.ArrayList

/** Activity demonstrating different image detector features with a still image from camera. */
@KeepName
class StillImageActivity : AppCompatActivity() {
  private var preview: ImageView? = null
  private var graphicOverlay: GraphicOverlay? = null
  private var selectedMode = OBJECT_DETECTION
  private var selectedSize: String? = SIZE_SCREEN
  private var isLandScape = false
  private var imageUri: Uri? = null
  // Max width (portrait mode)
  private var imageMaxWidth = 0
  // Max height (portrait mode)
  private var imageMaxHeight = 0
  private var imageProcessor: VisionImageProcessor? = null

  override fun onCreate(savedInstanceState: Bundle?) {
    super.onCreate(savedInstanceState)
    setContentView(R.layout.activity_still_image)
    findViewById<View>(R.id.select_image_button).setOnClickListener { view: View ->
      // Menu for selecting either: a) take new photo b) select from existing
      val popup = PopupMenu(this@StillImageActivity, view)
      popup.setOnMenuItemClickListener { menuItem: MenuItem ->
        val itemId = menuItem.itemId
        if (itemId == R.id.select_images_from_local) {
          startChooseImageIntentForResult()
          return@setOnMenuItemClickListener true
        } else if (itemId == R.id.take_photo_using_camera) {
          startCameraIntentForResult()
          return@setOnMenuItemClickListener true
        }
        false
      }
      val inflater = popup.menuInflater
      inflater.inflate(R.menu.camera_button_menu, popup.menu)
      popup.show()
    }
    preview = findViewById(R.id.preview)
    graphicOverlay = findViewById(R.id.graphic_overlay)

    populateFeatureSelector()
    populateSizeSelector()
    isLandScape = resources.configuration.orientation == Configuration.ORIENTATION_LANDSCAPE
    if (savedInstanceState != null) {
      imageUri = savedInstanceState.getParcelable(KEY_IMAGE_URI)
      imageMaxWidth = savedInstanceState.getInt(KEY_IMAGE_MAX_WIDTH)
      imageMaxHeight = savedInstanceState.getInt(KEY_IMAGE_MAX_HEIGHT)
      selectedSize = savedInstanceState.getString(KEY_SELECTED_SIZE)
    }

    val rootView = findViewById<View>(R.id.root)
    rootView.viewTreeObserver.addOnGlobalLayoutListener(
      object : ViewTreeObserver.OnGlobalLayoutListener {
        override fun onGlobalLayout() {
          rootView.viewTreeObserver.removeOnGlobalLayoutListener(this)
          imageMaxWidth = rootView.width
          imageMaxHeight = rootView.height - findViewById<View>(R.id.control).height
          if (SIZE_SCREEN == selectedSize) {
            tryReloadAndDetectInImage()
          }
        }
      }
    )

    val settingsButton = findViewById<ImageView>(R.id.settings_button)
    settingsButton.setOnClickListener {
      val intent = Intent(applicationContext, SettingsActivity::class.java)
      intent.putExtra(SettingsActivity.EXTRA_LAUNCH_SOURCE, LaunchSource.STILL_IMAGE)
      startActivity(intent)
    }
  }

  public override fun onResume() {
    super.onResume()
    Log.d(TAG, "onResume")
    createImageProcessor()
    tryReloadAndDetectInImage()
  }

  public override fun onPause() {
    super.onPause()
    imageProcessor?.run { this.stop() }
  }

  public override fun onDestroy() {
    super.onDestroy()
    imageProcessor?.run { this.stop() }
  }

  private fun populateFeatureSelector() {
    val featureSpinner = findViewById<Spinner>(R.id.feature_selector)
    val options: MutableList<String> = ArrayList()
    options.add(OBJECT_DETECTION)
    options.add(OBJECT_DETECTION_CUSTOM)
    options.add(CUSTOM_AUTOML_OBJECT_DETECTION)
    options.add(FACE_DETECTION)
    options.add(BARCODE_SCANNING)
    options.add(IMAGE_LABELING)
    options.add(IMAGE_LABELING_CUSTOM)
    options.add(CUSTOM_AUTOML_LABELING)
    options.add(POSE_DETECTION)
    options.add(SELFIE_SEGMENTATION)
    options.add(TEXT_RECOGNITION_LATIN)
    options.add(TEXT_RECOGNITION_CHINESE)
    options.add(TEXT_RECOGNITION_DEVANAGARI)
    options.add(TEXT_RECOGNITION_JAPANESE)
    options.add(TEXT_RECOGNITION_KOREAN)
    options.add(FACE_MESH_DETECTION)
    if (VERSION.SDK_INT >= Build.VERSION_CODES.N) {
      options.add(SUBJECT_SEGMENTATION)
    }

    // Creating adapter for featureSpinner
    val dataAdapter = ArrayAdapter(this, R.layout.spinner_style, options)
    // Drop down layout style - list view with radio button
    dataAdapter.setDropDownViewResource(android.R.layout.simple_spinner_dropdown_item)
    // attaching data adapter to spinner
    featureSpinner.adapter = dataAdapter
    featureSpinner.onItemSelectedListener =
      object : OnItemSelectedListener {
        override fun onItemSelected(
          parentView: AdapterView<*>,
          selectedItemView: View?,
          pos: Int,
          id: Long
        ) {
          if (pos >= 0) {
            selectedMode = parentView.getItemAtPosition(pos).toString()
            createImageProcessor()
            tryReloadAndDetectInImage()
          }
        }

        override fun onNothingSelected(arg0: AdapterView<*>?) {}
      }
  }

  private fun populateSizeSelector() {
    val sizeSpinner = findViewById<Spinner>(R.id.size_selector)
    val options: MutableList<String> = ArrayList()
    options.add(SIZE_SCREEN)
    options.add(SIZE_1024_768)
    options.add(SIZE_640_480)
    options.add(SIZE_ORIGINAL)
    // Creating adapter for featureSpinner
    val dataAdapter = ArrayAdapter(this, R.layout.spinner_style, options)
    // Drop down layout style - list view with radio button
    dataAdapter.setDropDownViewResource(android.R.layout.simple_spinner_dropdown_item)
    // attaching data adapter to spinner
    sizeSpinner.adapter = dataAdapter
    sizeSpinner.onItemSelectedListener =
      object : OnItemSelectedListener {
        override fun onItemSelected(
          parentView: AdapterView<*>,
          selectedItemView: View?,
          pos: Int,
          id: Long
        ) {
          if (pos >= 0) {
            selectedSize = parentView.getItemAtPosition(pos).toString()
            tryReloadAndDetectInImage()
          }
        }

        override fun onNothingSelected(arg0: AdapterView<*>?) {}
      }
  }

  public override fun onSaveInstanceState(outState: Bundle) {
    super.onSaveInstanceState(outState)
    outState.putParcelable(KEY_IMAGE_URI, imageUri)
    outState.putInt(KEY_IMAGE_MAX_WIDTH, imageMaxWidth)
    outState.putInt(KEY_IMAGE_MAX_HEIGHT, imageMaxHeight)
    outState.putString(KEY_SELECTED_SIZE, selectedSize)
  }

  private fun startCameraIntentForResult() { // Clean up last time's image
    imageUri = null
    preview!!.setImageBitmap(null)
    val takePictureIntent = Intent(MediaStore.ACTION_IMAGE_CAPTURE)
    if (takePictureIntent.resolveActivity(packageManager) != null) {
      val values = ContentValues()
      values.put(MediaStore.Images.Media.TITLE, "New Picture")
      values.put(MediaStore.Images.Media.DESCRIPTION, "From Camera")
      imageUri = contentResolver.insert(MediaStore.Images.Media.EXTERNAL_CONTENT_URI, values)
      takePictureIntent.putExtra(MediaStore.EXTRA_OUTPUT, imageUri)
      startActivityForResult(takePictureIntent, REQUEST_IMAGE_CAPTURE)
    }
  }

  private fun startChooseImageIntentForResult() {
    val intent = Intent()
    intent.type = "image/*"
    intent.action = Intent.ACTION_GET_CONTENT
    startActivityForResult(Intent.createChooser(intent, "Select Picture"), REQUEST_CHOOSE_IMAGE)
  }

  override fun onActivityResult(requestCode: Int, resultCode: Int, data: Intent?) {
    if (requestCode == REQUEST_IMAGE_CAPTURE && resultCode == Activity.RESULT_OK) {
      tryReloadAndDetectInImage()
    } else if (requestCode == REQUEST_CHOOSE_IMAGE && resultCode == Activity.RESULT_OK) {
      // In this case, imageUri is returned by the chooser, save it.
      imageUri = data!!.data
      tryReloadAndDetectInImage()
    } else {
      super.onActivityResult(requestCode, resultCode, data)
    }
  }

  private fun tryReloadAndDetectInImage() {
    Log.d(TAG, "Try reload and detect image")
    try {
      if (imageUri == null) {
        return
      }

      if (SIZE_SCREEN == selectedSize && imageMaxWidth == 0) {
        // UI layout has not finished yet, will reload once it's ready.
        return
      }

      val imageBitmap = BitmapUtils.getBitmapFromContentUri(contentResolver, imageUri) ?: return
      // Clear the overlay first
      graphicOverlay!!.clear()

      val resizedBitmap: Bitmap
      resizedBitmap =
        if (selectedSize == SIZE_ORIGINAL) {
          imageBitmap
        } else {
          // Get the dimensions of the image view
          val targetedSize: Pair<Int, Int> = targetedWidthHeight

          // Determine how much to scale down the image
          val scaleFactor =
            Math.max(
              imageBitmap.width.toFloat() / targetedSize.first.toFloat(),
              imageBitmap.height.toFloat() / targetedSize.second.toFloat()
            )
          Bitmap.createScaledBitmap(
            imageBitmap,
            (imageBitmap.width / scaleFactor).toInt(),
            (imageBitmap.height / scaleFactor).toInt(),
            true
          )
        }

      preview!!.setImageBitmap(resizedBitmap)
      if (imageProcessor != null) {
        graphicOverlay!!.setImageSourceInfo(
          resizedBitmap.width,
          resizedBitmap.height,
          /* isFlipped= */ false
        )
        imageProcessor!!.processBitmap(resizedBitmap, graphicOverlay)
      } else {
        Log.e(TAG, "Null imageProcessor, please check adb logs for imageProcessor creation error")
      }
    } catch (e: IOException) {
      Log.e(TAG, "Error retrieving saved image")
      imageUri = null
    }
  }

  private val targetedWidthHeight: Pair<Int, Int>
    get() {
      val targetWidth: Int
      val targetHeight: Int
      when (selectedSize) {
        SIZE_SCREEN -> {
          targetWidth = imageMaxWidth
          targetHeight = imageMaxHeight
        }
        SIZE_640_480 -> {
          targetWidth = if (isLandScape) 640 else 480
          targetHeight = if (isLandScape) 480 else 640
        }
        SIZE_1024_768 -> {
          targetWidth = if (isLandScape) 1024 else 768
          targetHeight = if (isLandScape) 768 else 1024
        }
        else -> throw IllegalStateException("Unknown size")
      }
      return Pair(targetWidth, targetHeight)
    }

  private fun createImageProcessor() {
    try {
      when (selectedMode) {
        OBJECT_DETECTION -> {
          Log.i(TAG, "Using Object Detector Processor")
          val objectDetectorOptions = PreferenceUtils.getObjectDetectorOptionsForStillImage(this)
          imageProcessor = ObjectDetectorProcessor(this, objectDetectorOptions)
        }
        OBJECT_DETECTION_CUSTOM -> {
          Log.i(TAG, "Using Custom Object Detector Processor")
          val localModel =
            LocalModel.Builder().setAssetFilePath("custom_models/object_labeler.tflite").build()
          val customObjectDetectorOptions =
            PreferenceUtils.getCustomObjectDetectorOptionsForStillImage(this, localModel)
          imageProcessor = ObjectDetectorProcessor(this, customObjectDetectorOptions)
        }
        CUSTOM_AUTOML_OBJECT_DETECTION -> {
          Log.i(TAG, "Using Custom AutoML Object Detector Processor")
          val customAutoMLODTLocalModel =
            LocalModel.Builder().setAssetManifestFilePath("automl/manifest.json").build()
          val customAutoMLODTOptions =
            PreferenceUtils.getCustomObjectDetectorOptionsForStillImage(
              this,
              customAutoMLODTLocalModel
            )
          imageProcessor = ObjectDetectorProcessor(this, customAutoMLODTOptions)
        }
        FACE_DETECTION -> {
          Log.i(TAG, "Using Face Detector Processor")
          val faceDetectorOptions = PreferenceUtils.getFaceDetectorOptions(this)
          imageProcessor = FaceDetectorProcessor(this, faceDetectorOptions)
        }
        BARCODE_SCANNING -> imageProcessor = BarcodeScannerProcessor(this, zoomCallback = null)
        TEXT_RECOGNITION_LATIN ->
          imageProcessor = TextRecognitionProcessor(this, TextRecognizerOptions.Builder().build())
        TEXT_RECOGNITION_CHINESE ->
          imageProcessor =
            TextRecognitionProcessor(this, ChineseTextRecognizerOptions.Builder().build())
        TEXT_RECOGNITION_DEVANAGARI ->
          imageProcessor =
            TextRecognitionProcessor(this, DevanagariTextRecognizerOptions.Builder().build())
        TEXT_RECOGNITION_JAPANESE ->
          imageProcessor =
            TextRecognitionProcessor(this, JapaneseTextRecognizerOptions.Builder().build())
        TEXT_RECOGNITION_KOREAN ->
          imageProcessor =
            TextRecognitionProcessor(this, KoreanTextRecognizerOptions.Builder().build())
        IMAGE_LABELING ->
          imageProcessor = LabelDetectorProcessor(this, ImageLabelerOptions.DEFAULT_OPTIONS)
        IMAGE_LABELING_CUSTOM -> {
          Log.i(TAG, "Using Custom Image Label Detector Processor")
          val localClassifier =
            LocalModel.Builder().setAssetFilePath("custom_models/bird_classifier.tflite").build()
          val customImageLabelerOptions = CustomImageLabelerOptions.Builder(localClassifier).build()
          imageProcessor = LabelDetectorProcessor(this, customImageLabelerOptions)
        }
        CUSTOM_AUTOML_LABELING -> {
          Log.i(TAG, "Using Custom AutoML Image Label Detector Processor")
          val customAutoMLLabelLocalModel =
            LocalModel.Builder().setAssetManifestFilePath("automl/manifest.json").build()
          val customAutoMLLabelOptions =
            CustomImageLabelerOptions.Builder(customAutoMLLabelLocalModel)
              .setConfidenceThreshold(0f)
              .build()
          imageProcessor = LabelDetectorProcessor(this, customAutoMLLabelOptions)
        }
        POSE_DETECTION -> {
          val poseDetectorOptions = PreferenceUtils.getPoseDetectorOptionsForStillImage(this)
          Log.i(TAG, "Using Pose Detector with options $poseDetectorOptions")
          val shouldShowInFrameLikelihood =
            PreferenceUtils.shouldShowPoseDetectionInFrameLikelihoodStillImage(this)
          val visualizeZ = PreferenceUtils.shouldPoseDetectionVisualizeZ(this)
          val rescaleZ = PreferenceUtils.shouldPoseDetectionRescaleZForVisualization(this)
          val runClassification = PreferenceUtils.shouldPoseDetectionRunClassification(this)
          imageProcessor =
            PoseDetectorProcessor(
              this,
              poseDetectorOptions,
              shouldShowInFrameLikelihood,
              visualizeZ,
              rescaleZ,
              runClassification,
              isStreamMode = false
            )
        }
        SELFIE_SEGMENTATION -> {
          imageProcessor = SegmenterProcessor(this, isStreamMode = false)
        }
        FACE_MESH_DETECTION -> {
          imageProcessor = FaceMeshDetectorProcessor(this)
        }
        SUBJECT_SEGMENTATION -> {
          if (VERSION.SDK_INT >= Build.VERSION_CODES.N) {
            imageProcessor = SubjectSegmenterProcessor(this)
          }
        }
        else -> Log.e(TAG, "Unknown selectedMode: $selectedMode")
      }
    } catch (e: Exception) {
      Log.e(TAG, "Can not create image processor: $selectedMode", e)
      Toast.makeText(
          applicationContext,
          "Can not create image processor: " + e.message,
          Toast.LENGTH_LONG
        )
        .show()
    }
  }

  companion object {
    private const val TAG = "StillImageActivity"
    private const val OBJECT_DETECTION = "Object Detection"
    private const val OBJECT_DETECTION_CUSTOM = "Custom Object Detection"
    private const val CUSTOM_AUTOML_OBJECT_DETECTION = "Custom AutoML Object Detection (Flower)"
    private const val FACE_DETECTION = "Face Detection"
    private const val BARCODE_SCANNING = "Barcode Scanning"
    private const val TEXT_RECOGNITION_LATIN = "Text Recognition Latin"
    private const val TEXT_RECOGNITION_CHINESE = "Text Recognition Chinese"
    private const val TEXT_RECOGNITION_DEVANAGARI = "Text Recognition Devanagari"
    private const val TEXT_RECOGNITION_JAPANESE = "Text Recognition Japanese"
    private const val TEXT_RECOGNITION_KOREAN = "Text Recognition Korean"
    private const val IMAGE_LABELING = "Image Labeling"
    private const val IMAGE_LABELING_CUSTOM = "Custom Image Labeling (Birds)"
    private const val CUSTOM_AUTOML_LABELING = "Custom AutoML Image Labeling (Flower)"
    private const val POSE_DETECTION = "Pose Detection"
    private const val SELFIE_SEGMENTATION = "Selfie Segmentation"
    private const val FACE_MESH_DETECTION = "Face Mesh Detection (Beta)"
    private const val SUBJECT_SEGMENTATION = "Subject Segmentation (Beta)"

    private const val SIZE_SCREEN = "w:screen" // Match screen width
    private const val SIZE_1024_768 = "w:1024" // ~1024*768 in a normal ratio
    private const val SIZE_640_480 = "w:640" // ~640*480 in a normal ratio
    private const val SIZE_ORIGINAL = "w:original" // Original image size
    private const val KEY_IMAGE_URI = "com.google.mlkit.vision.demo.KEY_IMAGE_URI"
    private const val KEY_IMAGE_MAX_WIDTH = "com.google.mlkit.vision.demo.KEY_IMAGE_MAX_WIDTH"
    private const val KEY_IMAGE_MAX_HEIGHT = "com.google.mlkit.vision.demo.KEY_IMAGE_MAX_HEIGHT"
    private const val KEY_SELECTED_SIZE = "com.google.mlkit.vision.demo.KEY_SELECTED_SIZE"
    private const val REQUEST_IMAGE_CAPTURE = 1001
    private const val REQUEST_CHOOSE_IMAGE = 1002
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/StillImageActivity.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/TaskExt.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin

import com.google.android.gms.tasks.OnCanceledListener
import com.google.android.gms.tasks.OnCompleteListener
import com.google.android.gms.tasks.OnFailureListener
import com.google.android.gms.tasks.OnSuccessListener
import com.google.android.gms.tasks.Task
import java.util.concurrent.Executor

/**
 * Quality-of-life helper to allow using trailing lambda syntax for adding a success listener to a
 * [Task].
 */
fun <TResult> Task<TResult>.addOnSuccessListener(
  executor: Executor,
  listener: (TResult) -> Unit
): Task<TResult> {
  return addOnSuccessListener(executor, OnSuccessListener(listener))
}

/**
 * Quality-of-life helper to allow using trailing lambda syntax for adding a failure listener to a
 * [Task].
 */
fun <TResult> Task<TResult>.addOnFailureListener(
  executor: Executor,
  listener: (Exception) -> Unit
): Task<TResult> {
  return addOnFailureListener(executor, OnFailureListener(listener))
}

/**
 * Quality-of-life helper to allow using trailing lambda syntax for adding a completion listener to
 * a [Task].
 */
fun <TResult> Task<TResult>.addOnCompleteListener(
  executor: Executor,
  listener: (Task<TResult>) -> Unit
): Task<TResult> {
  return addOnCompleteListener(executor, OnCompleteListener(listener))
}

/**
 * Quality-of-life helper to allow using trailing lambda syntax for adding a cancellation listener
 * to a [Task].
 */
fun <TResult> Task<TResult>.addOnCanceledListener(
  executor: Executor,
  listener: () -> Unit
): Task<TResult> {
  return addOnCanceledListener(executor, OnCanceledListener(listener))
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/TaskExt.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/VisionProcessorBase.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin

import android.app.ActivityManager
import android.content.Context
import android.graphics.Bitmap
import android.os.Build.VERSION_CODES
import android.os.SystemClock
import android.util.Log
import android.widget.Toast
import androidx.annotation.GuardedBy
import androidx.annotation.RequiresApi
import androidx.camera.core.ExperimentalGetImage
import androidx.camera.core.ImageProxy
import com.google.android.gms.tasks.OnFailureListener
import com.google.android.gms.tasks.OnSuccessListener
import com.google.android.gms.tasks.Task
import com.google.android.gms.tasks.TaskExecutors
import com.google.android.gms.tasks.Tasks
import com.google.android.odml.image.BitmapMlImageBuilder
import com.google.android.odml.image.ByteBufferMlImageBuilder
import com.google.android.odml.image.MediaMlImageBuilder
import com.google.android.odml.image.MlImage
import com.google.mlkit.common.MlKitException
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.demo.BitmapUtils
import com.google.mlkit.vision.demo.CameraImageGraphic
import com.google.mlkit.vision.demo.FrameMetadata
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.InferenceInfoGraphic
import com.google.mlkit.vision.demo.ScopedExecutor
import com.google.mlkit.vision.demo.VisionImageProcessor
import com.google.mlkit.vision.demo.preference.PreferenceUtils
import java.lang.Math.max
import java.lang.Math.min
import java.nio.ByteBuffer
import java.util.Timer
import java.util.TimerTask

/**
 * Abstract base class for ML Kit frame processors. Subclasses need to implement {@link
 * #onSuccess(T, FrameMetadata, GraphicOverlay)} to define what they want to with the detection
 * results and {@link #detectInImage(VisionImage)} to specify the detector object.
 *
 * @param <T> The type of the detected feature.
 */
abstract class VisionProcessorBase<T>(context: Context) : VisionImageProcessor {

  companion object {
    const val MANUAL_TESTING_LOG = "LogTagForTest"
    private const val TAG = "VisionProcessorBase"
  }

  private var activityManager: ActivityManager =
    context.getSystemService(Context.ACTIVITY_SERVICE) as ActivityManager
  private val fpsTimer = Timer()
  private val executor = ScopedExecutor(TaskExecutors.MAIN_THREAD)

  // Whether this processor is already shut down
  private var isShutdown = false

  // Used to calculate latency, running in the same thread, no sync needed.
  private var numRuns = 0
  private var totalFrameMs = 0L
  private var maxFrameMs = 0L
  private var minFrameMs = Long.MAX_VALUE
  private var totalDetectorMs = 0L
  private var maxDetectorMs = 0L
  private var minDetectorMs = Long.MAX_VALUE

  // Frame count that have been processed so far in an one second interval to calculate FPS.
  private var frameProcessedInOneSecondInterval = 0
  private var framesPerSecond = 0

  // To keep the latest images and its metadata.
  @GuardedBy("this") private var latestImage: ByteBuffer? = null
  @GuardedBy("this") private var latestImageMetaData: FrameMetadata? = null
  // To keep the images and metadata in process.
  @GuardedBy("this") private var processingImage: ByteBuffer? = null
  @GuardedBy("this") private var processingMetaData: FrameMetadata? = null

  init {
    fpsTimer.scheduleAtFixedRate(
      object : TimerTask() {
        override fun run() {
          framesPerSecond = frameProcessedInOneSecondInterval
          frameProcessedInOneSecondInterval = 0
        }
      },
      0,
      1000
    )
  }

  // -----------------Code for processing single still image----------------------------------------
  override fun processBitmap(bitmap: Bitmap?, graphicOverlay: GraphicOverlay) {
    val frameStartMs = SystemClock.elapsedRealtime()

    if (isMlImageEnabled(graphicOverlay.context)) {
      val mlImage = BitmapMlImageBuilder(bitmap!!).build()
      requestDetectInImage(
        mlImage,
        graphicOverlay,
        /* originalCameraImage= */ null,
        /* shouldShowFps= */ false,
        frameStartMs
      )
      mlImage.close()
      return
    }

    requestDetectInImage(
      InputImage.fromBitmap(bitmap!!, 0),
      graphicOverlay,
      /* originalCameraImage= */ null,
      /* shouldShowFps= */ false,
      frameStartMs
    )
  }

  // -----------------Code for processing live preview frame from Camera1 API-----------------------
  @Synchronized
  override fun processByteBuffer(
    data: ByteBuffer?,
    frameMetadata: FrameMetadata?,
    graphicOverlay: GraphicOverlay
  ) {
    latestImage = data
    latestImageMetaData = frameMetadata
    if (processingImage == null && processingMetaData == null) {
      processLatestImage(graphicOverlay)
    }
  }

  @Synchronized
  private fun processLatestImage(graphicOverlay: GraphicOverlay) {
    processingImage = latestImage
    processingMetaData = latestImageMetaData
    latestImage = null
    latestImageMetaData = null
    if (processingImage != null && processingMetaData != null && !isShutdown) {
      processImage(processingImage!!, processingMetaData!!, graphicOverlay)
    }
  }

  private fun processImage(
    data: ByteBuffer,
    frameMetadata: FrameMetadata,
    graphicOverlay: GraphicOverlay
  ) {
    val frameStartMs = SystemClock.elapsedRealtime()
    // If live viewport is on (that is the underneath surface view takes care of the camera preview
    // drawing), skip the unnecessary bitmap creation that used for the manual preview drawing.
    val bitmap =
      if (PreferenceUtils.isCameraLiveViewportEnabled(graphicOverlay.context)) null
      else BitmapUtils.getBitmap(data, frameMetadata)

    if (isMlImageEnabled(graphicOverlay.context)) {
      val mlImage =
        ByteBufferMlImageBuilder(
            data,
            frameMetadata.width,
            frameMetadata.height,
            MlImage.IMAGE_FORMAT_NV21
          )
          .setRotation(frameMetadata.rotation)
          .build()
      requestDetectInImage(mlImage, graphicOverlay, bitmap, /* shouldShowFps= */ true, frameStartMs)
        .addOnSuccessListener(executor) { processLatestImage(graphicOverlay) }

      // This is optional. Java Garbage collection can also close it eventually.
      mlImage.close()
      return
    }

    requestDetectInImage(
      InputImage.fromByteBuffer(
        data,
        frameMetadata.width,
        frameMetadata.height,
        frameMetadata.rotation,
        InputImage.IMAGE_FORMAT_NV21
      ),
      graphicOverlay,
      bitmap,
      /* shouldShowFps= */ true,
      frameStartMs
    )
      .addOnSuccessListener(executor) { processLatestImage(graphicOverlay) }
  }

  // -----------------Code for processing live preview frame from CameraX API-----------------------
  @RequiresApi(VERSION_CODES.LOLLIPOP)
  @ExperimentalGetImage
  override fun processImageProxy(image: ImageProxy, graphicOverlay: GraphicOverlay) {
    val frameStartMs = SystemClock.elapsedRealtime()
    if (isShutdown) {
      return
    }
    var bitmap: Bitmap? = null
    if (!PreferenceUtils.isCameraLiveViewportEnabled(graphicOverlay.context)) {
      bitmap = BitmapUtils.getBitmap(image)
    }

    if (isMlImageEnabled(graphicOverlay.context)) {
      val mlImage =
        MediaMlImageBuilder(image.image!!).setRotation(image.imageInfo.rotationDegrees).build()
      requestDetectInImage(
        mlImage,
        graphicOverlay,
        /* originalCameraImage= */ bitmap,
        /* shouldShowFps= */ true,
        frameStartMs
      )
        // When the image is from CameraX analysis use case, must call image.close() on received
        // images when finished using them. Otherwise, new images may not be received or the camera
        // may stall.
        // Currently MlImage doesn't support ImageProxy directly, so we still need to call
        // ImageProxy.close() here.
        .addOnCompleteListener { image.close() }

      return
    }

    requestDetectInImage(
      InputImage.fromMediaImage(image.image!!, image.imageInfo.rotationDegrees),
      graphicOverlay,
      /* originalCameraImage= */ bitmap,
      /* shouldShowFps= */ true,
      frameStartMs
    )
      // When the image is from CameraX analysis use case, must call image.close() on received
      // images when finished using them. Otherwise, new images may not be received or the camera
      // may stall.
      .addOnCompleteListener { image.close() }
  }

  // -----------------Common processing logic-------------------------------------------------------
  private fun requestDetectInImage(
    image: InputImage,
    graphicOverlay: GraphicOverlay,
    originalCameraImage: Bitmap?,
    shouldShowFps: Boolean,
    frameStartMs: Long
  ): Task<T> {
    return setUpListener(
      detectInImage(image),
      graphicOverlay,
      originalCameraImage,
      shouldShowFps,
      frameStartMs
    )
  }

  private fun requestDetectInImage(
    image: MlImage,
    graphicOverlay: GraphicOverlay,
    originalCameraImage: Bitmap?,
    shouldShowFps: Boolean,
    frameStartMs: Long
  ): Task<T> {
    return setUpListener(
      detectInImage(image),
      graphicOverlay,
      originalCameraImage,
      shouldShowFps,
      frameStartMs
    )
  }

  private fun setUpListener(
    task: Task<T>,
    graphicOverlay: GraphicOverlay,
    originalCameraImage: Bitmap?,
    shouldShowFps: Boolean,
    frameStartMs: Long
  ): Task<T> {
    val detectorStartMs = SystemClock.elapsedRealtime()
    return task
      .addOnSuccessListener(
        executor,
        OnSuccessListener { results: T ->
          val endMs = SystemClock.elapsedRealtime()
          val currentFrameLatencyMs = endMs - frameStartMs
          val currentDetectorLatencyMs = endMs - detectorStartMs
          if (numRuns >= 500) {
            resetLatencyStats()
          }
          numRuns++
          frameProcessedInOneSecondInterval++
          totalFrameMs += currentFrameLatencyMs
          maxFrameMs = max(currentFrameLatencyMs, maxFrameMs)
          minFrameMs = min(currentFrameLatencyMs, minFrameMs)
          totalDetectorMs += currentDetectorLatencyMs
          maxDetectorMs = max(currentDetectorLatencyMs, maxDetectorMs)
          minDetectorMs = min(currentDetectorLatencyMs, minDetectorMs)

          // Only log inference info once per second. When frameProcessedInOneSecondInterval is
          // equal to 1, it means this is the first frame processed during the current second.
          if (frameProcessedInOneSecondInterval == 1) {
            Log.d(TAG, "Num of Runs: $numRuns")
            Log.d(
              TAG,
              "Frame latency: max=" +
                maxFrameMs +
                ", min=" +
                minFrameMs +
                ", avg=" +
                totalFrameMs / numRuns
            )
            Log.d(
              TAG,
              "Detector latency: max=" +
                maxDetectorMs +
                ", min=" +
                minDetectorMs +
                ", avg=" +
                totalDetectorMs / numRuns
            )
            val mi = ActivityManager.MemoryInfo()
            activityManager.getMemoryInfo(mi)
            val availableMegs: Long = mi.availMem / 0x100000L
            Log.d(TAG, "Memory available in system: $availableMegs MB")
          }
          graphicOverlay.clear()
          if (originalCameraImage != null) {
            graphicOverlay.add(CameraImageGraphic(graphicOverlay, originalCameraImage))
          }
          this@VisionProcessorBase.onSuccess(results, graphicOverlay)
          if (!PreferenceUtils.shouldHideDetectionInfo(graphicOverlay.context)) {
            graphicOverlay.add(
              InferenceInfoGraphic(
                graphicOverlay,
                currentFrameLatencyMs,
                currentDetectorLatencyMs,
                if (shouldShowFps) framesPerSecond else null
              )
            )
          }
          graphicOverlay.postInvalidate()
        }
      )
      .addOnFailureListener(
        executor,
        OnFailureListener { e: Exception ->
          graphicOverlay.clear()
          graphicOverlay.postInvalidate()
          val error = "Failed to process. Error: " + e.localizedMessage
          Toast.makeText(
              graphicOverlay.context,
              """
          $error
          Cause: ${e.cause}
          """.trimIndent(),
              Toast.LENGTH_SHORT
            )
            .show()
          Log.d(TAG, error)
          e.printStackTrace()
          this@VisionProcessorBase.onFailure(e)
        }
      )
  }

  override fun stop() {
    executor.shutdown()
    isShutdown = true
    resetLatencyStats()
    fpsTimer.cancel()
  }

  private fun resetLatencyStats() {
    numRuns = 0
    totalFrameMs = 0
    maxFrameMs = 0
    minFrameMs = Long.MAX_VALUE
    totalDetectorMs = 0
    maxDetectorMs = 0
    minDetectorMs = Long.MAX_VALUE
  }

  protected abstract fun detectInImage(image: InputImage): Task<T>

  protected open fun detectInImage(image: MlImage): Task<T> {
    return Tasks.forException(
      MlKitException(
        "MlImage is currently not demonstrated for this feature",
        MlKitException.INVALID_ARGUMENT
      )
    )
  }

  protected abstract fun onSuccess(results: T, graphicOverlay: GraphicOverlay)

  protected abstract fun onFailure(e: Exception)

  protected open fun isMlImageEnabled(context: Context?): Boolean {
    return false
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/VisionProcessorBase.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/barcodescanner/BarcodeGraphic.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.barcodescanner

import android.graphics.Canvas
import android.graphics.Color
import android.graphics.Paint
import android.graphics.RectF
import com.google.mlkit.vision.barcode.common.Barcode
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.GraphicOverlay.Graphic
import kotlin.math.max
import kotlin.math.min

/** Graphic instance for rendering Barcode position and content information in an overlay view. */
class BarcodeGraphic constructor(overlay: GraphicOverlay?, private val barcode: Barcode?) :
  Graphic(overlay) {
  private val rectPaint: Paint = Paint()
  private val barcodePaint: Paint
  private val labelPaint: Paint

  init {
    rectPaint.color = MARKER_COLOR
    rectPaint.style = Paint.Style.STROKE
    rectPaint.strokeWidth = STROKE_WIDTH
    barcodePaint = Paint()
    barcodePaint.color = TEXT_COLOR
    barcodePaint.textSize = TEXT_SIZE
    labelPaint = Paint()
    labelPaint.color = MARKER_COLOR
    labelPaint.style = Paint.Style.FILL
  }

  /**
   * Draws the barcode block annotations for position, size, and raw value on the supplied canvas.
   */
  override fun draw(canvas: Canvas) {
    checkNotNull(barcode) { "Attempting to draw a null barcode." }
    // Draws the bounding box around the BarcodeBlock.
    val rect = RectF(barcode.boundingBox)
    // If the image is flipped, the left will be translated to right, and the right to left.
    val x0 = translateX(rect.left)
    val x1 = translateX(rect.right)
    rect.left = min(x0, x1)
    rect.right = max(x0, x1)
    rect.top = translateY(rect.top)
    rect.bottom = translateY(rect.bottom)
    canvas.drawRect(rect, rectPaint)
    // Draws other object info.
    val lineHeight = TEXT_SIZE + 2 * STROKE_WIDTH
    val textWidth = barcodePaint.measureText(barcode.displayValue)
    canvas.drawRect(
      rect.left - STROKE_WIDTH,
      rect.top - lineHeight,
      rect.left + textWidth + 2 * STROKE_WIDTH,
      rect.top,
      labelPaint
    )
    // Renders the barcode at the bottom of the box.
    canvas.drawText(barcode.displayValue!!, rect.left, rect.top - STROKE_WIDTH, barcodePaint)
  }

  companion object {
    private const val TEXT_COLOR = Color.BLACK
    private const val MARKER_COLOR = Color.WHITE
    private const val TEXT_SIZE = 54.0f
    private const val STROKE_WIDTH = 4.0f
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/barcodescanner/BarcodeGraphic.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/barcodescanner/BarcodeScannerProcessor.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.barcodescanner

import android.content.Context
import android.util.Log
import com.google.android.gms.tasks.Task
import com.google.mlkit.vision.barcode.BarcodeScanner
import com.google.mlkit.vision.barcode.BarcodeScannerOptions
import com.google.mlkit.vision.barcode.BarcodeScanning
import com.google.mlkit.vision.barcode.ZoomSuggestionOptions
import com.google.mlkit.vision.barcode.ZoomSuggestionOptions.ZoomCallback
import com.google.mlkit.vision.barcode.common.Barcode
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.kotlin.VisionProcessorBase

/** Barcode Detector Demo. */
class BarcodeScannerProcessor(context: Context, zoomCallback: ZoomCallback?) :
  VisionProcessorBase<List<Barcode>>(context) {

  private var barcodeScanner: BarcodeScanner

  init {
    // Note that if you know which format of barcode your app is dealing with, detection will be
    // faster to specify the supported barcode formats one by one, e.g.
    // BarcodeScannerOptions.Builder()
    //     .setBarcodeFormats(Barcode.FORMAT_QR_CODE)
    //     .build();
    barcodeScanner =
      if (zoomCallback != null) {
        val options =
          BarcodeScannerOptions.Builder()
            .setZoomSuggestionOptions(ZoomSuggestionOptions.Builder(zoomCallback).build())
            .build()
        BarcodeScanning.getClient(options)
      } else {
        BarcodeScanning.getClient()
      }
  }

  override fun stop() {
    super.stop()
    barcodeScanner.close()
  }

  override fun detectInImage(image: InputImage): Task<List<Barcode>> {
    return barcodeScanner.process(image)
  }

  override fun onSuccess(barcodes: List<Barcode>, graphicOverlay: GraphicOverlay) {
    if (barcodes.isEmpty()) {
      Log.v(MANUAL_TESTING_LOG, "No barcode has been detected")
    }
    for (i in barcodes.indices) {
      val barcode = barcodes[i]
      graphicOverlay.add(BarcodeGraphic(graphicOverlay, barcode))
      logExtrasForTesting(barcode)
    }
  }

  override fun onFailure(e: Exception) {
    Log.e(TAG, "Barcode detection failed $e")
  }

  companion object {
    private const val TAG = "BarcodeProcessor"

    private fun logExtrasForTesting(barcode: Barcode?) {
      if (barcode != null) {
        Log.v(
          MANUAL_TESTING_LOG,
          String.format(
            "Detected barcode's bounding box: %s",
            barcode.boundingBox!!.flattenToString()
          )
        )
        Log.v(
          MANUAL_TESTING_LOG,
          String.format("Expected corner point size is 4, get %d", barcode.cornerPoints!!.size)
        )
        for (point in barcode.cornerPoints!!) {
          Log.v(
            MANUAL_TESTING_LOG,
            String.format("Corner point is located at: x = %d, y = %d", point.x, point.y)
          )
        }
        Log.v(MANUAL_TESTING_LOG, "barcode display value: " + barcode.displayValue)
        Log.v(MANUAL_TESTING_LOG, "barcode raw value: " + barcode.rawValue)
        val dl = barcode.driverLicense
        if (dl != null) {
          Log.v(MANUAL_TESTING_LOG, "driver license city: " + dl.addressCity)
          Log.v(MANUAL_TESTING_LOG, "driver license state: " + dl.addressState)
          Log.v(MANUAL_TESTING_LOG, "driver license street: " + dl.addressStreet)
          Log.v(MANUAL_TESTING_LOG, "driver license zip code: " + dl.addressZip)
          Log.v(MANUAL_TESTING_LOG, "driver license birthday: " + dl.birthDate)
          Log.v(MANUAL_TESTING_LOG, "driver license document type: " + dl.documentType)
          Log.v(MANUAL_TESTING_LOG, "driver license expiry date: " + dl.expiryDate)
          Log.v(MANUAL_TESTING_LOG, "driver license first name: " + dl.firstName)
          Log.v(MANUAL_TESTING_LOG, "driver license middle name: " + dl.middleName)
          Log.v(MANUAL_TESTING_LOG, "driver license last name: " + dl.lastName)
          Log.v(MANUAL_TESTING_LOG, "driver license gender: " + dl.gender)
          Log.v(MANUAL_TESTING_LOG, "driver license issue date: " + dl.issueDate)
          Log.v(MANUAL_TESTING_LOG, "driver license issue country: " + dl.issuingCountry)
          Log.v(MANUAL_TESTING_LOG, "driver license number: " + dl.licenseNumber)
        }
      }
    }
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/barcodescanner/BarcodeScannerProcessor.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/facedetector/FaceDetectorProcessor.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.facedetector

import android.content.Context
import android.util.Log
import com.google.android.gms.tasks.Task
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.kotlin.VisionProcessorBase
import com.google.mlkit.vision.face.Face
import com.google.mlkit.vision.face.FaceDetection
import com.google.mlkit.vision.face.FaceDetector
import com.google.mlkit.vision.face.FaceDetectorOptions
import com.google.mlkit.vision.face.FaceLandmark
import java.util.Locale

/** Face Detector Demo.  */
class FaceDetectorProcessor(context: Context, detectorOptions: FaceDetectorOptions?) :
  VisionProcessorBase<List<Face>>(context) {

  private val detector: FaceDetector

  init {
    val options = detectorOptions
      ?: FaceDetectorOptions.Builder()
        .setClassificationMode(FaceDetectorOptions.CLASSIFICATION_MODE_ALL)
        .enableTracking()
        .build()

    detector = FaceDetection.getClient(options)

    Log.v(MANUAL_TESTING_LOG, "Face detector options: $options")
  }

  override fun stop() {
    super.stop()
    detector.close()
  }

  override fun detectInImage(image: InputImage): Task<List<Face>> {
    return detector.process(image)
  }

  override fun onSuccess(faces: List<Face>, graphicOverlay: GraphicOverlay) {
    for (face in faces) {
      graphicOverlay.add(FaceGraphic(graphicOverlay, face))
      logExtrasForTesting(face)
    }
  }

  override fun onFailure(e: Exception) {
    Log.e(TAG, "Face detection failed $e")
  }

  companion object {
    private const val TAG = "FaceDetectorProcessor"
    private fun logExtrasForTesting(face: Face?) {
      if (face != null) {
        Log.v(
          MANUAL_TESTING_LOG,
          "face bounding box: " + face.boundingBox.flattenToString()
        )
        Log.v(
          MANUAL_TESTING_LOG,
          "face Euler Angle X: " + face.headEulerAngleX
        )
        Log.v(
          MANUAL_TESTING_LOG,
          "face Euler Angle Y: " + face.headEulerAngleY
        )
        Log.v(
          MANUAL_TESTING_LOG,
          "face Euler Angle Z: " + face.headEulerAngleZ
        )
        // All landmarks
        val landMarkTypes = intArrayOf(
          FaceLandmark.MOUTH_BOTTOM,
          FaceLandmark.MOUTH_RIGHT,
          FaceLandmark.MOUTH_LEFT,
          FaceLandmark.RIGHT_EYE,
          FaceLandmark.LEFT_EYE,
          FaceLandmark.RIGHT_EAR,
          FaceLandmark.LEFT_EAR,
          FaceLandmark.RIGHT_CHEEK,
          FaceLandmark.LEFT_CHEEK,
          FaceLandmark.NOSE_BASE
        )
        val landMarkTypesStrings = arrayOf(
          "MOUTH_BOTTOM",
          "MOUTH_RIGHT",
          "MOUTH_LEFT",
          "RIGHT_EYE",
          "LEFT_EYE",
          "RIGHT_EAR",
          "LEFT_EAR",
          "RIGHT_CHEEK",
          "LEFT_CHEEK",
          "NOSE_BASE"
        )
        for (i in landMarkTypes.indices) {
          val landmark = face.getLandmark(landMarkTypes[i])
          if (landmark == null) {
            Log.v(
              MANUAL_TESTING_LOG,
              "No landmark of type: " + landMarkTypesStrings[i] + " has been detected"
            )
          } else {
            val landmarkPosition = landmark.position
            val landmarkPositionStr =
              String.format(Locale.US, "x: %f , y: %f", landmarkPosition.x, landmarkPosition.y)
            Log.v(
              MANUAL_TESTING_LOG,
              "Position for face landmark: " +
                landMarkTypesStrings[i] +
                " is :" +
                landmarkPositionStr
            )
          }
        }
        Log.v(
          MANUAL_TESTING_LOG,
          "face left eye open probability: " + face.leftEyeOpenProbability
        )
        Log.v(
          MANUAL_TESTING_LOG,
          "face right eye open probability: " + face.rightEyeOpenProbability
        )
        Log.v(
          MANUAL_TESTING_LOG,
          "face smiling probability: " + face.smilingProbability
        )
        Log.v(
          MANUAL_TESTING_LOG,
          "face tracking id: " + face.trackingId
        )
      }
    }
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/facedetector/FaceDetectorProcessor.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/facedetector/FaceGraphic.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.facedetector

import android.graphics.Canvas
import android.graphics.Color
import android.graphics.Paint
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.GraphicOverlay.Graphic
import com.google.mlkit.vision.face.Face
import com.google.mlkit.vision.face.FaceLandmark
import com.google.mlkit.vision.face.FaceLandmark.LandmarkType
import java.util.Locale
import kotlin.math.abs
import kotlin.math.max

/**
 * Graphic instance for rendering face position, contour, and landmarks within the associated
 * graphic overlay view.
 */
class FaceGraphic constructor(overlay: GraphicOverlay?, private val face: Face) : Graphic(overlay) {
  private val facePositionPaint: Paint
  private val numColors = COLORS.size
  private val idPaints = Array(numColors) { Paint() }
  private val boxPaints = Array(numColors) { Paint() }
  private val labelPaints = Array(numColors) { Paint() }

  init {
    val selectedColor = Color.WHITE
    facePositionPaint = Paint()
    facePositionPaint.color = selectedColor
    for (i in 0 until numColors) {
      idPaints[i] = Paint()
      idPaints[i].color = COLORS[i][0]
      idPaints[i].textSize = ID_TEXT_SIZE
      boxPaints[i] = Paint()
      boxPaints[i].color = COLORS[i][1]
      boxPaints[i].style = Paint.Style.STROKE
      boxPaints[i].strokeWidth = BOX_STROKE_WIDTH
      labelPaints[i] = Paint()
      labelPaints[i].color = COLORS[i][1]
      labelPaints[i].style = Paint.Style.FILL
    }
  }

  /** Draws the face annotations for position on the supplied canvas. */
  override fun draw(canvas: Canvas) {
    // Draws a circle at the position of the detected face, with the face's track id below.

    // Draws a circle at the position of the detected face, with the face's track id below.
    val x = translateX(face.boundingBox.centerX().toFloat())
    val y = translateY(face.boundingBox.centerY().toFloat())
    canvas.drawCircle(x, y, FACE_POSITION_RADIUS, facePositionPaint)

    // Calculate positions.
    val left = x - scale(face.boundingBox.width() / 2.0f)
    val top = y - scale(face.boundingBox.height() / 2.0f)
    val right = x + scale(face.boundingBox.width() / 2.0f)
    val bottom = y + scale(face.boundingBox.height() / 2.0f)
    val lineHeight = ID_TEXT_SIZE + BOX_STROKE_WIDTH
    var yLabelOffset: Float = if (face.trackingId == null) 0f else -lineHeight

    // Decide color based on face ID
    val colorID = if (face.trackingId == null) 0 else abs(face.trackingId!! % NUM_COLORS)

    // Calculate width and height of label box
    var textWidth = idPaints[colorID].measureText("ID: " + face.trackingId)
    if (face.smilingProbability != null) {
      yLabelOffset -= lineHeight
      textWidth =
        max(
          textWidth,
          idPaints[colorID].measureText(
            String.format(Locale.US, "Happiness: %.2f", face.smilingProbability)
          )
        )
    }
    if (face.leftEyeOpenProbability != null) {
      yLabelOffset -= lineHeight
      textWidth =
        max(
          textWidth,
          idPaints[colorID].measureText(
            String.format(Locale.US, "Left eye open: %.2f", face.leftEyeOpenProbability)
          )
        )
    }
    if (face.rightEyeOpenProbability != null) {
      yLabelOffset -= lineHeight
      textWidth =
        max(
          textWidth,
          idPaints[colorID].measureText(
            String.format(Locale.US, "Right eye open: %.2f", face.rightEyeOpenProbability)
          )
        )
    }

    yLabelOffset = yLabelOffset - 3 * lineHeight
    textWidth =
      Math.max(
        textWidth,
        idPaints[colorID].measureText(
          String.format(Locale.US, "EulerX: %.2f", face.headEulerAngleX)
        )
      )
    textWidth =
      Math.max(
        textWidth,
        idPaints[colorID].measureText(
          String.format(Locale.US, "EulerY: %.2f", face.headEulerAngleY)
        )
      )
    textWidth =
      Math.max(
        textWidth,
        idPaints[colorID].measureText(
          String.format(Locale.US, "EulerZ: %.2f", face.headEulerAngleZ)
        )
      )

    // Draw labels
    canvas.drawRect(
      left - BOX_STROKE_WIDTH,
      top + yLabelOffset,
      left + textWidth + 2 * BOX_STROKE_WIDTH,
      top,
      labelPaints[colorID]
    )
    yLabelOffset += ID_TEXT_SIZE
    canvas.drawRect(left, top, right, bottom, boxPaints[colorID])
    if (face.trackingId != null) {
      canvas.drawText("ID: " + face.trackingId, left, top + yLabelOffset, idPaints[colorID])
      yLabelOffset += lineHeight
    }

    // Draws all face contours.
    for (contour in face.allContours) {
      for (point in contour.points) {
        canvas.drawCircle(
          translateX(point.x),
          translateY(point.y),
          FACE_POSITION_RADIUS,
          facePositionPaint
        )
      }
    }

    // Draws smiling and left/right eye open probabilities.
    if (face.smilingProbability != null) {
      canvas.drawText(
        "Smiling: " + String.format(Locale.US, "%.2f", face.smilingProbability),
        left,
        top + yLabelOffset,
        idPaints[colorID]
      )
      yLabelOffset += lineHeight
    }

    val leftEye = face.getLandmark(FaceLandmark.LEFT_EYE)
    if (face.leftEyeOpenProbability != null) {
      canvas.drawText(
        "Left eye open: " + String.format(Locale.US, "%.2f", face.leftEyeOpenProbability),
        left,
        top + yLabelOffset,
        idPaints[colorID]
      )
      yLabelOffset += lineHeight
    }
    if (leftEye != null) {
      val leftEyeLeft =
        translateX(leftEye.position.x) - idPaints[colorID].measureText("Left Eye") / 2.0f
      canvas.drawRect(
        leftEyeLeft - BOX_STROKE_WIDTH,
        translateY(leftEye.position.y) + ID_Y_OFFSET - ID_TEXT_SIZE,
        leftEyeLeft + idPaints[colorID].measureText("Left Eye") + BOX_STROKE_WIDTH,
        translateY(leftEye.position.y) + ID_Y_OFFSET + BOX_STROKE_WIDTH,
        labelPaints[colorID]
      )
      canvas.drawText(
        "Left Eye",
        leftEyeLeft,
        translateY(leftEye.position.y) + ID_Y_OFFSET,
        idPaints[colorID]
      )
    }

    val rightEye = face.getLandmark(FaceLandmark.RIGHT_EYE)
    if (face.rightEyeOpenProbability != null) {
      canvas.drawText(
        "Right eye open: " + String.format(Locale.US, "%.2f", face.rightEyeOpenProbability),
        left,
        top + yLabelOffset,
        idPaints[colorID]
      )
      yLabelOffset += lineHeight
    }
    if (rightEye != null) {
      val rightEyeLeft =
        translateX(rightEye.position.x) - idPaints[colorID].measureText("Right Eye") / 2.0f
      canvas.drawRect(
        rightEyeLeft - BOX_STROKE_WIDTH,
        translateY(rightEye.position.y) + ID_Y_OFFSET - ID_TEXT_SIZE,
        rightEyeLeft + idPaints[colorID].measureText("Right Eye") + BOX_STROKE_WIDTH,
        translateY(rightEye.position.y) + ID_Y_OFFSET + BOX_STROKE_WIDTH,
        labelPaints[colorID]
      )
      canvas.drawText(
        "Right Eye",
        rightEyeLeft,
        translateY(rightEye.position.y) + ID_Y_OFFSET,
        idPaints[colorID]
      )
    }

    canvas.drawText("EulerX: " + face.headEulerAngleX, left, top + yLabelOffset, idPaints[colorID])
    yLabelOffset += lineHeight
    canvas.drawText("EulerY: " + face.headEulerAngleY, left, top + yLabelOffset, idPaints[colorID])
    yLabelOffset += lineHeight
    canvas.drawText("EulerZ: " + face.headEulerAngleZ, left, top + yLabelOffset, idPaints[colorID])

    // Draw facial landmarks
    drawFaceLandmark(canvas, FaceLandmark.LEFT_EYE)
    drawFaceLandmark(canvas, FaceLandmark.RIGHT_EYE)
    drawFaceLandmark(canvas, FaceLandmark.LEFT_CHEEK)
    drawFaceLandmark(canvas, FaceLandmark.RIGHT_CHEEK)
  }

  private fun drawFaceLandmark(canvas: Canvas, @LandmarkType landmarkType: Int) {
    val faceLandmark = face.getLandmark(landmarkType)
    if (faceLandmark != null) {
      canvas.drawCircle(
        translateX(faceLandmark.position.x),
        translateY(faceLandmark.position.y),
        FACE_POSITION_RADIUS,
        facePositionPaint
      )
    }
  }

  companion object {
    private const val FACE_POSITION_RADIUS = 8.0f
    private const val ID_TEXT_SIZE = 30.0f
    private const val ID_Y_OFFSET = 40.0f
    private const val BOX_STROKE_WIDTH = 5.0f
    private const val NUM_COLORS = 10
    private val COLORS =
      arrayOf(
        intArrayOf(Color.BLACK, Color.WHITE),
        intArrayOf(Color.WHITE, Color.MAGENTA),
        intArrayOf(Color.BLACK, Color.LTGRAY),
        intArrayOf(Color.WHITE, Color.RED),
        intArrayOf(Color.WHITE, Color.BLUE),
        intArrayOf(Color.WHITE, Color.DKGRAY),
        intArrayOf(Color.BLACK, Color.CYAN),
        intArrayOf(Color.BLACK, Color.YELLOW),
        intArrayOf(Color.WHITE, Color.BLACK),
        intArrayOf(Color.BLACK, Color.GREEN)
      )
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/facedetector/FaceGraphic.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/facemeshdetector/FaceMeshDetectorProcessor.kt ---

/*
 * Copyright 2022 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.google.mlkit.vision.demo.kotlin.facemeshdetector

import android.content.Context
import android.util.Log
import com.google.android.gms.tasks.Task
import com.google.android.odml.image.MlImage
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.kotlin.VisionProcessorBase
import com.google.mlkit.vision.demo.preference.PreferenceUtils
import com.google.mlkit.vision.facemesh.FaceMesh
import com.google.mlkit.vision.facemesh.FaceMeshDetection
import com.google.mlkit.vision.facemesh.FaceMeshDetector
import com.google.mlkit.vision.facemesh.FaceMeshDetectorOptions

/** Face Mesh Detector Demo. */
class FaceMeshDetectorProcessor(context: Context) :
  VisionProcessorBase<List<FaceMesh>>(context) {

  private val detector: FaceMeshDetector

  init {
    val optionsBuilder = FaceMeshDetectorOptions.Builder()
    if (PreferenceUtils.getFaceMeshUseCase(context) == FaceMeshDetectorOptions.BOUNDING_BOX_ONLY) {
      optionsBuilder.setUseCase(FaceMeshDetectorOptions.BOUNDING_BOX_ONLY)
    }
    detector = FaceMeshDetection.getClient(optionsBuilder.build())
  }

  override fun stop() {
    super.stop()
    detector.close()
  }

  override fun detectInImage(image: InputImage): Task<List<FaceMesh>> {
    return detector.process(image)
  }

  override fun onSuccess(faces: List<FaceMesh>, graphicOverlay: GraphicOverlay) {
    for (face in faces) {
      graphicOverlay.add(FaceMeshGraphic(graphicOverlay, face))
    }
  }

  override fun onFailure(e: Exception) {
    Log.e(TAG, "Face detection failed $e")
  }

  companion object {
    private const val TAG = "SelfieFaceProcessor"
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/facemeshdetector/FaceMeshDetectorProcessor.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/facemeshdetector/FaceMeshGraphic.kt ---

/*
 * Copyright 2022 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.google.mlkit.vision.demo.kotlin.facemeshdetector

import android.graphics.Canvas
import android.graphics.Color
import android.graphics.Paint
import android.graphics.RectF
import com.google.mlkit.vision.common.PointF3D
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.preference.PreferenceUtils
import com.google.mlkit.vision.facemesh.FaceMesh
import com.google.mlkit.vision.facemesh.FaceMeshDetectorOptions
import com.google.mlkit.vision.facemesh.FaceMeshPoint

/**
 * Graphic instance for rendering face position and mesh info within the associated graphic overlay
 * view.
 */
class FaceMeshGraphic(overlay: GraphicOverlay, private val faceMesh: FaceMesh) :
  GraphicOverlay.Graphic(overlay) {

  private val positionPaint: Paint
  private val boxPaint: Paint
  private val useCase: Int
  private var zMin: Float
  private var zMax: Float

  @FaceMesh.ContourType
  private val DISPLAY_CONTOURS =
    intArrayOf(
      FaceMesh.FACE_OVAL,
      FaceMesh.LEFT_EYEBROW_TOP,
      FaceMesh.LEFT_EYEBROW_BOTTOM,
      FaceMesh.RIGHT_EYEBROW_TOP,
      FaceMesh.RIGHT_EYEBROW_BOTTOM,
      FaceMesh.LEFT_EYE,
      FaceMesh.RIGHT_EYE,
      FaceMesh.UPPER_LIP_TOP,
      FaceMesh.UPPER_LIP_BOTTOM,
      FaceMesh.LOWER_LIP_TOP,
      FaceMesh.LOWER_LIP_BOTTOM,
      FaceMesh.NOSE_BRIDGE
    )

  /** Draws the face annotations for position on the supplied canvas. */
  override fun draw(canvas: Canvas) {

    // Draws the bounding box.
    val rect = RectF(faceMesh.boundingBox)
    // If the image is flipped, the left will be translated to right, and the right to left.
    val x0 = translateX(rect.left)
    val x1 = translateX(rect.right)
    rect.left = Math.min(x0, x1)
    rect.right = Math.max(x0, x1)
    rect.top = translateY(rect.top)
    rect.bottom = translateY(rect.bottom)
    canvas.drawRect(rect, boxPaint)

    // Draw face mesh
    val points =
      if (useCase == USE_CASE_CONTOUR_ONLY) getContourPoints(faceMesh) else faceMesh.allPoints
    val triangles = faceMesh.allTriangles

    zMin = Float.MAX_VALUE
    zMax = Float.MIN_VALUE
    for (point in points) {
      zMin = Math.min(zMin, point.position.z)
      zMax = Math.max(zMax, point.position.z)
    }

    // Draw face mesh points
    for (point in points) {
      updatePaintColorByZValue(
        positionPaint,
        canvas,
        /* visualizeZ= */true,
        /* rescaleZForVisualization= */true,
        point.position.z,
        zMin,
        zMax)
      canvas.drawCircle(
        translateX(point.position.x),
        translateY(point.position.y),
        FACE_POSITION_RADIUS,
        positionPaint
      )
    }

    if (useCase == FaceMeshDetectorOptions.FACE_MESH) {
      // Draw face mesh triangles
      for (triangle in triangles) {
        val point1 = triangle.allPoints[0].position
        val point2 = triangle.allPoints[1].position
        val point3 = triangle.allPoints[2].position
        drawLine(canvas, point1, point2)
        drawLine(canvas, point1, point3)
        drawLine(canvas, point2, point3)
      }
    }
  }

  private fun drawLine(canvas: Canvas, point1: PointF3D, point2: PointF3D) {
    updatePaintColorByZValue(
      positionPaint,
      canvas,
      /* visualizeZ= */true,
      /* rescaleZForVisualization= */true,
      (point1.z + point2.z) / 2,
      zMin,
      zMax)
    canvas.drawLine(
      translateX(point1.x),
      translateY(point1.y),
      translateX(point2.x),
      translateY(point2.y),
      positionPaint
    )
  }

  private fun getContourPoints(faceMesh: FaceMesh): List<FaceMeshPoint> {
    val contourPoints: MutableList<FaceMeshPoint> = ArrayList()
    for (type in DISPLAY_CONTOURS) {
      contourPoints.addAll(faceMesh.getPoints(type))
    }
    return contourPoints
  }

  companion object {
    private const val USE_CASE_CONTOUR_ONLY = 999
    private const val FACE_POSITION_RADIUS = 8.0f
    private const val BOX_STROKE_WIDTH = 5.0f
  }

  init {
    val selectedColor = Color.WHITE
    positionPaint = Paint()
    positionPaint.color = selectedColor

    boxPaint = Paint()
    boxPaint.color = selectedColor
    boxPaint.style = Paint.Style.STROKE
    boxPaint.strokeWidth = BOX_STROKE_WIDTH

    useCase = PreferenceUtils.getFaceMeshUseCase(applicationContext)
    zMin = java.lang.Float.MAX_VALUE
    zMax = java.lang.Float.MIN_VALUE
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/facemeshdetector/FaceMeshGraphic.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/labeldetector/LabelDetectorProcessor.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.labeldetector

import android.content.Context
import android.util.Log
import com.google.android.gms.tasks.Task
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.kotlin.VisionProcessorBase
import com.google.mlkit.vision.label.ImageLabel
import com.google.mlkit.vision.label.ImageLabeler
import com.google.mlkit.vision.label.ImageLabelerOptionsBase
import com.google.mlkit.vision.label.ImageLabeling
import java.io.IOException

/** Custom InputImage Classifier Demo.  */
class LabelDetectorProcessor(context: Context, options: ImageLabelerOptionsBase) :
  VisionProcessorBase<List<ImageLabel>>(context) {

  private val imageLabeler: ImageLabeler = ImageLabeling.getClient(options)

  override fun stop() {
    super.stop()
    try {
      imageLabeler.close()
    } catch (e: IOException) {
      Log.e(
        TAG,
        "Exception thrown while trying to close ImageLabelerClient: $e"
      )
    }
  }

  override fun detectInImage(image: InputImage): Task<List<ImageLabel>> {
    return imageLabeler.process(image)
  }

  override fun onSuccess(labels: List<ImageLabel>, graphicOverlay: GraphicOverlay) {
    graphicOverlay.add(LabelGraphic(graphicOverlay, labels))
    logExtrasForTesting(labels)
  }

  override fun onFailure(e: Exception) {
    Log.w(TAG, "Label detection failed.$e")
  }

  companion object {
    private const val TAG = "LabelDetectorProcessor"

    private fun logExtrasForTesting(labels: List<ImageLabel>?) {
      if (labels == null) {
        Log.v(MANUAL_TESTING_LOG, "No labels detected")
      } else {
        for (label in labels) {
          Log.v(
            MANUAL_TESTING_LOG,
            String.format("Label %s, confidence %f", label.text, label.confidence)
          )
        }
      }
    }
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/labeldetector/LabelDetectorProcessor.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/labeldetector/LabelGraphic.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.labeldetector

import android.graphics.Canvas
import android.graphics.Color
import android.graphics.Paint
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.GraphicOverlay.Graphic
import com.google.mlkit.vision.label.ImageLabel
import java.util.Locale

/** Graphic instance for rendering a label within an associated graphic overlay view.  */
class LabelGraphic(
  private val overlay: GraphicOverlay,
  private val labels: List<ImageLabel>
) : Graphic(overlay) {
  private val textPaint: Paint = Paint()
  private val labelPaint: Paint

  init {
    textPaint.color = Color.WHITE
    textPaint.textSize = TEXT_SIZE
    labelPaint = Paint()
    labelPaint.color = Color.BLACK
    labelPaint.style = Paint.Style.FILL
    labelPaint.alpha = 200
  }

  @Synchronized
  override fun draw(canvas: Canvas) {
    // First try to find maxWidth and totalHeight in order to draw to the center of the screen.
    var maxWidth = 0f
    val totalHeight = labels.size * 2 * TEXT_SIZE
    for (label in labels) {
      val line1Width = textPaint.measureText(label.text)
      val line2Width =
        textPaint.measureText(
          String.format(
            Locale.US,
            LABEL_FORMAT,
            label.confidence * 100,
            label.index
          )
        )

      maxWidth = Math.max(maxWidth, Math.max(line1Width, line2Width))
    }

    val x = Math.max(0f, overlay.width / 2.0f - maxWidth / 2.0f)
    var y = Math.max(200f, overlay.height / 2.0f - totalHeight / 2.0f)

    if (!labels.isEmpty()) {
      val padding = 20f
      canvas.drawRect(
        x - padding,
        y - padding,
        x + maxWidth + padding,
        y + totalHeight + padding,
        labelPaint
      )
    }

    for (label in labels) {
      if (y + TEXT_SIZE * 2 > overlay.height) {
        break
      }
      canvas.drawText(label.text, x, y + TEXT_SIZE, textPaint)
      y += TEXT_SIZE
      canvas.drawText(
        String.format(
          Locale.US,
          LABEL_FORMAT,
          label.confidence * 100,
          label.index
        ),
        x, y + TEXT_SIZE, textPaint
      )
      y += TEXT_SIZE
    }
  }

  companion object {
    private const val TEXT_SIZE = 70.0f
    private const val LABEL_FORMAT = "%.2f%% confidence (index: %d)"
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/labeldetector/LabelGraphic.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/objectdetector/ObjectDetectorProcessor.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.objectdetector

import android.content.Context
import android.util.Log
import com.google.android.gms.tasks.Task
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.kotlin.VisionProcessorBase
import com.google.mlkit.vision.objects.DetectedObject
import com.google.mlkit.vision.objects.ObjectDetection
import com.google.mlkit.vision.objects.ObjectDetector
import com.google.mlkit.vision.objects.ObjectDetectorOptionsBase
import java.io.IOException

/** A processor to run object detector.  */
class ObjectDetectorProcessor(context: Context, options: ObjectDetectorOptionsBase) :
  VisionProcessorBase<List<DetectedObject>>(context) {

  private val detector: ObjectDetector = ObjectDetection.getClient(options)

  override fun stop() {
    super.stop()
    try {
      detector.close()
    } catch (e: IOException) {
      Log.e(
        TAG,
        "Exception thrown while trying to close object detector!",
        e
      )
    }
  }

  override fun detectInImage(image: InputImage): Task<List<DetectedObject>> {
    return detector.process(image)
  }

  override fun onSuccess(results: List<DetectedObject>, graphicOverlay: GraphicOverlay) {
    for (result in results) {
      graphicOverlay.add(ObjectGraphic(graphicOverlay, result))
    }
  }

  override fun onFailure(e: Exception) {
    Log.e(TAG, "Object detection failed!", e)
  }

  companion object {
    private const val TAG = "ObjectDetectorProcessor"
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/objectdetector/ObjectDetectorProcessor.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/objectdetector/ObjectGraphic.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.objectdetector

import android.graphics.Canvas
import android.graphics.Color
import android.graphics.Paint
import android.graphics.RectF
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.GraphicOverlay.Graphic
import com.google.mlkit.vision.objects.DetectedObject
import java.util.Locale
import kotlin.math.abs
import kotlin.math.max
import kotlin.math.min

/** Draw the detected object info in preview.  */
class ObjectGraphic constructor(
  overlay: GraphicOverlay,
  private val detectedObject: DetectedObject
) : Graphic(overlay) {

  private val numColors = COLORS.size

  private val boxPaints = Array(numColors) { Paint() }
  private val textPaints = Array(numColors) { Paint() }
  private val labelPaints = Array(numColors) { Paint() }

  init {
    for (i in 0 until numColors) {
      textPaints[i] = Paint()
      textPaints[i].color = COLORS[i][0]
      textPaints[i].textSize = TEXT_SIZE
      boxPaints[i] = Paint()
      boxPaints[i].color = COLORS[i][1]
      boxPaints[i].style = Paint.Style.STROKE
      boxPaints[i].strokeWidth = STROKE_WIDTH
      labelPaints[i] = Paint()
      labelPaints[i].color = COLORS[i][1]
      labelPaints[i].style = Paint.Style.FILL
    }
  }

  override fun draw(canvas: Canvas) {
    // Decide color based on object tracking ID
    val colorID =
      if (detectedObject.trackingId == null) 0
      else abs(detectedObject.trackingId!! % NUM_COLORS)
    var textWidth =
      textPaints[colorID].measureText("Tracking ID: " + detectedObject.trackingId)
    val lineHeight = TEXT_SIZE + STROKE_WIDTH
    var yLabelOffset = -lineHeight

    // Calculate width and height of label box
    for (label in detectedObject.labels) {
      textWidth =
        max(textWidth, textPaints[colorID].measureText(label.text))
      textWidth = max(
        textWidth,
        textPaints[colorID].measureText(
          String.format(
            Locale.US,
            LABEL_FORMAT,
            label.confidence * 100,
            label.index
          )
        )
      )
      yLabelOffset -= 2 * lineHeight
    }

    // Draws the bounding box.
    val rect = RectF(detectedObject.boundingBox)
    val x0 = translateX(rect.left)
    val x1 = translateX(rect.right)
    rect.left = min(x0, x1)
    rect.right = max(x0, x1)
    rect.top = translateY(rect.top)
    rect.bottom = translateY(rect.bottom)
    canvas.drawRect(rect, boxPaints[colorID])

    // Draws other object info.
    canvas.drawRect(
      rect.left - STROKE_WIDTH,
      rect.top + yLabelOffset,
      rect.left + textWidth + 2 * STROKE_WIDTH,
      rect.top,
      labelPaints[colorID]
    )
    yLabelOffset += TEXT_SIZE
    canvas.drawText(
      "Tracking ID: " + detectedObject.trackingId,
      rect.left,
      rect.top + yLabelOffset,
      textPaints[colorID]
    )
    yLabelOffset += lineHeight
    for (label in detectedObject.labels) {
      canvas.drawText(
        label.text + " (index: " + label.index + ")",
        rect.left,
        rect.top + yLabelOffset,
        textPaints[colorID]
      )
      yLabelOffset += lineHeight
      canvas.drawText(
        String.format(
          Locale.US,
          LABEL_FORMAT,
          label.confidence * 100,
          label.index
        ),
        rect.left,
        rect.top + yLabelOffset,
        textPaints[colorID]
      )
      yLabelOffset += lineHeight
    }
  }

  companion object {
    private const val TEXT_SIZE = 54.0f
    private const val STROKE_WIDTH = 4.0f
    private const val NUM_COLORS = 10
    private val COLORS =
      arrayOf(
        intArrayOf(Color.BLACK, Color.WHITE),
        intArrayOf(Color.WHITE, Color.MAGENTA),
        intArrayOf(Color.BLACK, Color.LTGRAY),
        intArrayOf(Color.WHITE, Color.RED),
        intArrayOf(Color.WHITE, Color.BLUE),
        intArrayOf(Color.WHITE, Color.DKGRAY),
        intArrayOf(Color.BLACK, Color.CYAN),
        intArrayOf(Color.BLACK, Color.YELLOW),
        intArrayOf(Color.WHITE, Color.BLACK),
        intArrayOf(Color.BLACK, Color.GREEN)
      )
    private const val LABEL_FORMAT = "%.2f%% confidence (index: %d)"
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/objectdetector/ObjectGraphic.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/posedetector/PoseDetectorProcessor.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.posedetector

import android.content.Context
import android.util.Log
import com.google.android.gms.tasks.Task
import com.google.android.odml.image.MlImage
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.java.posedetector.classification.PoseClassifierProcessor
import com.google.mlkit.vision.demo.kotlin.VisionProcessorBase
import com.google.mlkit.vision.pose.Pose
import com.google.mlkit.vision.pose.PoseDetection
import com.google.mlkit.vision.pose.PoseDetector
import com.google.mlkit.vision.pose.PoseDetectorOptionsBase
import java.util.ArrayList
import java.util.concurrent.Executor
import java.util.concurrent.Executors

/** A processor to run pose detector. */
class PoseDetectorProcessor(
  private val context: Context,
  options: PoseDetectorOptionsBase,
  private val showInFrameLikelihood: Boolean,
  private val visualizeZ: Boolean,
  private val rescaleZForVisualization: Boolean,
  private val runClassification: Boolean,
  private val isStreamMode: Boolean
) : VisionProcessorBase<PoseDetectorProcessor.PoseWithClassification>(context) {

  private val detector: PoseDetector
  private val classificationExecutor: Executor

  private var poseClassifierProcessor: PoseClassifierProcessor? = null

  /** Internal class to hold Pose and classification results. */
  class PoseWithClassification(val pose: Pose, val classificationResult: List<String>)

  init {
    detector = PoseDetection.getClient(options)
    classificationExecutor = Executors.newSingleThreadExecutor()
  }

  override fun stop() {
    super.stop()
    detector.close()
  }

  override fun detectInImage(image: InputImage): Task<PoseWithClassification> {
    return detector
      .process(image)
      .continueWith(
        classificationExecutor,
        { task ->
          val pose = task.getResult()
          var classificationResult: List<String> = ArrayList()
          if (runClassification) {
            if (poseClassifierProcessor == null) {
              poseClassifierProcessor = PoseClassifierProcessor(context, isStreamMode)
            }
            classificationResult = poseClassifierProcessor!!.getPoseResult(pose)
          }
          PoseWithClassification(pose, classificationResult)
        }
      )
  }

  override fun detectInImage(image: MlImage): Task<PoseWithClassification> {
    return detector
      .process(image)
      .continueWith(
        classificationExecutor,
        { task ->
          val pose = task.getResult()
          var classificationResult: List<String> = ArrayList()
          if (runClassification) {
            if (poseClassifierProcessor == null) {
              poseClassifierProcessor = PoseClassifierProcessor(context, isStreamMode)
            }
            classificationResult = poseClassifierProcessor!!.getPoseResult(pose)
          }
          PoseWithClassification(pose, classificationResult)
        }
      )
  }

  override fun onSuccess(
    poseWithClassification: PoseWithClassification,
    graphicOverlay: GraphicOverlay
  ) {
    graphicOverlay.add(
      PoseGraphic(
        graphicOverlay,
        poseWithClassification.pose,
        showInFrameLikelihood,
        visualizeZ,
        rescaleZForVisualization,
        poseWithClassification.classificationResult
      )
    )
  }

  override fun onFailure(e: Exception) {
    Log.e(TAG, "Pose detection failed!", e)
  }

  override fun isMlImageEnabled(context: Context?): Boolean {
    // Use MlImage in Pose Detection by default, change it to OFF to switch to InputImage.
    return true
  }

  companion object {
    private val TAG = "PoseDetectorProcessor"
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/posedetector/PoseDetectorProcessor.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/posedetector/PoseGraphic.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.posedetector

import android.graphics.Canvas
import android.graphics.Color
import android.graphics.Paint
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.GraphicOverlay.Graphic
import com.google.mlkit.vision.pose.Pose
import com.google.mlkit.vision.pose.PoseLandmark
import java.lang.Math.max
import java.lang.Math.min
import java.util.Locale

/** Draw the detected pose in preview. */
class PoseGraphic
internal constructor(
  overlay: GraphicOverlay,
  private val pose: Pose,
  private val showInFrameLikelihood: Boolean,
  private val visualizeZ: Boolean,
  private val rescaleZForVisualization: Boolean,
  private val poseClassification: List<String>
) : Graphic(overlay) {
  private var zMin = java.lang.Float.MAX_VALUE
  private var zMax = java.lang.Float.MIN_VALUE
  private val classificationTextPaint: Paint
  private val leftPaint: Paint
  private val rightPaint: Paint
  private val whitePaint: Paint

  init {
    classificationTextPaint = Paint()
    classificationTextPaint.color = Color.WHITE
    classificationTextPaint.textSize = POSE_CLASSIFICATION_TEXT_SIZE
    classificationTextPaint.setShadowLayer(5.0f, 0f, 0f, Color.BLACK)

    whitePaint = Paint()
    whitePaint.strokeWidth = STROKE_WIDTH
    whitePaint.color = Color.WHITE
    whitePaint.textSize = IN_FRAME_LIKELIHOOD_TEXT_SIZE
    leftPaint = Paint()
    leftPaint.strokeWidth = STROKE_WIDTH
    leftPaint.color = Color.GREEN
    rightPaint = Paint()
    rightPaint.strokeWidth = STROKE_WIDTH
    rightPaint.color = Color.YELLOW
  }

  override fun draw(canvas: Canvas) {
    val landmarks = pose.allPoseLandmarks
    if (landmarks.isEmpty()) {
      return
    }

    // Draw pose classification text.
    val classificationX = POSE_CLASSIFICATION_TEXT_SIZE * 0.5f
    for (i in poseClassification.indices) {
      val classificationY =
        canvas.height -
          (POSE_CLASSIFICATION_TEXT_SIZE * 1.5f * (poseClassification.size - i).toFloat())
      canvas.drawText(
        poseClassification[i],
        classificationX,
        classificationY,
        classificationTextPaint
      )
    }

    // Draw all the points
    for (landmark in landmarks) {
      drawPoint(canvas, landmark, whitePaint)
      if (visualizeZ && rescaleZForVisualization) {
        zMin = min(zMin, landmark.position3D.z)
        zMax = max(zMax, landmark.position3D.z)
      }
    }

    val nose = pose.getPoseLandmark(PoseLandmark.NOSE)
    val lefyEyeInner = pose.getPoseLandmark(PoseLandmark.LEFT_EYE_INNER)
    val lefyEye = pose.getPoseLandmark(PoseLandmark.LEFT_EYE)
    val leftEyeOuter = pose.getPoseLandmark(PoseLandmark.LEFT_EYE_OUTER)
    val rightEyeInner = pose.getPoseLandmark(PoseLandmark.RIGHT_EYE_INNER)
    val rightEye = pose.getPoseLandmark(PoseLandmark.RIGHT_EYE)
    val rightEyeOuter = pose.getPoseLandmark(PoseLandmark.RIGHT_EYE_OUTER)
    val leftEar = pose.getPoseLandmark(PoseLandmark.LEFT_EAR)
    val rightEar = pose.getPoseLandmark(PoseLandmark.RIGHT_EAR)
    val leftMouth = pose.getPoseLandmark(PoseLandmark.LEFT_MOUTH)
    val rightMouth = pose.getPoseLandmark(PoseLandmark.RIGHT_MOUTH)

    val leftShoulder = pose.getPoseLandmark(PoseLandmark.LEFT_SHOULDER)
    val rightShoulder = pose.getPoseLandmark(PoseLandmark.RIGHT_SHOULDER)
    val leftElbow = pose.getPoseLandmark(PoseLandmark.LEFT_ELBOW)
    val rightElbow = pose.getPoseLandmark(PoseLandmark.RIGHT_ELBOW)
    val leftWrist = pose.getPoseLandmark(PoseLandmark.LEFT_WRIST)
    val rightWrist = pose.getPoseLandmark(PoseLandmark.RIGHT_WRIST)
    val leftHip = pose.getPoseLandmark(PoseLandmark.LEFT_HIP)
    val rightHip = pose.getPoseLandmark(PoseLandmark.RIGHT_HIP)
    val leftKnee = pose.getPoseLandmark(PoseLandmark.LEFT_KNEE)
    val rightKnee = pose.getPoseLandmark(PoseLandmark.RIGHT_KNEE)
    val leftAnkle = pose.getPoseLandmark(PoseLandmark.LEFT_ANKLE)
    val rightAnkle = pose.getPoseLandmark(PoseLandmark.RIGHT_ANKLE)

    val leftPinky = pose.getPoseLandmark(PoseLandmark.LEFT_PINKY)
    val rightPinky = pose.getPoseLandmark(PoseLandmark.RIGHT_PINKY)
    val leftIndex = pose.getPoseLandmark(PoseLandmark.LEFT_INDEX)
    val rightIndex = pose.getPoseLandmark(PoseLandmark.RIGHT_INDEX)
    val leftThumb = pose.getPoseLandmark(PoseLandmark.LEFT_THUMB)
    val rightThumb = pose.getPoseLandmark(PoseLandmark.RIGHT_THUMB)
    val leftHeel = pose.getPoseLandmark(PoseLandmark.LEFT_HEEL)
    val rightHeel = pose.getPoseLandmark(PoseLandmark.RIGHT_HEEL)
    val leftFootIndex = pose.getPoseLandmark(PoseLandmark.LEFT_FOOT_INDEX)
    val rightFootIndex = pose.getPoseLandmark(PoseLandmark.RIGHT_FOOT_INDEX)

    // Face
    drawLine(canvas, nose, lefyEyeInner, whitePaint)
    drawLine(canvas, lefyEyeInner, lefyEye, whitePaint)
    drawLine(canvas, lefyEye, leftEyeOuter, whitePaint)
    drawLine(canvas, leftEyeOuter, leftEar, whitePaint)
    drawLine(canvas, nose, rightEyeInner, whitePaint)
    drawLine(canvas, rightEyeInner, rightEye, whitePaint)
    drawLine(canvas, rightEye, rightEyeOuter, whitePaint)
    drawLine(canvas, rightEyeOuter, rightEar, whitePaint)
    drawLine(canvas, leftMouth, rightMouth, whitePaint)

    drawLine(canvas, leftShoulder, rightShoulder, whitePaint)
    drawLine(canvas, leftHip, rightHip, whitePaint)

    // Left body
    drawLine(canvas, leftShoulder, leftElbow, leftPaint)
    drawLine(canvas, leftElbow, leftWrist, leftPaint)
    drawLine(canvas, leftShoulder, leftHip, leftPaint)
    drawLine(canvas, leftHip, leftKnee, leftPaint)
    drawLine(canvas, leftKnee, leftAnkle, leftPaint)
    drawLine(canvas, leftWrist, leftThumb, leftPaint)
    drawLine(canvas, leftWrist, leftPinky, leftPaint)
    drawLine(canvas, leftWrist, leftIndex, leftPaint)
    drawLine(canvas, leftIndex, leftPinky, leftPaint)
    drawLine(canvas, leftAnkle, leftHeel, leftPaint)
    drawLine(canvas, leftHeel, leftFootIndex, leftPaint)

    // Right body
    drawLine(canvas, rightShoulder, rightElbow, rightPaint)
    drawLine(canvas, rightElbow, rightWrist, rightPaint)
    drawLine(canvas, rightShoulder, rightHip, rightPaint)
    drawLine(canvas, rightHip, rightKnee, rightPaint)
    drawLine(canvas, rightKnee, rightAnkle, rightPaint)
    drawLine(canvas, rightWrist, rightThumb, rightPaint)
    drawLine(canvas, rightWrist, rightPinky, rightPaint)
    drawLine(canvas, rightWrist, rightIndex, rightPaint)
    drawLine(canvas, rightIndex, rightPinky, rightPaint)
    drawLine(canvas, rightAnkle, rightHeel, rightPaint)
    drawLine(canvas, rightHeel, rightFootIndex, rightPaint)

    // Draw inFrameLikelihood for all points
    if (showInFrameLikelihood) {
      for (landmark in landmarks) {
        canvas.drawText(
          String.format(Locale.US, "%.2f", landmark.inFrameLikelihood),
          translateX(landmark.position.x),
          translateY(landmark.position.y),
          whitePaint
        )
      }
    }
  }

  internal fun drawPoint(canvas: Canvas, landmark: PoseLandmark, paint: Paint) {
    val point = landmark.position3D
    updatePaintColorByZValue(
      paint,
      canvas,
      visualizeZ,
      rescaleZForVisualization,
      point.z,
      zMin,
      zMax
    )
    canvas.drawCircle(translateX(point.x), translateY(point.y), DOT_RADIUS, paint)
  }

  internal fun drawLine(
    canvas: Canvas,
    startLandmark: PoseLandmark?,
    endLandmark: PoseLandmark?,
    paint: Paint
  ) {
    val start = startLandmark!!.position3D
    val end = endLandmark!!.position3D

    // Gets average z for the current body line
    val avgZInImagePixel = (start.z + end.z) / 2
    updatePaintColorByZValue(
      paint,
      canvas,
      visualizeZ,
      rescaleZForVisualization,
      avgZInImagePixel,
      zMin,
      zMax
    )

    canvas.drawLine(
      translateX(start.x),
      translateY(start.y),
      translateX(end.x),
      translateY(end.y),
      paint
    )
  }

  companion object {

    private val DOT_RADIUS = 8.0f
    private val IN_FRAME_LIKELIHOOD_TEXT_SIZE = 30.0f
    private val STROKE_WIDTH = 10.0f
    private val POSE_CLASSIFICATION_TEXT_SIZE = 60.0f
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/posedetector/PoseGraphic.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/segmenter/SegmentationGraphic.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.segmenter

import android.graphics.Bitmap
import android.graphics.Canvas
import android.graphics.Color
import android.graphics.Matrix
import androidx.annotation.ColorInt
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.segmentation.SegmentationMask
import java.nio.ByteBuffer

/** Draw the mask from SegmentationResult in preview.  */
class SegmentationGraphic(overlay: GraphicOverlay, segmentationMask: SegmentationMask) :
  GraphicOverlay.Graphic(overlay) {
  private val mask: ByteBuffer
  private val maskWidth: Int
  private val maskHeight: Int
  private val isRawSizeMaskEnabled: Boolean
  private val scaleX: Float
  private val scaleY: Float

  /** Draws the segmented background on the supplied canvas.  */
  override fun draw(canvas: Canvas) {
    val bitmap = Bitmap.createBitmap(
      maskColorsFromByteBuffer(mask), maskWidth, maskHeight, Bitmap.Config.ARGB_8888
    )
    if (isRawSizeMaskEnabled) {
      val matrix = Matrix(getTransformationMatrix())
      matrix.preScale(scaleX, scaleY)
      canvas.drawBitmap(bitmap, matrix, null)
    } else {
      canvas.drawBitmap(bitmap, getTransformationMatrix(), null)
    }
    bitmap.recycle()
    // Reset byteBuffer pointer to beginning, so that the mask can be redrawn if screen is refreshed
    mask.rewind()
  }

  /** Converts byteBuffer floats to ColorInt array that can be used as a mask.  */
  @ColorInt
  private fun maskColorsFromByteBuffer(byteBuffer: ByteBuffer): IntArray {
    @ColorInt val colors =
      IntArray(maskWidth * maskHeight)
    for (i in 0 until maskWidth * maskHeight) {
      val backgroundLikelihood = 1 - byteBuffer.float
      if (backgroundLikelihood > 0.9) {
        colors[i] = Color.argb(128, 255, 0, 255)
      } else if (backgroundLikelihood > 0.2) {
        // Linear interpolation to make sure when backgroundLikelihood is 0.2, the alpha is 0 and
        // when backgroundLikelihood is 0.9, the alpha is 128.
        // +0.5 to round the float value to the nearest int.
        val alpha = (182.9 * backgroundLikelihood - 36.6 + 0.5).toInt()
        colors[i] = Color.argb(alpha, 255, 0, 255)
      }
    }
    return colors
  }

  init {
    mask = segmentationMask.buffer
    maskWidth = segmentationMask.width
    maskHeight = segmentationMask.height
    isRawSizeMaskEnabled =
      maskWidth != overlay.getImageWidth() || maskHeight != overlay.getImageHeight()
    scaleX = overlay.getImageWidth() * 1f / maskWidth
    scaleY = overlay.getImageHeight() * 1f / maskHeight
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/segmenter/SegmentationGraphic.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/segmenter/SegmenterProcessor.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.segmenter

import android.content.Context
import android.util.Log
import com.google.android.gms.tasks.Task
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.kotlin.VisionProcessorBase
import com.google.mlkit.vision.demo.preference.PreferenceUtils
import com.google.mlkit.vision.segmentation.Segmentation
import com.google.mlkit.vision.segmentation.SegmentationMask
import com.google.mlkit.vision.segmentation.Segmenter
import com.google.mlkit.vision.segmentation.selfie.SelfieSegmenterOptions

/** A processor to run Segmenter.  */
class SegmenterProcessor :
  VisionProcessorBase<SegmentationMask> {
  private val segmenter: Segmenter

  constructor(context: Context) : this(context, /* isStreamMode= */ true)

  constructor(
    context: Context,
    isStreamMode: Boolean
  ) : super(
    context
  ) {
    val optionsBuilder = SelfieSegmenterOptions.Builder()
    optionsBuilder.setDetectorMode(
      if(isStreamMode) SelfieSegmenterOptions.STREAM_MODE
      else SelfieSegmenterOptions.SINGLE_IMAGE_MODE
    )
    if (PreferenceUtils.shouldSegmentationEnableRawSizeMask(context)) {
      optionsBuilder.enableRawSizeMask()
    }

    val options = optionsBuilder.build()
    segmenter = Segmentation.getClient(options)
    Log.d(TAG, "SegmenterProcessor created with option: " + options)
  }

  override fun detectInImage(image: InputImage): Task<SegmentationMask> {
    return segmenter.process(image)
  }

  override fun onSuccess(
    segmentationMask: SegmentationMask,
    graphicOverlay: GraphicOverlay
  ) {
    graphicOverlay.add(
      SegmentationGraphic(
        graphicOverlay,
        segmentationMask
      )
    )
  }

  override fun onFailure(e: Exception) {
    Log.e(TAG, "Segmentation failed: $e")
  }

  companion object {
    private const val TAG = "SegmenterProcessor"
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/segmenter/SegmenterProcessor.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/subjectsegmenter/SubjectSegmentationGraphic.kt ---

/*
 * Copyright 2023 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.subjectsegmenter

import android.graphics.Bitmap
import android.graphics.Canvas
import android.graphics.Color
import android.graphics.Matrix
import android.os.Build
import androidx.annotation.ColorInt
import androidx.annotation.RequiresApi
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.segmentation.subject.Subject
import com.google.mlkit.vision.segmentation.subject.SubjectSegmentationResult
import java.nio.FloatBuffer
import kotlin.collections.List

/** Draw the mask from [SubjectSegmentationResult] in preview. */
@RequiresApi(Build.VERSION_CODES.N)
class SubjectSegmentationGraphic(
  overlay: GraphicOverlay,
  segmentationResult: SubjectSegmentationResult,
  imageWidth: Int,
  imageHeight: Int
) : GraphicOverlay.Graphic(overlay) {
  private val subjects: List<Subject>
  private val imageWidth: Int
  private val imageHeight: Int
  private val isRawSizeMaskEnabled: Boolean
  private val scaleX: Float
  private val scaleY: Float

  /** Draws the segmented background on the supplied canvas. */
  override fun draw(canvas: Canvas) {
    val bitmap =
      Bitmap.createBitmap(
        maskColorsFromFloatBuffer(),
        imageWidth,
        imageHeight,
        Bitmap.Config.ARGB_8888
      )
    if (isRawSizeMaskEnabled) {
      val matrix = Matrix(getTransformationMatrix())
      matrix.preScale(scaleX, scaleY)
      canvas.drawBitmap(bitmap, matrix, null)
    } else {
      canvas.drawBitmap(bitmap, getTransformationMatrix(), null)
    }
    bitmap.recycle()
  }

  /**
   * Converts [FloatBuffer] floats from all subjects to ColorInt array that can be used as a mask.
   */
  @ColorInt
  private fun maskColorsFromFloatBuffer(): IntArray {
    @ColorInt val colors = IntArray(imageWidth * imageHeight)
    for (k in 0 until subjects.size) {
      val subject = subjects[k]
      val rgb = COLORS[k % COLORS.size]
      val color = Color.argb(128, rgb[0], rgb[1], rgb[2])
      val mask = subject.confidenceMask
      for (j in 0 until subject.height) {
        for (i in 0 until subject.width) {
          if (mask!!.get() > 0.5) {
            colors[(subject.startY + j) * imageWidth + subject.startX + i] = color
          }
        }
      }
      // Reset [FloatBuffer] pointer to beginning, so that the mask can be redrawn if screen is
      // refreshed
      mask!!.rewind()
    }
    return colors
  }

  init {
    subjects = segmentationResult.subjects
    this.imageWidth = imageWidth
    this.imageHeight = imageHeight

    isRawSizeMaskEnabled =
      imageWidth != overlay.imageWidth || imageHeight != overlay.imageHeight
    scaleX = overlay.imageWidth * 1f / imageWidth
    scaleY = overlay.imageHeight * 1f / imageHeight
  }

  companion object {
    private val COLORS =
      arrayOf(
        intArrayOf(255, 0, 255),
        intArrayOf(0, 255, 255),
        intArrayOf(255, 255, 0),
        intArrayOf(255, 0, 0),
        intArrayOf(0, 255, 0),
        intArrayOf(0, 0, 255),
        intArrayOf(128, 0, 128),
        intArrayOf(0, 128, 128),
        intArrayOf(128, 128, 0),
        intArrayOf(128, 0, 0),
        intArrayOf(0, 128, 0),
        intArrayOf(0, 0, 128)
      )
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/subjectsegmenter/SubjectSegmentationGraphic.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/subjectsegmenter/SubjectSegmenterProcessor.kt ---

/*
 * Copyright 2023 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.subjectsegmenter

import android.content.Context
import android.os.Build
import android.util.Log
import androidx.annotation.RequiresApi
import com.google.android.gms.tasks.Task
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.kotlin.VisionProcessorBase
import com.google.mlkit.vision.segmentation.subject.SubjectSegmentation
import com.google.mlkit.vision.segmentation.subject.SubjectSegmentationResult
import com.google.mlkit.vision.segmentation.subject.SubjectSegmenter
import com.google.mlkit.vision.segmentation.subject.SubjectSegmenterOptions

/** A processor to run Subject Segmenter. */
@RequiresApi(Build.VERSION_CODES.N)
class SubjectSegmenterProcessor : VisionProcessorBase<SubjectSegmentationResult> {
  private val subjectSegmenter: SubjectSegmenter
  private var imageWidth: Int = 0
  private var imageHeight: Int = 0

  constructor(context: Context) : super(context) {
    subjectSegmenter =
      SubjectSegmentation.getClient(
        SubjectSegmenterOptions.Builder()
          .enableMultipleSubjects(
            SubjectSegmenterOptions.SubjectResultOptions.Builder().enableConfidenceMask().build()
          )
          .build()
      )

    Log.d(TAG, "SubjectSegmenterProcessor created")
  }

  override fun detectInImage(image: InputImage): Task<SubjectSegmentationResult> {
    this.imageWidth = image.width
    this.imageHeight = image.height
    return subjectSegmenter.process(image)
  }

  override fun onSuccess(
    results: SubjectSegmentationResult,
    graphicOverlay: GraphicOverlay
  ) {
    graphicOverlay.add(
      SubjectSegmentationGraphic(graphicOverlay, results, imageWidth, imageHeight)
    )
  }

  override fun onFailure(e: Exception) {
    Log.e(TAG, "Segmentation failed: $e")
  }

  companion object {
    private const val TAG = "SbjSegmenterProcessor"
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/subjectsegmenter/SubjectSegmenterProcessor.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/textdetector/TextGraphic.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.textdetector

import android.graphics.Canvas
import android.graphics.Color
import android.graphics.Paint
import android.graphics.RectF
import android.util.Log
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.GraphicOverlay.Graphic
import com.google.mlkit.vision.text.Text
import java.util.Arrays
import kotlin.math.max
import kotlin.math.min

/**
 * Graphic instance for rendering TextBlock position, size, and ID within an associated graphic
 * overlay view.
 */
class TextGraphic
constructor(
  overlay: GraphicOverlay?,
  private val text: Text,
  private val shouldGroupTextInBlocks: Boolean,
  private val showLanguageTag: Boolean,
  private val showConfidence: Boolean
) : Graphic(overlay) {

  private val rectPaint: Paint = Paint()
  private val textPaint: Paint
  private val labelPaint: Paint

  init {
    rectPaint.color = MARKER_COLOR
    rectPaint.style = Paint.Style.STROKE
    rectPaint.strokeWidth = STROKE_WIDTH
    textPaint = Paint()
    textPaint.color = TEXT_COLOR
    textPaint.textSize = TEXT_SIZE
    labelPaint = Paint()
    labelPaint.color = MARKER_COLOR
    labelPaint.style = Paint.Style.FILL
    // Redraw the overlay, as this graphic has been added.
    postInvalidate()
  }

  /** Draws the text block annotations for position, size, and raw value on the supplied canvas. */
  override fun draw(canvas: Canvas) {
    Log.d(TAG, "Text is: " + text.text)
    for (textBlock in text.textBlocks) { // Renders the text at the bottom of the box.
      Log.d(TAG, "TextBlock text is: " + textBlock.text)
      Log.d(TAG, "TextBlock boundingbox is: " + textBlock.boundingBox)
      Log.d(TAG, "TextBlock cornerpoint is: " + Arrays.toString(textBlock.cornerPoints))
      if (shouldGroupTextInBlocks) {
        drawText(
          getFormattedText(textBlock.text, textBlock.recognizedLanguage, confidence = null),
          RectF(textBlock.boundingBox),
          TEXT_SIZE * textBlock.lines.size + 2 * STROKE_WIDTH,
          canvas
        )
      } else {
        for (line in textBlock.lines) {
          Log.d(TAG, "Line text is: " + line.text)
          Log.d(TAG, "Line boundingbox is: " + line.boundingBox)
          Log.d(TAG, "Line cornerpoint is: " + Arrays.toString(line.cornerPoints))
          Log.d(TAG, "Line confidence is: " + line.confidence)
          Log.d(TAG, "Line angle is: " + line.angle)
          // Draws the bounding box around the TextBlock.
          val rect = RectF(line.boundingBox)
          drawText(
            getFormattedText(line.text, line.recognizedLanguage, line.confidence),
            rect,
            TEXT_SIZE + 2 * STROKE_WIDTH,
            canvas
          )
          for (element in line.elements) {
            Log.d(TAG, "Element text is: " + element.text)
            Log.d(TAG, "Element boundingbox is: " + element.boundingBox)
            Log.d(TAG, "Element cornerpoint is: " + Arrays.toString(element.cornerPoints))
            Log.d(TAG, "Element language is: " + element.recognizedLanguage)
            Log.d(TAG, "Element confidence is: " + element.confidence)
            Log.d(TAG, "Element angle is: " + element.angle)
            for (symbol in element.symbols) {
            Log.d(TAG, "Symbol text is: " + symbol.text)
            Log.d(TAG, "Symbol boundingbox is: " + symbol.boundingBox)
            Log.d(TAG, "Symbol cornerpoint is: " + Arrays.toString(symbol.cornerPoints))
            Log.d(TAG, "Symbol confidence is: " + symbol.confidence)
            Log.d(TAG, "Symbol angle is: " + symbol.angle)
          }
          }
        }
      }
    }
  }

  private fun getFormattedText(text: String, languageTag: String, confidence: Float?): String {
    val res =
      if (showLanguageTag) String.format(TEXT_WITH_LANGUAGE_TAG_FORMAT, languageTag, text) else text
    return if (showConfidence && confidence != null) String.format("%s (%.2f)", res, confidence)
    else res
  }

  private fun drawText(text: String, rect: RectF, textHeight: Float, canvas: Canvas) {
    // If the image is flipped, the left will be translated to right, and the right to left.
    val x0 = translateX(rect.left)
    val x1 = translateX(rect.right)
    rect.left = min(x0, x1)
    rect.right = max(x0, x1)
    rect.top = translateY(rect.top)
    rect.bottom = translateY(rect.bottom)
    canvas.drawRect(rect, rectPaint)
    val textWidth = textPaint.measureText(text)
    canvas.drawRect(
      rect.left - STROKE_WIDTH,
      rect.top - textHeight,
      rect.left + textWidth + 2 * STROKE_WIDTH,
      rect.top,
      labelPaint
    )
    // Renders the text at the bottom of the box.
    canvas.drawText(text, rect.left, rect.top - STROKE_WIDTH, textPaint)
  }

  companion object {
    private const val TAG = "TextGraphic"
    private const val TEXT_WITH_LANGUAGE_TAG_FORMAT = "%s:%s"
    private const val TEXT_COLOR = Color.BLACK
    private const val MARKER_COLOR = Color.WHITE
    private const val TEXT_SIZE = 54.0f
    private const val STROKE_WIDTH = 4.0f
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/textdetector/TextGraphic.kt ---


--- START OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/textdetector/TextRecognitionProcessor.kt ---

/*
 * Copyright 2020 Google LLC. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.mlkit.vision.demo.kotlin.textdetector

import android.content.Context
import android.util.Log
import com.google.android.gms.tasks.Task
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.demo.GraphicOverlay
import com.google.mlkit.vision.demo.kotlin.VisionProcessorBase
import com.google.mlkit.vision.demo.preference.PreferenceUtils
import com.google.mlkit.vision.text.Text
import com.google.mlkit.vision.text.TextRecognition
import com.google.mlkit.vision.text.TextRecognizer
import com.google.mlkit.vision.text.TextRecognizerOptionsInterface

/** Processor for the text detector demo. */
class TextRecognitionProcessor(
  private val context: Context,
  textRecognizerOptions: TextRecognizerOptionsInterface
) : VisionProcessorBase<Text>(context) {
  private val textRecognizer: TextRecognizer = TextRecognition.getClient(textRecognizerOptions)
  private val shouldGroupRecognizedTextInBlocks: Boolean =
    PreferenceUtils.shouldGroupRecognizedTextInBlocks(context)
  private val showLanguageTag: Boolean = PreferenceUtils.showLanguageTag(context)
  private val showConfidence: Boolean = PreferenceUtils.shouldShowTextConfidence(context)

  override fun stop() {
    super.stop()
    textRecognizer.close()
  }

  override fun detectInImage(image: InputImage): Task<Text> {
    return textRecognizer.process(image)
  }

  override fun onSuccess(text: Text, graphicOverlay: GraphicOverlay) {
    Log.d(TAG, "On-device Text detection successful")
    logExtrasForTesting(text)
    graphicOverlay.add(
      TextGraphic(
        graphicOverlay,
        text,
        shouldGroupRecognizedTextInBlocks,
        showLanguageTag,
        showConfidence
      )
    )
  }

  override fun onFailure(e: Exception) {
    Log.w(TAG, "Text detection failed.$e")
  }

  companion object {
    private const val TAG = "TextRecProcessor"
    private fun logExtrasForTesting(text: Text?) {
      if (text != null) {
        Log.v(MANUAL_TESTING_LOG, "Detected text has : " + text.textBlocks.size + " blocks")
        for (i in text.textBlocks.indices) {
          val lines = text.textBlocks[i].lines
          Log.v(
            MANUAL_TESTING_LOG,
            String.format("Detected text block %d has %d lines", i, lines.size)
          )
          for (j in lines.indices) {
            val elements = lines[j].elements
            Log.v(
              MANUAL_TESTING_LOG,
              String.format("Detected text line %d has %d elements", j, elements.size)
            )
            for (k in elements.indices) {
              val element = elements[k]
              Log.v(
                MANUAL_TESTING_LOG,
                String.format("Detected text element %d says: %s", k, element.text)
              )
              Log.v(
                MANUAL_TESTING_LOG,
                String.format(
                  "Detected text element %d has a bounding box: %s",
                  k,
                  element.boundingBox!!.flattenToString()
                )
              )
              Log.v(
                MANUAL_TESTING_LOG,
                String.format(
                  "Expected corner point size is 4, get %d",
                  element.cornerPoints!!.size
                )
              )
              for (point in element.cornerPoints!!) {
                Log.v(
                  MANUAL_TESTING_LOG,
                  String.format(
                    "Corner point for element %d is located at: x - %d, y = %d",
                    k,
                    point.x,
                    point.y
                  )
                )
              }
            }
          }
        }
      }
    }
  }
}


--- END OF FILE app/src/main/java/com/google/mlkit/vision/demo/kotlin/textdetector/TextRecognitionProcessor.kt ---


--- START OF FILE app/src/main/res/drawable-hdpi/ic_switch_camera_white_48dp.xml ---

<?xml version="1.0" encoding="utf-8"?>
<!-- This is an example InsetDrawable. It should be manually reviewed. -->
<inset xmlns:android="http://schemas.android.com/apk/res/android"
    android:drawable="@drawable/ic_switch_camera_white_48dp_inset"
    android:insetTop="3.333333492dp"
    android:insetLeft="3.333333492dp"
    android:insetBottom="7.333333492dp"
    android:insetRight="3.333333492dp"
    android:visible="true" />


--- END OF FILE app/src/main/res/drawable-hdpi/ic_switch_camera_white_48dp.xml ---


--- START OF FILE app/src/main/res/drawable-mdpi/ic_switch_camera_white_48dp.xml ---

<?xml version="1.0" encoding="utf-8"?>
<!-- This is an example InsetDrawable. It should be manually reviewed. -->
<inset xmlns:android="http://schemas.android.com/apk/res/android"
    android:drawable="@drawable/ic_switch_camera_white_48dp_inset"
    android:insetTop="3dp"
    android:insetLeft="3dp"
    android:insetBottom="7dp"
    android:insetRight="3dp"
    android:visible="true" />


--- END OF FILE app/src/main/res/drawable-mdpi/ic_switch_camera_white_48dp.xml ---


--- START OF FILE app/src/main/res/drawable-xhdpi/ic_switch_camera_white_48dp.xml ---

<?xml version="1.0" encoding="utf-8"?>
<!-- This is an example InsetDrawable. It should be manually reviewed. -->
<inset xmlns:android="http://schemas.android.com/apk/res/android"
    android:drawable="@drawable/ic_switch_camera_white_48dp_inset"
    android:insetTop="3.5dp"
    android:insetLeft="3.5dp"
    android:insetBottom="7.5dp"
    android:insetRight="3.5dp"
    android:visible="true" />


--- END OF FILE app/src/main/res/drawable-xhdpi/ic_switch_camera_white_48dp.xml ---


--- START OF FILE app/src/main/res/drawable-xxhdpi/ic_switch_camera_white_48dp.xml ---

<?xml version="1.0" encoding="utf-8"?>
<!-- This is an example InsetDrawable. It should be manually reviewed. -->
<inset xmlns:android="http://schemas.android.com/apk/res/android"
    android:drawable="@drawable/ic_switch_camera_white_48dp_inset"
    android:insetTop="3.666666746dp"
    android:insetLeft="3.666666746dp"
    android:insetBottom="7.666666985dp"
    android:insetRight="3.666666746dp"
    android:visible="true" />


--- END OF FILE app/src/main/res/drawable-xxhdpi/ic_switch_camera_white_48dp.xml ---


--- START OF FILE app/src/main/res/drawable-xxxhdpi/ic_switch_camera_white_48dp.xml ---

<?xml version="1.0" encoding="utf-8"?>
<!-- This is an example InsetDrawable. It should be manually reviewed. -->
<inset xmlns:android="http://schemas.android.com/apk/res/android"
    android:drawable="@drawable/ic_switch_camera_white_48dp_inset"
    android:insetTop="3.75dp"
    android:insetLeft="3.75dp"
    android:insetBottom="7.75dp"
    android:insetRight="3.75dp"
    android:visible="true" />


--- END OF FILE app/src/main/res/drawable-xxxhdpi/ic_switch_camera_white_48dp.xml ---


--- START OF FILE app/src/main/res/drawable/list_item_background.xml ---

<?xml version="1.0" encoding="utf-8"?>
<shape xmlns:android="http://schemas.android.com/apk/res/android"
    android:shape="rectangle" >
    <stroke android:width="3dip" android:color="@color/blue"/>
    <corners android:bottomRightRadius="16dp"
        android:bottomLeftRadius="16dp"
        android:topLeftRadius="16dp"
        android:topRightRadius="16dp" />
</shape>


--- END OF FILE app/src/main/res/drawable/list_item_background.xml ---


--- START OF FILE app/src/main/res/drawable/logo_mlkit.xml ---

<?xml version="1.0" encoding="utf-8"?>
<vector xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:tools="http://schemas.android.com/tools"
    tools:ignore="NewApi"
    android:width="172dp"
    android:height="129dp"
    android:viewportWidth="172.43"
    android:viewportHeight="129.06">
    <path
        android:fillColor="#4285f4"
        android:pathData="M1.102,114.948l57.68,-109.632l16.815,8.847l-57.68,109.632z" />
    <path
        android:fillColor="#0d47a1"
        android:pathData="M9.5,119.43m-9.5,0a9.5,9.5 0,1 1,19 0a9.5,9.5 0,1 1,-19 0" />
    <path
        android:fillColor="#abccfc"
        android:pathData="M57.69,9.76h19v109.85h-19z" />
    <path
        android:fillColor="#0d47a1"
        android:pathData="M67.19,9.76m-9.5,0a9.5,9.5 0,1 1,19 0a9.5,9.5 0,1 1,-19 0" />
    <path
        android:fillColor="#4285f4"
        android:pathData="M58.779,114.962l57.68,-109.632l16.815,8.847l-57.68,109.632z" />
    <path
        android:fillColor="#abccfc"
        android:pathData="M115.38,9.76h19v109.85h-19z" />
    <path
        android:fillColor="#4285f4"
        android:pathData="M124.88,109.93h38.39v19h-38.39z" />
    <path
        android:fillColor="#0d47a1"
        android:pathData="M124.88,119.43m-9.5,0a9.5,9.5 0,1 1,19 0a9.5,9.5 0,1 1,-19 0" />
    <path
        android:fillColor="#0d47a1"
        android:pathData="M163.26,119.43m-9.5,0a9.5,9.5 0,1 1,19 0a9.5,9.5 0,1 1,-19 0" />
    <path
        android:fillColor="#0d47a1"
        android:pathData="M124.88,9.76m-9.5,0a9.5,9.5 0,1 1,19 0a9.5,9.5 0,1 1,-19 0" />
    <path
        android:fillColor="#0d47a1"
        android:pathData="M67.19,119.43m-9.5,0a9.5,9.5 0,1 1,19 0a9.5,9.5 0,1 1,-19 0" />
</vector>


--- END OF FILE app/src/main/res/drawable/logo_mlkit.xml ---


--- START OF FILE app/src/main/res/layout-land/activity_vision_camerax_live_preview.xml ---

<?xml version="1.0" encoding="utf-8"?>
<androidx.constraintlayout.widget.ConstraintLayout
    xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:app="http://schemas.android.com/apk/res-auto"
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    android:keepScreenOn="true">

  <androidx.camera.view.PreviewView
      android:id="@+id/preview_view"
      android:layout_width="0dp"
      android:layout_height="match_parent"
      app:layout_constraintStart_toStartOf="parent"
      app:layout_constraintEnd_toStartOf="@+id/control"/>

  <com.google.mlkit.vision.demo.GraphicOverlay
      android:id="@+id/graphic_overlay"
      android:layout_width="0dp"
      android:layout_height="match_parent"
      app:layout_constraintStart_toStartOf="@id/preview_view"
      app:layout_constraintEnd_toEndOf="@id/preview_view" />

  <FrameLayout
      android:id="@id/control"
      android:layout_width="220dp"
      android:layout_height="match_parent"
      app:layout_constraintStart_toEndOf="@id/preview_view"
      app:layout_constraintEnd_toEndOf="parent"
      android:background="#000">

    <Spinner
        android:id="@+id/spinner"
        android:layout_width="match_parent"
        android:layout_height="wrap_content"
        android:layout_gravity="top"/>

    <ToggleButton
        android:id="@+id/facing_switch"
        android:layout_width="48dp"
        android:layout_height="48dp"
        android:layout_gravity="bottom|start"
        android:background="@layout/toggle_style"
        android:checked="false"
        android:textOff=""
        android:textOn=""/>

    <ImageView
        android:id="@+id/settings_button"
        android:layout_width="wrap_content"
        android:layout_height="wrap_content"
        android:layout_gravity="bottom|end"
        android:padding="12dp"
        android:contentDescription="@string/menu_item_settings"
        android:src="@drawable/ic_settings_white_24dp"/>
  </FrameLayout>

</androidx.constraintlayout.widget.ConstraintLayout>


--- END OF FILE app/src/main/res/layout-land/activity_vision_camerax_live_preview.xml ---


--- START OF FILE app/src/main/res/layout-land/activity_vision_cameraxsource_demo.xml ---

<?xml version="1.0" encoding="utf-8"?>
<androidx.constraintlayout.widget.ConstraintLayout
    xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:app="http://schemas.android.com/apk/res-auto"
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    android:keepScreenOn="true">

  <androidx.camera.view.PreviewView
      android:id="@+id/preview_view"
      android:layout_width="0dp"
      android:layout_height="match_parent"
      app:layout_constraintStart_toStartOf="parent"
      app:layout_constraintEnd_toStartOf="@+id/control"/>

  <com.google.mlkit.vision.demo.GraphicOverlay
      android:id="@+id/graphic_overlay"
      android:layout_width="0dp"
      android:layout_height="match_parent"
      app:layout_constraintStart_toStartOf="@id/preview_view"
      app:layout_constraintEnd_toEndOf="@id/preview_view" />

  <FrameLayout
      android:id="@id/control"
      android:layout_width="220dp"
      android:layout_height="match_parent"
      app:layout_constraintStart_toEndOf="@id/preview_view"
      app:layout_constraintEnd_toEndOf="parent"
      android:background="#000">

    <TextView
        android:id="@+id/detector"
        android:layout_width="match_parent"
        android:layout_height="wrap_content"
        android:layout_gravity="top"
        android:gravity="center"
        android:maxLines="2"
        android:textStyle="bold"
        android:textColor="#FFF"
        android:textSize="16sp"
        android:text="@string/custom_object_detection"/>

    <ToggleButton
        android:id="@+id/facing_switch"
        android:layout_width="48dp"
        android:layout_height="48dp"
        android:layout_gravity="bottom|start"
        android:background="@layout/toggle_style"
        android:checked="false"
        android:textOff=""
        android:textOn=""/>

    <ImageView
        android:id="@+id/settings_button"
        android:layout_width="wrap_content"
        android:layout_height="wrap_content"
        android:layout_gravity="bottom|end"
        android:padding="12dp"
        android:contentDescription="@string/menu_item_settings"
        android:src="@drawable/ic_settings_white_24dp"/>
  </FrameLayout>

</androidx.constraintlayout.widget.ConstraintLayout>


--- END OF FILE app/src/main/res/layout-land/activity_vision_cameraxsource_demo.xml ---


--- START OF FILE app/src/main/res/layout-land/activity_vision_live_preview.xml ---

<?xml version="1.0" encoding="utf-8"?>
<androidx.constraintlayout.widget.ConstraintLayout
    xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:app="http://schemas.android.com/apk/res-auto"
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    android:keepScreenOn="true">

  <com.google.mlkit.vision.demo.CameraSourcePreview
      android:id="@+id/preview_view"
      android:layout_width="0dp"
      android:layout_height="match_parent"
      app:layout_constraintStart_toStartOf="parent"
      app:layout_constraintEnd_toStartOf="@+id/control"/>

  <com.google.mlkit.vision.demo.GraphicOverlay
      android:id="@+id/graphic_overlay"
      android:layout_width="0dp"
      android:layout_height="match_parent"
      app:layout_constraintStart_toStartOf="@id/preview_view"
      app:layout_constraintEnd_toEndOf="@id/preview_view" />

  <FrameLayout
      android:id="@id/control"
      android:layout_width="220dp"
      android:layout_height="match_parent"
      app:layout_constraintEnd_toEndOf="parent"
      android:background="#000">

    <Spinner
        android:id="@+id/spinner"
        android:layout_width="match_parent"
        android:layout_height="wrap_content"
        android:layout_gravity="top"/>

    <ToggleButton
        android:id="@+id/facing_switch"
        android:layout_width="48dp"
        android:layout_height="48dp"
        android:layout_gravity="bottom|start"
        android:background="@layout/toggle_style"
        android:checked="false"
        android:textOff=""
        android:textOn=""/>

    <ImageView
        android:id="@+id/settings_button"
        android:layout_width="wrap_content"
        android:layout_height="wrap_content"
        android:layout_gravity="bottom|end"
        android:padding="12dp"
        android:contentDescription="@string/menu_item_settings"
        android:src="@drawable/ic_settings_white_24dp"/>
  </FrameLayout>

</androidx.constraintlayout.widget.ConstraintLayout>


--- END OF FILE app/src/main/res/layout-land/activity_vision_live_preview.xml ---


--- START OF FILE app/src/main/res/layout/activity_chooser.xml ---

<?xml version="1.0" encoding="utf-8"?>
<LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:app="http://schemas.android.com/apk/res-auto"
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    android:paddingTop="@dimen/activity_vertical_margin"
    android:paddingBottom="@dimen/activity_vertical_margin"
    android:paddingLeft="@dimen/activity_horizontal_margin"
    android:paddingRight="@dimen/activity_horizontal_margin"
    android:orientation="vertical">

  <ImageView
      android:id="@+id/imageView"
      android:contentDescription="@string/app_name"
      android:layout_width="match_parent"
      android:layout_height="50dp"
      android:layout_marginTop="32dp"
      app:srcCompat="@drawable/logo_mlkit" />

  <TextView
      android:layout_width="match_parent"
      android:layout_height="wrap_content"
      android:paddingTop="32dp"
      android:paddingBottom="32dp"
      android:fontFamily="google-sans"
      android:gravity="center_horizontal"
      android:text="@string/app_name"
      android:textColor="@color/white"
      android:textSize="18sp"/>

  <ListView
      android:id="@+id/test_activity_list_view"
      android:layout_width="match_parent"
      android:layout_height="match_parent"/>
</LinearLayout>


--- END OF FILE app/src/main/res/layout/activity_chooser.xml ---


--- START OF FILE app/src/main/res/layout/activity_settings.xml ---

<?xml version="1.0" encoding="utf-8"?>
<LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"
    android:id="@+id/settings_container"
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    android:orientation="vertical">

</LinearLayout>


--- END OF FILE app/src/main/res/layout/activity_settings.xml ---


--- START OF FILE app/src/main/res/layout/activity_still_image.xml ---

<?xml version="1.0" encoding="utf-8"?>
<androidx.constraintlayout.widget.ConstraintLayout
    xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:app="http://schemas.android.com/apk/res-auto"
    android:id="@+id/root"
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    android:keepScreenOn="true">

  <ImageView
      android:id="@+id/preview"
      android:layout_width="wrap_content"
      android:layout_height="wrap_content"
      android:adjustViewBounds="true"
      app:layout_constraintBottom_toTopOf="@+id/control"
      app:layout_constraintEnd_toEndOf="parent"
      app:layout_constraintStart_toStartOf="parent"
      app:layout_constraintTop_toTopOf="parent" />

  <com.google.mlkit.vision.demo.GraphicOverlay
      android:id="@+id/graphic_overlay"
      android:layout_width="0dp"
      android:layout_height="0dp"
      app:layout_constraintLeft_toLeftOf="@id/preview"
      app:layout_constraintRight_toRightOf="@id/preview"
      app:layout_constraintTop_toTopOf="@id/preview"
      app:layout_constraintBottom_toBottomOf="@id/preview"/>

  <LinearLayout
      android:id="@id/control"
      android:layout_width="match_parent"
      android:layout_height="wrap_content"
      app:layout_constraintBottom_toBottomOf="parent"
      android:background="#000"
      android:orientation="vertical">

    <Button
        android:id="@+id/select_image_button"
        android:layout_width="wrap_content"
        android:layout_height="wrap_content"
        android:layout_gravity="center"
        android:layout_margin="12dp"
        android:text="@string/select_image"/>

    <LinearLayout
        android:id="@+id/control2"
        android:layout_width="match_parent"
        android:layout_height="60dp"
        app:layout_constraintBottom_toBottomOf="parent"
        android:background="#000"
        android:orientation="horizontal">

      <Spinner
          android:id="@+id/size_selector"
          android:layout_width="0dp"
          android:layout_weight="1"
          android:layout_height="wrap_content"
          android:layout_gravity="center"/>

      <Spinner
          android:id="@+id/feature_selector"
          android:layout_width="0dp"
          android:layout_weight="1"
          android:layout_height="wrap_content"
          android:layout_gravity="center"/>
    </LinearLayout>
  </LinearLayout>

  <include
      layout="@layout/settings_style"
      android:id="@+id/settings_button"
      android:layout_width="wrap_content"
      android:layout_height="wrap_content"
      app:layout_constraintRight_toRightOf="@id/root"
      app:layout_constraintTop_toTopOf="@id/root"/>

</androidx.constraintlayout.widget.ConstraintLayout>


--- END OF FILE app/src/main/res/layout/activity_still_image.xml ---


--- START OF FILE app/src/main/res/layout/activity_vision_camerax_live_preview.xml ---

<?xml version="1.0" encoding="utf-8"?>
<androidx.constraintlayout.widget.ConstraintLayout
    xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:app="http://schemas.android.com/apk/res-auto"
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    android:keepScreenOn="true">

  <androidx.camera.view.PreviewView
      android:id="@+id/preview_view"
      android:layout_width="match_parent"
      android:layout_height="0dp"
      app:layout_constraintTop_toTopOf="parent"
      app:layout_constraintBottom_toTopOf="@+id/control"/>

  <com.google.mlkit.vision.demo.GraphicOverlay
      android:id="@+id/graphic_overlay"
      android:layout_width="0dp"
      android:layout_height="0dp"
      app:layout_constraintLeft_toLeftOf="@id/preview_view"
      app:layout_constraintRight_toRightOf="@id/preview_view"
      app:layout_constraintTop_toTopOf="@id/preview_view"
      app:layout_constraintBottom_toBottomOf="@id/preview_view"/>

  <include
      android:id="@+id/settings_button"
      layout="@layout/settings_style"
      android:layout_width="wrap_content"
      android:layout_height="wrap_content"
      app:layout_constraintRight_toRightOf="@id/preview_view"
      app:layout_constraintTop_toTopOf="@id/preview_view" />

  <LinearLayout
      android:id="@id/control"
      android:layout_width="match_parent"
      android:layout_height="60dp"
      app:layout_constraintBottom_toBottomOf="parent"
      android:background="#000"
      android:orientation="horizontal">

    <ToggleButton
        android:id="@+id/facing_switch"
        android:layout_width="48dp"
        android:layout_height="48dp"
        android:layout_gravity="center_vertical"
        android:background="@layout/toggle_style"
        android:checked="false"
        android:textOff=""
        android:textOn=""/>

    <Spinner
        android:id="@+id/spinner"
        android:layout_width="0dp"
        android:layout_weight="1"
        android:layout_height="wrap_content"
        android:layout_gravity="center"/>

  </LinearLayout>

</androidx.constraintlayout.widget.ConstraintLayout>


--- END OF FILE app/src/main/res/layout/activity_vision_camerax_live_preview.xml ---


--- START OF FILE app/src/main/res/layout/activity_vision_cameraxsource_demo.xml ---

<?xml version="1.0" encoding="utf-8"?>
<androidx.constraintlayout.widget.ConstraintLayout
    xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:app="http://schemas.android.com/apk/res-auto"
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    android:keepScreenOn="true">

  <androidx.camera.view.PreviewView
      android:id="@+id/preview_view"
      android:layout_width="match_parent"
      android:layout_height="0dp"
      app:layout_constraintTop_toTopOf="parent"
      app:layout_constraintBottom_toTopOf="@+id/control"/>

  <com.google.mlkit.vision.demo.GraphicOverlay
      android:id="@+id/graphic_overlay"
      android:layout_width="0dp"
      android:layout_height="0dp"
      app:layout_constraintLeft_toLeftOf="@id/preview_view"
      app:layout_constraintRight_toRightOf="@id/preview_view"
      app:layout_constraintTop_toTopOf="@id/preview_view"
      app:layout_constraintBottom_toBottomOf="@id/preview_view"/>

  <include
      android:id="@+id/settings_button"
      layout="@layout/settings_style"
      android:layout_width="wrap_content"
      android:layout_height="wrap_content"
      app:layout_constraintRight_toRightOf="@id/preview_view"
      app:layout_constraintTop_toTopOf="@id/preview_view" />

  <LinearLayout
      android:id="@id/control"
      android:layout_width="match_parent"
      android:layout_height="60dp"
      app:layout_constraintBottom_toBottomOf="parent"
      android:background="#000"
      android:orientation="horizontal">

    <ToggleButton
        android:id="@+id/facing_switch"
        android:layout_width="48dp"
        android:layout_height="48dp"
        android:layout_gravity="center_vertical"
        android:background="@layout/toggle_style"
        android:checked="false"
        android:textOff=""
        android:textOn=""/>

    <TextView
        android:id="@+id/detector"
        android:layout_width="0dp"
        android:layout_weight="1"
        android:layout_height="wrap_content"
        android:layout_gravity="center"
        android:gravity="center"
        android:maxLines="1"
        android:textStyle="bold"
        android:textColor="#FFF"
        android:textSize="16sp"
        android:text="@string/custom_object_detection"/>
  </LinearLayout>

</androidx.constraintlayout.widget.ConstraintLayout>


--- END OF FILE app/src/main/res/layout/activity_vision_cameraxsource_demo.xml ---


--- START OF FILE app/src/main/res/layout/activity_vision_entry_choice.xml ---

<?xml version="1.0" encoding="utf-8"?>
<LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"
    android:layout_width="wrap_content"
    android:layout_height="wrap_content"
    android:layout_gravity="center"
    android:paddingTop="@dimen/activity_vertical_margin"
    android:paddingBottom="@dimen/activity_vertical_margin"
    android:paddingLeft="@dimen/activity_horizontal_margin"
    android:paddingRight="@dimen/activity_horizontal_margin"
    android:orientation="vertical">

    <TextView
        android:background="@drawable/list_item_background"
        android:id="@+id/java_entry_point"
        android:layout_width="wrap_content"
        android:layout_height="wrap_content"
        android:layout_margin="@dimen/activity_vertical_margin"
        android:padding="20dp"
        android:text="@string/java_entry_title"
        android:textSize="26sp" />

    <TextView
        android:background="@drawable/list_item_background"
        android:id="@+id/kotlin_entry_point"
        android:layout_width="wrap_content"
        android:layout_height="wrap_content"
        android:layout_margin="@dimen/activity_vertical_margin"
        android:padding="20dp"
        android:text="@string/kotlin_entry_title"
        android:textSize="26sp" />
</LinearLayout>


--- END OF FILE app/src/main/res/layout/activity_vision_entry_choice.xml ---


--- START OF FILE app/src/main/res/layout/activity_vision_live_preview.xml ---

<?xml version="1.0" encoding="utf-8"?>
<androidx.constraintlayout.widget.ConstraintLayout
    xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:app="http://schemas.android.com/apk/res-auto"
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    android:keepScreenOn="true">

  <com.google.mlkit.vision.demo.CameraSourcePreview
      android:id="@+id/preview_view"
      android:layout_width="match_parent"
      android:layout_height="0dp"
      app:layout_constraintTop_toTopOf="parent"
      app:layout_constraintBottom_toTopOf="@+id/control"/>

  <com.google.mlkit.vision.demo.GraphicOverlay
      android:id="@+id/graphic_overlay"
      android:layout_width="0dp"
      android:layout_height="0dp"
      app:layout_constraintLeft_toLeftOf="@id/preview_view"
      app:layout_constraintRight_toRightOf="@id/preview_view"
      app:layout_constraintTop_toTopOf="@id/preview_view"
      app:layout_constraintBottom_toBottomOf="@id/preview_view"/>

  <include
      android:id="@+id/settings_button"
      layout="@layout/settings_style"
      android:layout_width="wrap_content"
      android:layout_height="wrap_content"
      app:layout_constraintRight_toRightOf="@id/preview_view"
      app:layout_constraintTop_toTopOf="@id/preview_view" />

  <LinearLayout
      android:id="@id/control"
      android:layout_width="match_parent"
      android:layout_height="60dp"
      app:layout_constraintBottom_toBottomOf="parent"
      android:background="#000"
      android:orientation="horizontal">

    <ToggleButton
        android:id="@+id/facing_switch"
        android:layout_width="48dp"
        android:layout_height="48dp"
        android:layout_gravity="center_vertical"
        android:background="@layout/toggle_style"
        android:checked="false"
        android:textOff=""
        android:textOn=""/>

    <Spinner
        android:id="@+id/spinner"
        android:layout_width="0dp"
        android:layout_weight="1"
        android:layout_height="wrap_content"
        android:layout_gravity="center"/>

  </LinearLayout>

</androidx.constraintlayout.widget.ConstraintLayout>


--- END OF FILE app/src/main/res/layout/activity_vision_live_preview.xml ---


--- START OF FILE app/src/main/res/layout/settings_style.xml ---

<?xml version="1.0" encoding="utf-8"?>
<ImageView
    xmlns:android="http://schemas.android.com/apk/res/android"
    android:layout_width="wrap_content"
    android:layout_height="wrap_content"
    android:padding="12dp"
    android:contentDescription="@string/menu_item_settings"
    android:src="@drawable/ic_settings_white_24dp"/>


--- END OF FILE app/src/main/res/layout/settings_style.xml ---


--- START OF FILE app/src/main/res/layout/spinner_style.xml ---

<?xml version="1.0" encoding="utf-8"?>
<TextView
    xmlns:android="http://schemas.android.com/apk/res/android"
    android:textStyle="bold"
    android:layout_width="match_parent"
    android:layout_height="wrap_content"
    android:padding="1dip"
    android:gravity="center"
    android:textColor="#FFF"
    android:textSize="16sp"/>


--- END OF FILE app/src/main/res/layout/spinner_style.xml ---


--- START OF FILE app/src/main/res/layout/toggle_style.xml ---

<?xml version="1.0" encoding="utf-8"?>
<selector xmlns:android="http://schemas.android.com/apk/res/android">
  <item
      android:drawable="@drawable/ic_switch_camera_white_48dp"
      android:state_checked="true"/>
  <item
      android:drawable="@drawable/ic_switch_camera_white_48dp"
      android:state_checked="false"/>
</selector>


--- END OF FILE app/src/main/res/layout/toggle_style.xml ---


--- START OF FILE app/src/main/res/menu/camera_button_menu.xml ---

<?xml version="1.0" encoding="utf-8"?>
<menu xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:app="http://schemas.android.com/apk/res-auto">
  <item android:id="@+id/select_images_from_local"
      android:title="Select image from album"
      android:orderInCategory="100"
      app:showAsAction="never" />
  <item android:id="@+id/take_photo_using_camera"
      android:title="Take photo"
      android:orderInCategory="100"
      app:showAsAction="never" />
</menu>


--- END OF FILE app/src/main/res/menu/camera_button_menu.xml ---


--- START OF FILE app/src/main/res/values/arrays.xml ---

<?xml version="1.0" encoding="utf-8"?>
<resources>

  <string-array name="pref_entries_face_detector_landmark_mode">
    <item>@string/pref_entries_face_detector_landmark_mode_no_landmarks</item>
    <item>@string/pref_entries_face_detector_landmark_mode_all_landmarks</item>
  </string-array>

  <string-array name="pref_entry_values_face_detector_landmark_mode">
    <item>@string/pref_entry_values_face_detector_landmark_mode_no_landmarks</item>
    <item>@string/pref_entry_values_face_detector_landmark_mode_all_landmarks</item>
  </string-array>

  <string-array name="pref_entries_face_detector_contour_mode">
    <item>@string/pref_entries_face_detector_contour_mode_no_contours</item>
    <item>@string/pref_entries_face_detector_contour_mode_all_contours</item>
  </string-array>

  <string-array name="pref_entry_values_face_detector_contour_mode">
    <item>@string/pref_entry_values_face_detector_contour_mode_no_contours</item>
    <item>@string/pref_entry_values_face_detector_contour_mode_all_contours</item>
  </string-array>

  <string-array name="pref_entries_face_detector_classification_mode">
    <item>@string/pref_entries_face_detector_classification_mode_no_classifications</item>
    <item>@string/pref_entries_face_detector_classification_mode_all_classifications</item>
  </string-array>

  <string-array name="pref_entry_values_face_detector_classification_mode">
    <item>@string/pref_entry_values_face_detector_classification_mode_no_classifications</item>
    <item>@string/pref_entry_values_face_detector_classification_mode_all_classifications</item>
  </string-array>

  <string-array name="pref_entries_face_detector_performance_mode">
    <item>@string/pref_entries_face_detector_performance_mode_fast</item>
    <item>@string/pref_entries_face_detector_performance_mode_accurate</item>
  </string-array>

  <string-array name="pref_entry_values_face_detector_performance_mode">
    <item>@string/pref_entry_values_face_detector_performance_mode_fast</item>
    <item>@string/pref_entry_values_face_detector_performance_mode_accurate</item>
  </string-array>

  <string-array name="pref_entries_values_pose_detector_performance_mode">
    <item>@string/pref_entries_pose_detector_performance_mode_fast</item>
    <item>@string/pref_entries_pose_detector_performance_mode_accurate</item>
  </string-array>

  <string-array name="pref_entry_values_pose_detector_performance_mode">
    <item>@string/pref_entry_values_pose_detector_performance_mode_fast</item>
    <item>@string/pref_entry_values_pose_detector_performance_mode_accurate</item>
  </string-array>

  <string-array name="pref_entry_titles_face_mesh_use_case">
    <item>Bounding Box Only</item>
    <item>Face Mesh</item>
    <item>Face Mesh(Simplified Contours only)</item>
  </string-array>

  <string-array name="pref_entry_values_face_mesh_use_case">
    <item>0</item>
    <item>1</item>
    <item>999</item>
  </string-array>
</resources>


--- END OF FILE app/src/main/res/values/arrays.xml ---


--- START OF FILE app/src/main/res/values/colors.xml ---

<?xml version="1.0" encoding="utf-8"?>
<resources>
    <color name="colorPrimary">#4CAF50</color>
    <color name="colorPrimaryDark">#388E3C</color>
    <color name="colorAccent">#7C4DFF</color>

    <color name="blue_grey_400">#78909C</color>
    <color name="light_grey_400">#E6E6E6</color>
    <color name="light_green_700">#689F38</color>
    <color name="gray">#BFBFBF</color>
    <color name="white">#FFFFFF</color>
    <color name="blue">#4286f4</color>
    <color name="red">#f44242</color>
</resources>


--- END OF FILE app/src/main/res/values/colors.xml ---


--- START OF FILE app/src/main/res/values/dimens.xml ---

<resources>
    <!-- Default screen margins, per the Android Design guidelines. -->
    <dimen name="activity_horizontal_margin">16dp</dimen>
    <dimen name="activity_vertical_margin">16dp</dimen>
    <dimen name="padding_standard">10dp</dimen>
</resources>


--- END OF FILE app/src/main/res/values/dimens.xml ---


--- START OF FILE app/src/main/res/values/strings.xml ---

<?xml version="1.0" encoding="utf-8"?>
<resources>
    <string name="app_name" translatable="false">MLKit-Vision</string>
    <string name="java_entry_title" translatable="false">Run the ML Kit quickstart written in Java</string>
    <string name="kotlin_entry_title" translatable="false">Run the ML Kit quickstart written in Kotlin</string>
    <string name="ok" translatable="false">OK</string>
    <string name="permission_camera_rationale" translatable="false">Access to the camera is needed for detection</string>
    <string name="no_camera_permission" translatable="false">This application cannot run because it does not have the camera permission.  The application will now exit.</string>
    <string name="low_storage_error" translatable="false">Face detector dependencies cannot be downloaded due to low device storage</string>
    <string name="toggle_turn_on" translatable="false">Front</string>
    <string name="toggle_turn_off" translatable="false">Back</string>
    <string name="desc_camera_source_activity" translatable="false">Vision detectors demo with live camera preview</string>
    <string name="desc_still_image_activity" translatable="false">Vision detectors demo with a still image</string>
    <string name="desc_camerax_live_preview_activity" translatable="false">Vision detectors demo with live preview using CameraX. Note that CameraX is only supported on API 21+</string>
    <string name="desc_cameraxsource_demo_activity" translatable="false">Object detection with custom classifier using ML Kit CameraXSource API. Note that CameraX is only supported on API 21+</string>
    <string name="download_error" translatable="false">Download error</string>
    <string name="start_over" translatable="false">Start over</string>
    <string name="menu_item_settings" translatable="false">Settings</string>
    <string name="select_image" translatable="false">Select image</string>
    <string name="custom_object_detection" translatable="false">Custom Object Detector</string>

    <!-- Settings related strings. -->
    <string name="pref_screen" translatable="false">ps</string>
    <string name="pref_screen_title_live_preview" translatable="false">Live preview settings</string>
    <string name="pref_screen_title_still_image" translatable="false">Still image settings</string>
    <string name="pref_screen_title_camerax_live_preview" translatable="false">CameraX live preview settings</string>
    <string name="pref_screen_title_cameraxsource_demo" translatable="false">CameraXSource live preview settings</string>
    <string name="pref_category_info" translatable="false">Detection Info</string>
    <string name="pref_category_face_detection" translatable="false">Face Detection</string>
    <string name="pref_category_object_detection" translatable="false">Object Detection / Custom Object Detection</string>
    <string name="pref_category_automl" translatable="false">AutoML Image Labeling</string>
    <string name="pref_category_pose_detection" translatable="false">Pose Detection</string>
    <string name="pref_category_segmentation" translatable="false">Selfie Segmentation</string>
    <string name="pref_category_text_recognition" translatable="false">Text Recognition</string>
    <string name="pref_category_barcode_scanning" translatable="false">Barcode Scanning</string>

    <!-- Strings for camera settings. -->
    <string name="pref_category_key_camera" translatable="false">pckc</string>
    <string name="pref_category_title_camera" translatable="false">Camera</string>
    <string name="pref_key_rear_camera_preview_size" translatable="false">rcpvs</string>
    <string name="pref_key_rear_camera_picture_size" translatable="false">rcpts</string>
    <string name="pref_key_front_camera_preview_size" translatable="false">fcpvs</string>
    <string name="pref_key_front_camera_picture_size" translatable="false">fcpts</string>
    <string name="pref_key_camerax_rear_camera_target_resolution" translatable="false">crctas</string>
    <string name="pref_key_camerax_front_camera_target_resolution" translatable="false">cfctas</string>
    <string name="pref_key_camera_live_viewport" translatable="false">clv</string>
    <string name="pref_title_rear_camera_preview_size" translatable="false">Rear camera preview size</string>
    <string name="pref_title_front_camera_preview_size" translatable="false">Front camera preview size</string>
    <string name="pref_title_camerax_rear_camera_target_resolution" translatable="false">CameraX rear camera target resolution</string>
    <string name="pref_title_camerax_front_camera_target_resolution" translatable="false">CameraX front camera target resolution</string>
    <string name="pref_title_camera_live_viewport" translatable="false">Enable live viewport</string>
    <string name="pref_summary_camera_live_viewport" translatable="false">Do not block camera preview drawing on detection</string>

    <!-- Strings for info preference. -->
    <string name="pref_title_info_hide" translatable="false">Hide detection info</string>
    <string name="pref_key_info_hide" translatable="false">ih</string>

    <!-- Strings for barcode scanning preference. -->
    <string name="pref_key_enable_auto_zoom" translatable="false">eaz</string>
    <string name="pref_title_enable_auto_zoom" translatable="false">Enable auto zoom</string>

    <!-- Strings for text recognition preference. -->
    <string name="pref_title_group_recognized_text_in_blocks" translatable="false">Group recognized text in paragraphs</string>
    <string name="pref_key_group_recognized_text_in_blocks" translatable="false">grtib</string>
    <string name="pref_title_show_language_tag" translatable="false">Show language identified</string>
    <string name="pref_key_show_language_tag" translatable="false">slt</string>
    <string name="pref_title_show_text_confidence" translatable="false">Show confidence score</string>
    <string name="pref_key_show_text_confidence" translatable="false">stc</string>

    <!-- Strings for object detector enable multiple objects preference. -->
    <string name="pref_title_object_detector_enable_multiple_objects" translatable="false">Enable multiple objects</string>
    <string name="pref_key_live_preview_object_detector_enable_multiple_objects" translatable="false">lpodemo</string>
    <string name="pref_key_still_image_object_detector_enable_multiple_objects" translatable="false">siodemo</string>

    <!-- Strings for object detector enable classification preference. -->
    <string name="pref_title_object_detector_enable_classification" translatable="false">Enable classification</string>
    <string name="pref_key_live_preview_object_detector_enable_classification" translatable="false">lpodec</string>
    <string name="pref_key_still_image_object_detector_enable_classification" translatable="false">siodec</string>

    <!-- Strings for face detector landmark mode preference. -->
    <string name="pref_title_face_detector_landmark_mode" translatable="false">Landmark mode</string>
    <string name="pref_key_live_preview_face_detection_landmark_mode" translatable="false">lpfdlm</string>
    <string name="pref_entries_face_detector_landmark_mode_no_landmarks" translatable="false">No landmarks</string>
    <string name="pref_entries_face_detector_landmark_mode_all_landmarks" translatable="false">All landmarks</string>
    <!-- The following entry values must match the ones in FaceDetectorOptions#LandmarkMode -->
    <string name="pref_entry_values_face_detector_landmark_mode_no_landmarks" translatable="false">1</string>
    <string name="pref_entry_values_face_detector_landmark_mode_all_landmarks" translatable="false">2</string>

    <!-- Strings for face detector contour mode preference. -->
    <string name="pref_title_face_detector_contour_mode" translatable="false">Contour mode</string>
    <string name="pref_key_live_preview_face_detection_contour_mode" translatable="false">lpfdcm</string>
    <string name="pref_entries_face_detector_contour_mode_no_contours" translatable="false">No contours</string>
    <string name="pref_entries_face_detector_contour_mode_all_contours" translatable="false">All contours</string>
    <!-- The following entry values must match the ones in FaceDetectorOptions#ContourMode -->
    <string name="pref_entry_values_face_detector_contour_mode_no_contours" translatable="false">1</string>
    <string name="pref_entry_values_face_detector_contour_mode_all_contours" translatable="false">2</string>

    <!-- Strings for face detector classification mode preference. -->
    <string name="pref_title_face_detector_classification_mode" translatable="false">Classification mode</string>
    <string name="pref_key_live_preview_face_detection_classification_mode" translatable="false">lpfdcfm</string>
    <string name="pref_entries_face_detector_classification_mode_no_classifications" translatable="false">No classifications</string>
    <string name="pref_entries_face_detector_classification_mode_all_classifications" translatable="false">All classifications</string>
    <!-- The following entry values must match the ones in FaceDetectorOptions#ClassificationMode -->
    <string name="pref_entry_values_face_detector_classification_mode_no_classifications" translatable="false">1</string>
    <string name="pref_entry_values_face_detector_classification_mode_all_classifications" translatable="false">2</string>

    <!-- Strings for face detector performance mode preference. -->
    <string name="pref_title_face_detector_performance_mode" translatable="false">Performance mode</string>
    <string name="pref_key_live_preview_face_detection_performance_mode" translatable="false">lpfdpm</string>
    <string name="pref_entries_face_detector_performance_mode_fast" translatable="false">Fast</string>
    <string name="pref_entries_face_detector_performance_mode_accurate" translatable="false">Accurate</string>
    <!-- The following entry values must match the ones in FaceDetectorOptions#PerformanceMode -->
    <string name="pref_entry_values_face_detector_performance_mode_fast" translatable="false">1</string>
    <string name="pref_entry_values_face_detector_performance_mode_accurate" translatable="false">2</string>

    <!-- Strings for face detector face tracking preference. -->
    <string name="pref_title_face_detector_face_tracking" translatable="false">Face tracking</string>
    <string name="pref_key_live_preview_face_detection_face_tracking" translatable="false">lpfdft</string>

    <!-- Strings for face detector min face size preference. -->
    <string name="pref_title_face_detector_min_face_size" translatable="false">Minimum face size</string>
    <string name="pref_key_live_preview_face_detection_min_face_size" translatable="false">lpfdmfs</string>
    <string name="pref_dialog_message_face_detector_min_face_size" translatable="false">Proportion of the head width to the image width, and the valid value range is [0.0, 1.0]</string>
    <string name="pref_toast_invalid_min_face_size" translatable="false">Minimum face size must be a float value and in the range [0.0, 1.0]</string>

    <!-- Strings for pose detector performance mode preference. -->
    <string name="pref_title_pose_detector_performance_mode" translatable="false">Performance mode</string>
    <string name="pref_key_live_preview_pose_detection_performance_mode" translatable="false">lppdpm</string>
    <string name="pref_key_still_image_pose_detection_performance_mode" translatable="false">sipdpm</string>
    <string name="pref_entries_pose_detector_performance_mode_fast" translatable="false">Fast</string>
    <string name="pref_entries_pose_detector_performance_mode_accurate" translatable="false">Accurate</string>
    <string name="pref_entry_values_pose_detector_performance_mode_fast" translatable="false">1</string>
    <string name="pref_entry_values_pose_detector_performance_mode_accurate" translatable="false">2</string>

    <!-- Strings for pose detector prefer gpu preference. -->
    <string name="pref_title_pose_detector_prefer_gpu" translatable="false">Prefer using GPU</string>
    <string name="pref_key_pose_detector_prefer_gpu" translatable="false">pdpg</string>
    <string name="pref_summary_pose_detector_prefer_gpu" translatable="false">If enabled, GPU will be used as long as it is available, stable and returns correct results. In this case, the detector will not check if GPU is really faster than CPU.</string>

    <!-- Strings for pose detector showInFrameLikelihood preference. -->
    <string name="pref_title_pose_detector_show_in_frame_likelihood" translatable="false">Show in-frame likelihood</string>
    <string name="pref_key_live_preview_pose_detector_show_in_frame_likelihood" translatable="false">lppdsifl</string>
    <string name="pref_key_still_image_pose_detector_show_in_frame_likelihood" translatable="false">sipdsifl</string>

    <!-- Strings for pose detector z value visualization preference. -->
    <string name="pref_title_pose_detector_visualize_z" translatable="false">Visualize z value</string>
    <string name="pref_key_pose_detector_visualize_z" translatable="false">pdvz</string>
    <string name="pref_title_pose_detector_rescale_z" translatable="false">Rescale z value for visualization</string>
    <string name="pref_key_pose_detector_rescale_z" translatable="false">pdrz</string>

    <!-- Strings for pose classification preference. -->
    <string name="pref_title_pose_detector_run_classification" translatable="false">Run Classification</string>
    <string name="pref_key_pose_detector_run_classification" translatable="false">pdrc</string>
    <string name="pref_summary_pose_detector_run_classification" translatable="false">Classify squat and pushup poses. Count reps in streaming mode. To get the best classification results based on the current sample data, face the camera side way and make sure your full body is in the frame.</string>

    <!-- Strings for segmentation preference. -->
    <string name="pref_title_segmentation_raw_size_mask" translatable="false">Enable raw size mask</string>
    <string name="pref_key_segmentation_raw_size_mask" translatable="false">srsm</string>

    <string name="pref_group_title_face_mesh" translatable="false">Face Mesh Detection</string>
    <string name="pref_title_face_mesh_use_case" translatable="false">Use Case</string>
    <string name="pref_key_face_mesh_use_case" translatable="false">face_mesh_use_case</string>

</resources>


--- END OF FILE app/src/main/res/values/strings.xml ---


--- START OF FILE app/src/main/res/values/styles.xml ---

<resources>
    <style name="AppTheme" parent="Theme.AppCompat.NoActionBar"/>
</resources>


--- END OF FILE app/src/main/res/values/styles.xml ---


--- START OF FILE app/src/main/res/xml/preference_live_preview_quickstart.xml ---

<?xml version="1.0" encoding="utf-8"?>
<PreferenceScreen
  xmlns:android="http://schemas.android.com/apk/res/android"
  android:key="@string/pref_screen">

  <PreferenceCategory
      android:enabled="true"
      android:key="@string/pref_category_key_camera"
      android:title="@string/pref_category_title_camera">

    <ListPreference
        android:key="@string/pref_key_rear_camera_preview_size"
        android:persistent="true"
        android:title="@string/pref_title_rear_camera_preview_size"/>

    <ListPreference
        android:key="@string/pref_key_front_camera_preview_size"
        android:persistent="true"
        android:title="@string/pref_title_front_camera_preview_size"/>

    <ListPreference
        android:key="@string/pref_key_camerax_rear_camera_target_resolution"
        android:persistent="true"
        android:title="@string/pref_title_camerax_rear_camera_target_resolution"/>

        <ListPreference
        android:key="@string/pref_key_camerax_front_camera_target_resolution"
        android:persistent="true"
        android:title="@string/pref_title_camerax_front_camera_target_resolution"/>

    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_camera_live_viewport"
        android:persistent="true"
        android:summary="@string/pref_summary_camera_live_viewport"
        android:title="@string/pref_title_camera_live_viewport"/>

  </PreferenceCategory>

  <PreferenceCategory android:title="@string/pref_category_info">
    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_info_hide"
        android:persistent="true"
        android:title="@string/pref_title_info_hide"/>
  </PreferenceCategory>

  <PreferenceCategory android:title="@string/pref_category_barcode_scanning">

    <SwitchPreference
        android:defaultValue="true"
        android:key="@string/pref_key_enable_auto_zoom"
        android:persistent="true"
        android:title="@string/pref_title_enable_auto_zoom"/>

  </PreferenceCategory>

  <PreferenceCategory android:title="@string/pref_category_text_recognition">

    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_group_recognized_text_in_blocks"
        android:persistent="true"
        android:title="@string/pref_title_group_recognized_text_in_blocks"/>

    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_show_language_tag"
        android:persistent="true"
        android:title="@string/pref_title_show_language_tag"/>

    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_show_text_confidence"
        android:persistent="true"
        android:title="@string/pref_title_show_text_confidence"/>

  </PreferenceCategory>

  <PreferenceCategory
      android:title="@string/pref_category_object_detection">

    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_live_preview_object_detector_enable_multiple_objects"
        android:persistent="true"
        android:title="@string/pref_title_object_detector_enable_multiple_objects"/>

    <SwitchPreference
        android:defaultValue="true"
        android:key="@string/pref_key_live_preview_object_detector_enable_classification"
        android:persistent="true"
        android:title="@string/pref_title_object_detector_enable_classification"/>

  </PreferenceCategory>

  <PreferenceCategory
      android:title="@string/pref_category_face_detection">

    <ListPreference
        android:defaultValue="@string/pref_entry_values_face_detector_landmark_mode_no_landmarks"
        android:entries="@array/pref_entries_face_detector_landmark_mode"
        android:entryValues="@array/pref_entry_values_face_detector_landmark_mode"
        android:key="@string/pref_key_live_preview_face_detection_landmark_mode"
        android:persistent="true"
        android:title="@string/pref_title_face_detector_landmark_mode"
        android:summary="%s"/>

    <ListPreference
        android:defaultValue="@string/pref_entry_values_face_detector_contour_mode_all_contours"
        android:entries="@array/pref_entries_face_detector_contour_mode"
        android:entryValues="@array/pref_entry_values_face_detector_contour_mode"
        android:key="@string/pref_key_live_preview_face_detection_contour_mode"
        android:persistent="true"
        android:title="@string/pref_title_face_detector_contour_mode"
        android:summary="%s"/>

    <ListPreference
        android:defaultValue="@string/pref_entry_values_face_detector_classification_mode_no_classifications"
        android:entries="@array/pref_entries_face_detector_classification_mode"
        android:entryValues="@array/pref_entry_values_face_detector_classification_mode"
        android:key="@string/pref_key_live_preview_face_detection_classification_mode"
        android:persistent="true"
        android:title="@string/pref_title_face_detector_classification_mode"
        android:summary="%s"/>

    <ListPreference
        android:defaultValue="@string/pref_entry_values_face_detector_performance_mode_fast"
        android:entries="@array/pref_entries_face_detector_performance_mode"
        android:entryValues="@array/pref_entry_values_face_detector_performance_mode"
        android:key="@string/pref_key_live_preview_face_detection_performance_mode"
        android:persistent="true"
        android:title="@string/pref_title_face_detector_performance_mode"
        android:summary="%s"/>

    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_live_preview_face_detection_face_tracking"
        android:persistent="true"
        android:title="@string/pref_title_face_detector_face_tracking"/>

    <EditTextPreference
        android:defaultValue="0.1"
        android:dialogMessage="@string/pref_dialog_message_face_detector_min_face_size"
        android:key="@string/pref_key_live_preview_face_detection_min_face_size"
        android:persistent="true"
        android:title="@string/pref_title_face_detector_min_face_size"/>
  </PreferenceCategory>

  <PreferenceCategory android:title="@string/pref_category_pose_detection">
    <ListPreference
        android:defaultValue="@string/pref_entry_values_pose_detector_performance_mode_fast"
        android:entries="@array/pref_entries_values_pose_detector_performance_mode"
        android:entryValues="@array/pref_entry_values_pose_detector_performance_mode"
        android:key="@string/pref_key_live_preview_pose_detection_performance_mode"
        android:persistent="true"
        android:title="@string/pref_title_pose_detector_performance_mode"
        android:summary="%s"/>
    <SwitchPreference
        android:defaultValue="true"
        android:key="@string/pref_key_pose_detector_prefer_gpu"
        android:persistent="true"
        android:title="@string/pref_title_pose_detector_prefer_gpu"
        android:summary="@string/pref_summary_pose_detector_prefer_gpu"/>
    <SwitchPreference
        android:defaultValue="true"
        android:key="@string/pref_key_live_preview_pose_detector_show_in_frame_likelihood"
        android:persistent="true"
        android:title="@string/pref_title_pose_detector_show_in_frame_likelihood"/>
    <SwitchPreference
        android:defaultValue="true"
        android:key="@string/pref_key_pose_detector_visualize_z"
        android:persistent="true"
        android:title="@string/pref_title_pose_detector_visualize_z"/>
    <SwitchPreference
        android:defaultValue="true"
        android:key="@string/pref_key_pose_detector_rescale_z"
        android:persistent="true"
        android:title="@string/pref_title_pose_detector_rescale_z"/>
    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_pose_detector_run_classification"
        android:persistent="true"
        android:title="@string/pref_title_pose_detector_run_classification"
        android:summary="@string/pref_summary_pose_detector_run_classification"/>
  </PreferenceCategory>

  <PreferenceCategory android:title="@string/pref_category_segmentation">
    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_segmentation_raw_size_mask"
        android:persistent="true"
        android:title="@string/pref_title_segmentation_raw_size_mask"/>
  </PreferenceCategory>
  <PreferenceCategory android:title="@string/pref_group_title_face_mesh">
    <ListPreference
        android:key="@string/pref_key_face_mesh_use_case"
        android:persistent="true"
        android:title="@string/pref_title_face_mesh_use_case"
        android:defaultValue="1"
        android:entries="@array/pref_entry_titles_face_mesh_use_case"
        android:entryValues="@array/pref_entry_values_face_mesh_use_case"
        android:summary="%s"/>
  </PreferenceCategory>

</PreferenceScreen>


--- END OF FILE app/src/main/res/xml/preference_live_preview_quickstart.xml ---


--- START OF FILE app/src/main/res/xml/preference_still_image.xml ---

<?xml version="1.0" encoding="utf-8"?>
<PreferenceScreen xmlns:android="http://schemas.android.com/apk/res/android">

  <PreferenceCategory android:title="@string/pref_category_info">
    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_info_hide"
        android:persistent="true"
        android:title="@string/pref_title_info_hide"/>
  </PreferenceCategory>

  <PreferenceCategory android:title="@string/pref_category_text_recognition">

    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_group_recognized_text_in_blocks"
        android:persistent="true"
        android:title="@string/pref_title_group_recognized_text_in_blocks"/>

    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_show_language_tag"
        android:persistent="true"
        android:title="@string/pref_title_show_language_tag"/>

    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_show_text_confidence"
        android:persistent="true"
        android:title="@string/pref_title_show_text_confidence"/>

  </PreferenceCategory>

  <PreferenceCategory
      android:title="@string/pref_category_object_detection">

    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_still_image_object_detector_enable_multiple_objects"
        android:persistent="true"
        android:title="@string/pref_title_object_detector_enable_multiple_objects"/>

    <SwitchPreference
        android:defaultValue="true"
        android:key="@string/pref_key_still_image_object_detector_enable_classification"
        android:persistent="true"
        android:title="@string/pref_title_object_detector_enable_classification"/>

  </PreferenceCategory>

  <PreferenceCategory android:title="@string/pref_category_pose_detection">
    <ListPreference
        android:defaultValue="@string/pref_entry_values_pose_detector_performance_mode_fast"
        android:entries="@array/pref_entries_values_pose_detector_performance_mode"
        android:entryValues="@array/pref_entry_values_pose_detector_performance_mode"
        android:key="@string/pref_key_still_image_pose_detection_performance_mode"
        android:persistent="true"
        android:title="@string/pref_title_pose_detector_performance_mode"
        android:summary="%s"/>
    <SwitchPreference
        android:defaultValue="true"
        android:key="@string/pref_key_pose_detector_prefer_gpu"
        android:persistent="true"
        android:title="@string/pref_title_pose_detector_prefer_gpu"
        android:summary="@string/pref_summary_pose_detector_prefer_gpu"/>
    <SwitchPreference
        android:defaultValue="true"
        android:key="@string/pref_key_still_image_pose_detector_show_in_frame_likelihood"
        android:persistent="true"
        android:title="@string/pref_title_pose_detector_show_in_frame_likelihood"/>
    <SwitchPreference
        android:defaultValue="true"
        android:key="@string/pref_key_pose_detector_visualize_z"
        android:persistent="true"
        android:title="@string/pref_title_pose_detector_visualize_z"/>
    <SwitchPreference
        android:defaultValue="true"
        android:key="@string/pref_key_pose_detector_rescale_z"
        android:persistent="true"
        android:title="@string/pref_title_pose_detector_rescale_z"/>
    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_pose_detector_run_classification"
        android:persistent="true"
        android:title="@string/pref_title_pose_detector_run_classification"/>
  </PreferenceCategory>

  <PreferenceCategory android:title="@string/pref_category_segmentation">
    <SwitchPreference
        android:defaultValue="false"
        android:key="@string/pref_key_segmentation_raw_size_mask"
        android:persistent="true"
        android:title="@string/pref_title_segmentation_raw_size_mask"/>
  </PreferenceCategory>

  <PreferenceCategory
      android:title="@string/pref_category_face_detection">

    <ListPreference
        android:defaultValue="@string/pref_entry_values_face_detector_landmark_mode_no_landmarks"
        android:entries="@array/pref_entries_face_detector_landmark_mode"
        android:entryValues="@array/pref_entry_values_face_detector_landmark_mode"
        android:key="@string/pref_key_live_preview_face_detection_landmark_mode"
        android:persistent="true"
        android:title="@string/pref_title_face_detector_landmark_mode"
        android:summary="%s"/>

    <ListPreference
        android:defaultValue="@string/pref_entry_values_face_detector_contour_mode_all_contours"
        android:entries="@array/pref_entries_face_detector_contour_mode"
        android:entryValues="@array/pref_entry_values_face_detector_contour_mode"
        android:key="@string/pref_key_live_preview_face_detection_contour_mode"
        android:persistent="true"
        android:title="@string/pref_title_face_detector_contour_mode"
        android:summary="%s"/>

    <ListPreference
        android:defaultValue="@string/pref_entry_values_face_detector_classification_mode_no_classifications"
        android:entries="@array/pref_entries_face_detector_classification_mode"
        android:entryValues="@array/pref_entry_values_face_detector_classification_mode"
        android:key="@string/pref_key_live_preview_face_detection_classification_mode"
        android:persistent="true"
        android:title="@string/pref_title_face_detector_classification_mode"
        android:summary="%s"/>

    <ListPreference
        android:defaultValue="@string/pref_entry_values_face_detector_performance_mode_fast"
        android:entries="@array/pref_entries_face_detector_performance_mode"
        android:entryValues="@array/pref_entry_values_face_detector_performance_mode"
        android:key="@string/pref_key_live_preview_face_detection_performance_mode"
        android:persistent="true"
        android:title="@string/pref_title_face_detector_performance_mode"
        android:summary="%s"/>
  </PreferenceCategory>

  <PreferenceCategory android:title="@string/pref_group_title_face_mesh">
    <ListPreference
        android:key="@string/pref_key_face_mesh_use_case"
        android:persistent="true"
        android:title="@string/pref_title_face_mesh_use_case"
        android:defaultValue="1"
        android:entries="@array/pref_entry_titles_face_mesh_use_case"
        android:entryValues="@array/pref_entry_values_face_mesh_use_case"
        android:summary="%s"/>
  </PreferenceCategory>

</PreferenceScreen>


--- END OF FILE app/src/main/res/xml/preference_still_image.xml ---


--- START OF FILE build.gradle ---

// Top-level build file where you can add configuration options common to all sub-projects/modules.

buildscript {
    ext.kotlin_version = "1.9.0"

    repositories {
        mavenCentral()
        mavenLocal()
        google()
    }

    dependencies {
        classpath 'com.android.tools.build:gradle:8.10.0'
        classpath "org.jetbrains.kotlin:kotlin-gradle-plugin:$kotlin_version"

        // NOTE: Do not place your application dependencies here; they belong
        // in the individual module build.gradle files
    }
}

allprojects {
    repositories {
        mavenCentral()
        mavenLocal()
        google()
    }
}

task clean(type: Delete) {
    delete rootProject.buildDir
}


--- END OF FILE build.gradle ---


--- START OF FILE settings.gradle ---

rootProject.name='ML Kit Vision Quickstart'
include ':app'


--- END OF FILE settings.gradle ---
