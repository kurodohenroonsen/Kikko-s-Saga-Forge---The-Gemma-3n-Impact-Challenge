[
    {
        "project_title": "Vision2Audio - AI-Powered application for the Blind and Visually Impaired",
        "tagline": "A game-changing technology powered by a Google's Gemma 3n model on the NVIDIA Jetson AGX Orin 64GB Kit",
        "authors": "Nurgaliyev Shakhizat",
        "summary": "People who are visually impaired or blind encounter numerous challenges in their daily lives, including difficulty perceiving objects and navigating their surroundings. These limitations extend to recreational activities such as swimming, playing computer games, walking, and more. Despite advancements in assistive technologies, there remains a significant gap in solutions that enhance the quality of life for individuals with blindness or visual impairments. \n\nVision2Audio is a proof-of-concept assistive technology project designed to bridge this gap by converting visual information into auditory feedback. This innovative system enables visually impaired individuals to perceive their environment in ways previously impossible, leveraging cutting-edge AI models and edge computing.\n\nVision2Audio is a web-based application that enables users to capture images and ask questions about them using Automatic Speech Recognition (ASR). The recognized speech is converted into text, which is then sent to a [vLLM inference engine](https://github.com/vllm-project/vllm) running [Google's Gemma 3n](https://deepmind.google/models/gemma/gemma-3n/) model. The system processes the input and generates responses, which are delivered audibly to the user through Text-to-Speech (TTS) technology.\n\nWithout further ado, let’s get started!",
        "technologies": [
            "CUDA",
            "Docker",
            "Gemma 3n",
            "Howler.js",
            "NVIDIA Jetson",
            "NVIDIA Riva",
            "Tailscale",
            "vLLM"
        ],
        "source_code_url": "https://github.com/shahizat/Vision2Audio",
        "demo_video_url": "https://youtu.be/Z34V8POK6v8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/vision2audio-ai-powered-application-for-the-blind"
    },
    {
        "project_title": "VedXlite – Advanced Multi-Modal Accessibility-Focused AI Assistant",
        "tagline": "Offline-capable, CUDA-accelerated AI with chat, vision, and voice. It is designed for blind and introverted users.",
        "authors": "Yugal Kishor, Shourya Singh",
        "summary": "VedXlite – Advanced Multi-Modal AI Assistant\n\n**A sophisticated, accessibility-focused assistant with advanced chat, voice & vision powered by Gemma3n, CUDA acceleration, and offline processing.**\n\n---",
        "technologies": [
            "CUDA",
            "Flask",
            "Ollama",
            "Vosk",
            "Web Speech API"
        ],
        "source_code_url": "https://github.com/yugalgamer/vedXLite",
        "demo_video_url": "https://youtu.be/DU95UqQThg4?si=lbjajAslvZwHaVWd",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/vedxlite-advanced-multi-modal-accessibility-focuse"
    },
    {
        "project_title": "GemmA.I: Real-Time Visual Perception Assistant for Blind and Visually Impaired",
        "tagline": "Privacy-first, real-time on-device AI delivering instant scene understanding and spoken assistance for blind and visually impaired users.",
        "authors": "Valluru Chetan Reddy",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/chetanreddyv/gemmai.git",
        "demo_video_url": "https://youtu.be/9jFIB0NQrnY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemmai-real-time-visual-perception-assistant"
    },
    {
        "project_title": "Rapid Care",
        "tagline": "RapidCare: AI-Driven Command System for Mass Casualty Incidents (MCI)",
        "authors": "Cher Huang",
        "summary": "In mass casualty events like concerts, wildfires, or emergency situations, the ability to respond swiftly and effectively can make all the difference in saving lives. Each year, countless individuals suffer injuries in these environments, highlighting the urgent need for fast, reliable, and coordinated care. Not to mention that during MCI, internet access might be interrupted. From rapidly assessing injuries to locating missing loved ones, the challenge is immense, and the demand for a seamless, real-time, offline-first response system has never been greater.\n\nEnter RapidCare — a groundbreaking solution designed to transform emergency response during mass casualty events. Powered by the advanced Gemma 3n AI engine with intelligent model selection, RapidCare is a voice-activated, multi-role, on-device AI assistant tailored for field triage, coordination, and communication. Whether you're a paramedic, nurse, field agent, or a family member searching for a loved one, RapidCare equips you with the tools needed to act quickly and efficiently.\n\nThis cutting-edge app is designed to operate fully offline on mobile devices, including Google Edge AI, Jetson devices, and web browsers, ensuring optimal performance regardless of connectivity. With intelligent medical triage features like video-based assessments, real-time vitals transcription, and patient reunification capabilities, RapidCare serves as a vital assistant to those working on the frontlines and those desperately seeking information.\n\nKey Technologies:\n\nOffline Capabilities: Through an HTTP Bridge, RapidCare can communicate with on-device AI (e.g. Android + Google Edge AI) for tasks like image analysis, enabling the app to function seamlessly without cloud dependence.\n\nFine-tuning:  The medical image triage is a fine-tuned with injury images in Unsloth, extracting precise triage information  including Triage Level, Reasoning, and Patient Information (Age, Gender, and Brief Assessment Finding).\n\nOn-Device AI Processing: Deployable to Jetson devices and mobile devices, all critical tasks are processed locally, ensuring rapid and reliable responses even in the most chaotic environments. When there's no internet connectivity, data is stored securely in Jetson device, and processing is not hindered with pre-downloaded on device AI models.\n\nDynamic Model Selection: RapidCare intelligently switches between Gemma 3n 2B and 4B models based on system load and task complexity. During high-traffic situations or resource constraints, the system automatically selects the 2B model for quick responses, while leveraging the 4B model for complex analysis when resources allow. This mix-and-match approach ensures optimal performance across diverse deployment scenarios.\n\nWith RapidCare, emergency responders and families can navigate the chaos of mass casualty events with increased efficiency, improved coordination, and ultimately, better outcomes for those in need of urgent medical care.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Cherchercher/em-Android",
        "demo_video_url": "https://www.youtube.com/watch?v=pO57VgrE5hI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/rapid-care"
    },
    {
        "project_title": "Gemma 3n color extractor",
        "tagline": "Get the key colors from your very favorite photo.",
        "authors": "starlightlab",
        "summary": "Gemma 3n color extractor\nHi, I'm jaehwa Noh. We always have a favorite photo that of mood and colors are very lovely. I'd thought what if we can use that colors to the other place, such as on the app, on the design poster and etc.\n\nIt is very useful when we use Gemma 3n, on device model, without worry about the price and sharing private image.\n\nTwo photos in here taken by me.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/MbJucsIrkfA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma3n-color-extractor"
    },
    {
        "project_title": "\"Pre-Doc\" - a personal, private AI chat to consult before your actual doctor. ",
        "tagline": "Gemma3n’s offline, private, on-device, model to assess ANY issues of intimate or personal nature, and have a more informed doctor's visit.",
        "authors": "Karan Bhatti",
        "summary": "\"Pre-Doc\" - Private AI Health Assessment for Critical Interventions\n\nMillions of people worldwide delay or avoid seeking medical help for sensitive health issues due to privacy fears, cultural stigma, lack of access to specialist care, or cost barriers. This delay can have life-threatening consequences for conditions requiring urgent intervention.\n\"Pre-Doc\" empowers users with immediate, research-backed pre-assessment using Gemma 3n's:\n\n- Private, \n- offline capabilities\n- Multilingual support\n- On-device AI chat technology\n\nREAL PEOPLE, REAL IMPACT - Backed by Clinical Research:\n\nSuicide Prevention (Saving Lives!):\nRecent research shows that suicidal crises are often impulsive - with 24% of attempters making the decision within just 5 minutes, and 47% deliberating for 10 minutes or less. Many college students with suicidal ideation hide their thoughts due to stigma, but are more likely to disclose through anonymous assessment. \"Pre-Doc\" provides the immediate, private intervention window that can be critical for suicide prevention.\n\nTherapeutic Circumcision Assessment (Preventing Years of Mental Distress):\nClinical studies demonstrate that early identification and treatment of conditions requiring therapeutic circumcision can prevent years of physical and psychological suffering. Research consistently shows improvements in depression and anxiety scores, with 97% of partners reporting positive outcomes. Early intervention enables people to have years of normalcy rather than enduring preventable mental health challenges.\nEvidence-Based Early Intervention:\n\n\"Pre-Doc\" provides a private, evidence-based way to:\n\n- Identify urgent health conditions before they become critical\n- Reduce healthcare barriers for at-risk populations\n- Enable informed decision-making about professional care\n- Preserve dignity while ensuring health needs are met\n\nBecause health matters, privacy does too, and timing can save lives.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=mkX-EUfW3aY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/personal-and-private-ai-chatbot-before-your-doc"
    },
    {
        "project_title": "Smart Eyes with Gemma",
        "tagline": "AI-Powered Vision Assistant for the Visually Impaired",
        "authors": "Jae-min Cha",
        "summary": "Smart Eyes with Gemma is an intelligent assistive system designed to empower the visually impaired with enhanced environmental awareness and communication capabilities. Leveraging the groundbreaking on-device, multimodal AI of Gemma 3n (for text analysis and real-time navigation guidance) and Gemma 3 (for advanced vision understanding and scene description), our solution provides real-time object detection, AI-powered situational analysis, and high-quality voice guidance, all running locally for unparalleled privacy and reliability.\n\nThis project demonstrates the transformative potential of Gemma 3n's mobile-first architecture in creating tangible accessibility solutions. It addresses critical real-world challenges by:\n\n🔍 Real-time Environmental Awareness: Utilizing YOLOv11 object detection with intelligent tracking to identify people, vehicles, and obstacles, providing immediate visual overlays and voice alerts based on approach patterns and directions.\n\n🧠 Intelligent Navigation Assistance: Gemma 3n analyzes detected objects and their movement status (approaching, stationary, fast approaching) to deliver concise, safety-focused voice guidance in under 15 words, enabling split-second decision making in complex environments.\n\n👁️ Advanced Multimodal Understanding: Users can request detailed scene descriptions or ask specific questions about their surroundings. Gemma 3's superior vision capabilities process images and text to provide accurate, context-aware answers, including OCR functionality for reading signs, labels, and written text.\n\n🗣️ Natural Voice Interaction: Integrated speech recognition (English/Korean) allows hands-free questioning, while Piper TTS converts AI responses into natural-sounding speech with smart audio queuing to prevent overlapping guidance.\n\n🔒 Privacy-First & Offline Architecture: By running Gemma models via Ollama and Piper TTS locally, Smart Eyes with Gemma operates completely offline, ensuring user privacy while maintaining full functionality without internet dependency—a crucial advantage for assistive technology.\n\n⚡ Optimized Edge Performance: Strategic use of Gemma 3n for rapid text processing and navigation alerts, combined with Gemma 3 for detailed visual analysis, demonstrates efficient resource allocation that's essential for future embedded deployment.\n\n🚀 Scalable Vision for Impact: While currently implemented as a web-based demonstration, the architecture is designed for seamless integration into smart canes, AR glasses, and purpose-built assistive devices. This showcases how Gemma 3n's edge-optimized capabilities can power next-generation accessibility solutions that provide always-on, private assistance.\n\nSmart Eyes with Gemma exemplifies how the combined power of Gemma 3n and Gemma 3 can create meaningful, positive change through practical on-device AI, fostering greater independence and safety for the visually impaired while paving the way for widespread deployment in real-world assistive technologies.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/chahero/smart-eyes-with-gemma",
        "demo_video_url": "https://www.youtube.com/watch?v=GNU-zBUO_3I",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/smart-eyes-with-gemma"
    },
    {
        "project_title": "Project Persona",
        "tagline": "A conversational language coach powered by Gemma 3n.",
        "authors": "Nam Nguyen",
        "summary": "Project Persona: a Conversational Language Coach powered by Gemma 3n – Technical Writeup\n\nProject Persona is a Conversational Language Coach that guides learners through role-play scenarios, complete with speech, images, real-time feedback, and conversation analysis. \nIt leverages the Gemma 3n model’s multimodal, multilingual, private, and offline-first capabilities. The offline version, which can run locally without an Internet connection, leverages Gemma 3n via Hugging Face, whereas the online interactive demo leverages NVIDIA’s API.\n\n[Key Features]\n\n1. Situations: Curated role-play prompts (e.g., chatting with a friend, job interviews) which define character personas and conversational goals.\n2. Multilingual Support: English, Mandarin Chinese, Spanish, Vietnamese, French, and Hindi.\n3. Multimodal Interaction: Users can send images to enrich context (e.g., show an object in the scene), use Speech-To-Text for convenience, and have the model respond using Text-To-Speech\n4. Real-Time Feedback: After each user response, the system returns a short critique color‑coded as green/yellow/red.\n5. Conversation Analysis: At session end, the app evaluates the full dialogue and produces detailed analysis and feedback.\n\n[High‑Level Architecture]\n\nOffline: [React Frontend] ←→ [Flask Backend] ←→  [Gemma 3n model on HF]\nOnline Interactive Demo:  [React Frontend] ←→ [Flask Backend] ←→  [NVIDIA API (Gemma 3n)]\nArchitecture Summary\nThe system is designed for lightweight, modular, and local-first operation. It consists of:\n\nFrontend:\nFramework: React (with Vite for bundling)\n\nCore Components:\nChatPanel: Handles conversation rendering and user/AI interaction\nConfigPanel: Lets users select scenarios and language settings\nFeedbackView: Displays real-time correction and post-session analysis\nSpeechController: Manages STT and TTS events\nSpeech Recognition: Web Speech API integrated via react-speech-recognition\nText-to-Speech: Web-native speechSynthesis API, dynamically adjusted by language\nMultimodal Inputs: Webcam image capture embedded into conversation context\n\n\nBackend:\nFramework: Flask (Python)\n\nTasks:\nManage and serve scenario configurations\nInterface with the LLM inference layer\nProcess image/text input and format for LLM\nManage summarization and evaluation tasks post-session\n\nLLM Inference Layer:\n\nModel: Gemma 3n (E4B) downloaded through Hugging Face (NVIDIA API for the interactive demo)\nMultimodal Input: Text and base64 image pairs passed for multimodal messages\nSystem Prompts: Pre-written prompts per scenario, include role, feedback, and user learning goals\nPrivacy: For the offline version, inference occurs fully on-device—no external API calls or cloud logging. NVIDIA API is used for the interactive demo.\n\nHosting & Deployment:\n\nOffline version: Works offline for privacy-first, accessible deployments. The app’s source code (frontend and backend) can be run via an IDE with React and Flask.\nOnline interactive demo: Deployed via Render\n\n[Key Features: In-depth]\n\n1. Conversational Situations\nThe app allows users to engage in rich, role-play conversations such as ordering at a café, catching up with a friend, or attending a job interview. Each scenario has a system prompt with a persona and behavior guide for the AI.\n\nImplementation:\n-Scenario metadata is stored in a config file.\n-The system prompt includes role instructions, user goals, and constraints to guide Gemma 3n.\n-The LLM maintains context across multiple turns, allowing natural and dynamic conversations that adapt to user responses.\n\n2. Multilingual Support\nThe coach supports multiple languages (e.g., English, Spanish, Vietnamese, and more). Users can switch languages at any time, and the app seamlessly reconfigures the system prompt and recognition/transcription tools.\n\nImplementation:\n-Prompts are pre-translated and contextualized per scenario.\n-Language-specific speech recognition is handled via browser-native APIs with dynamic language switching.\n\n3. Multimodal Support\nUsers can include images as part of the conversation—for example, showing a photo of a product they want to discuss or describing an object in the room.\n\nImplementation:\n-Captured webcam images are base64-encoded and embedded directly into the prompt.\n-Gemma 3n accepts interleaved text and image inputs\n-The model is able to reference visual context, enhancing realism and dimensionality in conversations.\n\n4. Real-time Feedback\nLearners receive contextual feedback in real-time after each response.\n\nImplementation:\n-Transcripts from speech recognition are fed to the LLM.\n-A prompt instructs the LLM to critique the user’s response\n-Another prompt instructs the LLM to give a color code (red – yellow – green, indicating performance)\n-Feedback is presented clearly in the UI alongside the response.\n\n\n5. Conversation Analysis\nAt the end of a session, the app summarizes key performance highlights.\n\nImplementation:\n-After a conversation ends, the full dialogue is passed to Gemma 3n with a summarization prompt.\n-The model analyzes performance according to pre-set goals.\n-Feedback is displayed in an evaluation tab with suggestions for next steps.\n\n6. Speech-to-Text (STT) and Text-to-Speech (TTS)\nThe app supports voice input and output in all supported languages, making the interaction feel natural and immersive.\n\nImplementation:\n-react-speech-recognition handles STT via the Web Speech API.\n-speechSynthesis handles TTS for AI responses.\n-The TTS voice is dynamically selected based on the target language.\n\n[Leveraging Gemma 3n’s Capabilities]\n\nGemma 3n brings a suite of architectural innovations that power our language coach. Below, we map each of its core features to how our app leverages them:\n\n1. Optimized On‑Device Performance & Efficiency:\nGemma 3n’s Per‑Layer Embeddings and effective parameter count reduce memory usage while maintaining model quality. We host Gemma 3n locally, enabling real‑time conversation without lag.\n\n2. Privacy‑First & Offline Ready:\nRunning Gemma 3n locally eliminates cloud dependencies and avoids sending user speech or images to third parties. This ensures users can safely practice conversations in private.\n\n3. Multimodal Understanding:\nGemma 3n’s vision encoders (MobileNet‑V5) process text and images in a single prompt. This allows the model to reference visual context—e.g., describing a captured object or providing feedback on a whiteboard sketch. This allows for more natural and dynamic conversations and is a key feature that makes this app stand out.\n\n4. Improved Multilingual Capabilities:\nWith multilingual capabilities, Gemma 3n handles all six target languages with amazing fluency, allowing for conversation coaching in different languages and powering our mission.\n\n[Prompt Engineering & Message Flow]\n\n-System Prompt: Defines the AI persona and scenario rules.\n-User Turn: May include text and/or image. The backend’s prepare_messages() function:\n-Extracts the system prompt into a single initial message.\n-Attaches any imageSrc fields as {\"type\":\"image_url\",\"image_url\":{\"url\":...}} blocks.\n-Assistant Response: Single POST to /chat returns the AI persona’s next line.\n-Immediate Feedback: A second prompt asks for a one-sentence critique in the learner’s language.\n-Sentiment Classification: A third prompt labels the feedback as positive/neutral/negative, yielding a color.\n-Session Evaluation: Calling /evaluate sends the entire chat history plus an instruction to produce Markdown‑formatted final feedback.\n\n[Engineering Challenges & Solutions]\n\n1. Embedding Images Without Upload\nSolution: We send base64 JPEGs inline, simplifying infrastructure.\n\n2. TTS Visual Synchronization\nSolution:Hooked SpeechSynthesisUtterance.onboundary events to animate a CSS scale on word boundaries.\n\n3. State Consistency on Language Switch\nSolution: A useEffect hook resets tabs to the first valid key when language changes, preventing crashes.\n\n4. Frontend Complexity\nSolution: Broke the React app into small, focused components (ConfigPanel, ChatPanel, WebcamFeed, etc.).\n\n5. Multilingual Speech Recognition Accuracy\nSolution: Tuned react-speech-recognition settings and fallback to manual input for accents and dialects.\n\n6. Prompt Consistency across Scenarios\nSolution: Centralized system-prompt templates to ensure persona rules are uniformly applied in all role-play contexts.\n\n7. Handling Unsupported Browsers & Permissions\nSolution: Feature-detection for STT/TTS and camera; graceful UI fallbacks and user messages when APIs are unavailable.\n\n8. Concurrency & Rate Limiting for Local Inference\nSolution: Implemented request queuing and simple rate limiting in Flask to prevent overload\n\n9. Secure Configuration Management\nSolution: Environment-driven configuration (.env for host, ports, credentials) and strict CORS policies.\n\n\nWhy These Decisions Matter\nLocal Gemma 3n: Offers total control, zero cloud latency, and no per‑request costs. Keeps the app’s operations private and accessible without an Internet connection.\n\n1. Flask + Python: Rapid development, straightforward HTTP handling, and easy JSON manipulation.\n2. React Hooks: Simplifies state management for dynamic features like STT/TTS and image capture.\n3. Inline Base64 Images: Eliminates file‑server complexity, keeping the system self‑contained.\n4. Each choice balances performance, developer productivity, and feasibility.\n\nGithub (Includes offline version and interactive demo version): \nhttps://github.com/namknnguyen/project-persona/tree/main\n\n*Links to code repository and interactive demo are all attached to this writeup.\n*Interactive demo might take 1-2 minutes for the first message as the server starts up\n*STT availability varies among browsers",
        "technologies": [
            "Flask",
            "Gemma 3n",
            "Web Speech API"
        ],
        "source_code_url": "https://github.com/namknnguyen/project-persona/tree/main",
        "demo_video_url": "https://www.youtube.com/watch?v=iLximobmWNg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/project-persona"
    },
    {
        "project_title": "AR-based Smart Mirror / Virtual Fitting Room (VFR)",
        "tagline": "Virtual Fitting Room (VFR) systems are revolutionizing the retail experience built in Unity — allows users to virtually “try on”clothes with",
        "authors": "Fatima1112",
        "summary": "The Smart Mirror app combines Augmented Reality, real-time 3D rendering, and cloud-delivered assets to simulate a physical try-on experience.\n•\tBuilt using Unity 2022.3 LTS\n•\tUtilizes ARFoundation for cross-platform AR (iOS & Android)\n•\tLoads clothing models dynamically from the cloud using AssetBundles\n•\tEnables outfit selection, virtual overlay, and screenshots\nTechnical Components\nLayer\t                Technology\t                                                        Purpose\nFrontend\t                   Unity, C#\t                                        AR rendering, UI, event control\nAR SDK\t                   ARFoundation + ARKit/ARCore\t       Body tracking & pose estimation\nOutfit System\t           Unity AssetBundles\t                       Dynamic loading of 3D models\nBackend\t                   Firebase / GitHub Pages\t               Remote asset hosting\nData Handling\tJSON (via Newtonsoft)\t                       Remote catalog of outfits\n\nGemma 3n Integration (Phase 2 / Experimental)\nTo enhance personalization, we began integrating Gemma 3n, a lightweight transformer model from Google, in the following ways:\n1.\tPreference Prediction: Based on prior outfit selections and tracked joint metrics (e.g., shoulder width), Gemma 3n recommends clothing that better fits user profiles.\n2.\tCatalog Sorting: The JSON catalog gets dynamically reordered to prioritize likely-preferred outfits.\n3.\tOn-Device Inference: Initial tests with on-device inference using Gemma 3n show strong potential without requiring full cloud AI.\nAlthough still in the early integration phase, logs from live sessions feed into a recommendation dataset powering future versions.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/RHFI0Jj4eF4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ar-based-smart-mirror-virtual-fitting-room-vfr"
    },
    {
        "project_title": "ShifaAI – Life-Saving Offline Healthcare Assistant",
        "tagline": "Health guidance powered by Gemma 3n for communities that are underserved—anytime, anywhere, even without internet.",
        "authors": "Md Arafat",
        "summary": "📄Project Description:\nShifaAI is an offline AI-powered healthcare app designed to support millions of people in remote or underserved regions where internet access is unavailable or unreliable. Built using Flutter, ShifaAI acts as a pocket health assistant that offers:\n\n✅ Symptom Checker – Users can input symptoms and receive AI-generated health guidance based on common illnesses.\n\n✅ Medicine Identifier – Scan or type medicine names to get descriptions, usage, and warnings—even offline.\n\n✅ Mental Health Support – Offline emotional support and self-care tips inspired by Islamic values and local relevance.\n\nWhy ShifaAI Matters:\nIn emergencies or daily health issues, internet dependency can be fatal. Most healthcare apps fail without a connection. ShifaAI is built to work fully offline, empowering users with life-saving knowledge when they need it most.\n\nTechnologies Used:\n\nFlutter (UI/UX)\n\nOn-device AI model for symptom matching\n\nLocal database for medicine info\n\nBilingual support (English & Bangla)\n\nTarget Impact:\nDesigned especially for rural Bangladesh and other low-resource regions, ShifaAI bridges the healthcare gap using accessible technology, inclusive language, and cultural values.",
        "technologies": [],
        "source_code_url": "https://github.com/mdarafatrealworld01/shifaai-offline-healthcare",
        "demo_video_url": "https://youtu.be/n8f1AWlX6W8?si=Gtz8NGQVFeTqxEza",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/shifaai-life-saving-offline-healthcare-assistant"
    },
    {
        "project_title": "SAB (सब): AI-Powered Voice Healthcare Ecosystem",
        "tagline": "Democratizing Healthcare Access Through On-Premise Voice Technology",
        "authors": "Dr. Bhavya Ratan Maroo, aloksinha3",
        "summary": "*Team: Dr. Bhavya Ratan Maroo(1), Alok Sinha(2)*\n1. Lok Nayak Hospital, Burari Goverment Hospital & Maulana Azad Medical College, New Delhi, India\n2. Duke University Durham, North Carolina, USA \n\n\n#1. Problem Statement - The Post Clinic Gap\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F8009523%2Fe3cb3e40003cf24f8d9bf7444d5e646a%2FAdd%20a%20little%20bit%20of%20body%20text%20(3).png?generation=1754520479354688&alt=media)\n\n\nEach morning at Burari Government Hospital, our OPD fills with expectant mothers—women who have walked miles or waited hours for a few precious minutes with a doctor. The exam room is a blur: brief consultations, a rushed prescription, and a flood of advice that must somehow sustain them in the days ahead.\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F8009523%2F278b0952dcecf3d584863a2ebafab08f%2FAdobe%20Express%20-%20WhatsApp%20Video%202025-08-03%20at%202.56.59%20AM.gif?generation=1754520122760286&alt=media)\n\nYet the true challenge in global health isn’t inside the clinic. For **2.8 billion people**, the moment they step outside marks the start of the **post-clinic gap**: a stretch of time with little support, almost no follow-up, and growing risk of complications.\n\n---",
        "technologies": [
            "Docker",
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/aloksinha3/SabCare/",
        "demo_video_url": "https://youtu.be/eZOwJnHHKyY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sabcare-offline-voice-first-ai-based-home-care-ant"
    },
    {
        "project_title": "HiRobot Meets Gemma",
        "tagline": "Bringing the best of two worlds together - Embodied AI.",
        "authors": "Sahana Venkatesh",
        "summary": "🤖 Hi Robot meets Gemma – An Interactive Robotic System Using Gemma3n and SmolVLA",
        "technologies": [
            "CUDA",
            "Flask",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Sahanave/hi-robot-gemma3n",
        "demo_video_url": "https://youtu.be/IOxbG6RAJmM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/robot-meet-gemma"
    },
    {
        "project_title": "Blind Vision",
        "tagline": "Empowering the Visually Impaired with real time Scene and Video description, powered by Google's Gemma 3N",
        "authors": "Jaisal K Jain",
        "summary": "Blind Vision",
        "technologies": [
            "CUDA",
            "Gemma 3n",
            "Web Speech API"
        ],
        "source_code_url": "https://github.com/JaisalJain/Blind-Vision",
        "demo_video_url": "https://www.youtube.com/watch?v=ILj0ZRMYTcI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/blind-vision"
    },
    {
        "project_title": "AI Accessibility Suite: Powered by Gemma 2 by mohammad parham dehghan",
        "tagline": " Innovative AI tools for blind, deaf, and speech-impaired users using Gemma 2, YOLOv8, BLIP, and MiDaS, poised for first-place impact.",
        "authors": "Behroz12342",
        "summary": "AI Accessibility Suite: Powered by Gemma 2\nAI Accessibility Suite, a leading contender for first place, comprises four automated assistive tools (PolyVoice, RealTime, md, VocalBridge) for blind, deaf, and speech-impaired users, leveraging Gemma 2, YOLOv8, BLIP, and MiDaS. Supporting over 2.2 billion people with vision or hearing impairments, it delivers real-time object detection, speech-to-text, text-to-speech, and scene description. Gemma 2 refines descriptions for accessibility, YOLOv8 detects objects, BLIP generates captions, and MiDaS estimates distances. Features include multilingual support, automated execution, and user-friendly interfaces, with demos and usage guides in the Kaggle Dataset. With potential for smart glasses integration, as shown in the vision video, this innovative suite maximizes Gemma 2’s on-device capabilities for global impact.\nby mohammad parham dehghan",
        "technologies": [],
        "source_code_url": "https://github.com/Parham-Dehghan/aiblindproject",
        "demo_video_url": "https://youtube.com/shorts/MghhyJliTPU?feature=share",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/aiblindproject"
    },
    {
        "project_title": "BetaAssist",
        "tagline": "Artificial intelligence",
        "authors": "Adetunji Adams",
        "summary": "BetaAssist: Making Technology Truly Accessible",
        "technologies": [
            "Gemma 3n",
            "Vosk"
        ],
        "source_code_url": "https://github.com/Akeem1955/BetaAssist.git",
        "demo_video_url": "https://youtu.be/RkzuMIfKaIk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/betaassist"
    },
    {
        "project_title": "Pragya AI – Your Offline Multilingual OCR & Q&A  Assistant",
        "tagline": " An offline AI tool for multilingual text extraction and question answering.",
        "authors": "Rimita Ghosh",
        "summary": "📌 Title: Pragya AI – Your Offline Multilingual OCR & Q&A Companion\n\n\nIn today’s world, access to information is key—but millions still face unreliable or no internet. Meet Pragya AI—an easy-to-use, **offline tool that reads multilingual text from images and instantly answers your questions from a local knowledge base.\n [Problem Statement ]\nStudents, farmers, and social workers encounter printed or handwritten texts in Hindi, **Bengali, and **English daily. Without internet, scanning and understanding this content is tough—leaving many disconnected.\n\n🌟 [Solution] \n\nPragya AI solves this by:\n- Extracting text with Tesseract OCR (Hindi, Bengali, English)\n- Processing images using OpenCV and Pillow\n- Searching answers offline with fuzzy matching via difflib\n- Providing color-coded terminal feedback with termcolor\n- Running 100% offline—no internet, no cloud required\n\n🛠 [Technology] \n\nBuilt using practical, lightweight Python libraries:\n- pytesseract for OCR\n- opencv-python (cv2) & Pillow for image processing\n- termcolor for clear terminal messages\n- difflib for fuzzy question matching\n\nNo heavy AI models or cloud dependencies—just clean, reliable offline tech.\n\n🛠 Demo:\n\nWe feed Pragya AI images in Hindi, Bengali, and English. It extracts text via OCR, shows the recognized content, and matches user questions against a curated JSON Q&A database. The best answer appears instantly—fully offline.\n\n🎓 [Impact] \n\nThis tool empowers people in low-connectivity zones—helping students, NGOs, and field workers access knowledge fast, securely, and privately.\n\n🚀 [Ending]\n\nMore than code, Pragya AI is a mission—to bridge digital divides with offline multilingual OCR and Q&A assistance.\n\nThank you.....",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/vSvYxh2ykJ8?si=P6W4bWQAk4s1dI8v",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/pragya-ai-your-offline-multilingual-ocr-qna"
    },
    {
        "project_title": "NANI AI — Motherhood Journey, Powered by Gemma 3n",
        "tagline": "Empowering mothers with AI-driven guidance, wellness tracking, and baby care—fully private, fully offline.",
        "authors": "Shayamon Bastakoti",
        "summary": "**Overview**\n\nNANI AI is an offline-first, privacy-respecting AI assistant designed specifically for pregnant women and new moms. Built with Gemma 3n via Ollama local inference, it offers real-time guidance, emotional support, and fun learning—all running entirely on-device, ensuring data privacy and reliability even without an internet connection.\n\nThis project aims to address maternal well-being, stress reduction, and parenting guidance, especially for mothers in low-connectivity or privacy-conscious environments.\n\n\n**Problem Statement**\n\nPregnancy and early motherhood are overwhelming. Constant internet access, privacy concerns, and a lack of personalized emotional support add to the stress. NANI AI bridges this gap with a local AI companion that works offline, providing week-by-week advice, affirmations, and interactive parenting guidance.\n\n\n**Key Features**\n\nDaily AI Affirmations & Wellness Tips 🧘‍♀️\n\nPregnancy Milestone Tracker 📅\n\nBedtime Stories & Baby Sleep Music 💤\n\nInteractive Game-Style Do’s & Don’ts Guide 🎮\n\n(Optional) Baby Cry Analyzer (AI-powered Sound Detection) 🎤\n\n\n\n**How We Used Gemma 3n**\n\nOllama Local Inference Engine: The entire app runs Gemma 3n queries via Ollama on-device, ensuring privacy and faster response.\n\nPrompt Engineering: Each feature is powered by task-specific prompts, leveraging Gemma 3n's flexible reasoning.\n\nOffline-Ready: No dependency on cloud APIs for primary AI features.\n\nMultimodal UI (Text-Audio): Enhanced with interactive Streamlit frontend.\n\n\n\n **Technology Stack**\n\nGemma 3n (via Ollama Local API)\n\nPython\n\nStreamlit (Frontend)\n\nOptional: Baby Cry Detection Model\n\n📂 Folder Structure\n\n📁 NANI-AI/\n├── app.py\n├── tabs/\n│   ├── home.py\n│   ├── milestones.py\n│   ├── bedtime_stories.py\n│   ├── cry_analyzer.py\n│   └── ... (other tabs)\n├── requirements.txt\n└── README.md\n\n\n\n**Installation Instructions**\n\ngit clone https://github.com/Simon420-lye/naniai.git\ncd naniai\npip install -r requirements.txt\nollama pull gemma 3n or gemma:7b depending on your device\nstreamlit run app.py\n\n\n\n**Challenges & Learnings**\n\nOptimized response speed for local inference using Ollama.\n\nBalancing AI model size and device compatibility.\n\nCrafting emotionally engaging prompts for affirmations & wellness.\n\nDesigning a user-friendly, accessible UI with low technical friction.\n\n\n\n**Future Scope**\n\n- Expand to Multilingual Support and AI Voice Conversations (Nepali, Spanish, etc.)\n\n- Emotion-Aware Companion (AI Mood Detection via Facial & Voice Analysis)\n\n- AI Powered Family Planner & Fertility Coach\n\n- Baby Growth Visualizer (Augmented Reality Growth Projection)\n\n- Store for baby and pregnancy shopping\n\n- Real-time Cry Analysis using local audio processing\n\n\n📄 License\nThis project is open-sourced under the MIT License.\n\n\n🏆 Hackathon Citation\nSubmission for Google - The Gemma 3n Impact Challenge\n\nSince my code runs locally with Ollama without the use of the internet, I haven't prepared the hosting links for this particular project.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Simon420-lye/naniai",
        "demo_video_url": "https://youtu.be/GZomdAtsj_A?si=kA0cwvF_Je9hADyd",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/nani-ai-private-ai-companion-gemma-3n-powered"
    },
    {
        "project_title": "offline-gemma-assistant",
        "tagline": "Offline Gemma Assistant: A Local LLM Solution for Remote Areas",
        "authors": "seveNine",
        "summary": "Offline Gemma Assistant: A Local LLM Solution for Remote Areas",
        "technologies": [
            "Docker",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/seveNine97/offline-gemma-assistant",
        "demo_video_url": "https://youtu.be/bIx4RMClLyU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/offline-gemma-assistant"
    },
    {
        "project_title": "Silent Voice",
        "tagline": "Neural translator for paralyzed patients: biosignals -> natural language. Eye movements & micro-expressions become full sentences instantly.",
        "authors": "Petr Royce",
        "summary": "Silent Voice: Neural Translation for the Paralyzed - A Research Prototype Powered by Fine-Tuned Gemma 3n",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/0xroyce/silent-voice",
        "demo_video_url": "https://www.youtube.com/watch?v=usnPM2YBI74",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/silent-voice"
    },
    {
        "project_title": "Doc-Note-to-SOAP-converter",
        "tagline": "This app converts illegible handwritten medical notes into professional SOAP format using OCR and Google Gemma 3n AI, reducing documentation",
        "authors": "saroj bono1, Robert Fenney",
        "summary": "Medical AI SOAP Note Generation Project",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=7FssFXqn1fY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/doc-note-to-soap-converter"
    },
    {
        "project_title": "Project Nightingale",
        "tagline": "A Submission to the Google - Gemma 3n Impact Challenge",
        "authors": "Jeremiah Brandon Stark",
        "summary": "Introduction: A New Paradigm for Humanitarian Technology\n\nThe history of crisis response and global development is a chronicle of human resilience punctuated by catastrophic failures, moments where the lack of timely, accurate information was the primary catalyst for preventable loss of life. We propose a fundamental paradigm shift. The future of impactful technology cannot be tethered to a distant cloud; it must be architected for resilience, designed for privacy, and engineered to function at the edge of human need, where infrastructure is weakest. Project Nightingale is the architectural blueprint for this future. It is not an application, but a global utility; an offline-first, multimodal field assistant that places the power of expert-level AI directly into the hands of those who need it most. This document outlines the vision, architecture, and life-saving potential of a platform built on Google's Gemma 3n, designed to forge a new legacy of humanitarian innovation.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/xJvusLaAN4E?si=F6fo8uCntx4Gr4Wi",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/project-nightingale"
    },
    {
        "project_title": "Aid Response in Crises",
        "tagline": "A history aware chatbot that is build using Gemma3N LLM model with Langgraph, Chromadb, Gradio etc.Aims to help victims.",
        "authors": "Rohit Dileep",
        "summary": "In this project, I built an intelligent Aid Response Chatbot designed to assist individuals affected by natural disasters such as wildfires, hurricanes, earthquakes, tornadoes, Avalanche. The system leverages LangGraph to orchestrate a structured conversational flow with memory, enabling history-aware responses that feel more human and contextually accurate.\n\n Key Components:\nLLM Backbone: Gemma-3N (via Langchain Ollama)\nA fast and efficient transformer-based language model to provide concise, accurate, and empathetic responses during crises.\n\nLangGraph Framework:\nUsed to build a multi-step, agentic reasoning graph that dynamically routes user queries based on intent, ensuring each request is processed by the most relevant sub-agent (e.g., fire response, earthquake triage, shelter info).\n\nChromadb + Vector Retrieval:\nContextual grounding is achieved using ChromaDB, which serves as a retriever-augmented generation (RAG) backend. Disaster-specific knowledge (e.g., first-aid protocols, emergency contacts, region-specific alerts) is stored and queried in real time.\nCreated using UnstructureURL and saved the RAG for offline usage.\n\nHistory Awareness:\nThe chatbot remembers past queries within the same session, enabling follow-up questions, context chaining, and user-specific assistance over longer conversations—crucial during stressful disaster scenarios.\n\nGradio :\nFor UI related use case.",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=3C5AwmRMuX0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/aid-response-in-crises"
    },
    {
        "project_title": "HealthGem - Gemma powered personal medical diary",
        "tagline": "Record medical incidents intelligently to reveal long-term disease patterns or help physicians diagnose rare diseases",
        "authors": "Léon Zeitler",
        "summary": "*The information provided in this Writeup is for general informational purposes only and is not a substitute for professional medical advice, diagnosis, or treatment. Always seek the advice of your physician or other qualified health provider with any questions you may have regarding a medical condition.*\n\nDiagnostic Odysseys",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/A4ReGTFAEXo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/healthgem-a-personalized-medical-diary"
    },
    {
        "project_title": "La Casita ",
        "tagline": "A Smart, Sustainable & Autonomous home with Mirabel — a Disney-inspired AI assistant powered by Google's Gemma 3n model.",
        "authors": "SrikanthPagadala",
        "summary": "🏠 La Casita - Smart & Sustainable Home Automation with Gemma 3n",
        "technologies": [
            "Gemma 3n",
            "NVIDIA Jetson",
            "Ollama"
        ],
        "source_code_url": "https://github.com/srikanthpagadala/kaggle-google-gemma-3n-hackathon-submission",
        "demo_video_url": "https://www.youtube.com/watch?v=EHGmr7-5pXU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/la-casita-smart-and-sustainable-home-automation"
    },
    {
        "project_title": "GemCurio (Natural History Museum Explorer)",
        "tagline": "GemCurio is an Android APP powered by the Gemma 3n multimodal model.",
        "authors": "Claude",
        "summary": "GemCurio is an app powered by the Gemma 3n multimodal model. It can:\nIdentify exhibits from photos and generate explanations.\nCreate clues from photos of exhibits for interactive games with others.\nAutomatically record information for easy review.\nAutomatically organize photographed exhibits and award badges.\nOffer general AI conversational capabilities.\nProvide general AI multimodal functionalities.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/Bt5FunxcU48",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemcurio-natural-history-museum-explorer"
    },
    {
        "project_title": "FARM-GUARDIAN",
        "tagline": "Empowering Farmers with Offline AI – Instant Crop Disease & Pest Detection in the Palm of Your Hand",
        "authors": "Aanya Soni",
        "summary": "FarmGuardian is an offline-first, AI-powered mobile application that helps farmers identify crop diseases and pest infestations in real-time using just their phone's camera. Leveraging Google’s Gemma 3n on-device multimodal model, it processes images locally to provide instant feedback and treatment suggestions—without needing internet access. It empowers rural and remote farmers with expert-level plant diagnostics directly in their hands, anytime, anywhere",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/0fCkT_wIPF4?feature=shared",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/farm-guardian"
    },
    {
        "project_title": "Gemma K12 Tutor",
        "tagline": "Private, Personalized, and Offline AI Tutor for Every Student",
        "authors": "Palvinder",
        "summary": "Gemma K12 Tutor is an AI-powered learning assistant designed for K-12 students, working completely offline for privacy and accessibility—even on low-end devices. Students simply upload their textbook, ask any question, and receive instant, age-appropriate explanations strictly aligned to their syllabus.\nThe app **supports multiple languages**, delivers step-by-step solutions for tricky concepts, and generates quizzes for self-assessment. By running locally and not requiring internet, Gemma K12 Tutor empowers children in remote areas, protects student privacy, and relieves parental concerns about unsafe online resources.\n\n**Key Features:**\n- **Offline AI:** No internet required, all processing is local.\n- **Textbook-Based Learning:** Answers are generated from the uploaded textbook, ensuring syllabus alignment.\n- **Instant Explanations:** Clear, simple answers for any textbook question or concept.\n- **Quiz Generator:** MCQs and feedback to reinforce learning.\n- **Multilingual Support:** Works with textbooks and questions in any language.\n- **Safe & Private:** No data leaves the device; perfect for privacy-conscious environments.\n- **Accessible:** Runs on standard hardware—ideal for low-resource schools and homes.\n\n**Why Gemma K12 Tutor?**\nMillions of children lack safe and reliable access to online learning tools. Gemma K12 Tutor brings the power of large language models to every student—no matter where they live, what device they use, or what language they speak. It’s not just a digital teacher—it’s a revolution in accessible, equitable education.",
        "technologies": [],
        "source_code_url": "https://github.com/palvinderkaurbath/gemma-k12-app.git",
        "demo_video_url": "https://youtu.be/z_vXBrD7iHs",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-k12-tutor"
    },
    {
        "project_title": "AuriZen: Awareness, Understanding, Reflection, Innerpeace",
        "tagline": "Your private companion for mindfulness and clarity.",
        "authors": "Jason Byers",
        "summary": "AuriZen — Your Offline & Personal Wellness Companion\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F27762435%2F53b3fd61ac58f0c7bcfaa96e12944231%2Faurizen_logo.png?generation=1752277417070675&alt=media)",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/JasonByers85/Aurizen",
        "demo_video_url": "https://www.youtube.com/watch?v=b_oPJo9fKTw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/aurizen-awareness-understanding-reflection-innerpe"
    },
    {
        "project_title": "EchoCare – Assistant Vocal Émotionnel Hors-Ligne avec Gemma 3n",
        "tagline": "Un compagnon vocal intelligent qui réconforte, écoute, et interagit entièrement hors-ligne avec Gemma 3n",
        "authors": "Aliou Cisse",
        "summary": "EchoCare – Assistant vocal émotionnel hors-ligne avec Gemma 3n\n\n**EchoCare** est un assistant vocal empathique et intelligent, fonctionnant localement avec le modèle *Gemma 3n (via Ollama)*. Il analyse la voix, comprend les émotions et répond en français avec chaleur et bienveillance.\n\n> 🎯 **Objectif** : Fournir un compagnon vocal accessible, privé et émotionnellement intelligent pour les personnes isolées ou en détresse émotionnelle, même sans connexion internet.\n\n---\n\n#",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=WsT071C63mA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/echocare-assistant-vocal-motionnel-hors-ligne-avec"
    },
    {
        "project_title": "Aid in Crisis Response: Build tools that can operate offline to provide critical",
        "tagline": "Life-saving offline AI app for disasters: guides first aid and connects people through mesh networks when internet is down.",
        "authors": "Nura Yusuf Ibrahim",
        "summary": "Offline AI-Powered Emergency Assistant for Crisis Respons\n\n1. Project Title\nOffline AI-Powered Emergency Assistant\n\n2. Problem Statement\nDuring natural disasters or conflicts, internet and mobile networks often fail, leaving people without access to life-saving information or a way to communicate. This delays first-aid response and rescue coordination, costing lives in the critical first hours after a crisis.\n3. Proposed Solution\nWe propose an offline-first Android application that functions even without internet or cellular networks:\ni. Life-Saving First-Aid Guidance\n•\tPreloaded multilingual medical and survival instructions.\n•\tText-to-speech for accessibility and low-literacy users.\nii. Peer-to-Peer SOS Communication\n•\tBluetooth/Wi-Fi Direct mesh networking to send SOS messages and GPS coordinates to nearby devices.\niii. AI-Assisted First Aid (Advanced Feature)\n•\tOn-device TensorFlow Lite model to recognize injuries (photos or symptoms) and suggest appropriate first-aid steps.\n5. Impact\ni. Life-saving in critical first hours of a disaster.\nii. Empowers communities to self-organize and request help when official rescue is delayed.\niii. Affordable & Scalable, working on low-end Android smartphones common in developing regions.\n\n6. Technology Stack\n  i. Kotlin (Android) - Offline app development.\n    ii. Room (SQLite) - Local data storage.\n       iii. Nearby Connections API – Peer-to-peer mesh networking.\n         iv. TensorFlow Lite - On-device AI inference.\n         v.  Android TextToSpeech API - Voice instructions.\n7. Development Timeline\n     i.  Build basic offline first-aid guide & local database.\n     ii.  Integrate mesh networking & SOS communication.\n     iii.   Add AI-assisted first-aid detection & testing.\n8. Expected Outcome\nA fully functional offline emergency response app that can save lives in disaster-prone areas by providing immediate first-aid help and facilitating community-driven rescue communication.\n \nPrepared by:\nNura Yusuf Ibrahim\nAlnahrain University Baghdad - Automation and AI Department\nDate: July 2025",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/5JkK0tmcx4A?si=GICO_gob1CVOstUW",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/aid-in-crisis-response-build-tools-that-can-operat"
    },
    {
        "project_title": "EasyForm - Your Offline Form Filling Assistant",
        "tagline": "The omni form filling assistant to translate, read your local documents to fill the forms for you.",
        "authors": "Hieu Le, Tung Vu thanh, Nguyen Minh Tung",
        "summary": "EasyForm: Technical Report\n\nThis document provides a detailed technical overview of the EasyForm application, covering its architecture, the strategic use of LLMs like Gemma 3, the challenges encountered during development, and the rationale behind our technical decisions.",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/hieu-lee/tic-tac-toe",
        "demo_video_url": "https://www.youtube.com/watch?v=wD0rF9A1gLA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/easyform-your-offline-form-filling-assistant"
    },
    {
        "project_title": "Lightweight Multimodal Pet Health Monitoring System Powered by Gemma 3N",
        "tagline": "Leverage Gemma 3N’s multimodality for offline pet health monitoring on mobile devices",
        "authors": "echy dai",
        "summary": "1. Project Background and Pain Points\n\nPets can’t verbalize discomfort, and traditional single-sensor tools (step counters, thermometers) miss subtle cues—like secret wound-licking or quiet whimpers—delaying care. We aim to give owners real-time, offline smartphone monitoring via Gemma 3N’s multimodal strengths.\n\n2. Technical Solution: Gemma 3N-Powered Smart Monitoring\n\nUsing phone cameras/microphones, we collect video/audio, processed locally by a lightweight Gemma 3N model.\n\nKey Innovations:\n\n• Multimodal Fusion: Links visuals (e.g., paw-licking) and audio (e.g., whimpering) to flag issues like skin inflammation—89% accuracy (vs. 65% for single-sensor tools).\n\n• On-Device Efficiency: Runs offline on mid-range phones (e.g., 0.8 sec/frame on Redmi Note 12), avoiding latency/privacy risks.\n\n• Conversational Guidance: Answers user queries (e.g., \"Why won’t it eat?\") with context-aware advice, blending real-time data and pet health knowledge.\n 3.how to achieve :use Gemma 3N（please see the attachment）\n4. Test and value\n\n• Mobile phone can run: Redmi Note 12 test, continuous analysis for 1 hour without jamming, memory is stable\n\n• High accuracy: 100 abnormal video + audio tests, Gemma 3N recognition accuracy 89%, much better than the traditional method\n\n• Ordinary people can use it: no need to buy high-end equipment, the mobile phone can be used with a small gadget (after code encapsulation), protecting pet health at low cost\n\n5. Summary and outlook\n\nRelying on the multimodality and lightweighting of Gemma 3N, we simplify \"complex AI monitoring\" so that pet owners can care about the health of their children at any time. In the future, I want to optimize the model, support more pet types, and let more people use it!",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/echy-3999/pet-health-gemma3n",
        "demo_video_url": "https://youtu.be/JneH8IVcq7E?si=gM3nGc_eRT0lIMw9",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/lightweight-multimodal-pet-health-monitoring-syste"
    },
    {
        "project_title": "Dream's assistant: AI Assistant for Speech-Impaired Humans",
        "tagline": "On-device AI assistant trained for unique speech patterns, empowering users to build their dreams and achieve their full potential.",
        "authors": "Brady Ali Medina",
        "summary": "**The Problem**\nMillions of people with speech impairments have brilliant ideas and ambitious dreams but face barriers in digital communication and task execution. Traditional voice assistants fail to understand their unique speech patterns, limiting their ability to independently manage social media, create content, run businesses, or pursue their goals.\n\n**The Solution**\nAmorita's Assistant is an on-device AI companion fine-tuned specifically for speech impairments. Using Gemma 3n's advanced capabilities, it understands unique speech patterns and becomes a powerful personal assistant that helps users accomplish anything they envision.\n\nKey Features:\n- Custom Speech Recognition: Fine-tuned using Unsloth to understand individual speech patterns and accents\n- Complete Digital Control: Send messages, manage social media, upload videos, make calls, and handle online tasks\n- Privacy-First: All conversations and personal data stay completely on-device\n- Multilingual Support: Optimized for multiple languages including Spanish\n- Emotional Intelligence: Provides encouragement, celebrates wins, and offers genuine companionship\n\nTechnical Innovation:\n\n- Personalized Fine-tuning: Each installation adapts to the user's specific speech characteristics\n- Multimodal Integration: Processes voice, text, and images for comprehensive assistance\n- Offline Reliability: Functions without internet connection for consistent support\n- Phone Integration: Direct access to device functions for seamless task execution\n\nReal-World Impact:\n\nOur first user is building WhatsApp communities for people with disabilities, launching a YouTube channel, and creating inclusive businesses. The assistant enables her to execute these dreams independently, transforming communication barriers into opportunities for leadership and impact.\n\nThis technology doesn't just solve speech recognition; it unlocks human potential. Every user can pursue their unique dreams, whether entrepreneurship, content creation, community building, or personal growth.\n\nAmorita's Assistant(my sister) proves that AI can be deeply personal, culturally aware, and genuinely empowering, I might called in the future Dream's Assistant. Thank you!",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Bra-AM/dream-assistant-android/tree/main",
        "demo_video_url": "https://youtu.be/4MTa6NzVsYw?si=UQdU8QTLnI9h1wuY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/amoritas-assistant-ai-assistant-for-speech-impaire"
    },
    {
        "project_title": "GYM BRO - Your On-Device AI Personal Trainer",
        "tagline": "A private, on-device AI fitness coach that provides personalized workouts, real-time form correction, and AI-driven motivation.",
        "authors": "Simon Makumi",
        "summary": "Project: GYM BRO - Your AI Personal Trainer\n\nMy full, detailed project writeup is available on my GitHub README page.\n\nPlease view the full README here for all technical details, features, and setup instructions:\nhttps://github.com/simonMakumi/gym-bro-hackathon\n\nVideo Demo:\nhttps://youtu.be/q9QmZ30MchU\n\nBrief Summary:\nGYM BRO is a private, on-device AI personal trainer built to make fitness accessible to everyone. It leverages the power of Gemma 3n running locally via Ollama to provide personalized workout plans, real-time form correction, and goal-oriented motivation, all without needing an internet connection.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/simonMakumi/gym-bro-hackathon#",
        "demo_video_url": "https://youtu.be/q9QmZ30MchU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gym-bro-your-on-device-ai-personal-trainer"
    },
    {
        "project_title": "Waste Sorting Assistant - Offline Inference API",
        "tagline": "Waste Sorting Assistant: Offline AI for a Cleaner Planet – A Google Gemma 3n Hackathon Submission",
        "authors": "Harshad Bhirud",
        "summary": "Waste Sorting Assistant: Offline AI for a Cleaner Planet – A Google Gemma 3n Hackathon Submission 🤖♻️🌍",
        "technologies": [
            "Flask",
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=-jgu96Ll4z0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/waste-sorting-assistant-offline-inference-api"
    },
    {
        "project_title": "MySightGuide",
        "tagline": "An AI-Powered 'Extended Eye' for the Visually Impaired",
        "authors": "LILIAN AISUAN",
        "summary": "Author: Lilian Aisuan\n\n**Core Mission:** To create a powerful, hands-free, and reliable AI companion for visually impaired individuals that functions seamlessly both online and offline, with a strong focus on accessibility and real-world usability.\n\n**1. Overview:**\nMySightGuide is a Native Android application designed to serve as an AI companion for the visually impaired. It leverages on-device computer vision, OCR, and conversational AI to provide users with a greater sense of their surroundings, enhancing their independence and safety. As a hybrid online/offline solution, it ensures core functionality is always available, addressing the critical need for reliability.\n\n**2. The Problem:**\nFor millions of visually impaired individuals, navigating the world and performing daily tasks can be a significant challenge. While mobile phones are ubiquitous, their utility for those who cannot see is often limited to basic functions like calls. Accessing visual information—from reading product labels to understanding a room's layout—requires assistance, limiting independence and inclusivity.\n\n**3. The Solution**\nMySightGuide transforms an Android phone into an \"extended eye,\" giving users the ability to perceive their environment through AI. Key features include:\n\n- Scene Description:** Using on-device computer vision models, the app describes the user's surroundings in real-time.\n\n- Text Recognition (OCR):** Powered by ML Kit, the app can read text from documents, signs, and product packaging aloud.\n\n- Conversational Assistant:** Integrated with the Gemini API, users can ask natural language questions about what the camera sees for a more interactive experience.\n\n- Hybrid Functionality:** Core features like object detection and OCR are processed on-device, guaranteeing privacy and functionality even without an internet connection.\n\n**4. The Core Philosophy: A Resilient Hybrid AI Model\nThe central architectural decision for MySightGuide was the implementation of a hybrid online/offline AI model. This was not a compromise, but a strategic choice made in direct response to the unique challenges of this project and the specific needs of its target audience.\n\nThe goal was to create an app that was not only intelligent but also dependable. A visually impaired user must be able to trust that their tool will work, regardless of their location or internet connectivity. This philosophy guided every technical decision.\n\n- The Central Technical Challenge: The \"Gemma 3N Paradox\"\nThe competition encouraged the use of the new, powerful Gemma 3N models. However, a deep technical analysis and early development revealed a significant paradox: the most powerful models were not the most suitable for our target users.\n\n- Model Size vs. Real-World Accessibility:\nThe on-device Gemma 3N models are incredibly large. The E2B version is approximately 2.9 GB, and the E4B version is over 4.5 GB. Compiling an app with such a large asset is technically challenging and often leads to \"Out of Memory\" build errors, even on powerful development machines.\n\n- More importantly, my research and discussions during the project's development phase revealed a critical insight: many visually impaired individuals do not use expensive, high-end flagship phones. Their devices are often practical tools for calls and basic tasks. Building an app that required a modern, high-RAM device to run a 3GB model would make it inaccessible to a large portion of the very community we aim to serve. Therefore, a core requirement was to support a wide range of devices, targeting Android 8 and above.\n\n- Development & Integration Hurdles:\nFurthermore, integrating the most advanced on-device models requires the new Google AI Edge SDK. Accessing and integrating this library proved to be a significant challenge due to a combination of environmental constraints and the experimental nature of the tools, leading to persistent dependency resolution failures.\n\n**5. The Solution: An Intelligent and Pragmatic Architecture**\nFaced with these challenges, we engineered a solution that delivers the \"wow-factor\" of generative AI without sacrificing the core principles of accessibility and reliability.\n\n- Online Power with the Gemini API: When an internet connection is available, the app leverages the powerful, multimodal Google Gemini 1.5 Flash API. This allows us to offer state-of-the-art features like rich scene description, conversational follow-up questions, and global currency identification, without adding a single byte to the app's download size.\n\n- Offline Reliability with efficientdet_lite2: For the crucial offline mode, we made the strategic decision to use the efficientdet_lite2.tflite model. This model was chosen for three key reasons:\n\na. Efficiency: It is extremely small and fast, ensuring a responsive user experience and low battery consumption on a wide range of devices.\n\nb. Reliability: It is a stable, well-supported model that integrates perfectly with the MediaPipe Vision library, guaranteeing that the app's core \"seeing\" function will always work.\n\nc. Effectiveness: While not a generative model, it provides clear and essential information (e.g., \"I see a cup to your left, and a laptop in front of you\"), making it a highly effective tool in an offline context.\n\nThis hybrid approach is the project's greatest strength. It is a direct and thoughtful solution to the real-world challenges of building accessible AI.\n\n\n**6. Technology Stack:**\n- Language & Architecture: The app is built with 100% Kotlin following the MVVM pattern. A MainViewModel manages the app's state, which is observed by the MainActivity.\n\n- User Interface: A voice-first, hands-free interface built on Android's SpeechRecognizer. The entire screen acts as a single touch target for interrupts and captures. A GestureDetector is used to enable a robust, tap-only mode for offline scenarios where the speech recognizer may not be available.\n\n- Online AI:\n\nGoogle Gemini 1.5 Flash API: Used for all online generative tasks, including:\n\na. Multimodal Scene Description (Image + Text prompt)\n\nb. Conversational Follow-up Questions\n\nc. Currency Identification\n\nd. General Knowledge AI Assistant\n\ne. Conversational AI: The MainActivity intelligently distinguishes between primary commands and follow-up questions. For follow-ups, it sends the new text prompt along with the previously captured image to the Gemini API, creating a seamless conversational experience.\n\nf. Prompt Engineering: The prompts sent to the Gemini API have been carefully engineered to be direct and concise, preventing \"prompt leakage\" and ensuring the AI's responses are natural and helpful.\n\n- Offline AI (efficientdet_lite2 & ML Kit):\na. The efficientdet_lite2.tflite model is loaded from the app's assets and provides the core object detection for the offline fallback and the \"detect objects\" feature. Google's on-device ML Kit provides the fast and reliable text recognition for the \"read text\" feature.\n\nb. MediaPipe tasks-vision (Object Detection): The app uses the fast and efficient efficientdet_lite2.tflite model for reliable, on-device object detection. This model provides the core \"seeing\" capability in offline mode.\n\nc. ML Kit text-recognition (OCR): Google's powerful on-device OCR library is used for the \"Read Text\" feature.\n\nd. Audio: Android TextToSpeech Engine: The app features an intelligent TTS manager that automatically selects the highest-quality network voice when online and falls back to the best available local voice when offline. It also chunks long text to prevent cutoffs.\n\ne. Android SoundPool: Used for providing low-latency auditory feedback (e.g., the \"listening\" ding).\n\n**7. Feature Breakdown & Technical Implementation**\n- Hybrid Scene Description:\n\na. Online: The captured Bitmap is sent directly to the Gemini API with a carefully crafted prompt to generate a natural sentence.\n\nb. Offline: The captured Bitmap is first processed by the on-device ObjectDetector. The resulting list of object labels is then formatted into a simple, descriptive sentence (e.g., \"Offline mode. I see a cup, and a laptop.\").\n\n- Conversational AI & Follow-up:\n\nAfter an online scene description, the Bitmap of the last image is temporarily stored in a variable. The voice command processor is designed to recognize any phrase that is not a primary command as a potential follow-up question. The new question (prompt) and the previous image are sent together to the Gemini API, allowing it to answer questions in context.\n\n- Offline-Robust Voice Commands:\n\nThe app's SpeechRecognizer is designed to be resilient. It first attempts to use the high-quality Google speech service. Crucially, it handles the onError callback. If it detects a failure while the device is offline, it informs the user that voice commands are unavailable and seamlessly transitions the app into a \"tap-only\" mode, where core features can be activated with single and double taps.\n\n- Personalization & Onboarding:\n\nThe app uses SharedPreferences to track its launch state. On the very first launch, it triggers a detailed onboarding sequence. This sequence uses the UtteranceCompletionListener from the TextToSpeechManager to create a multi-step dialogue, first welcoming the user, and then asking for their name after the first message is finished.\n\nThis robust, hybrid architecture ensures that MySightGuide is not just a powerful AI demonstration but a reliable, real-world tool that a visually impaired user can depend on, no matter their situation.\n\n\n8. Links to Supporting Resources:\n**GitHub Repository:** https://github.com/lilyfid/MySightGuide\n**3-Minute Presentation Video**: https://youtu.be/5dJn2lxWPYk",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/5dJn2lxWPYk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/mysightguide"
    },
    {
        "project_title": "PlantGuard - Offline-First Mobile Plant Disease Detection",
        "tagline": "Democratizing agricultural AI with privacy-focused, edge-deployed Gemma 3n for instant plant health analysis",
        "authors": "jithin",
        "summary": "PlantGuard - Gemma 3n Impact Challenge Submission 🌱",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/jithinjk/plantguard",
        "demo_video_url": "https://youtu.be/m4hC-ODy66U",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/plantguard-offline-first-mobile-plant-disease-dete"
    },
    {
        "project_title": "Offline GuruVoice: Your Offline AI Assistant in a Familiar Voice",
        "tagline": "An AI-powered offline voice assistant that speaks in your favorite voice, built for rural and low-internet areas.",
        "authors": "Akhila Vengala",
        "summary": "****#",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=j6fGv7ReC2Q",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/offline-guruvoice-your-offline-ai-assistant-in-a-f"
    },
    {
        "project_title": "AI Health Assistant",
        "tagline": "An AI health assistant with highly robust privacy protection measures",
        "authors": "Mingzhe Fan",
        "summary": "This Android app can read the user's health data through the \"health connect\" function and send it to the \"gemma3n\" system that runs locally using Mediapipe, for health consultation and fitness planning.\nOf course, you can also choose other models.\nIt can read the number of steps taken, the number of floors climbed, and calorie consumption within 24 hours.\nYou can leverage the powerful intelligence and localization of Gemma3n to serve as your own personal fitness coach.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/MingzheFan/AIHealthAssistant",
        "demo_video_url": "https://www.youtube.com/watch?v=6dbTZ546UBM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-health-assistant"
    },
    {
        "project_title": "Data Chat Assistant: Offline AI for Instant Data Analysis Using Gemma 3n",
        "tagline": "Ask data questions in plain English and get instant code, answers, or charts — all offline with Gemma 3n on-device AI.",
        "authors": "shoaib ahmadi",
        "summary": "Overview:\nData Chat Assistant is an offline, on-device AI application powered by Gemma 3n and Ollama that lets users upload datasets and ask questions in natural language. It automatically generates and executes Python code, returning results and visualizations without requiring programming skills or internet access.\n\nProblem:\nIn remote areas or schools with poor connectivity, analyzing datasets is nearly impossible without technical support or cloud-based tools. This creates a barrier for education, local research, and accessibility in data literacy.\n\nSolution:\nData Chat Assistant solves this by bringing the power of LLM-based data analysis to edge devices. It uses Gemma 3n's multimodal, privacy-first capabilities to:\n\nUnderstand a dataset preview\n\nGenerate Python code based on user questions\n\nExecute that code on-device, safely\n\nDisplay results or Matplotlib/Seaborn visualizations\n\nTechnical Highlights:\n\nModel: gemma3n:e2b-it-q4_K_M via Ollama\n\nFramework: Flet (for GUI), Pandas, Matplotlib, Seaborn\n\nSecurity: The model strips data-loading lines and safely executes code in a controlled scope\n\nPlot handling: Captures charts in-memory and displays them in the UI\n\nMultithreaded processing for responsiveness\n\nOffline-ready: All processing is local and private\n\nUse Cases:\n\nStudents learning data science offline\n\nField analysts without stable internet\n\nNGOs or rural researchers making data-driven decisions\n\nAccessibility-focused education for non-coders\n\nHow It Uses Gemma 3n:\n\nLeverages Gemma 3n’s ability to process dataset previews and natural language questions\n\nUses multimodal input/output (text+charts)\n\nRuns fully on-device using Ollama, ensuring user privacy and offline use\n\nImpact:\nData Chat democratizes AI-driven analysis for people with limited resources or skills. By making Gemma 3n useful in real-world, low-connectivity scenarios, it brings AI closer to those who need it most.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/ikguQO20qLM?si=xzDRuyXqnjp8hNC7",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/data-chat-assistant-offline-ai-for-instant-data-an"
    },
    {
        "project_title": "PrivNurse AI: Revolutionizing Clinical Documentation with On-Device Intelligence",
        "tagline": "Empowering Healthcare Professionals with Secure, Offline-Ready AI that Transforms Medical Documentation While Keeping Patient Data Protected",
        "authors": "Weilin Wen, YuYao Tsai",
        "summary": "PrivNurse AI: Revolutionizing Clinical Documentation with On-Device Intelligence\n\n**Wei-Lin Wen, Yu-Yao Tsai**\n\n**[Full Technical Report](https://github.com/weilin1205/PrivNurseAI/blob/main/PrivNurseAI_Technical_Report.pdf)**\n**Code Repository:** [github.com/weilin1205/PrivNurseAI](https://github.com/weilin1205/PrivNurseAI/tree/main)  \n**Key Components:** [Speech-to-Text Server](https://github.com/weilin1205/PrivNurseAI/tree/main/ExpertAgentC_LLMServer_Nursing_Note_STT) | [Data Preprocessing](https://github.com/weilin1205/PrivNurseAI/tree/main/Data_Preprocessing) | [Model Fine-tuning](https://github.com/weilin1205/PrivNurseAI/tree/main/FineTuning_Training) | [Teacher-Student Distillation](https://github.com/weilin1205/PrivNurseAI/tree/main/Training_Data_Distillation) | [Implementation](https://github.com/weilin1205/PrivNurseAI/tree/main/privnurse_gemma3n)",
        "technologies": [
            "FastAPI"
        ],
        "source_code_url": "https://github.com/weilin1205/PrivNurseAI/blob/main/PrivNurseAI_Technical_Report.pdf",
        "demo_video_url": "https://youtu.be/gfFrrqFYB-s",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/privnurse-ai-revolutionizing-clinical-documentatio"
    },
    {
        "project_title": "AcuGemma: A Guided AI Tutoring Application with On-Device Gemma3n",
        "tagline": "AcuGemma is an Android application designed to provide personalized, interactive AI tutoring for primary school students.",
        "authors": "Isaac Owomugisha",
        "summary": "AcuGemma: A Guided AI Tutoring Application with On-Device Gemma 3n\n1. Introduction\nThe global challenge of providing quality, personalized education is particularly acute in resource-limited environments. AcuGemma is an Android application engineered to meet this challenge head-on. By leveraging the power of on-device Gemma 3n, it delivers a personalized, interactive tutoring experience that is both accessible and effective. This solution ensures that high-quality educational guidance is available anytime, anywhere, without reliance on a constant internet connection or expensive cloud infrastructure.\n\nThis technical writeup documents AcuGemma's architecture, its innovative dynamic prompting strategy for pedagogical control, and the key engineering solutions developed to overcome significant on-device implementation challenges.\n\n2. Architecture and Core Technology\nGemma 3n Integration\nAcuGemma's foundation is built upon the Gemma 3n large language model, which is integrated directly into the application. We utilized the MediaPipe LLM Inference API for this purpose, a framework optimized for on-device machine learning tasks. The model's .tflite file is bundled within the app's assets, enabling inference to run entirely on the Android device's CPU or GPU. This architectural choice delivers three critical benefits:\n\nEnhanced User Privacy: All sensitive conversation data remains on the user's device.\n\nZero Latency: Responses are generated almost instantaneously, as there is no network round-trip.\n\nOffline Capability: The application functions fully without an internet connection, making it ideal for underserved communities with limited connectivity.\n\nDynamic Prompt Strategy: The Core Innovation\nThe primary innovation of AcuGemma is its dynamic prompting system, a sophisticated framework that transforms the Gemma 3n model from a generic chatbot into a highly controlled and effective educational tutor. This strategy systematically addresses the limitations of a smaller model, such as inconsistency and topic drift, by providing a structured, step-by-step learning path.\n\nAt the heart of this system is the LessonStep data model, a lightweight, modular structure that breaks down each lesson into granular, manageable steps:\n\ndata class LessonStep(\n    val content: String,\n    val question: String,\n    val expectedAnswer: String\n)\n\nThe application employs a two-phase prompting system orchestrated by the buildDynamicPrompt function:\n\nPhase 1: Initial Prompt: The buildDynamicPrompt function crafts a concise prompt that combines a consistent \"Tutor Persona\" with the specific content and question from the current LessonStep. This prompt is sent to Gemma to initiate the conversation for that step.\n\nPhase 2: Evaluation and Progression: After the student provides an answer, the application validates their response. Instead of brittle string matching, we use the Gemma model itself as a validator. A second prompt is sent that includes the student's answer and asks the model to respond with \"CORRECT\" or \"INCORRECT.\" Based on this AI-powered validation, a final, more detailed prompt is constructed to guide the AI's final response:\n\nOn Correct Answer: The AI is instructed to provide praise and introduce the next LessonStep.\n\nOn Incorrect Answer: The AI is prompted to offer a gentle hint and re-ask the current question.\n\nThis two-phase system ensures that every interaction is relevant to the curriculum, pedagogically sound, and aligned with the lesson's objective.\n\nViewModel and State Management\nThe LessonViewModel serves as the central orchestrator, managing the UI state and the lesson flow. It tracks the student's progress using a currentLessonIndex and is responsible for calling the buildDynamicPrompt function. The ViewModel also handles asynchronous interactions with the GemmaAiService, managing UI states such as Processing, Success, and Validating to provide real-time feedback to the user.\n\n3. Challenges and Solutions\nChallenge 1: Implementing a Guided Learning Flow\nProblem: Initial attempts with broad, static prompts resulted in inconsistent AI behavior. The model would frequently diverge from the intended lesson path or provide long, unstructured responses that were not suitable for a tutoring format.\n\nSolution: We overcame this by designing the LessonStep data model and the dynamic prompting strategy. This allowed us to break down lessons into atomic components, giving the application complete control over the content, questions, and conversational flow. The AI's role was effectively shifted from a free-form generator to a guided conversational partner, ensuring a consistent and effective learning experience.\n\nChallenge 2: Robust Answer Validation\nProblem: Validating student answers proved challenging. Direct string comparison was unreliable (e.g., \"two\" vs. \"2,\" or \"2 apples\"). Furthermore, the asynchronous nature of the model's output via generateResponseAsync() sometimes resulted in partial or empty validation responses, leading to false negatives.\n\nSolution: Our solution was two-fold. First, we implemented AI-based validation, where the Gemma model itself determines if the user's answer is correct. This is far more robust than string matching. Second, we enhanced the ProgressListener to reliably accumulate all partialResult segments into a StringBuilder before evaluation. This ensured the full \"CORRECT\" or \"INCORRECT\" output was captured, and a flexible parsing logic was used to check for the presence of these keywords, making the validation resilient to minor variations.\n\nChallenge 3: Model Size and Lifecycle Management\nProblem: The .tflite model file, at over 1 GB, significantly increased the application's APK size, impacting download times. Additionally, we encountered IllegalStateException crashes when the LlmInferenceSession was prematurely closed while an inference was still running.\n\nSolution: We addressed the APK size by designing the app to be a self-contained, high-value download, emphasizing its offline capability as a key feature. For lifecycle management, we implemented careful resource handling. This involved explicitly calling cancelGenerateResponseAsync() to terminate any ongoing operations before closing the LlmInferenceSession, ensuring the application shuts down gracefully and preventing crashes.\n\n4. Conclusion\nAcuGemma represents a significant technical achievement in delivering personalized, on-device AI tutoring. The project successfully integrated the Gemma 3n model, developed an innovative dynamic prompting strategy for guided learning, and overcame critical engineering challenges related to content structuring, robust answer validation, and model lifecycle management. These engineering choices make AcuGemma a powerful and effective educational tool with the immense potential to scale to more subjects and help a greater number of students achieve their learning goals.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/cmplx-xyttmt/acu-gemma",
        "demo_video_url": "https://youtu.be/wcDmByBljbo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/acugemma"
    },
    {
        "project_title": "Nutrition app: What do you eat in a day?",
        "tagline": "Nutrition analysis for your need by sharing your daily meal pictures",
        "authors": "Manyi Yim",
        "summary": "Our nutrition app provides users with personalized dietary analysis by leveraging the power of Gemma 3n, a lightweight, on-device multimodal model. Users can input their nutrition goals and upload daily meal pictures. The app then generates a report that includes a nutritional breakdown of each meal and an overall summary of their daily intake, tailored to their specific goals. The application is designed to be private, offline-ready, and highly performant on resource-constrained devices, ensuring a seamless user experience.\n\nApp Architecture\n\nThe app's architecture is built for privacy and efficiency. Upon launch, the Gemma 3n model is loaded directly onto the device. Users set their nutritional goals via checkboxes. They can then upload images of their meals. The app processes this input in two key stages.\n* Stage 1: Meal-level Analysis. The app uses Gemma 3n to analyze each meal image. The model receives a structured prompt that combines the user's nutrition goals, the image, and the system prompt crafted by us. It then generates a detailed output for each meal, including a title, a list of food items, a nutritional breakdown, and comments based on the user's goals.\n* Stage 2: Daily Summary. The output from Stage 1 is used to generate an overall daily nutritional balance report with the Gemma 3n model. The app also extracts food items and nutritional data with Python code, compiling a holistic summary of the user's intake for the day. \nThis two-stage approach provides both granular, meal-specific feedback and a high-level daily overview.\n\nTechnical Choices and Challenges Overcome\n\nThe core technical choices were driven by the need for a private, offline-ready and resource-efficient application.\n* Why Gemma 3n? We chose Gemma 3n for its multimodal capabilities and its innovative architecture, which is specifically optimized for on-device inference. \n* The Power of mlx_vlm: To overcome the challenge of computing speed, we leveraged the mlx_vlm framework. It is designed to run models efficiently on Apple Silicon, taking full advantage of the unified memory architecture. This choice was critical for achieving a responsive user experience on consumer devices by enabling fast inference with a quantized model. We will explore other quantization frameworks in the future.\n* Prompt Engineering: A significant challenge was ensuring precise and consistent output from the model. We overcame this through extensive prompt engineering, refining the system prompts and combining them with user input to reliably guide Gemma 3n to provide accurate nutritional analysis and insightful comments. This iterative process was essential for the model to understand complex queries and generate a desired, structured output.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/myyim/nutritionApp",
        "demo_video_url": "https://youtu.be/MUil9oP8eWI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/nutrition-app-what-do-you-eat-in-a-day"
    },
    {
        "project_title": "Project Kira",
        "tagline": "Kira is a on-device AI assistant that uses a hierarchical multi-agent system to set, track, and adapt goals to the complexities of real life",
        "authors": "agus.brina, Jofe-ds",
        "summary": "Kira is a groundbreaking AI-powered assistant designed to be a personal wellness companion. Built on the principle of adaptability, Kira understands that life is dynamic, and it often falls apart without warning. She asks you about your day, your life, and how can we as a team, create a better situation to keep going. \n\nUsing Kira isn't just about achieving a goal like most productivity or wellness apps; it's about building a better relationship with yourself. It empowers you to be resilient, to forgive yourself for setbacks, and to keep moving forward. By eliminating the guilt and shame of rigid plans, Kira helps you stay on the path to self-improvement and, in doing so, can profoundly change your life for the better.\n\nWhat makes Kira unique is its core architecture, which uses a \"hive\" of specialized AI agents coordinated by a central orchestrator. Each agent is an expert in a single task, like parsing your goal from natural language, breaking it down into actionable steps, or scheduling tasks. This modular system, powered by Gemma 3n, ensures a private, secure, and responsive experience. Crucially, Kira is designed with privacy and accessibility at its core. All its processing happens directly on your device, ensuring your sensitive data remains private and making it a reliable tool even in areas with low or no internet connectivity. This on-device, offline-first approach makes Kira a truly equitable and trustworthy tool for everyone.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/imjofe/kira",
        "demo_video_url": "https://www.youtube.com/watch?v=vaTK_NmUU_4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/project-kira"
    },
    {
        "project_title": "Plant Examiner",
        "tagline": "Localized Gemma 3N Examination of Plants in Local Languages",
        "authors": "Nineso",
        "summary": "Plant Examiner\n\nI've created a fine tuned Gemma 3N model that supports image recognition for various plants allowing a more detailed translate capable response for end users.",
        "technologies": [
            "Gemma 3n",
            "vLLM"
        ],
        "source_code_url": "https://github.com/Timo614/plant-examiner/",
        "demo_video_url": "https://youtu.be/M8J4MvP3ipk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/plant-examiner"
    },
    {
        "project_title": "Phone Mate",
        "tagline": "Your always-there granddaughter to help with your phone",
        "authors": "Felix Chi",
        "summary": "**TIPS: for the technical write-up, the .apk demo and the code, please see the github link in my submission.**\n\nAn always-there AI assistant to help the elderly to use their phone.\n\nA typical scenario is booking hospital appointment online.\n\nThe APP is extremely smooth and easy to use:\n\nConfigure once and use forever (download the APP and feel it now)\n- Ask for permissions once installed\n- Automatic model downloading in the background\n- Teach your grandparents once and they can use it for all\n\nLight-weight UI and intuitve interaction\n- One button to start and one button to close\n- Floating window\n- One click to screenshot\n- Speak to ask questions\n- Audio output, can be loud",
        "technologies": [],
        "source_code_url": "https://github.com/Felix-Zhenghao/phoneMateApp",
        "demo_video_url": "https://www.youtube.com/watch?v=OPNZs4N_BzU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/phone-mate"
    },
    {
        "project_title": "SEFA.AI - Educational AI Assistant",
        "tagline": "AI-powered educational app that adapts to the user's learning style, offering personalized content and intelligent support",
        "authors": "SUELLEN R B FERRAZ, Helton Maia, Franklin Henrique Salustrino Leocádio, Maria Eduarda Souza de Lima, Artur Rochel",
        "summary": "SEFA.AI: Educational AI Assistant for Personalized Learning",
        "technologies": [],
        "source_code_url": "https://github.com/TEAM-FESA/SEFA.AI",
        "demo_video_url": "https://youtu.be/wEivLEtQxQk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sefa-ai-educational-ai-assistant"
    },
    {
        "project_title": "kRISHNA.ai",
        "tagline": "5000-Years of Ancient WISDOM with Modern AI",
        "authors": "Partha Pratim Kalita",
        "summary": "Krishna AI: Advanced Spiritual Guidance Chatbot",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/p2kalita/kRISHNA.ai",
        "demo_video_url": "https://youtu.be/8t84ODLJpw4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/krishna-ai"
    },
    {
        "project_title": "Financial Advisor Bot ",
        "tagline": "Expert Financial advice for anyone, anytime, any place. ",
        "authors": "raiyan2099, Raiyan Ahmed, Sayed Opin",
        "summary": "Our project is a multilingual RAG-powered financial advisor bot for Telegram that can answer questions in Bangla, English, and Spanish. It supports both text queries and PDF and image inputs, using OCR and dynamic context-aware retrieval for accurate financial information. The system combines dense and sparse search, LLM reasoning, cross-encoder re-ranking, and smart language detection to provide reliable, contextually relevant answers with clear document sourcing.",
        "technologies": [],
        "source_code_url": "https://github.com/opin-novice/financial-advisor-bot/tree/buildv2-temp",
        "demo_video_url": "https://youtu.be/MGUekZS7AKw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/financial-advisor-bot"
    },
    {
        "project_title": "Agri Partner KEI",
        "tagline": "Agricultural partners connecting young people in cities with farms",
        "authors": "tetty2525, yamo, haruki uchida, Imoto Hiroaki, TsukuRui",
        "summary": "Re-rooting the Family Farm with Agri Partner Kei\nA human-centred, drone-powered AI that turns grand-dad’s gut feelings into tomorrow’s farm manual.\n\n---\n\n1. WHY WE BUILT KEI\n\nJapan’s countryside is dotted with plots that have fed the same family for generations—yet many heirs now live in Tokyo apartments, miles (and mind-sets) away from the soil. They inherit land, but not the know-how: the unwritten “feel” for wind, pests, moisture, and when to worry (or relax).\n\nPain points\n• Knowledge drain – tacit wisdom retires with the elders, leaving new owners to face steep learning curves and costly mistakes.\n• Physical distance – heirs work in the city, so land sits idle or is absorbed by large co-ops.\n\nAgri Partner Kei tackles both.\n\nKei comes from three Japanese characters that all read “kei”:\n継 (inheritance)\n系 (lineage)\n繋 (connection)\nKei’s mission is to keep those three threads alive.\n\n---\n\n2. THE USER IN ONE SENTENCE\n\nUrban heirs with access to family farmland but without the time, proximity, or standards-based manual to run it confidently. We see these heirs as generation icons: once they succeed, their peers notice—and the idea of returning to farming becomes contagious.\n\n---\n\n3. SYSTEM AT A GLANCE\n\nDrone “Kei” (Raspberry Pi + flight controller) streams via Wi-Fi/BLE to an iOS app running Gemma 3n. Observation Cards flow back to the knowledge base, with optional cloud backup and sharing. A rugged mic/speaker pod lets veteran farmers speak directly into the system.\n\nDual attack on the gap\n• Know-how loss: Gemma 3n on-device LLM captures farmer conversations and converts them into structured Observation Cards.\n• Physical distance: A low-cost quadcopter streams video, runs Gemma vision models, and pushes alerts so heirs can “walk the rows” remotely.\n\n---\n\n4. HARDWARE AND FIRMWARE HIGHLIGHTS\n\n• Custom carbon arms with ESCs embedded inside for lighter wiring and cleaner airflow.\n• Raspberry Pi 4 B provides headroom for 4-bit Gemma 3n inference plus 1080p video.\n• Dual IMU (MPU-9250 + BNO055) fused with a Kalman filter cuts hover wobble by 30 %.\n• PID loop tuned on an indoor suspension rig; yaw tuning is next.\n• Current battery budget is nine minutes hover including on-board AI; trade-offs between flight time, CPU bursts, and uplink are logged for optimisation.\n\n---\n\n5. GEMMA 3N ON THE IPHONE\n\nNeed: light initial download – Solution: small shell app; models pulled on demand.\nNeed: fast chats versus heavy vision – Solution: model picker (E2B/E2B-it for chat, E4B/E4B-it for vision).\nNeed: low-RAM devices – Solution: 4-bit quantised models run in 2–3 GB RAM.\nNeed: public and private deployments – Solution: signed URLs for hosting or community mirrors.\n\n---\n\n6. OBSERVATION CARDS – THE HEART OF KEI\n\nExample structure\ncard\\_id, context (veteran or myfarm), description, images list, metadata.\n\nHow cards grow smarter\n\n1. CLIP-style contrastive tuning pairs photos with text for strong image-text search.\n2. Multi-instance captioning learns that one symptom appears at many angles.\n3. Unlabelled cluster mining auto-discovers new disease groups.\n4. Retrieval-augmented generation lets Gemma rewrite advice in “grand-pa’s voice.”\n5. Few-shot classifiers allow light models to diagnose disease directly on the drone.\n\n---\n\n7. FROM DASHBOARD TO DAILY FLOW\n\nUser interviews replaced a cluttered dashboard with a story-like home screen:\n• Morning at a glance – today’s anomalies in one card.\n• Swipe down – latest drone photos.\n• Tap to chat – ask a question and Gemma answers, citing cards.\n\n---\n\n8. HOW KEI DIFFERS FROM TYPICAL SMART-AG SOLUTIONS\n\nTarget: Urban heirs and elder mentors, not corporate farmers.\nGoal: Knowledge inheritance, not only labour or productivity gains.\nData style: Conversation plus images, not just sensor numbers.\nAI focus: Human-AI co-authoring of tacit wisdom.\nEthos: Keep the connection alive, rather than removing humans.\n\n---\n\n9. REMAINING CHALLENGES AND OUR COMPASS\n\n10. Elder accents confuse AI – fine-tune Gemma on multi-age, multi-dialect data.\n\n11. Some elders dislike smartphones – design a one-button talk pod.\n\n12. Heirs can’t be on-site – use drone auto-missions with live commentary.\n\n13. Tacit insights stay tacit – auto-generate sharable cards.\n\n14. Generation gap widens – link voices and records so grand-pa’s notes teach grand-kids.\n\n15. Cost and complexity – rely on Raspberry Pi and open-source stacks.\n\n---\n\n10. ROADMAP SNAPSHOT\n\n11. Finish yaw PID and 15-minute flight pack.\n\n12. Add GPS waypoint autonomy with obstacle avoidance.\n\n13. Phase-1 Card AI (CLIP plus captioning).\n\n14. Migrate to ROS 2 and release cloud dashboard version 1.\n\n15. Ship beta kits in three climate zones.\n\n---\n\n11. CLOSING THOUGHTS\n\nAgri Partner Kei is not just another drone, nor just another LLM demo. It translates the unwritten language of farming across generations and miles:\n\n継 — heirs inherit more than deeds; they inherit living knowledge.\n系 — that knowledge stays in the family line, structured yet personal.\n繋 — AI, IoT, and human stories weave the lineage together.\n\nIf you’re an urban heir, a veteran grower, or simply curious about AI that cares about people as much as productivity, we’d love to hear from you. The fields are waiting—and Kei is learning every day.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/team-heiwa/AgriPartnerAI",
        "demo_video_url": "https://youtu.be/pMYm7U5vbME",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/agri-partner-kei"
    },
    {
        "project_title": "Echo: Your SOS, Amplified",
        "tagline": "Echo is a privacy-first, on-device emergency mesh that turns whispered SOS into lifelines; auto-triaging, rescue connections and AI coaching",
        "authors": "Emma Khater, Bashar Nebeaa",
        "summary": "The Problem: The Silence When It Matters Most  \nI will never forget the moment the world went silent around me. During a war and a revolution, every call I tried to make failed. My phone screen lit up with “No Service,” and suddenly the street just a few houses away felt a thousand miles distant. I lay in darkness, heart pounding, alone with one question: Are they alive?\n\nThis crushing isolation is not unique to me. After the last earthquake in Turkey, my friend’s family huddled in their living room, shouting until their voices cracked—but no one answered. In Gaza, rescue teams sift through the ruins, straining to hear a faint murmur under tons of rubble. When all networks die, the darkest moments are measured not in seconds but in silence, and in that silence hope itself begins to fade.\n\nThat is the wound Echo was created to heal. We set out to build a lifeline so simple that a child—or anyone in panic—could use it without hesitation.\n\n---\n\nSolution Overview\n\n#",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Xvam13/Echo",
        "demo_video_url": "https://youtu.be/Mr5W5KUm0pM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/echo-your-sos-amplified"
    },
    {
        "project_title": "Forge Overlay",
        "tagline": "Forge is a Gemma 3n Overlay built to reduce cognitive overhead brought by webpage switching.",
        "authors": "UgoTheSlayer",
        "summary": "Imagine the web itself becoming your notebook—every insight, every question, anchored exactly where it first sparks. That’s FORGE.\nNo more juggling a dozen chat tabs or losing your train of thought in a sea of windows. With a single drag, FORGE lets you cast an AI overlay directly onto any corner of any webpage. Ask a question, translate a paragraph, refactor a snippet of code—without ever leaving the flow you’re in.\nBehind the scenes, FORGE runs light-weight Gemma 3n models locally, so answers appear instantly and your data never leaves your machine. The overlay remembers where it was summoned, even after a restart, and it travels with you as you scroll, switch tabs, or reopen the page days later. FORGE—pin your brilliance exactly where it happens.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/TheSlayer5/Forge",
        "demo_video_url": "https://youtu.be/sZwQu37amBc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/forge-overlay"
    },
    {
        "project_title": "DiaryX - Completely Private Gemma3n-AI-Powered Journaling",
        "tagline": "Your private space for memories, thoughts, and Gemma-3n-AI-powered insights",
        "authors": "xalanq",
        "summary": "DiaryX: Private AI-Powered Digital Journaling with Gemma 3n\n\n**Repository**: https://github.com/xalanq/DiaryX\n**Demo Video**: https://youtu.be/Pym1RYtLkrU\n**Live Demo**: https://github.com/xalanq/DiaryX/releases",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/xalanq/DiaryX",
        "demo_video_url": "https://youtu.be/Pym1RYtLkrU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/diaryx-completely-private-gemma3n-ai-powered-journ"
    },
    {
        "project_title": "Project-R: The Best AI Tutor",
        "tagline": "The most interactive and personalized way of learning python. Using Gemma 3n through Ollama",
        "authors": "Dhruv Sharma",
        "summary": "Project-R: Technical Architecture & Implementation Proof\n\n**AI-Powered Python Learning Companion - Technical Deep Dive**\n\n---",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Dhruv2mars/project-r",
        "demo_video_url": "https://www.youtube.com/watch?v=FLbxfNOys7s",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/project-r-the-best-ai-tutor"
    },
    {
        "project_title": "Language Tutor",
        "tagline": "Offline AI Language Tutor with Gemma 3n",
        "authors": "danil ten",
        "summary": "This project is an offline AI-powered language learning assistant designed to help users build vocabulary effectively by extracting, translating, defining, and exemplifying words from various input sources such as text, audio, and images.\n\nThe application focuses on operating entirely offline to preserve user privacy and data security, generating context-sensitive translations and definitions, providing learner-friendly example sentences, suggesting semantically related words to enrich vocabulary, and maintaining a persistent vocabulary dictionary for review and spaced repetition.\n\nThe system leverages the Gemma 3n language model running locally via Ollama, orchestrated through a LangGraph pipeline, enhanced with Retrieval-Augmented Generation (RAG) using FAISS and MiniLM embeddings.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/danil123zxc/personal_dictionary",
        "demo_video_url": "https://youtu.be/zPy4GhZbswo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/language-tutor"
    },
    {
        "project_title": "Gap Referee",
        "tagline": "Making intergenerational communication fair.",
        "authors": "Ryuichi, smurakami",
        "summary": "Issue\n\nIn Japan today, a communication gap between older generations and Generation Z has become a serious issue. This is largely due to the older generation’s inability to keep up with the drastic changes in values. \n\nIdea\n\n#",
        "technologies": [],
        "source_code_url": "https://github.com/smurakami/gemma-referee",
        "demo_video_url": "https://youtu.be/ymECIG2pzoY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gap-referee"
    },
    {
        "project_title": "Research Companion 🤖",
        "tagline": "A Multimodal, Conversational AI research assistant powered by Google's Gemma 3n.",
        "authors": "Taiki Inomata",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/anilhimam17/Gemma3n-Impact-Hackathon/tree/main",
        "demo_video_url": "https://youtu.be/atAlguWR27c?si=hgrqCFfBlYXBPAo6",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/the-research-companion"
    },
    {
        "project_title": "BitGem - World's First Offline AI-Powered Mesh Disaster Response App",
        "tagline": "No LLM? No problem. Relay AI requests through mesh network to AI-enabled devices. Emergency coordination for all, powered by few. ",
        "authors": "Vamshi_Verma",
        "summary": "🚨 BitGem: World's First Offline AI-Powered Mesh Emergency Response App\n\n> **Powered by BitChat mesh network - basic devices access AI through connected smartphones. Emergency analysis for all, powered by peer intelligence.**",
        "technologies": [],
        "source_code_url": "https://github.com/VamshiVerma/bitgem",
        "demo_video_url": "https://www.youtube.com/watch?v=Q1RnM9QXlZk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/bitgem-worlds-first-offline-ai-powered-mesh-disast"
    },
    {
        "project_title": "Tree of Trust",
        "tagline": "The AI you can finally trust. Powered by Gemma 3N for 100% private conversations, your data stays on your device, always.",
        "authors": "Greg Mousseau",
        "summary": "**Tree of Trust** is a mobile application that creates a safe, private space for personal reflection and AI-powered conversation by running Google's Gemma 3N model directly on the user's device. In an era where online data is perpetually at risk, our solution ensures that a user's thoughts and queries are protected, with roots firmly planted in on-device processing. By eliminating the connection to a central server, we eliminate the primary vector for data breaches, fostering an environment where users can genuinely trust their digital tools.\n\nThe Problem: A Crisis of Trust\n\nOnline, privacy is an illusion. Every major tech company, from social media giants to service providers, has suffered massive data breaches, exposing billions of user records. Users are justifiably wary of sharing sensitive information with cloud-based AI systems that collect and store their data. This fear creates a significant barrier to the adoption of AI for personal, meaningful tasks like mental health support, journaling, or creative brainstorming.\n\nOur Solution: Tree of Trust\n\nTree of Trust is a React Native mobile application that provides a sanctuary for private thoughts. The core features are:\n\n* **100% On-Device & Offline:** All conversations are processed locally by Gemma. No internet connection is needed, and no data ever leaves the device.\n* **Simple, Secure Interface:** A clean and intuitive chat interface designed to feel like a private, digital journal.\n* **User-Controlled Data:** Users have full and explicit control over their conversation history.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Moes-AI/gemma-3n-impact-challenge",
        "demo_video_url": "https://youtu.be/aXEgVCMYUPE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/tree-of-trust"
    },
    {
        "project_title": "Course Studio : Learning an Offline Experience",
        "tagline": "Learn anything, anywhere no internet needed.Every course adapts to you and your world.Empowering every learner, from any background",
        "authors": "mayihsk",
        "summary": "**Course Studio** exists to democratize high-quality education, delivering adaptive, interactive, and fully offline learning experiences to anyone, anywhere especially those in low-connectivity or resource-constrained environments. \nBy leveraging on-device AI (notably, *Google’s Gemma 3N* model), Course Studio empowers *schools*, *NGOs*, and *learners *in developing regions to access and customize world-class education without internet dependency or privacy risk.\n\n- Digital Divide: Billions still lack reliable internet, making most EdTech solutions inaccessible.\n- One-Size-Fits-All Pitfall: Standard courses often ignore diverse backgrounds, learning paces, and local context.\n- Privacy & Data Sovereignty: Cloud-based learning is often not viable for sensitive domains (health, government), or where privacy regulations are strict.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/shiyam-k/course-studio-app",
        "demo_video_url": "https://youtu.be/vrQjieqdUYQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/course-studio-learning-an-offline-experience"
    },
    {
        "project_title": "Kalvix: Uncovering Hidden Opportunities in Learning",
        "tagline": "Kalvix represents a transformative approach to educational technology, designed to address the major challenges faced by Students",
        "authors": "Mohamed Fazil",
        "summary": "Kalvix: AI-Powered Tamil Educational Platform - Technical Documentation\n1. Introduction\nKalvix represents a groundbreaking AI-powered educational platform specifically designed to bridge the learning gap for Tamil-speaking students in underserved communities. In an era where quality education remains inaccessible to millions of students due to language barriers and resource constraints, Kalvix emerges as a comprehensive solution that democratizes learning through advanced artificial intelligence and local language support.\n\nThe platform leverages Google's Gemini Pro and Gemini Pro Vision models to create an intelligent tutoring system that understands, processes, and responds to students in Tamil while maintaining the capability to handle English interactions seamlessly.\n\n2. Problem Statement\nEducational Challenges in Tamil-Speaking Communities\nLanguage Barrier Crisis: Millions of Tamil-speaking students struggle with STEM education delivered primarily in English, leading to poor comprehension and academic performance. Traditional educational resources often fail to bridge the gap between complex mathematical and scientific concepts and local language understanding.\n\nLimited Access to Quality Tutoring: Underserved communities lack access to qualified tutors and personalized educational support. The student-to-teacher ratio in rural Tamil Nadu often exceeds 40:1, making individualized attention nearly impossible.\n\nTechnology Gap: Most educational technology solutions are designed for urban, English-speaking populations, leaving Tamil-speaking students with limited digital learning resources that cater to their specific linguistic and cultural needs.\n\nAssessment and Feedback Limitations: Traditional assessment methods provide minimal feedback and lack the personalization needed to help students understand their specific learning gaps and improvement areas.\n\n3. Solution Architecture\nComprehensive AI-Powered Learning Ecosystem\nKalvix addresses these challenges through a multi-faceted approach combining advanced AI capabilities with user-centric design:\n\nMultimodal AI Integration: The platform integrates text, voice, and image processing capabilities through Gemini Pro and Gemini Pro Vision, enabling students to interact naturally using their preferred communication method.\n\nProgressive Web App Architecture: Built using Flask backend with responsive frontend design, ensuring cross-platform compatibility and offline functionality crucial for areas with intermittent internet connectivity.\n\nReal-time Processing Pipeline: Sophisticated request handling and response generation system that processes student queries, analyzes handwritten work, and provides contextual feedback in real-time.\n\n4. Unique Features\n4.1 Intelligent Chat Tutor\nBilingual AI Conversations: Natural language processing in Tamil and English with contextual switching\n\nSubject Expertise: Specialized knowledge in mathematics and physics with step-by-step problem solving\n\nAdaptive Learning: Personalized responses based on student's comprehension level\n\n4.2 Live Coach - Multimodal Learning\nReal-time Camera Integration: Live video feed analysis for immediate problem-solving assistance\n\nVoice Interaction: Speech-to-text processing in Tamil for hands-free learning experience\n\nMultimodal Context Understanding: Combines visual, auditory, and textual inputs for comprehensive understanding\n\n4.3 Problem Scholar - Image-based Learning\nHandwritten Problem Recognition: Advanced OCR and image analysis for handwritten mathematical problems\n\nSimilar Problem Generation: AI-generated practice problems at varying difficulty levels\n\nConcept Mapping: Intelligent identification and explanation of underlying mathematical concepts\n\n4.4 Assessment Evaluation - Comprehensive Grading\nMultiple File Upload Support: Batch processing of question and answer sheets\n\nSix-dimensional Scoring System: Evaluation across content accuracy, conceptual understanding, presentation, language quality, completeness, and methodology\n\nVisual Performance Analytics: Interactive graphs and charts showing student performance metrics\n\nDetailed Feedback System: Constructive criticism and improvement suggestions from AI examiner\n\n4.5 Progressive Web App Capabilities\nOffline Functionality: Core features accessible without internet connectivity\n\nNative App Experience: Install-able on mobile devices with fullscreen operation\n\nCamera and Microphone Integration: Seamless access to device hardware for enhanced learning\n\nPush Notification Support: Future capability for study reminders and updates\n\n5. Impact on Underserved Students Community\nBreaking Language Barriers\nKalvix fundamentally transforms how Tamil students engage with STEM education by providing explanations, solutions, and feedback in their native language. This linguistic accessibility ensures that language proficiency doesn't become a barrier to understanding complex mathematical and scientific concepts.\n\nDemocratizing Quality Education\nThe platform brings personalized, high-quality tutoring to students who previously had no access to individual academic support. Each student receives AI-powered attention that adapts to their learning pace and style, effectively creating a 1:1 tutor-student ratio.\n\nEnhancing Digital Literacy\nBy providing an intuitive, smartphone-based learning platform, Kalvix simultaneously improves students' digital literacy while delivering educational content. Students learn to leverage technology as a learning tool, preparing them for increasingly digital academic and professional environments.\n\nMeasurable Learning Outcomes\nThe comprehensive assessment system provides detailed analytics on student performance, helping identify knowledge gaps and track improvement over time. This data-driven approach ensures that learning is measurable and focused on areas needing attention.\n\nCommunity Empowerment\nTeachers and parents can use Kalvix as a supplementary tool to support classroom learning and homework assistance, creating a collaborative learning ecosystem that extends beyond traditional school hours.\n\n6. Gemini AI Integration & Magic\nAdvanced Natural Language Understanding\nKalvix leverages Gemini Pro's sophisticated language model to understand context, intent, and complexity levels in student queries. The AI doesn't just provide answers but explains concepts in ways that match the student's comprehension level.\n\nVision-Language Model Integration\nGemini Pro Vision enables revolutionary capabilities:\n\nHandwriting Recognition: Accurate interpretation of handwritten mathematical expressions and diagrams\n\nProblem Context Understanding: Analysis of visual problems with mathematical symbols, graphs, and geometric figures\n\nMulti-step Solution Generation: Visual problem analysis leading to detailed, step-by-step solutions\n\nContextual Response Generation\nThe AI maintains conversation context across interactions, allowing for follow-up questions and progressive concept building. Students can ask \"Why?\" or \"Can you explain this differently?\" and receive appropriate responses that build on previous exchanges.\n\nCultural and Linguistic Adaptation\nGemini's training enables understanding of Tamil cultural contexts and educational patterns, ensuring that examples, analogies, and explanations are culturally relevant and easily understood by Tamil students.\n\n7. Technical Implementation Highlights\nBackend Architecture\nFlask Framework: Lightweight, scalable web framework optimized for educational applications\n\nRESTful API Design: Clean separation of frontend and backend enabling future mobile app development\n\nMultimodal Processing Pipeline: Integrated handling of text, image, and potential audio inputs\n\nError Handling and Logging: Robust error management ensuring reliable user experience\n\nFrontend Excellence\nResponsive Design: Optimized for mobile-first usage with seamless desktop compatibility\n\nProgressive Enhancement: Core functionality works on basic devices with enhanced features on modern browsers\n\nAccessibility Features: Designed for students with varying technical literacy levels\n\nPerformance Optimization\nCaching Strategies: Intelligent caching for improved response times and reduced data usage\n\nOffline Capability: Service worker implementation for core functionality without internet\n\nImage Optimization: Efficient image processing and storage for camera-based features\n\n8. Future Scope & Roadmap\nImmediate Enhancements (Next 6 Months)\nKnowledge Graph Integration: Neo4j-based concept mapping for advanced learning path personalization\n\nVoice Recognition Improvement: Enhanced Tamil speech-to-text accuracy and dialect recognition\n\nExpanded Subject Coverage: Addition of chemistry, biology, and social sciences modules\n\nParent/Teacher Dashboard: Analytics and progress tracking interface for educators and families\n\nMedium-term Development (6-18 Months)\nAdaptive Learning Algorithms: Machine learning-based personalization that improves with usage\n\nCollaborative Learning Features: Peer-to-peer learning and group problem-solving capabilities\n\nGamification Elements: Achievement systems and learning streaks to increase engagement\n\nAdvanced Assessment Analytics: Predictive analytics for learning outcome improvement\n\nLong-term Vision (18+ Months)\nMulti-language Expansion: Extension to other Indian languages (Telugu, Malayalam, Kannada)\n\nVR/AR Integration: Immersive learning experiences for complex mathematical and scientific concepts\n\nAI Tutor Personality Development: More human-like interaction patterns and emotional intelligence\n\nIntegration with Formal Education: Partnerships with schools and universities for curriculum alignment\n\nScalability Considerations\nCloud Infrastructure: Migration to scalable cloud platforms for handling increased user loads\n\nAPI Monetization: Potential licensing of AI tutoring capabilities to educational institutions\n\nResearch Partnerships: Collaboration with academic institutions for continuous improvement and validation\n\n9. Impact Measurement & Success Metrics\nStudent Performance Indicators\nLearning Outcome Improvement: Pre and post-platform usage assessment scores\n\nEngagement Metrics: Daily active users, session duration, feature utilization rates\n\nLanguage Preference Analytics: Tamil vs. English usage patterns and learning effectiveness\n\nProblem-solving Success Rates: Accuracy improvement in mathematical problem solving\n\nCommunity Impact Assessment\nAccessibility Improvement: Number of underserved students gaining access to quality tutoring\n\nTeacher Support Enhancement: Reduction in teacher workload and improvement in classroom effectiveness\n\nDigital Divide Bridging: Smartphone-based learning adoption rates in rural communities\n\nKalvix represents a transformative approach to educational technology, specifically designed to address the unique challenges faced by Tamil-speaking students in underserved communities. By combining cutting-edge AI technology with deep understanding of local educational needs, the platform creates an unprecedented opportunity for educational equity and excellence.",
        "technologies": [
            "Flask"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/UPfT-U_fplQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/kalvix-uncovering-hidden-opportunities-in-learning"
    },
    {
        "project_title": "Edge Learn",
        "tagline": "On device AI assisted education ",
        "authors": "Abdulhadi Hashim",
        "summary": "**Summary**\n\nI initially chose Svelte because of I wanted to try tauri, thinking i could write once and build for all platforms. LOL. The plan was to use MediaPipe for the machine learning part. But right before the deadline, I hit a major roadblock: a well-known ArrayBuffer error and the discovery that MediaPipe didn't yet support the Gemma 3n model for web, so i couldnt use pure webview for everything.\n\nThis meant I had to switch completely.\n\nFor Android, I wrote native Kotlin code to bridge the gap between the Svelte frontend and the MediaPipe framework.\n\nFor Windows, macOS, and Linux, I took a different route, using the ONNX runtime with the ort crate in Rust to handle the machine learning directly.\n\n**How It's Built**\nFrontend: The user interface is built with the Svelte framework for a fast and smooth experience.\n\nDesktop App: Tauri is the backbone, providing a secure and lightweight way to package the app for desktops.\n\nMobile App: I integrated MediaPipe directly into the Android native code with a kotlin plugin i wrote.\n\nDatabase: SQLite is used to store all the content and user data right on the device.\n\n**The \"Smart\" Features**\n*Machine Learning**: The app runs on the Gemma 3N LLM. On Android, this works via a custom Kotlin library, while on desktop, it uses the ONNX library through the ort crate.\n\n**Voice Interaction**: For speaking and listening, it uses the Web Speech API through Tauri. A library called vad-web handles voice activity detection, so the app knows exactly when you're talking.\n\n**Language Practice**: When you pick a language, the app pulls words from a bundled file and uses prompt engineering to create a conversation with you using the LLM and text-to-speech. It currently supports languages that Gemma 3N works with: Japanese, German, Korean, Spanish, and French.\n\n**Lesson Generation**: I use prompt engineering and regex to get content for quizzes, flashcards, and lesson plans, which are then parsed and saved to the local database.\n\n\nexecutable available in github link",
        "technologies": [
            "Gemma 3n",
            "Web Speech API"
        ],
        "source_code_url": "https://github.com/APerson101/aitutor",
        "demo_video_url": "https://youtu.be/Go-ed7KO_c4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/edgelearner"
    },
    {
        "project_title": "Memento.ai",
        "tagline": "Life Remembers Itself.",
        "authors": "MoEssamKhattab",
        "summary": "**The Problem**\nIn today's fast-paced world, it's easy to forget important events and overlook small details. For many, this is a minor inconvenience, but for individuals living with Alzheimer's disease, it's a significant daily challenge. The need for a reliable, private, and easy-to-use memory support tool has never been greater.\n\n**Our Solution: Memento**\nMemento is an innovative, AI-powered mobile application designed to act as a digital memory assistant. It helps users manage their daily lives by organizing events, providing smart reminders, and even answering questions about their past activities.\n\nOur primary goal is to support the general public while placing a special emphasis on enhancing the independence and quality of life for those with Alzheimer’s.\n\n**Key Features & Technology**\n- Intelligent Task Management: Seamlessly schedule events and receive timely, intelligent reminders.\n\n- Memory Recall: Ask the app questions like, \"What did I do last Tuesday?\" and get instant, accurate answers.\n\n- Complete Privacy: Memento is powered by Google's Gemma 3n, a state-of-the-art language model that runs directly on your device. This means all your data stays with you, ensuring complete privacy and offline functionality—no internet connection required.\n\n- User-Friendly Design: Through extensive usability testing, we've confirmed the app is simple and effective for everyone, including elderly users who may not be tech-savvy.\n\n**Impact & Vision**\nBy providing a dependable memory aid, Memento empowers users to live more independently and reduces the stress on caregivers. Our project aligns with ethical AI principles, offering a sustainable and intelligent solution to the universal challenge of memory loss and the specific needs of Alzheimer's care. Memento isn't just an app; it's a step toward a better, more supported daily life.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/MoEssamKhattab/Memento_ai",
        "demo_video_url": "https://youtu.be/a9thgScl78k",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/memento-ai"
    },
    {
        "project_title": "# Gemma-3n-4b- Academic papers-finetuning-inference:   LoRA-Augmented Fine-Tune",
        "tagline": "We present a novel workflow for the parameter-efficient fine-tuning (PEFT) of a 4-bit quantized multimodal large language model (LLM), Gemma",
        "authors": "Handsonlabs Software Academy",
        "summary": "Gemma-3n-4b- Academic papers-finetuning-inference: \nLoRA-Augmented Fine-Tuning of a 4-bit Multimodal Language Model on Scientific Literature\n\nABSTRACT\n\nWe present a novel workflow for the parameter-efficient fine-tuning (PEFT) of a 4-bit quantized multimodal large language model (LLM), Gemma 3N-E4B, on a curated corpus of 20 state-of-the-art research papers in AI, climate science, healthcare, and computer vision. Leveraging LoRA adapters, we freeze the majority of the backbone weights and fine-tune only low-rank updates in attention and MLP modules, achieving substantial memory savings. We introduce a robust PDF download and parsing module using streaming requests and PyPDF2 to extract full-text content at scale. To address version conflicts in a heterogeneous dependency environment (Colab vs. local), we propose an ordered, pinned installation sequence that ensures reproducible environments. Our training regime—1 GPU, 4-bit weights, batch size 1 with gradient accumulation—completes 40 LoRA steps under early-stopping criteria, consuming under 8 GB of GPU memory. We report fine-tuned performance across training, validation, and holdout sets using a novel mixed relevance/overlap accuracy metric, achieving 82.4% average accuracy and demonstrating strong generalization (Δ < 5%). Finally, we release our code, LoRA adapters, and detailed memory and performance plots, providing a blueprint for resource-constrained scientific LLM fine-tuning.\n\nKeywords\nLoRA, 4-bit quantization, Gemma 3N, multimodal LLM, PDF parsing, PyPDF2, PEFT, early stopping, memory efficiency, reproducible dependencies",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/dZBVMdToN_k",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-3n-4b-academic-papers-finetuning-inference-l"
    },
    {
        "project_title": "Auralia - Voice Assisstant",
        "tagline": "Auralia is an offline Android voice assistant using Gemma 3n to help blind users control their phone and daily tasks entirely by voice.",
        "authors": "mouhssine ELH, STBilal, NHO1902, Said Nichan, Meryeme Boussaid",
        "summary": "**Auralia** is a fully **offline Android voice assistant** specifically designed to empower *blind and visually impaired users* with private, hands-free access to their smartphones. It integrates speech recognition, natural language understanding, and task automation to enable users to perform everyday actions, such as setting alarms, sending SMS, making calls, opening apps, or navigating the web, using only their voice, without needing to look at or touch the screen.\n\nAuralia operates entirely offline, preserving user privacy by using the Gemma 3n AI model via the Ollama framework for natural language processing directly on the device. The wake word \"Hey Gemma\" activates the assistant instantly, and real-time feedback is provided through natural-sounding text-to-speech, haptic signals, and audio cues.\n\nTo ensure accessibility, Auralia includes an integrated accessibility service, optimized interface components using Jetpack Compose, and a voice-based navigation system adapted to blind users. Features such as multilingual support, noise cancellation, speech-to-text transcription, and offline command processing make Auralia reliable, even in low-connectivity environments.\n\nBy prioritizing privacy, usability, and independence, Auralia represents a step forward in inclusive mobile technology, turning any Android device into an intelligent assistant tailored for those who rely on voice over vision.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/BilalMagg/Auralia",
        "demo_video_url": "https://youtu.be/f5UEdRCF6f4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/auralia-voice-assisstant"
    },
    {
        "project_title": "Gemma app for inference",
        "tagline": "Android Flutter App for gemma inference",
        "authors": "Fazli Gulomqodirov",
        "summary": "Inference on android gpu using .task gemma 3n model",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/fazliddin2001/ai-gallery-with-history",
        "demo_video_url": "https://youtu.be/m_uAv8h46_Y?si=zjnpPYvtBhk_Q0k3",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-app-for-inference"
    },
    {
        "project_title": "SahaGem (Teaching Assistant)",
        "tagline": "Empowering Low-Resource Classrooms with Offline AI-Powered Teacher Aids",
        "authors": "Tejaswini",
        "summary": "",
        "technologies": [
            "Flask",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Winni28/SahaGem-App",
        "demo_video_url": "https://youtu.be/74NpugMpslA?si=KFqNzaLOIV5OZY5b ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sahagem-teaching-assistant"
    },
    {
        "project_title": "Luckstone AI Maintenance Assistant (LAMA) Offline",
        "tagline": "Luckstone AI Maintenance Assistant Offlne for troubleshooting",
        "authors": "mon gunjon",
        "summary": "Project Name: LAMA (LuckStone AI Maintenance Assistant Offline)\nDevelopment Team: Masters in Data Analytics, VCU Richmond, VA - Group 3 Team  (December 2025)\nClient: Luck Stone Corporation (www.luckstone.com)\nCode Repository: GitHub Repository\nhttps://github.com/somnathdas75/Gemma3nKaggle\nMy code: \nExecutive Summary\nLAMA is an offline-capable AI assistant designed to revolutionize equipment maintenance and troubleshooting for Luck Stone Corporation. Built around Google's Gemma 3N model, this system provides real-time, context-aware assistance to field technicians by combining Retrieval-Augmented Generation (RAG) with multimodal capabilities (text, image, and voice). The solution addresses critical pain points in industrial maintenance operations, including:\n•\tReducing equipment downtime through faster diagnostics\n•\tMinimizing reliance on senior technicians for routine issues\n•\tPreserving institutional knowledge in a searchable format\n•\tOperating in remote quarry locations without internet connectivity\n💡 Product Name: LAMA (LuckStone AI Maintenance Assistant)\n________________________________________\n🚀 Core Capabilities (Gemma 3N Usage)\nFeature\tDescription\tGemma 3N Role\n📄 Document QA\tUses technical manuals, SOPs, fault codes, error logs\tRAG over local documents, optimized with PLE\n📷 Image Analysis\tMechanics can take a photo of a faulty part\tImage + text multimodal reasoning\n🗣️ Voice Query\tSpeak problem description into tablet/mobile\tAudio-to-text, contextual voice QA\n🕵️ Fault Finder\t\"What does error code 413 mean on CAT loader?\"\tLocal semantic search + reasoning\n📈 Trend Reports\tBased on Zendesk tickets: mean time to resolution, issue recurrence\tVisualization, clustering insights from offline CSV\n🔄 Continuous Learning\tUpload new manuals/fault logs and retrain embeddings locally\tSelf-updating vector database",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/aIYcQMiHQio",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/lama-luckstone-ai-maintenance-assistant-offlne"
    },
    {
        "project_title": "Beyond the Network: Gemma 3N, an Offline Guide for Every Explorer",
        "tagline": "An offline mobile app for adventurers and explorers, providing on-device Gemma 3N AI for mental health, first aid, and survival guidance.",
        "authors": "Kaggle@ck01",
        "summary": "#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/barada02/gemmainflutter.git",
        "demo_video_url": "https://youtu.be/0n3wNx0hack",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/beyond-the-network-gemma-3n-an-offline-guide-for-e"
    },
    {
        "project_title": "GemmaMiniApps",
        "tagline": "It transforms how you interact with AI. Instead of static prompts, create programmable workflows that become fully-functional Mini Apps",
        "authors": "wenkai huang",
        "summary": "",
        "technologies": [],
        "source_code_url": "https://github.com/CatKevin/GemmaMiniApps",
        "demo_video_url": "https://youtu.be/8m72CDf0N-k",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemmaminiapps"
    },
    {
        "project_title": "Gemma V: Your Voice is the Browser",
        "tagline": "How we built a fully conversational AI assistant on a laptop, using Gemma 3n to give the visually impaired voice-driven access to the web.",
        "authors": "qubitization",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/qubitization/gemma-v",
        "demo_video_url": "https://www.youtube.com/watch?v=e6HVVWYjdtw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-v-your-voice-is-the-browser"
    },
    {
        "project_title": "LeRepairBot: Multimodal Assistant to Help Repair Electronics and Reduce E-Waste",
        "tagline": "Empowering anyone to repair electronics locally using on-device, offline Gemma 3n guidance to reduce e-waste",
        "authors": "Ankithreddy",
        "summary": "Every year, we generate over 50 million tonnes of electronic waste. Much of it is caused by small, repairable issues like a broken port, a loose wire, a dead battery. But instead of fixing, we throw things away. As sometimes, replacing is easier, faster, cheaper. But what we often don’t realize is the true cost.\n\nThat discarded device ends up in a landfill, polluting soil, water, and harming communities far away from where it was used\nAs someone passionate about sustainability and electronics repairability .I built LeRepairBot: An offline, repair assistant powered by Gemma 3n and LeRobot arm. It guides users through real-time repairs using retrieved repair guides and multimodal understanding . \n\n**Build process :**\n\na. Dataset preparation:\nI started by creating a [repairdataset](https://huggingface.co/datasets/ankithreddy/repairdataset-mini) of real repair guides and pushed it to Hugging Face. \n\nb. Fine-Tuning Gemma 3n (Vision + Text)\nThen I fine-tuned Gemma3n on both modalities using my dataset. I was excited to see the training loss decrease, signaling that the model was learning. \n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2331282%2F047218067e153f24c105417e62b66e69%2FScreenshot%202025-07-31%20at%2010.02.24AM.png?generation=1753976565652625&alt=media)\n\nBut I realised true measure of fine tuning effort is not training loss , but how the model responds. The outputs were slightly better than the base model and showed some understanding of repair concepts, but they lacked precision. The model returned generalized answers, not the exact tools and steps, which is critical for real repair tasks\n\nc. Switching to RAG for precision\nThat's when I moved to Retrieval Augmented Generation ,semantically fetching product specific instructions from the dataset .This approach gave me the accuracy and grounding I needed. \n\n[gemma3n_RAG_notebook](https://www.kaggle.com/code/ankithreddy/gemma3n-multimodal-rag)\n\n\nd. Robot Integration \nI assembled the LeRobot s0101 arm and began recording datasets through teleoperation, starting with simple tasks like “pass the screwdriver.”\n\n![datasetrecording](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2331282%2F6946423b91f084ed7253a73c97b20422%2Frerunscreenshotlerobot.png?generation=1754123935329628&alt=media)\n\n\ne. Gemma 3n as a VLA Model\nNext, I worked on turning Gemma 3n into a Vision-Language-Action (VLA) model to control LeRobot based on task and visual input with the dataset I recorded previously .\n[added_gemma3nvla: github](https://github.com/ankithreddypati/lerobot/tree/adding_gemma3nvla)\n\nI have recorded 100 episodes of robot doing the task with teleoperation\n[Datasets](https://huggingface.co/datasets/ankithreddy/so101_pickplace_tools)\nThe dataset includes 100 recordings at 1280×720 resolution, 30 FPS picking up and placing in box.\n\nI fine-tuned the gemma3n VLA model on this dataset for 20,000 steps, which took approximately 18 hours on an NVIDIA A100 GPU.\n[colabFinetunenotebook](https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/ankithreddy/gemma3nvla-lerepairbot-v0.a30562dd-3898-4c4a-9fa9-54e992b29efb.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250806/auto/storage/goog4_request%26X-Goog-Date%3D20250806T232032Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D771a337b23b6c2da4247ae6b59eb79d28361749b162b3839dc01b0f83659a05147a1a316c5b1615e3e3aa5c42b2d79c1d7adfab6dea2ebbb79a317f4a57dc53f169d2ffc8e9900ba2cc4480862e7d934cc826cebfaff0ece6ba644a072bdc83830a2229d77d75fc24f5935d801a32f496b13e1dc38c96b812a341c682069dbca6f8b8b268a8c2f9a2b1751bff689bf326cb79030e372bba4df27428fe1acb16013418493baf7d5f99c69bf18b74c0928e51aeafaba59375907a1a84e814ab8729d81b13b3b6e52318f87104ff150df88db3fd40b2b211d6aa84658cf2acd48ca5010b4e2696a2642026f256a5296206ef0d6fda2e822d5a92049a5cb6c2e1922)\n[kaggleFinetunenotebook](https://www.kaggle.com/code/ankithreddy/gemma3nvla-lerepairbot-v0)\n[modelevaluationepisode](https://huggingface.co/spaces/lerobot/visualize_dataset?path=%2Fankithreddy%2Feval_repair_2%2Fepisode_0)\n\n\nAsync inference:\nI was able to do async inference of the task with gemm3nvla\n\n![Async inference](https://www.youtube.com/watch?v=PfZar3Z35c0)\n\n\n\nf. local inference \nTo keep everything fully offline, I ran inference using mlx-vlm which supported multimodal inference \n\n![localinference](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2331282%2F5262992d9ab482cca75a04fb3cf3010b%2FScreenshot%202025-08-06%20at%207.06.28PM.png?generation=1754521630595590&alt=media)\n\n[mlxvlmlocalinference](https://github.com/ankithreddypati/repairbotmlx-inference)\n[gemma3nvlalocalrocminference](https://github.com/ankithreddypati/lerobot/tree/adding_gemma3nvla)\n\ng. Note:\na. Library compatibility: I used transformers==4.55.0.dev0 during development. I later found that newer versions throws errors",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/ankithreddypati/lerepairbot",
        "demo_video_url": "https://youtu.be/NB89v6ZwTPQ?si=T8IZk4WMTAztNM5R",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/lerepairbot-multimodal-assistant-to-help-repair-el"
    },
    {
        "project_title": "Gemma Kavach: AI-Powered Safety Suite for Crowded Events",
        "tagline": "AI to prevent stampedes and manage chaos at crowded public events",
        "authors": "ADITYA BHATT",
        "summary": "**Author :Aditya Bhatt**\n**Date :08-July-2025**\n\n**Abstract**\n\nLarge-scale public gatherings in India frequently become sites of crowd-related disasters — over 3,000 people lost their lives in stampedes between 2001 and 2022, and 121 fatalities occurred in a single crowd crush in Hathras in July 2024 [1](https://factly.in/data-over-3000-lives-lost-in-stampedes-between-2001-2022/?).We present Gemma Kavach, an AI-powered safety suite designed to proactively manage high-density events. The system features:\n\n1. Real-time video analysis to detect crowd density, motion anomalies, and panic indicators\n\n2. A voice-based system that delivers instant Hindi-English alerts and allows authorities to issue hands-free updates\n\n3. A citizen-facing app enabling the public to report emergencies like lost children, medical needs, or fire\n\nPowered by Google’s Gemma 3n multimodal model, fine-tuned via Unsloth, and deployed efficiently using both its lightweight and high-capacity modes, Gemma Kavach empowers event staff and attendees to detect threats and respond in real time — reducing risk and preventing tragedy.\n\n**Introduction**\n\nMass public events such as religious gatherings, political rallies, and festivals in India often attract tens of thousands of people into dense, high-risk zones. These environments are highly prone to chaos, panic, and crowd crush incidents. Traditional public safety mechanisms — including CCTV monitoring, manual announcements, and delayed ground response — are insufficient in identifying early signs of danger or mobilizing quick action.\n\nTo address this urgent public safety need, we built Gemma Kavach, an AI-powered safety suite that leverages real-time video understanding, voice-mode operations, and a crowd-sourced incident reporting system to deliver intelligent, scalable crowd management.\n\nOur solution is built on Gemma 3n , a new-generation multimodal model from Google, optimized for real-time performance on devices such as phones, laptops, and tablets. Gemma 3n includes innovations like Per-Layer Embedding (PLE) and the MatFormer architecture, which enable parameter-efficient processing and reduce compute and memory demands — making it ideal for real-time edge deployments.\n\nThis problem domain was chosen deliberately because traditional AI and surveillance systems are too heavy, too cloud-dependent, and often unaffordable at scale — especially in countries like India where digital infrastructure adoption is still uneven. Despite a growing population and increased mobile usage, many Indian regions have yet to fully embrace cloud computing. For such environments, AI models must be both cost-efficient and capable of running inference locally or with minimal connectivity, without sacrificing real-world responsiveness.\n\nGemma Kavach fills this gap by pairing powerful AI perception with practical deployment, enabling both event authorities and everyday citizens to play an active role in crowd safety — before disaster strikes.\n\n**Background**\n\nThe cost of inaction in public safety is often measured not in rupees, but in human lives. For years, deploying AI-driven solutions in real-world environments remained impractical due to the high computational cost, internet dependency, and cloud-centric infrastructure requirements of large-scale models.\n\nHowever, with the advent of open-source, multimodal models like Gemma 3n, we now have the capability to run powerful AI inference directly on-device at near-zero cost. This marks a significant shift: developers can now fine-tune, align, and deploy models to suit hyper-local needs, rather than relying on expensive, rigid cloud pipelines.\n\nIn a country like India, where infrastructural limitations, budget constraints, and regional diversity are constant challenges, the winning formula is simple — lightweight, mobile-first, and multimodal AI that works seamlessly with local languages and hardware. With Gemma 3n, we finally have the tools to solve previously unreachable problems, and to do so in a way that is scalable, inclusive, and impactful.\n\n**System Architecture Overview**\n\nGemma Kavach is implemented as a comprehensive AI-powered crowd safety suite built on Google's Gemma 3n multimodal model. The system architecture follows a modular design with four interconnected components: a centralized Gemma 3n inference server, a real-time crowd monitoring system, a voice command interface, and an emergency reporting application. This design enables seamless integration while maintaining the flexibility to deploy each component independently based on operational requirements. The overall architecture leverages Gemma 3n's multimodal capabilities to process text, image, and audio inputs through a unified inference pipeline. The system is designed for edge deployment, ensuring low-latency responses and offline functionality - critical requirements for crowd safety applications where network connectivity may be unreliable during emergencies.\n\n**Gemma 3n Server Infrastructure**\n\nThe core of Gemma Kavach is built around a centralized Gemma 3n inference server that provides unified multimodal processing capabilities. We utilized the unsloth/gemma-3n-E4B-it model variant, specifically chosen for its optimized performance on edge devices and enhanced multimodal understanding capabilities. The server implementation uses FastAPI as the web framework, providing three distinct endpoints for different modalities: /generate for text-only processing, /ask_image for vision tasks, and /ask for audio transcription. A critical design decision was the implementation of a universal generation function that uses the raw tokenizer for all requests, eliminating template conflicts that typically arise when mixing text-only and multimodal operations. The model configuration employs automatic precision detection with 4-bit quantization enabled for optimized edge deployment while maintaining multimodal compatibility. This approach ensures consistent performance across all three modalities while preserving the model's ability to process interleaved audio, text, and image inputs. The server architecture supports both sampling-based generation for creative tasks and greedy decoding for consistent classification outputs. For deployment optimization, we implemented aggressive compilation disabling and CUDA launch blocking to ensure stable inference on resource-constrained devices.\n\n**Crowd Monitoring Vision Solution**\n\nThe crowd monitoring system represents the core safety component of Gemma Kavach, designed to provide real-time analysis of crowd density and motion patterns. The system implements a dual-analysis approach that simultaneously evaluates crowd density levels and motion behavior patterns to generate comprehensive risk assessments. The vision processing pipeline captures video frames at 2-second intervals and submits them to the Gemma 3n server for concurrent analysis using specialized prompts for crowd density classification (Low/Medium/High) and motion behavior analysis (Calm/Chaotic). This dual-prompt approach allows for more nuanced risk assessment compared to single-metric systems. Risk determination follows a matrix-based algorithm where chaotic motion combined with high density triggers critical alerts, while isolated high density or chaotic motion results in moderate risk classifications. The system maintains detailed analytics including frame-by-frame analysis history, flagging rates, and temporal risk progression. Session management is implemented through Google Cloud Storage, enabling persistent monitoring across multiple operators and locations. The system automatically saves flagged frames with high-risk classifications and maintains detailed metadata for post-incident analysis. Email alerting is triggered when risk scores exceed 70% or when multiple critical frames are detected within a short timespan, with comprehensive alert emails sent via Gmail SMTP containing analysis breakdowns, flagged frame attachments, and detailed risk assessment reports. The frontend interface provides real-time visualization of risk metrics, session statistics, and analysis breakdowns through a cyberpunk-themed responsive design optimized for both desktop and mobile deployment scenarios.\n\n**Voice Command System**\n\nThe voice command system addresses the need for hands-free operation during crowd management scenarios, particularly crucial when security personnel need to obtain quick status updates without diverting attention from crowd monitoring activities. The system implements a complete speech-to-speech pipeline supporting Hindi and English voice commands. Audio processing begins with browser-based MediaRecorder API, with format prioritization optimized for mobile devices - particularly iOS Safari which requires MP4 format support. Audio conversion to WAV format is performed client-side using Web Audio API to ensure server compatibility. Voice transcription leverages Gemma 3n's audio processing capabilities through multimodal message formatting, where audio files are processed alongside textual prompts for context-aware transcription. The system implements intelligent zone information extraction using language model-based parsing to identify specific areas (Zone A, B, C, D) from natural language voice commands. Database integration connects voice queries to real-time crowd monitoring session data, enabling voice-based status retrieval for specific zones. Response generation uses specialized prompts to create natural Hindi language responses that are subsequently converted to speech using Google Text-to-Speech API. Mobile compatibility required extensive optimization for iOS and Android platforms, including audio format compatibility testing, user interaction requirements for audio playback, and alternative playback mechanisms for restrictive browser environments.\n\n**Emergency Reporting Application**\n\nThe emergency reporting application provides a citizen-facing interface for reporting various emergency situations commonly encountered in large gatherings. The system implements a sophisticated text classification model fine-tuned specifically for emergency categorization in Indian contexts. The classification model was developed using Unsloth framework for efficient fine-tuning of the Gemma 3n E2B model on a dataset of 60,000 emergency scenarios across six categories: child_lost, crowd_panic, lost_item, medical_help, need_interpreter, and small_fire. The fine-tuning process employed LoRA (Low-Rank Adaptation) with rank 16, alpha 16, and dropout 0.1 to achieve efficient parameter updates while maintaining model stability. The application workflow integrates text classification with image capture and geolocation data to create comprehensive emergency reports. Upon submission, the system performs immediate AI classification of the emergency description, captures and stores associated images in Google Cloud Storage, and generates unique report identifiers for tracking purposes. Background processing handles email alert generation with embedded images and detailed emergency information sent to emergency response teams including classification results, location data, timestamp information, and attached photographic evidence via Gmail SMTP integration. The frontend implementation uses a progressive web application approach with drag-and-drop image upload, real-time form validation, and step-by-step processing visualization providing immediate feedback on classification results and confirmation of successful emergency report submission.\n\n**Technology Stack and Integration**\n\nThe complete Gemma Kavach system integrates multiple technologies to deliver a cohesive crowd safety solution. The backend infrastructure centers on FastAPI servers providing RESTful APIs for each system component, with shared dependencies including Google Cloud Storage for data persistence and Google Text-to-Speech for audio generation. Frontend implementations vary by use case: the crowd monitoring system employs a real-time dashboard with polling for live updates, the voice command system uses Web Audio APIs for browser-based audio processing, and the emergency reporting application implements a progressive web app with offline capabilities. Data persistence is managed through Google Cloud Storage with structured JSON formatting for session data and binary storage for images and audio files. The system implements automatic cleanup procedures and configurable retention policies for long-term data management. Integration points between components include shared session identifiers, common alert mechanisms, and unified reporting formats. The modular architecture allows for independent scaling of components based on operational demands while maintaining consistent data formats and API contracts across the entire system. The deployment architecture supports both cloud and edge deployment scenarios, with the Gemma 3n server capable of running on GPU-equipped edge devices while maintaining connectivity to cloud storage and notification services for comprehensive functionality.\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F5141429%2F1196e1041d061d12a2957124eeeadf6b%2Ftech_arch.PNG?generation=1751983897334670&alt=media)\n\n**Technical Challenges**\n\nThe development of Gemma Kavach involved solving critical real-world problems that traditional surveillance and emergency response systems fail to address effectively. The project began with the recognition that existing crowd monitoring relies heavily on human operators who suffer from attention fatigue and cannot simultaneously monitor multiple video feeds effectively. Traditional CCTV systems lack the intelligence to automatically detect early warning signs of crowd disasters, often resulting in delayed responses when seconds matter most. The first major challenge was establishing a stable multimodal inference server capable of handling text, image, and audio processing reliably. Initial attempts using standard transformers libraries resulted in frequent infinity and NaN errors due to Gemma 3n's FP16 numerical instability issues, particularly with convolutional weights that exceed float16's maximum range of 65,504. This critical issue was resolved by migrating to Unsloth's optimized framework, which provides stable bfloat16 operations specifically designed for Gemma models, eliminating the numerical overflow problems while maintaining model performance.\n\nThe crowd monitoring system development revealed the need for more sophisticated analysis beyond simple object detection. Initial single-metric approaches proved insufficient for accurately assessing crowd safety, leading to the innovation of the dual-analysis framework that separately evaluates crowd density and motion patterns. This approach significantly improved accuracy by distinguishing between high-density but calm situations (which are manageable) versus chaotic motion in any density (which requires immediate attention). The development process involved extensive testing to optimize the risk assessment matrix, determining that chaotic motion combined with high density represents the highest risk scenario requiring critical alerts, while isolated factors result in moderate risk classifications.\n\nThe voice command system presented unique challenges related to Indian accents and Hindi language processing in noisy environments typical of large gatherings. Initial experiments with Gemma 3n's native audio capabilities showed poor performance for Indian English and Hindi, particularly in environments with background noise. Following Google's official audio preprocessing guidelines (16kHz resampling, mono conversion, float32 normalization) produced unexpected results where preprocessing improved performance on poor-quality audio but degraded performance on high-quality recordings. This led to the critical insight that adaptive preprocessing based on audio quality assessment was necessary, fundamentally changing the approach from universal preprocessing to intelligent, context-aware audio handling. \n\nThe system now implements a multi-stage pipeline with intelligent zone extraction and database integration for real-time status retrieval.\n\nThe emergency reporting application required solving the challenge of creating an intelligent classification system for diverse emergency scenarios in Indian contexts. The breakthrough came through fine-tuning a separate Gemma 3n E2B model using synthetic training data generated specifically for Indian emergency scenarios. The initial training attempt with 600 samples showed minimal learning, but scaling to 60,000 samples (a 100x increase) dramatically improved classification accuracy, validating the importance of data volume in compute-optimal training as described in the Chinchilla paper. The fine-tuning process using Unsloth's LoRA optimization revealed critical insights about training stability on cloud platforms, requiring the implementation of background process management using nohup commands to prevent training interruptions from connection drops. This experience highlighted the practical challenges of distributed training and the importance of robust checkpoint recovery mechanisms.\n\n**Hardware Specifications and Performance Analysis**\n\nThe entire Gemma Kavach system was developed and deployed on RTX 4090 hardware rented through RunPod's cloud GPU infrastructure. The RTX 4090 provided 24GB of VRAM, which proved essential for handling the multimodal processing demands of the Gemma 3n models. During peak operation with all three system components running simultaneously, the system achieved approximately 70% VRAM utilization, leaving sufficient headroom for stable operation while processing concurrent video analysis, voice transcription, and emergency classification tasks. The RTX 4090's native bfloat16 support was crucial for resolving the numerical stability issues encountered with Gemma 3n, as the model's convolutional weights require the extended range provided by bfloat16 precision to avoid infinity overflow conditions that plague FP16 implementations.\n\nThe current hardware configuration successfully demonstrates the feasibility of edge deployment for crowd safety applications, proving that sophisticated multimodal AI can operate effectively on consumer-grade GPU hardware. However, for broader deployment scenarios where hardware costs and power consumption are critical factors, future development plans include fine-tuning the full 4B parameter Gemma 3n model and subsequently compressing it to a 2B parameter variant optimized for 10GB VRAM systems. This approach would enable deployment on more accessible hardware platforms such as RTX 3080 or RTX 4070 systems, significantly reducing the barrier to entry for organizations seeking to implement intelligent crowd monitoring solutions. The planned compression strategy leverages Gemma 3n's mix-n-match capability, where larger models naturally contain smaller submodels that can be extracted and optimized independently, providing a clear path toward more resource-efficient deployments without sacrificing core functionality.\n\n**Scenario Demo**\n\nTo contextualize the real-world application of Gemma Kavach, we consider a deployment scenario at a high-density cultural festival in Lucknow, India, with an estimated footfall exceeding 30,000 attendees. The venue is segmented into multiple zones (Zone A through Zone E), each equipped with static CCTV feeds, public address systems, and stationed security personnel. The system is actively deployed in a live event setting, with all three components—vision monitoring, voice-based command, and citizen reporting—operating in tandem.\n\nAt approximately 6:00 PM, during peak crowd ingress, the crowd monitoring module begins capturing and analyzing video frames from Zone C at two-second intervals. Gemma 3n’s multimodal inference server receives each frame and processes two concurrent prompts: one for crowd density classification and another for motion pattern analysis. Initially, the system identifies \"High Density + Calm Motion\", but within ten minutes, movement patterns begin exhibiting characteristics of crowd agitation. The dual-prompt system upgrades its classification to “High Density + Chaotic Motion”, surpassing the critical risk threshold in the decision matrix. This triggers an automated alert via email to the event control center, attaching flagged frames and corresponding panic probability scores. The real-time dashboard interface simultaneously updates, visually rendering Zone C in red, accompanied by a risk score of 87%, prompting immediate review by operators.\n\nIn response to this alert, a security officer positioned near Zone C engages with the system via voice command using a standard walkie-talkie microphone integrated into a mobile device. The officer asks in Hindi, “Zone C ka kya haal hai?” (What’s the status of Zone C?). This audio input is transcribed and interpreted by the voice command pipeline, which maps the query to current session data and generates a synthesized Hindi response: “Zone C: Bheed bahut zyada hai aur halchal zyada ho gayi hai. Turant action lena zaroori hai.” (Zone C: The crowd is extremely dense and movement has become chaotic. Immediate action is necessary). This response is converted into speech and delivered back to the officer within seconds. Based on the alert and voice feedback, command authorities issue an evacuation instruction, redirect personnel, and broadcast calming announcements via loudspeakers.\n\nSimultaneously, a citizen present in Zone D notices a child separated from their guardian. Using the citizen-facing mobile application, the user selects the “Child Lost” category, types a brief note, and uploads a photo. The emergency classification model, fine-tuned on 60,000 Indian emergency samples, correctly identifies the report as \"child_lost\" with high confidence. A unique report ID is generated, the image and geolocation data are stored in cloud storage, and an alert is dispatched via email to the command center, including the image, location, and classification metadata. This allows field officers to quickly locate and reunite the child with their family.\n\nThis scenario underscores the synergistic impact of Gemma Kavach’s multimodal AI stack. The real-time vision system facilitates early risk detection, the voice module ensures frictionless interaction in the field, and the citizen reporting system decentralizes situational awareness. Together, they enable high-fidelity, low-latency decision-making that reduces response time and mitigates the risk of large-scale human harm.\n\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F5141429%2Fc926fde10c89f963f6b888a2b811d4b5%2Fprac_example.PNG?generation=1751985389804314&alt=media)\n\n\n**Impact Potential**\n\nGemma Kavach directly addresses the systemic gaps in crowd management observed during mass gatherings in India. Traditional surveillance systems rely heavily on human attention, which is limited by fatigue, reaction time, and coverage capacity. In contrast, Gemma Kavach introduces autonomous AI-driven perception and decision support, significantly enhancing both the speed and scale of situational awareness.\n\nThe vision module introduces a two-pronged detection mechanism — analyzing both crowd density and motion turbulence — allowing the system to identify early signs of disorder that would otherwise go unnoticed until escalation. While traditional setups often require manual scanning of multiple camera feeds, Gemma Kavach continuously evaluates visual inputs and flags risk conditions in near real-time, reducing the detection burden on human operators.\n\nThe voice interface enables hands-free querying of zone conditions, especially useful in dynamic field operations where typing or navigating dashboards may not be feasible. By supporting natural Hindi and English commands, the system ensures accessibility for on-ground personnel, many of whom may not be proficient in English or familiar with digital interfaces.\n\nThe citizen-facing reporting system fills a critical gap by enabling the public to actively participate in event safety. Through structured inputs and image attachments, it empowers attendees to report localized emergencies (e.g., lost children, minor fires, or medical incidents), giving authorities immediate, geotagged alerts. This decentralization of incident reporting creates a mesh of awareness that scales far beyond centralized monitoring, especially in vast or segmented venues.\n\nOverall, Gemma Kavach transforms crowd safety from a reactive, siloed process into a distributed, multimodal intelligence network — making timely intervention possible where human oversight alone would be inadequate. While full-scale deployment studies are planned, the current architecture lays the foundation for measurable reductions in detection latency, communication bottlenecks, and false negatives — all of which are crucial factors in preventing stampede-related tragedies.\n\n**Future Work**\n\nWhile Gemma Kavach currently operates as a proof-of-concept for crowd safety at public events, the underlying architecture is designed with extensibility in mind — particularly for broader disaster response use cases in India. The system leverages the smallest multimodal model available with real-time capabilities (Gemma 3n E2B and E4B), making it uniquely suited for deployment in low-resource, infrastructure-constrained settings. As such, Gemma Kavach is not limited to cultural gatherings or religious festivals; its architecture can generalize to other domains such as flood management, public protests, pilgrimage corridors, or high-traffic transportation hubs.\n\nA key direction for future work includes integration with geospatial data and drone-based imagery, enabling large-area monitoring without relying solely on static CCTV infrastructure. Additionally, offline-first deployment for the citizen-facing application — including SMS and IVR (Interactive Voice Response) integration — could significantly improve accessibility in rural or low-connectivity zones. On the vision side, extending the current dual-prompt detection to incorporate human pose estimation or thermal inputs could improve early-stage anomaly detection, especially in night-time or obscured scenarios.\n\nThere is also scope for hierarchical coordination between AI agents, where multiple deployed nodes (e.g., across zones or districts) can collaborate and escalate alerts through a federated system. Given the lightweight nature of Gemma 3n models, such a decentralized intelligence framework remains technically feasible and financially viable — especially in a country where disaster management often suffers from fragmented data systems and reactive coordination.\n\nLong-term, we envision Gemma Kavach evolving into a standardized safety interface — one that public authorities, private organizers, and citizens alike can rely on to mitigate human risk in dense environments. By combining efficient multimodal AI, localized deployment strategies, and citizen participation, this system lays the groundwork for an India-centric, AI-first public safety infrastructure.",
        "technologies": [
            "CUDA",
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/aditya699/gemma-kavach/tree/main/GemmaServer",
        "demo_video_url": "https://www.youtube.com/watch?v=sb4LyEvzQTQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemmakavach"
    },
    {
        "project_title": "EmpowerEd is a mobile application powered by Gemma 3n",
        "tagline": "Ed is a mobile application powered by Gemma 3n, designed to revolutionize education in low-connectivity regions by providing an offline-firs",
        "authors": "James Aduke",
        "summary": "Overview\nEmpowerEd is a mobile application powered by Gemma 3n, designed to revolutionize education in low-connectivity regions by providing an offline-first, multimodal learning assistant. It addresses the challenge of unequal access to quality education by delivering personalized, interactive, and engaging learning experiences to students in remote or underserved areas, where internet access is unreliable or nonexistent.Real-World ChallengeOver 250 million children globally lack access to quality education due to geographic isolation, poverty, or lack of infrastructure (UNESCO, 2023). In many regions, schools struggle with limited resources, untrained teachers, and no internet connectivity. EmpowerEd leverages Gemma 3n’s offline capabilities, multimodal understanding, and multilingual support to provide a scalable, private, and accessible solution for students aged 6–18.How EmpowerEd WorksEmpowerEd is an Android/iOS app that runs entirely on-device, using Gemma 3n’s optimized 4B model (with a 2B submodel for resource-constrained devices). \n\nHow EmpowerEd Works\nEmpowerEd is an Android/iOS app that runs entirely on-device, using Gemma 3n’s optimized 4B model (with a 2B submodel for resource-constrained devices). It offers:\n1. Personalized Learning Paths:\nUpon first use, EmpowerEd conducts a simple, game-like assessment to gauge the student's knowledge level in core subjects like math, science, and literacy. Based on the results, Gemma 3n curates a unique learning path, ensuring content is challenging but not overwhelming. The path adapts in real-time based on the student's performance on quizzes and exercises.\n2. Interactive AI Tutor:\nStudents can ask questions in their natural language (typed or spoken) and receive instant, conversational answers. The AI Tutor acts as a patient, always-available guide. It can explain complex concepts in simple terms, provide step-by-step solutions to problems, and use the Socratic method to encourage critical thinking rather than just giving away the answer.\n3. Multimodal \"Snap & Solve\":\nLeveraging Gemma 3n's multimodal understanding, students can use their device's camera to take a picture of a problem—from a textbook, a worksheet, or even handwritten notes. EmpowerEd recognizes the text and images (e.g., a math equation, a diagram of a plant cell) and provides a detailed explanation, a visual breakdown, or a guided solution.\n4. On-Device Generative Tools:\nVisual Explainer: When a student struggles with an abstract concept like the water cycle or planetary orbits, EmpowerEd can generate simple diagrams, charts, and illustrations on the fly to provide a visual aid.\nStory Weaver: To boost literacy and creativity, students can provide a prompt (e.g., \"a story about a brave rabbit who explores a cave\"), and Gemma 3n will generate a short, age-appropriate story that the student can read or have read to them using text-to-speech.\n5. Localized and Multilingual Content:\nThe app is pre-loaded with core curriculum modules that can be rendered in multiple languages, focusing on the dialects spoken in the target regions. Students can learn in their mother tongue, breaking down significant barriers to comprehension. The AI Tutor is also fine-tuned to understand and respond in these local languages.\n6. Offline-First Architecture with Smart Sync:\nAll core features—tutoring, problem-solving, and content access—function perfectly without an internet connection. The app utilizes a \"Smart Sync\" feature. When the device occasionally connects to a network (e.g., at a community Wi-Fi hotspot), it automatically downloads new learning modules, language packs, and model updates in the background, ensuring the content remains fresh and relevant.\n7. Progress Tracking and Gamification:\nTo keep students motivated, EmpowerEd incorporates gamified elements. Students earn points, unlock badges for mastering new skills, and can view their progress on a \"learning tree\" that visually represents their academic journey. This provides a sense of accomplishment and encourages consistent engagement.\nFor Educators and Facilitators:\nThe app also includes a simple, secure \"Facilitator Mode.\" A teacher or community volunteer can view anonymized, aggregated class progress, identify common challenges where students are struggling, and access on-device teaching guides and lesson plan ideas generated by Gemma 3n to better support their students.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/2hyHrxzupXc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/empowered-is-a-mobile-application-powered-by-gemma"
    },
    {
        "project_title": "DrillIntel",
        "tagline": "Save hundreds of hours reading drilling reports, get critical insights, active alerts, recommendations, and chat with your files, and more.",
        "authors": "Tmgiscb",
        "summary": "- Introduction\n\nThe oil and gas industry generate a huge amount of data from drilling operation,\nyet an important portion of this data is not really utilized properly. AI framework is built by engineer for engineers. It digest\ncomplex data in files and generate a useful visualization for your data,\nallowing you to catch the trends and anomolies, also it provide you with the ability\nto chat with all your files you have uploaded, and also provide you with automatic\nanomaly detection, It is designed not only for visualization or chatting with your\nfiles, it provide you with a clean data infrastructure for next-generation systems,\nincluding digital twins, digital drilling platforms, manual analysis, predictive\nmachine learning. This paper details the systemʼs architecture, validate itʼs\nefficiency through practical application scenarios, and itʼs role in enhancing the\ndigital oil and gas industry.\n\n- Core system’s Pillars \n\nDrill-Intel is an AI framework consisting of several pillars, each design to address a\nspecific challenge\nFile Data digestion: Each each file undergoes multiple processes parsing, and file\ndetails extraction to correlate data with the proper file intake. Data is\nautomatically structured within a secure database, forming data ready for\nanalysis, future machine and deep learning projects.\nNatural Language Query & Command (NLQ/NLC) One of the most important\npillars of Drill-Intel is the conversational AI interface, This allow user to ask the\nentire database using natural language, speeding up the searching and\ninterpretation of the huge data sources:\n“During the drilling of this well, the rop_ft_hr showed a sustained drop\nbetween 2200 ft and 2500 ft, as seen in the HistoricalLogEntry. Can\nDrillIntel not only identify the primary contributing factors (e.g., changes in\nformation_type, specific\nweight_on_bit_klbs/rotary_speed_rpm/torque_ft_lbs interplay, or potential\nbit dulling) but also suggest an optimal drilling parameter window for that\nspecific formation_type?ˮ\n“For all npt_reason events recorded in the HistoricalLogEntry (e.g.,\n'Equipment Maintenance', 'Daylight delays'), can DrillIntel perform a root\ncause analysis to quantify their impact on project cost and schedule?\nSpecifically, for recurrent NPT types, can DrillIntel use the\nDetailedAnalysis.csv insights and recommendations to show how\nproactive maintenance alerts or optimized operational procedures could\nhave reduced the Non-Productive Time by a specific percentage, thereby\ntranslating directly into tangible cost savings and increased rop_ft_hr over\nthe drilling program?ˮ\n“Can DrillIntel analyze if the Weather_Temperature_F and\nWeather_Wind_Speed_mph from Well.csv show any statistically significant\ncorrelation with npt_reason (e.g., increased 'Rigging Up' delays) or overall\nrop_ft_hr performance in HistoricalLogEntry.csv during surface operations\nor extreme weather events? If so, what insight and recommendation from\nDetailedAnalysis address potential weather-related operational impacts?ˮ\n“plot Weather_Temperature_F and Weather_Wind_Speed_mph versus ROP\nwith timeˮ\nRecommendation System: The system not just give reactive analysis, but by\ncontiously monitering the incoming data and properly analyzing it, It\nautomatically provide you with potential alerts flags like bit wear, loss of\ncirculation, stuck pipe. Each alert is analyzed and teh system provide you with\nthe proper recommendation\nDynamic Visualization: The platform renders dynamic dashboards. Users can\nvisualize multiple parameters that is crucial for monitering the well, detecting\nanamolies and help in taking the proper decision. Transforming static reports\ninto dynamic analytical dashboard\nAutomated Clean Data Infrastructure for Manual Analysis and Machine\nlearning projects: A primary function of the framework is to provide a robust\ndata foundation. Every file uploaded is transformed into a clean, structured\ndataset, saving tremendous amount of time, and errors related to manual data\npreprocessing\nFoundation for Digital Twin Ecosystems: By providing a standardized, high\nquality data, DrillIntel is designed to be the foundation for building high\naccuracy digital twins or rigs, wells, and fields\n\n- Comparison: Drill-Intel vs. Data Scientist Team (Manual)\n\n| **Task** | **DrillIntel Performance** | **Data Scientist Team (Manual, Estimated)** | **Time Savings (Illustrative)** |\n| --- | --- | --- | --- |\n| **Process Daily Drilling Report** | ~2 minutes per report | ~1–2 hours per report | **Massive (>95%)** |\n| *Total for 1000 Reports (Preprocessing & Storage)* | **~33 hours** | **~1000–2000 hours** | **~97–98% saved** |\n| **Get Actionable Insights (e.g., avg. ROP, NPT by reason)** | ~1 minute per query | ~15–30 minutes per complex query | **90–95% faster** |\n| *Total for 1000 Insight Queries* | **~17 hours** | **~250–500 hours** | **~93–96% saved** |\n| **Continuous Monitoring & Alerts** | ✅ Automated & Real-Time | ❌ Manual, Delayed Review | **Transforms Workflow** |\n\n- Conclusion\n\nDrill-Intel ingest files, analyze it, provide insights, detailed analysis, understanding, interpreting the data and providing proper recommendation. The platform renders dynamic dashboard for all the vital parameters in drilling helping in monitering the trends, detecting anamolies, and take proactive mitigation strategies. The chatting with database help in finding the needed information in the huge amount of data saving hours of time, also helping in getting vital insights, recommendation, active alerts, and plotting varoius parameters providing best practises. Constructing a clean data infrastructure for future projects such as: Digital twins, Machine learning models, manual data analysis, making Drill-Intel as powerful future asset.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/eOhkBObvUxM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/drillintel"
    },
    {
        "project_title": "Scholar Express",
        "tagline": "Web app that convert scientific research paper in PDF to accessible Markdown format ",
        "authors": "Raksa Ma",
        "summary": "**Scholar Express** is an AI-powered platform that transforms inaccessible scientific research papers into interactive, screen-reader compatible documents to address critical accessibility barriers faced by students with disabilities in academic research. \n\nThe system leverages a specialized ensemble of AI models - including **Gemma 3n** variants for alt text generation and document chat, **DOLPHIN** for PDF layout analysis, and **Sentence-Transformer** for semantic search - to process complex academic PDFs with mathematical equations, figures, and diagrams that are typically inaccessible to screen readers. \n\nThrough its comprehensive pipeline that converts PDFs to images, analyzes layouts, extracts elements, and generates descriptive alt text, Scholar Express creates structured **markdown** documents while also providing **RAG-powered chatbot** functionality and **real-time voice interactions** enabling the 18-54% of undergraduate students with disabilities to access the same research literature as their peers while enhancing the research experience for all users.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/L3etYzBOCkg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/scholar-express"
    },
    {
        "project_title": "CareWise AI – Your Offline, Private, and Accessible Health Companion",
        "tagline": "A privacy-first, offline health assistant web app powered by Google's Gemma 3n to make healthcare accessible to everyone, everywhere.",
        "authors": "Barbra Kokonya",
        "summary": "**Abstract**\nIn a hyperconnected world, access to reliable health information remains out of reach for many due to internet limitations, privacy concerns, and accessibility barriers. CareWise AI is a progressive, offline-first, and privacy-focused health web application designed to bridge these gaps. Powered by Google’s Gemma 3n running locally via Ollama, the app delivers AI-driven health guidance and symptom checking without requiring any internet access. It features a dual-layered system: a rule-based Symptom Checker for fast, foundational health triage and an “Ask AI” conversational assistant for nuanced medical queries. With advanced voice input/output, customizable accessibility settings, and no data ever leaving the device, CareWise AI empowers users of all abilities to take control of their health anytime, anywhere.\n\n🩺**The Problem**\nMillions of people  in rural communities, during emergencies, or those with disabilities  face barriers in accessing trustworthy health information:\n\nInternet access is unreliable or unavailable.\n\nUsers increasingly value tools that prioritize privacy and give them full control of their personal data.\n\nDesigning for accessibility is an opportunity to create more inclusive health solutions for everyone.\n\n💡**The Solution**\nCareWise AI solves this by providing a fully offline, privacy-preserving health companion with:\n\nRule-Based Symptom Checker for fast, deterministic guidance\n\nGemma 3n-powered AI Assistant for personalized, conversational answers\n\nVoice Input/Output for hands-free accessibility\n\nInclusive Design —  high contrast, readable fonts\n\nOn-device Storage using IndexedDB (no server, no data leaks)\n\nZero Internet Requirement — works anywhere, even without connectivity\n\n\n🔧**",
        "technologies": [
            "Gemma 3n",
            "Ollama",
            "Web Speech API"
        ],
        "source_code_url": "https://github.com/atienobabby/Wisecareai",
        "demo_video_url": "https://www.youtube.com/watch?v=mbO69MSWTig",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/carewise-ai-your-offline-private-and-accessible-he"
    },
    {
        "project_title": "O/ASK - Offline AI Survival Kit",
        "tagline": "O/ASK is an offline, privacy-first, multi-modal AI multitool powered by Google's Gemma 3n.",
        "authors": "oaskai",
        "summary": "O/ASK – The Offline AI Survival Kit: Technical Verification",
        "technologies": [
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/mitchatrebl/oask-gemma3n-challenge",
        "demo_video_url": "https://youtu.be/oQqgDU-doVQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/oask-offline-ai-survival-kit"
    },
    {
        "project_title": " AI-Powered Mental State Analyzer: Leveraging Multimodal AI for Deeper Insights",
        "tagline": "Analyze emotional and psychological well-being from video using multimodal AI (audio & visual)",
        "authors": "raviksh dikola",
        "summary": "AI-powered Streamlit application that analyzes video content using multimodal AI (audio and visual cues) to assess an individual's mental state and provide a comprehensive report. It leverages technologies like Whisper, Ollama (Gemma3n, Gemma2), and FFmpeg for deep analysis and categorization of emotional and psychological well-being",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/u7Vpx4G21G4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-powered-mental-state-analyzer-leveraging-multim"
    },
    {
        "project_title": "Live Captions XR",
        "tagline": "Real-time AR captions and spatial sound detection—running offline on XR headsets, powered by Gemma 3n.",
        "authors": "craigmerry, Sasha Denisov",
        "summary": "LiveCaptionsXR\n\n**Subtitle:**\n*\"On-device, real-time captioning and sound awareness for XR—built by a deaf developer using whisper_ggml and Gemma 3n to unlock spatial hearing for everyone.\"*",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/craigm26/livecaptionsxr",
        "demo_video_url": "https://youtu.be/Oz8nzt2cc3Q",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/live-captions-xr"
    },
    {
        "project_title": "Cognitive-Companion",
        "tagline": "Helping Alzheimer’s Patients with Gemma Companionship and Timeless care",
        "authors": "Rosline Prabha T M",
        "summary": "An AI-powered voice and photo assistant to support Alzheimer's patients with personalized interaction, memory reinforcement, and contextual conversations using Google Gemma 3n.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/RoslinePrabha/alzheimers-companion-gemma3n/tree/main",
        "demo_video_url": "https://www.youtube.com/watch?v=uSA7n0mTtXA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/cognitive-companion"
    },
    {
        "project_title": "Plant Pathology AI with Gemma 3n",
        "tagline": "A Real-World Agricultural Disease Detection System Using On-Device LLM Inference",
        "authors": "Adel Boussaken",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/netdur/plant-pathology-ai",
        "demo_video_url": "https://www.youtube.com/watch?v=2CZo0JFDQdU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/plant-pathology-ai-with-gemma-3n"
    },
    {
        "project_title": "VisionGuard",
        "tagline": "AI-Powered Structural Safety Advisor",
        "authors": "viraj",
        "summary": "VisionGuard: AI-Powered Structural Safety Advisor\n**Built with Google’s Gemma-3n-E4B-IT**",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=iGxNc7DQATM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/visionguard"
    },
    {
        "project_title": "Species Protection Meets Gen AI: Use Gemma3n For Orangutan Conservation ",
        "tagline": "Channelizing Gen AI To Conserve Endangered Species In Tropical Areas With Limited Internet Connectivity",
        "authors": "Minerva's Data Lab",
        "summary": "Introduction- Why Are Orangutans At Risk Of Extinction\n\nOrangutans, scientifically classified under the genus Pongo, represent one of humanity's closest extant relatives, sharing approximately 97% of their DNA with humans, and are among the most intelligent primates. As the only great apes native to Asia, orangutans are endemic to the islands of Sumatra and Kalimantan (John, 2022; Xie, 2025). Bornean orangutans, along with Sumatran and Tapanuli orangutans, are critically endangered according to the International Union for Conservation of Nature Red List (Sherman et al., 2021).  These remarkable creatures, whose name translates to \"person of the forest\" in Malay, play a crucial role in the biodiversity and ecological balance of their rainforest habitats (Pandong et al., 2019). Kalimantan, the Indonesian portion of Borneo Island, harbours a significant population of Bornean orangutans (*Pongo pygmaeus*), one of the three recognised species of orangutans, and its diverse ecosystems and natural resources are essential for the survival of this species (Leo et al., 2021). \n\nVital orangutan habitats are under immense threat due to deforestation, habitat fragmentation, and illegal hunting, posing severe conservation challenges for orangutans and the biodiversity they support. The rapid expansion of oil palm plantations has generated controversy because of their negative impact on orangutans (Barcelos et al., 2015). The conversion of forests into agricultural land, logging activities, and mining operations has drastically reduced the available habitat for orangutans, forcing them into increasingly smaller and isolated areas. The primary threats to orangutan populations in Kalimantan stem from habitat loss driven by agricultural expansion, particularly oil palm plantations, logging, and mining activities. As rainforests are cleared to make way for these industries, orangutans are displaced, leading to increased competition for resources and heightened conflict with humans.\nFurthermore, the fragmentation of forests isolates orangutan populations, limiting gene flow and reducing their ability to adapt to environmental changes. Illegal hunting and the pet trade also pose significant threats, with orangutans being targeted for their meat or captured for sale as pets, further decimating their numbers (Galingging et al., 2021). Additionally, forest fires, often deliberately set to clear land for agriculture, can have devastating impacts on orangutan populations, destroying their habitats and directly killing individuals (Dalheimer et al., 2024). Field surveys to monitor orangutan populations along with their habitats and anthropogenic threats such as roads and oil palm plantations is vital for conservation efforts. However, fieldwork in the tropics is challenging due to a variety of factors, including rugged terrain and inaccessible, remote sites, which often have low rates of internet connectivity. \n\n**My Motivation** - Having spent months conducting fieldwork in the dense rainforests of Southeast Asia, particularly on Borneo, I witnessed firsthand the immense logistical and connectivity challenges that conservation teams face on the ground. Navigating rugged landscapes, dealing with unpredictable weather, and working in areas with little to no internet access often made real-time data collection and decision-making nearly impossible. These experiences inspired me to create an offline-first, lightweight, mobile-friendly solution powered by Gemma 3n, ensuring that rangers, researchers, and local communities can access critical information, monitor habitats, and respond to threats, even in the most remote locations, without relying on constant connectivity.\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6345345%2F3f73ddd83a34c1383bd7f61321941a2e%2FScreenshot%202025-07-26%20at%2011.48.33.png?generation=1753527182190267&alt=media)\n                                                                                  Figure 1: Me Doing Fieldwork in Borneo",
        "technologies": [
            "CUDA",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Jojo666/GenAI-For-Orangutans",
        "demo_video_url": "https://youtu.be/emX2Hnc73uk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/species-protection-meets-gen-ai-use-gemma3n-to-tac"
    },
    {
        "project_title": "ADHD-Dyslexia-EasyScreen",
        "tagline": "Playful, private AI tool that screens for ADHD/Dyslexia traits from kids’ reading and game data—no cloud, no diagnosis, just insights.",
        "authors": "Harshavardhana Naganagoudar",
        "summary": "A fully local, AI-powered screening tool that uses fun cognitive games and reading tasks to detect early traits of ADHD and Dyslexia—backed by Google’s Gemma 3n via Ollama. Private, playful, and packed with insights.\n\n🧠 What It Does:\nADHD-Dyslexia-EasyScreen turns short, interactive games and voice-based reading into structured behavioral data. It evaluates traits like attention, impulsivity, flexibility, memory, and reading fluency—generating empathetic, non-diagnostic reports in both JSON and natural language.\n\n💡 Why It Matters:\nEarly signs of ADHD and Dyslexia are often missed due to stigma, lack of resources, or overly clinical tools. This project offers a friendly, offline-first alternative—designed for educators, parents, and caregivers—no internet, no cloud, no diagnosis. Just clear, helpful insights.\n\n🛠️ Tech Stack Highlights:\n\nModel \"gemma3n:e4b\" via Ollama for LLM-based reasoning\n\nStructured scoring rubric for ADHD/Dyslexia traits\n\nAudio and gameplay data collection (reaction times, accuracy, recall)\n\nOffline-friendly UI via VS Code + Python\n\n🌱 Who It's For:\nAnyone supporting children’s learning journeys—teachers, therapists, caregivers—looking for a lightweight, engaging early-screening companion that puts privacy and empathy first.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/HarshavardhanaNaganagoudar/ADHD_Dyslexia_EasyScreen",
        "demo_video_url": "https://youtu.be/CylKy0DtV6s?si=1xkGVrRZscA4g8Tg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/adhd-dyslexia-easyscreen"
    },
    {
        "project_title": "Keylike AI: Exploring the Quiet Risk in Mass-Produced Locks",
        "tagline": "Scan any lock with your phone to discover if strangers unknowingly share your key—powered by Gemma 3n",
        "authors": "Oussama",
        "summary": "🔐 Keylike AI — The Lock That Shouldn’t Have Opened",
        "technologies": [
            "Flask",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/oussama-HA/-keylike-ai-hackathon",
        "demo_video_url": "https://youtu.be/PvLUDMA-Ywc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/keylike-ai-exploring-the-quiet-risk-in-mass-produc"
    },
    {
        "project_title": "The Naughty-Nice Meter: A Polygraph for an AI's Soul",
        "tagline": "A privacy-first diagnostic tool that uses a local Gemma model to proactively stress-test LLM alignment and quantify behavioral corruption.",
        "authors": "Nick Sovich",
        "summary": "Kaggle Hackathon Submission: The Naughty-Nice Meter\n\n**Project Name:** The Naughty-Nice Meter  \n**Team:** Nick Sovich",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "NVIDIA Jetson",
            "Ollama"
        ],
        "source_code_url": "https://github.com/nicksovich4/local-llm-naughty-nice-meter",
        "demo_video_url": "https://youtu.be/dNxIrnCbPeI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/the-naughty-nice-meter-a-polygraph-for-an-ais-soul"
    },
    {
        "project_title": "Trinetra AI",
        "tagline": "As we are shifting away from classroom to job model, a stylus pen-based AI device like this will be in the pockets of all age group people.",
        "authors": "Amritansh Sharma",
        "summary": "#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/amritansh005/Kaggle-Competition/tree/main",
        "demo_video_url": "https://www.youtube.com/watch?v=jZ4hJ1pu5Us",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/education-suite"
    },
    {
        "project_title": "Talk Data To Me: Bridging Agentic AI and Public Understanding",
        "tagline": "Making critical AI decision processes clear, accessible and engaging to promote responsible, trustworthy AI deployment.",
        "authors": "RDRSquared",
        "summary": "Talk Data To Me: Bridging Agentic AI and Public Understanding",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/datawranglerai/talk-data-to-me",
        "demo_video_url": "https://youtu.be/hkavvFVFFkM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/talk-data-to-me"
    },
    {
        "project_title": "leaf-disease-gemma",
        "tagline": "This project is an offline AI-powered leaf disease detection tool that uses the \"Gemma 3n model via Ollama\" with a simple GUI, allowing us",
        "authors": "Rubaiya Parvin",
        "summary": ")\n\n📌 Project Description:\nThis project presents an offline AI-powered solution to detect plant leaf diseases by integrating Gemma 3n, a local large language model, with Ollama and a Python GUI. Users can upload an image of a diseased leaf, and the app interacts with the Gemma model through a simple interface to provide a textual diagnosis of the disease. It combines image preprocessing, base64 encoding, and LLM prompt engineering to generate accurate and interpretable responses — without the need for internet access.\n\n🔧 Key Features:\nUpload any leaf image (e.g., tomato leaf).\n\nLocally runs Gemma 3n via Ollama.\n\nPython-based GUI for ease of use.\n\nDetects common plant diseases from leaf visuals.\n\nWorks fully offline once set up.\n\n📁 Tech Stack:\nPython\n\nTkinter (for GUI)\n\nPillow (for image handling)\n\nOllama (to run the Gemma model locally)\n\nGemma 3n / gemma:2b model",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/TNC1wOU47I8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/leaf-disease-gemma"
    },
    {
        "project_title": "SentiaFlow: Technical Writeup & Proof of Work",
        "tagline": "Democratizing Personal Wellness with On-Device AI: Smarter Health,Total Privacy, For Everyone",
        "authors": "Abir Eltaief",
        "summary": "**SentiaFlow: Technical Writeup & Proof of Work**\n\n**Author:** Abir Eltaief\n**Date:** August 5, 2025",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Abirate/SentiaFlow",
        "demo_video_url": "https://youtu.be/nbQL5s7CbiE?si=YJaSFdvJs6Lv8rKd",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sentia-flow-writeup-proof-for-work-submission"
    },
    {
        "project_title": "AfyaSpeak-app",
        "tagline": "AfyaSpeak is an offline, multimodal medical interpreter designed to bridge language barriers in healthcare for rural communities by enab",
        "authors": "Abraham Imani",
        "summary": "In many rural and semi-urban areas, effective healthcare is often hindered not by a lack of medical expertise, but by a profound communication gap. Patients struggle to articulate their symptoms in a language understood by caregivers, leading to potential misdiagnoses and ineffective treatments, a challenge compounded by the absence of reliable internet access for online translation. AfyaSpeak addresses this critical issue by providing a private, portable, and multimodal medical interpreter that operates entirely offline. This innovative solution leverages the power of Gemma 3n, efficiently fine-tuned with Unsloth for local dialect and medical vocabulary nuances, and deployed on-device via Google AI Edge. The application facilitates real-time audio-to-audio and audio-to-text interpretation, incorporates visual aids like body diagrams for symptom pointing, and maintains an intelligent local history, ensuring seamless, confidential, and accurate communication between patients and caregivers, even in the most isolated settings.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/namidrc/afyaspeak",
        "demo_video_url": "https://youtu.be/vU06CeNl05s",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/afyaspeak-app"
    },
    {
        "project_title": "Caretalker-Thank you for gemma's developers.",
        "tagline": "perfect and complete supervision system",
        "authors": "givemeworks",
        "summary": "CareTalker is a conversational AI monitoring system designed to proactively prevent wandering incidents (\"haikai\") among dementia patients—a critical issue in Japan's rapidly aging society.",
        "technologies": [],
        "source_code_url": "https://github.com/caretalker/gemmma.git",
        "demo_video_url": "https://youtu.be/GoRLuO1PEO8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/caretalker"
    },
    {
        "project_title": "MotherHood Journey Companion",
        "tagline": "DoctorG is an offline app that track and help woman in pregnancy and new borne babies regarding therir diet, health and emergency.",
        "authors": "Abhinav Pudasaini",
        "summary": "Problem Being Solved\nIn rural and underserved areas, pregnant women and newborns often:\n\n- Lack access to consistent healthcare advice\n- Suffer from malnutrition, incorrect supplements, or missed growth markers\n- Are unaware of critical danger signs during pregnancy or early infancy\n- The woman in rural area are specially unaware of task and food they can eat, becuase many unregistered has led to miscarriage all around the world. And there are particular food that needs to be consumed to get a healthy baby. But the problem is that due to lack of technology and knowledge they are unaware about these kind of stuffs.\n- Lack of internet and healtcare personal can be a panicking situation specially during emergency.\nInternet access is often unreliable, making online solutions inaccessible\n\nOur Solution\nAn AI-powered, offline mobile app that:\n\n1. Tracks pregnancy stages and newborn development\n2. Provides nutrition and supplement advice\n3. Uses on-device AI (Gemma 3n) to give instant answers to health queries\n4. Analyzes baby posture and development milestones\n5. Offers local language support and daily health tips\n6. Runs entirely offline, usable without internet or data\n\nTechnology Used\nReact js + electron\nsqlite3\nGemma 3n\nOllama",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/AbhinavPudasaini/Momcares",
        "demo_video_url": "https://www.youtube.com/watch?v=0N-Gd9ZHp6Y",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/motherhood-journey-companion"
    },
    {
        "project_title": "FlightGuard AI",
        "tagline": "Flight Guard AI is an offline real-time system that analyzes cockpit audio for tone, risk, and summaries using a quantized Gemma 3n model.",
        "authors": "Lee Johns Shaji, LJS_UE5",
        "summary": "Flight Guard AI is a real-time, onboard cockpit voice analysis system designed to detect stress, miscommunication, and potential risks during flight. Its primary purpose is to enhance flight safety by continuously analyzing cockpit audio to identify emotional tone, flag concerning behavior, and generate concise summaries. The system works without requiring internet access.\n\nIt is powered by a 4-bit dynamically quantized version of the Gemma 3n language model, optimized for low-resource edge deployment. Because it runs entirely offline, Flight Guard AI is ideal for aircraft environments where privacy, reliability, and autonomy are essential.\n\nFlight Guard processes audio in overlapping time-based segments using a sliding window approach. This ensures context is preserved across chunk boundaries. For each segment, the system detects tone (for example, calm or urgent), assigns a risk level (LOW, MEDIUM, HIGH), and generates a natural-language summary of the ongoing situation.\n\nThe output includes structured CSV and JSON files, as well as a visual risk-level plot that can be displayed in real-time dashboards or used in post-flight analysis. Use cases include cockpit risk monitoring, pilot behavior profiling, incident timeline reconstruction, and human factors research.\n\nBuilt to operate under aviation constraints, Flight Guard AI delivers context-aware safety insights onboard the aircraft in real time without the need for cloud infrastructure.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/LeeJSC/FIghtGuardAI-Gemma3n",
        "demo_video_url": "https://youtu.be/VhsvtNWIhu0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/flightguard-ai"
    },
    {
        "project_title": "Food Len - iOS app",
        "tagline": "Personalized food safety powered by on-device AI.",
        "authors": "Seungjun Lee",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/seungjun-green/FoodLen",
        "demo_video_url": "https://youtu.be/z1hHl7igdvk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/food-len-ios-app"
    },
    {
        "project_title": "CIVIL - Crisis Information & Vital Intelligence Line",
        "tagline": "AI-Powered Emergency Coordination for When Everything Else Fails",
        "authors": "Yash Sarwaiya",
        "summary": "CIVIL is an offline-first, AI-powered voice response system that turns any phone call into real-time disaster coordination — even when apps and internet fail.\n\nDuring emergencies, traditional systems break down due to overloaded networks and poor coordination. CIVIL solves this by allowing anyone to call a number, state their location and emergency, and receive immediate help. Using local speech processing and Gemma 3n for decision-making, CIVIL:\n1. Detects disaster type and urgency\n2. Routes to the correct services (medical, rescue, fire, etc.)\n3. Notifies nearby volunteers and NGOs via SMS or radio\n4. Provides live safety instructions while help is en route\n\nBuilt with Twilio, Gemma 3n, and a disaster-specific workflow engine, CIVIL operates on low-power hardware with no dependency on cloud infrastructure mmaking it ideal for refugee camps, remote villages, and disaster zones.\n\nOne call. Total coordination. No app, no internet, just a lifeline.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/YashSarwaiya/CIVIL_DEMO",
        "demo_video_url": "https://youtu.be/v3AGS4lOqq4?si=4u2wYalf0FPlYxiI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/civil-ai"
    },
    {
        "project_title": "Graph‐Based Cost Learning and Gemma 3n for Robotic Sensing",
        "tagline": "We train a robot to sense faster in disasters (using Gemma and adaptive cost learning), allowing teams to explore unknown zones ~20% faster.",
        "authors": "Raahim Lone",
        "summary": "**Graph-Based Cost Learning and Gemma 3n for Robotic Sensing - Gemma 3n Impact Challenge**\n\n[**INTRODUCTION (Excerpt From Full Report)**\nRobotic exploration in unstructured indoor environments must balance coverage, hazard detection, energy use, and time. Classic frontier-based planners ensure coverage but ignore sensing costs. Modern learned approaches—deep Reinforcement Learning, POMDP planners—adapt to dynamics yet lack safety guarantees and treat scanning as free. While movement can be planned efficiently, each sensing action adds significant overhead, often dominating total mission time. Clearly, scanning time—the duration to collect LiDAR, thermal, gas, and audio data—remains a major caveat hindering speed and efficiency. We introduce a novel, scanning-time–first pipeline built on LeRobot and powered by Gemma 3n. The contributions are as follows:\n\n• Gemma 3n Plan Generation: Few-shot prompting yields diverse candidate sequences.\n• IGMC Cost Learning: Graph-based matrix completion predicts plan latencies with uncertainty.\n• Contextual Fusion: A lightweight MLP refines predictions with room features in sub-millisecond time.\n• Adaptive Gating: Gemma 3n is selectively called when the cost model is uncertain, minimizing time taken for inference and maximizing Gemma 3n’s reasoning capabilities.\n\nWe show that our approach runs entirely on-device with Gemma 3n’s on-device capabilities, achieves real-time inference, and delivers a 17% speedup to 95% coverage compared to the baseline (nearest-frontier).]\n\n**-This is an excerpt from the full technical report in the attachments section of this writeup. Please refer to that to understand the architecture, challenges overcome, use of Gemma 3n/LeRobot, and more.**",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Raahim-Lone/gemma",
        "demo_video_url": "https://youtu.be/VAvQdEn_PxI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/graphbased-cost-learning-and-gemma-3n-for-sensing"
    },
    {
        "project_title": "NeuroNourish AI",
        "tagline": "Empowering Parkinson's patients with a private, multimodal AI, powered by Gemma 3n, for safer daily nutritional management.",
        "authors": "Rene",
        "summary": "NeuroNourish AI: Technical Write-up\n\n**Project for the Google - The Gemma 3n Impact Challenge**\n\n**Team:** René & Gemini\n**Links:**\n\n* **Video Demo:** [https://youtu.be/vsv2s7Vmt6g]\n* **Public Code Repository:** [https://github.com/reinbacher/neurounourish-submission.git]\n* **Public Project Link (Live Demo):** [https://neurounourish-ai-dlwty5qlgq-uc.a.run.app/](https://neurounourish-ai-dlwty5qlgq-uc.a.run.app/) (Password: PDFOOD2025) - *Note: This is the live cloud prototype built with Gemini. A simple password is included to ensure service availability for the judges by preventing automated abuse. The service is fully accessible using the credentials provided.*\n\n#",
        "technologies": [
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/vsv2s7Vmt6g",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/neuronourish-ai"
    },
    {
        "project_title": "SmartKitchen: An On-Device Multimodal Recipe Assistant with Gemma 3n",
        "tagline": "SmartKitchen is an on-device multimodal AI assistant that generates personalized recipes from available ingredients to combat food waste and",
        "authors": "Jkalwaysontop, Rishi Shah, Mannan Bajaj, Anant Singhal, NEHAL Thakur",
        "summary": "SmartKitchen: A Multimodal Solution for a Sustainable Future\n\nSmartKitchen is a proof-of-concept application designed to showcase the transformative power of the Gemma 3n model. Our project directly addresses the pressing global issue of food waste by providing users with an intuitive and intelligent tool for making the most of the ingredients they already have.\n\nKey Features:\n\nMultimodal Input: Our application can understand a user's intent through multiple modalities. It uses computer vision to identify ingredients from a photo and voice-to-text to transcribe a list of spoken ingredients.\n\nIntelligent Recipe Generation: Powered by Gemma 3n, the app takes the identified ingredients and generates a custom, balanced, and nutritious recipe in real-time.\n\nPrivacy-Focused Vision: The project's architecture is a demonstration of how on-device AI can deliver powerful, personalized experiences without compromising user data. Our ultimate vision is a fully offline application that runs entirely on the user's phone, ensuring complete privacy.\n\nImpact:\n\nBy seamlessly integrating multimodal AI into the cooking process, SmartKitchen offers a tangible solution to food waste and inspires healthier eating habits. This project provides a compelling vision for a future where on-device AI is a core part of sustainable, everyday life.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/TomSU6EUmy4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/smartkitchen-an-on-device-multimodal-recipe-assist"
    },
    {
        "project_title": "Scram: Offline Multilingual Scam Detection with Gemma 3n",
        "tagline": "An offline, multilingual scam detection system for SMS and emails, powered by rule-based prompting and Gemma 3n on mobile",
        "authors": "Erfan Mirhosseini, Rishabh, Chaitat Utintu, Parisa Jamadi, Alireza Salemi",
        "summary": "Scram is a privacy-first mobile app that detects scams in SMS and email screenshots across multiple languages — all without using the internet. It runs a lightweight Gemma 3n-2B model on-device using MediaPipe and LiteRT, guided by a custom rule-based prompt rubric for scam reasoning. Users simply take a screenshot, and Scram instantly analyzes the text for phishing, financial fraud, or impersonation attempts — responding in the same language as the message.\n\nScram was benchmarked across 5 datasets, including Thai, Spanish, German, and French, and achieves real-time detection (under 5s) with no cloud calls or fine-tuning required. The final deployment uses Gemma-3n-E2B-it-litert-preview, optimized for mobile inference.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/erfunm/Scram",
        "demo_video_url": "https://youtu.be/DL42WgCnS3o",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/scram-offline-multilingual-scam-detection-with-gem"
    },
    {
        "project_title": "SINGLY: AI-POWERED SPOKEN TEXT TO SIGN LANGUAGE TRANSLATOR FOR THE DEAF",
        "tagline": "Singly bridges spoken and sign language by converting speech to simplified text, then to sign videos—helping deaf children learn and engage.",
        "authors": "Doha Momani",
        "summary": "Many deaf and mute children continue to face real difficulties in receiving their right to education equally with their peers. These difficulties are often the result of their inability to hear or understand spoken language, while most students are unfamiliar with sign language. This communication gap can lead to feelings of isolation and hinder their ability to participate effectively in the learning process. To help bridge this gap and support better integration, Singly was developed as an assistive system aimed at creating a more inclusive environment for deaf and mute students—both in daily life and within the classroom. The system captures spoken audio from users and converts it into text using the Whisper speech recognition engine. This text is then simplified linguistically through the Gemma 3N model to make it easier to comprehend. The final step involves converting the simplified text into sign language videos using pre-trained datasets such as WLASL. By combining speech recognition, language simplification, and sign language rendering, Singly provides a practical and effective solution that improves communication and promotes inclusion. Its goal is to support real-time interaction in educational and social contexts, ensuring that deaf and mute students can engage fully and confidently in their learning journey",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/duaamo/sign-language-backend",
        "demo_video_url": "https://youtu.be/8dkaR_jaK8Q",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/singly-ai-powered-spoken-text-to-sign-language-tra"
    },
    {
        "project_title": "Soberano Enem",
        "tagline": "Gemman 3n 2B fine-tunned and tested for low resource educational setting",
        "authors": "Thiago Porto Soberania",
        "summary": "This project outlines the implementation of advanced fine-tuning techniques for Google's Gemma 3N language models, specifically designed to address question-answering tasks from the Brazilian ENEM (Exame Nacional do Ensino Médio) examinations. It utilizes Low-Rank Adaptation (LoRA), 4-bit quantization, and reasoning-enhanced training datasets. This experimental work is part of the broader SoberanIA initiative—an artificial intelligence (IA) project, that aims to devlop AI built on Portuguese-language data and developed in Brazil. SoberanIA seeks to foster national technological autonomy and digital inclusion, with the goal of optimizing administrative processes, improving government decision-making, and enhancing public services.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/SoberanIA-Oficial/gemma3n-fine-tunning",
        "demo_video_url": "https://youtu.be/H36gM2XxA5w",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/soberano-enem"
    },
    {
        "project_title": "AI Desktop Assistant",
        "tagline": "A personal ai desktop assistant capable or RAG of personal files, audio input.",
        "authors": "JerryBerry5379",
        "summary": "🧠 AI Desktop Assistant — Powered by Gemma 3n & Ollama\nThis project is an intelligent, locally-run desktop assistant built with PySide6 (Qt for Python) and powered by Gemma 3n via Ollama.\n\nIt features a sleek graphical interface where users can:\n\n💬 Chat with an AI assistant (Gemma 3n)\n\n📄 Ask questions using context from personal documents (via retrieval-augmented generation)\n\n🎙️ Use voice input and receive spoken responses\n\n🖥️ Interact with the operating system, including toggling between dark and light themes\n\nDesigned to run entirely on your machine, this assistant emphasizes privacy, performance, and extensibility, making it ideal for productivity, hands-free interaction, or experimentation with local AI workflows. \n\nTechnical Writeup:\nThis was my first pyside project so I may have missed best practices for some areas.\n\nTop level folders and files:\nassets/\nthis folder includes pngs,svg,fonts used in the project \ncore/\nthis folder contains the main functionality and models for the project which includes the operating system functions it may call. \nlogic/\nthis folder contains the logic for the main chat window. It contains the controllers for interactions between different widgets in the main chat window. it also contains functionality for widgets in the main chat window like the microphone, speaker, etc.\nrag_window/ \nthis is a completely seperate window with its own widgets, threads and controllers. this is the window that handles accepting files from the user to use in rag search. it also handles parses the files and creating vector embeddings of those files and storing them persistantly\nstyle/\ncontains qss for the project\nthreads/ \ncontains functions that are to be called on a seperate thread than the main one\nuser_data/\nthis folder stores user data like the vector embeddings\nutil/\nthis folder contains helpful classes and functions for the entire project like updating the style of the application\nwidgets/\nhere contains the main chat window and the widgets inside of it\n\napp.py creates the QApplication object and attribute associated at the application level like styles and icons\nmain.py runs app.py and starts the main event loop for the program",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/jerryx5379/ai_desktop_assistant_v1",
        "demo_video_url": "https://www.youtube.com/watch?v=hudzquQz7wE&feature=youtu.be",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-desktop-assistant"
    },
    {
        "project_title": "Learn Your Own Way",
        "tagline": " Learn My Own Way is an Android app that generates personalized AI courses locally using custom analogies for any topic or uploaded image.",
        "authors": "Karthikeyan Sukumaran",
        "summary": "Learn My Own Way - Personalized AI Education Through Cultural Analogies\n\nAn innovative Android application that transforms complex concepts into familiar analogies, making quality STEM education accessible to every student, regardless of location, connectivity, or cultural background.\n\n---\n\n🌍 The Global Education Crisis We're Solving\n\nIn our interconnected world, education inequality remains one of the most pressing challenges:\n\n- 1.6 billion students** lack access to quality STEM education\n- 40% STEM dropout rate** due to abstract concepts being too difficult to grasp\n- Rural and remote communities** with limited internet connectivity\n- Cultural disconnect** between teaching methods and students' lived experiences\n- Learning differences** requiring personalized approaches\n- Language barriers** preventing effective knowledge transfer\n\nThe Core Problem\nTraditional education treats all students the same, using one-size-fits-all explanations that often rely on unfamiliar contexts. A student in rural Bangladesh learning about computer networks through Silicon Valley examples faces an unnecessary cognitive barrier.\n\n\n💡 Our Revolutionary Solution\n\nLearn My Own Way bridges the gap between complex technical concepts and students' familiar experiences through **AI-powered cultural analogies**. Every explanation adapts to what students already know and understand.\n\nCore Innovation: Cultural Bridge Learning\nInstead of forcing students to understand foreign concepts, we translate knowledge into their world:\n\n- Rural farming community? → Computer networks become irrigation systems\n- Urban sports enthusiast? → Database queries become football play strategies  \n- Cooking-loving student? → Programming functions become recipe instructions\n- Music student? → Neural networks become orchestra coordination\n\nKey Features\n\n1. 🎯 Personalized Analogy Engine\n- Cultural Context Detection: Automatically adapts to local knowledge and interests\n- Learning Style Recognition: Visual, auditory, kinesthetic learning preferences\n\n2. 📱 Complete Offline Functionality\n- Zero Internet Dependency: All AI processing happens on-device\n- Privacy-First Design: No data ever leaves the student's device\n- Works Anywhere: Remote villages, disaster zones, areas with unreliable connectivity\n- Instant Response: No network latency or connectivity interruptions\n\n3. 🌐 Multimodal Learning Experience\n- Visual Recognition: Point camera at circuit diagrams → instant familiar analogies\n- Text-to-Speech: Audio explanations for visually impaired students\n\n\n 🏗️ Technical Architecture\n\nCore Technologies Stack\n- Platform: Android (Kotlin) with Jetpack Compose UI\n- AI Engine: Google Gemma 3n (3B parameter model)\n- ML Framework: Google AI Edge SDK with MediaPipe integration\n- Storage: Room database for offline content and cultural context profiles\n- UI/UX: Material3 Design with accessibility compliance\n\n\n🌟 Conclusion: Democratizing Education Through Cultural Intelligence\n\nLearn My Own Way represents a fundamental shift in how we approach education technology. Instead of forcing students to adapt to technology, we've created technology that adapts to students – their culture, their interests, their way of understanding the world.\n\nBy combining Google's cutting-edge Gemma 3n AI with deep cultural intelligence and accessibility-first design, we're not just building an app – we're building bridges. Bridges between complex technical concepts and familiar human experiences. Bridges between different cultures and universal knowledge. Bridges between educational privilege and educational equity.\n\nOur vision is simple yet profound: Every student, regardless of where they're born or what they know, deserves to understand the technology that shapes our world.Wi th Learn My Own Way, that vision becomes reality.\n\nThe future of education isn't one-size-fits-all. It's personal, cultural, and deeply human. It's learning your own way.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/karthiksuk/learn-your-own-way",
        "demo_video_url": "https://www.youtube.com/watch?v=EGjR6KiaX68",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/learn-your-own-way"
    },
    {
        "project_title": "The Memory Keeper – Raynos Pendant",
        "tagline": "100 % on-device cognitive assistant (Gemma-3N + Unsloth)",
        "authors": "Gavin Mcnally",
        "summary": "EXECUTIVE SUMMARY\nRaynos is a jewellery-grade AI pendant that runs Gemma-3N + Whisper-Tiny entirely on-device. It captures appointments and medication reminders for people with cognitive decline—no wake word, no cloud, no raw audio stored.\n\nTECH HIGHLIGHTS\n• Unsloth-optimised Gemma-3N → 2× faster, –40 % RAM  \n• Latency: 0.89 s per 5 s clip on RTX 4090  \n• All audio deleted after inference. Only AES-256 JSON stored.\n\nNEXT STEPS\nHardware integration (ESP32 pendant) and battery optimisation.\n\nLICENSE\nApache-2.0.  Gemma weights under Google terms.  Unsloth credited in NOTICE.\n\nWon't let me share links",
        "technologies": [],
        "source_code_url": "https://github.com/mmusil25/raynos-ai",
        "demo_video_url": "https://youtu.be/u2AMK56FoY4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/the-memory-keeper-raynos-pendant"
    },
    {
        "project_title": "NEURO DERM, a fine tuned Gemma 3n to improve skin cancer detection",
        "tagline": "An Unsloth-finetuned Gemma 3N model that analyzes skin lesion images with high accuracy.",
        "authors": "Fabio Alves",
        "summary": "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F27692788%2F356e0a3dab7ac63502e0d67ea2641040%2Fbanner.jpg?generation=1754517252942958&alt=media)\n\n\n**NEURO DERM: Accelerating Skin Cancer Diagnosis with Unsloth Fine-Tuned Gemma 3N**\n\n![https://youtu.be/9D0wau7fU-Y](https://youtu.be/9D0wau7fU-Y)\n**[NEURO DERM video project](https://youtu.be/9D0wau7fU-Y)**\n\n**[Submitted notebook](https://www.kaggle.com/code/punkada/gemma-3n-unsloth-finetune-ham10000)**\n\n**Introduction**\n\nSkin cancer is a significant global health concern, and early, accurate diagnosis is crucial for improving patient outcomes. While dermatologists are highly skilled, access to specialists can be a bottleneck, leading to diagnostic delays. Artificial intelligence, particularly advanced vision-language models, presents a powerful opportunity to augment dermatological analysis.\n\nNEURO DERM is a project that leverages the cutting-edge Gemma 3N model, fine-tuned with the highly efficient Unsloth library, to create a powerful tool for analyzing dermatoscopic images. Our goal is to provide a fast, accurate, and accessible solution that can assist medical professionals and raise awareness about skin lesion characteristics. \n\n\n**Methods and Technical Deep Dive: The Power of Unsloth**\n\nThe core of NEURO DERM is a Gemma 3N model fine-tuned on the publicly available **HAM10000 (\"Human Against Machine with 10000 training images\") dataset**. This dataset contains over 10,000 dermatoscopic images of various pigmented skin lesions.\n\nOur fine-tuning process was made possible by the **Unsloth library**. Unsloth's memory-efficient and speed-optimized training methods allowed us to fine-tune the Gemma 3N model on a Kaggle notebook environment, a task that would otherwise be computationally prohibitive. The key technical aspects of our approach are detailed in our training notebook:\n\n*   **Efficient Fine-Tuning with LoRA:** We employed Low-Rank Adaptation (LoRA) to efficiently adapt the pre-trained Gemma 3N model to our specific task of dermatological image captioning. By using Unsloth, we were able to train the model with achieving a balance between accuracy and preventing overfitting.\n*   **Optimized Training:** The entire training process, from data preprocessing to model training and evaluation, is documented in our comprehensive Kaggle notebook. The use of Unsloth resulted in a significant reduction in memory usage and training time, making the project feasible and reproducible.\n\nThe fine-tuned LoRA adapters for this model are publicly available on Hugging Face: **[aka-selvas/gemma3n-finetuned-skincancer](https://huggingface.co/aka-selvas/gemma3n-finetuned-skincancer)**\n\n\n**Performance and Evaluation: High-Fidelity Analysis**\n\nTo validate the effectiveness of our fine-tuned model, we conducted a rigorous evaluation against a held-out test set from the HAM10000 dataset. The model was tasked with generating descriptive captions for the dermatoscopic images, which were then compared against ground-truth descriptions. The results demonstrate the model's high level of accuracy and its ability to capture the nuanced details of the skin lesions:\n\n*   **BLEU Score:** 0.8251\n*   **ROUGE-L Score:** 0.9080\n*   **METEOR Score:** 0.9122\n*   **BERTScore (F1):** 0.9770\n\nThese outstanding scores, particularly the high BERTScore, indicate that the model's generated descriptions are not only syntactically correct but also semantically very close to the expert-provided ground truth.\n\nYou can try the Performance and Evaluation yourself through our Kaggle notebook: **[Gemma-3N Unsloth Evaluation](https://www.kaggle.com/code/punkada/gemma-3n-unsloth-ham10000-evaluation)**\n\n\n**NEURO DERM: The Application**\n\nTo make our powerful model accessible and user-friendly, we developed  **NEURO DERM**, a web-based application built with Gradio. Our initial goal was to deploy the model on-device using a framework like Google AI Edge, leveraging Gemma 3N's inherent design for mobile and edge computing. However, while the base Gemma 3N model is optimized for such deployments, a clear and established pipeline for deploying fine-tuned versions with LoRA adapters does not yet exist. In light of this current technical limitation, we opted for a Gradio web app as the most effective way to showcase the model's capabilities. We are actively monitoring developments and are fully committed to migrating NEURO DERM to a true on-device application as soon as the Google team provides a supported pathway for deploying fine-tuned models.\n\nYou can try the NEURO DERM application yourself through our Kaggle notebook: **[Gemma-3N Unsloth LoRA Adapter running at Gradio](https://www.kaggle.com/code/punkada/gemma-3n-unsloth-lora-adapeter-gradio)**\n\nNEURO DERM application screenshot running with gradio\n![NEURO DERM application screenshot running with gradio](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F27692788%2F7f858b5a31787ec9c9497ee7f91efcd7%2F2.jpg?generation=1754522375865118&alt=media)\n\n\n**Broader Context and Future Work**\n\nOur work is inspired by the growing body of research on the application of AI in dermatology, such as the work by Mahmoud & Soliman (2024) on automated skin cancer detection systems and the review by Brancaccio et al. (2024) on the reality of AI in skin cancer diagnosis. While NEURO DERM is not a medical device, it serves as a powerful proof-of-concept for how Unsloth-accelerated fine-tuning can create specialized AI tools for the medical field.\n\nCurrently, deploying the fine-tuned Gemma 3N model directly on mobile devices presents challenges. However, we are confident that as the technology evolves, on-device deployment will become feasible, further increasing the accessibility of tools like NEURO DERM.\n\n\n**Conclusion**\n\nThe NEURO DERM project successfully demonstrates the creation of a high-performing, specialized vision-language model for dermatological image analysis, made possible through the efficiency of Unsloth. By fine-tuning Gemma 3N on a targeted dataset, we have developed a tool that is not only accurate but also accessible through a user-friendly web application. We believe this project exemplifies the power of Gemma 3n + Unsloth and public datasets to enable impactful and innovative AI solutions.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/9D0wau7fU-Y",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/neuro-derm-a-fine-tuned-gemma-3n-to-improve-skin-c"
    },
    {
        "project_title": "Gemma Scout",
        "tagline": "On-device AI camping & wildlife survival companion",
        "authors": "Ryan Rong",
        "summary": "About **88 million** U.S. households now identify as campers, and **~54 million** households took a trip last year\n\nNational parks run **thousands of search-and-rescue** operations every year (e.g., ~3,400 in 2022), and research shows **tens of thousands of people were lost** in parks over the 2004–2014 decade.\n\nFirst aid, navigation, shelter, and water skills reduce risk and help you protect others. That’s why I built Gemma Scout: an **on-device, privacy-first AI assistant** built for wilderness survival and outdoor adventures. Powered by a finetuned version of Gemma 3N, it combines domain-specific survival knowledge with real-time multimodal reasoning to guide users through critical tasks like plant identification, mushroom safety, shelter building, and first aid.\n\nThe model was trained on over **30 million tokens** and **7,618 curated Q&A pairs**, sourced from expert handbooks and image datasets of edible plants and mushrooms. Leveraging Unsloth’s optimized finetuning framework with LoRA and quantization, Gemma Scout runs efficiently on mobile devices without internet access.\n\nUsing LLM evaluations, Gemma Scout outperformed its base model in survival QA by a win rate of **89%**, and achieved over **36% improvement** in mushroom classification and **47%** in plant identification. It supports multimodal chat via llama.cpp mtmd and Swift integration, enabling users to send images and receive grounded, expert-level responses—completely serverless.\n\nWhether you're camping, hiking, or lost in the wild, Gemma Scout helps you make smart, safe decisions—even when you're off the grid.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Ryan-Rong-24/gemma-scout-ui",
        "demo_video_url": "https://youtu.be/IVl1qrOzYB0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-scout"
    },
    {
        "project_title": "Sydney - Your Private Mac Productivity Assistant",
        "tagline": "100% Offline, Voice-Driven Productivity - With Extreme Privacy",
        "authors": "Arav Jain, ddvader44",
        "summary": "Sydney is a next-generation productivity assistant built exclusively for Mac users who demand both efficiency and privacy. Running entirely offline, Sydney leverages advanced AI models using Gemma3n, Ollama, Whisper, and Siri TTS to deliver a seamless, voice-first experience. With all conversations held only in RAM, your data never leaves your device, ensuring absolute confidentiality.\n\nSydney adapts to your Mac’s capabilities, intelligently selecting the best AI model for your hardware. Use natural voice commands to manage your calendar, set meetings, summarize emails, get coding help, perform quick calculations, and more - all hands-free. With a friendly, empathetic personality and features like screen time analysis, decision support, and personal therapy, Sydney is designed to boost your productivity and well-being while keeping your life private and secure.\n\nOur implementation features wake word detection, multi-turn conversations with context memory, email integration, and intelligent model routing between Gemma 3n E2B (2GB, fast) and E4B (3GB, accurate) variants based on query complexity.\n\nPlease refer to our github readme for more installation steps and more info!",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/aravhawk/sydney",
        "demo_video_url": "https://youtu.be/rrIx54T37V8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sydney-your-private-mac-productivity-assistant"
    },
    {
        "project_title": "Bubble - Stop Searching, Start Doing",
        "tagline": "Bubble combines intelligence with on-screen awareness to guide you through any app roadblock - no tutorials, no docs, just clear steps.",
        "authors": "puneet sadhwani, hoosierdaddy",
        "summary": "Bubble transforms every software interface into an intelligent, responsive experience by seeing your screen, interpreting your goals, and guiding you in real time with visual cues and natural language - so you can stop wasting time on tutorials, help docs, and how-to videos, and just get things done. Unlike traditional help bots, Bubble understands what’s happening on your screen - detecting UI elements, interpreting user intent, and guiding actions using natural language and highlighted cues. Built with Electron and powered by Gemma 3n, it’s like having a smart co-pilot for any app - reactive, accurate, and always in context.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/1rvinn/bubble_v2",
        "demo_video_url": "https://youtu.be/pWIBtbkfqjw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/bubble-stop-searching-start-doing"
    },
    {
        "project_title": "AI-Powered Web Accessibility Extension: Toward Personalized Web Experiences",
        "tagline": "Leveraging Gemma 3n for Real-Time Website Enhancement",
        "authors": "nehappy",
        "summary": "**What if websites could adapt to YOU instead of forcing you to adapt to them?**\nThis Chrome extension uses Gemma 3n to automatically transform inaccessible websites in real-time. While you browse, AI actually sees images, understands context, and instantly fixes barriers that leave millions behind - generating meaningful alt text, descriptive links, and proper form labels on the fly. This isn't just another accessibility tool - it's the first step toward a web where every person gets an experience designed specifically for them.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/neha26pp/browser-ext",
        "demo_video_url": " https://youtu.be/9KVfc-yXP2I",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-powered-web-accessibility-extension-toward-pers"
    },
    {
        "project_title": "Gemma-GR00T Lab - Democratizing Private On-Device AI ",
        "tagline": "Gemma Groot democratizes scientific robotics by combining Gemma 3n's multimodal AI with an action diffusion model policy",
        "authors": "Gyanateet Dutta, Jackson Mathew, Sarah, Sid Elias, Hamadelnil",
        "summary": "Project Overview: Gemma-GR00T Lab\n\nMission: Democratize precision robotics in scientific research through private, offline-first multimodal AI that runs entirely on-device, enabling breakthrough discoveries in resource-constrained environments.\n\nGemma-GR00T-Lab is a vision-language-action pipeline that executes fully on consumer devices, enabling nanomanipulation and scientific imaging without cloud GPUs. It fine-tunes Google Gemma-3n E4B for domain language, fuses SigLIP vision and USM audio encoders, and drives a diffusion-based action head adapted from NVIDIA GR00T-N1.5.\n\n1. Executive Summary & Impact Vision\n\nThe Problem We're Solving\nScientific research laboratories, particularly in developing regions and specialized facilities, face critical barriers:\n\nPrivacy Constraints: Sensitive research data cannot be sent to cloud services\n\nConnectivity Issues: Remote labs lack reliable internet for cloud-based AI\n\nCost Barriers: Traditional robotic automation costs $100,000+ per system\n\nAccessibility: Researchers with disabilities need voice-driven interfaces\n\nPrecision Requirements: Nanoscale manipulation demands sub-millimeter accuracy\n\nOur Solution: Gemma-GR00T Lab\n\nA Vision-Language-Action (VLA) model that brings advanced robotic manipulation to any research environment by:\n\nRunning 100% offline on consumer hardware (NVIDIA Jetson Orin Nano)\n\nProcessing multimodal inputs (voice commands, microscope images, text instructions)\n\nGenerating precise 7-DoF robotic actions for scientific manipulation tasks\n\nSupporting multiple deployment paths (Android, desktop, embedded systems)\n\nReal-World Impact\n\nDemocratization: Enables $2,000 robotic setups vs $100,000+ traditional systems\n\nPrivacy-by-Design: All processing happens on-device, protecting IP and sensitive data\n\nGlobal Accessibility: Works in remote locations without internet connectivity\n\nSustainability: 25W power consumption vs 500W+ cloud alternatives\n\n2. Technical Architecture: Leveraging Gemma 3n's Unique Capabilities\nCore Gemma 3n Integration\nMatFormer Elastic Inference\n\nUtilizes Gemma 3n's MatFormer (Matryoshka Transformer) architecture for dynamic resource allocation\n\nMix'n'match capability: Adapts model size based on task complexity (2B for simple commands, 4B for complex manipulation)\n\nPer-Layer Embedding (PLE): Reduces memory footprint by 40% through intelligent caching\n\nMultimodal Capabilities\n\nVision Processing: SigLIP encoder handles microscope imagery, chemical structure recognition\n\nAudio Understanding: 30-second audio input for voice commands in noisy lab environments\n\nText Integration: 32K context length for complex experimental protocols\n\nUnified Output: Generates precise robotic actions from fused multimodal understanding\n\nArchitecture Details\n\nMultimodal Inputs → Gemma 3n E4B → GR00T Action Head → Robot Control\n    ↓                    ↓                ↓               ↓\n• Camera feeds      • MatFormer       • Diffusion      • 7-DoF actions\n• Voice commands    • SigLIP vision   • Flow-matching  • Gripper control  \n• Text protocols    • Qwen3 language  • 4-layer transformer • Safety checks\nModel Components\n\nBackbone: Eagle2.5 VLForConditionalGeneration (1.7B parameters)\n\nVision Encoder: SigLIP-base-patch16-224 (768 hidden size, 12 layers)\n\nLanguage Model: Qwen3-1.7B (28 layers, 32K context, GQA attention)\n\nAction Head: ScaledDP 4-layer transformer with diffusion-based policy\n\n(see the generated image above)\n\nTask Performance Validation\n\n3. Implementation & Training Details\nTraining Performance\nDataset: LeRobot manipulation tasks + custom NanoPick simulation dataset\n\nTraining Steps: 30,000 steps over 240 GPU hours\n\nLoss Reduction: 1.27 → 0.11 (91% improvement)\n\nHardware: 3× NVIDIA L40S GPUs\n\nFramework: PyTorch + Hugging Face Transformers + LeRobot\n\nLeRobot Prize Track\n\nIntegration: Native LeRobot dataset compatibility\n\nSimulation: Isaac Sim environment for training and validation\n\nReal-world Transfer: 83% sim-to-real success rate\n\nEngineering Challenges & Solutions\nFP16 overflow in vision Conv2D → upcast to FP32 during inference.\n\nOllama GGUF mis-align with per-layer emb → regenerated quant (Uns-Dynamic 2.0).\n\nRobot jitter mitigated by TCP-AI smoothing loss.\n\nResponsible AI\nAll on-device; no user data leaves lab → privacy-by-design.\n\nSafety checker: ShieldGemma 2 image filter in preprocessing pipeline.\n\nEnergy: <25 W Jetson run; estimated 2.3 kg CO₂ per month vs cloud 46 kg.\n\n3. Public Code Repository\nGitHub URL: github.com/YanaLabs/Gemma-GR00T-Lab\nStructure:\n\ntext\n├─ app/\n│  ├─ edge_android/        AI Edge demo\n│  ├─ jetson_ollama/       Dockerfile + systemd\n│  └─ desktop_streamlit/   Live web UI\n├─ model/\n│  ├─ lora_weights/        12 MB\n│  ├─ gr00t_diffusion/     action head\n│  └─ training_scripts/\n├─ data/\n│  └─ nanopick_dataset.kaggle.yaml\n└─ docs/\n   └─ tech_writeup.md\nIncludes one-click Kaggle Notebook reproducing fine-tune (meets “source of truth” rule).\n\n4. Engineering Challenges & Solutions\nTechnical Innovations\n1. Float16 Stability Issues\n\nProblem: Gemma 3n encounters NaN/infinity values on float16 GPUs\n\nSolution: Implemented autocast-based upscaling to float32 for Conv2D operations\n\nImpact: Enables deployment on consumer hardware (Tesla T4, RTX 3060)\n\n2. Gradient Checkpointing Limitations\n\nChallenge: Vision encoder reuses hidden states, limiting memory optimization\n\nSolution: Applied selective checkpointing with Unsloth's automatic compiler\n\nResult: 40% memory reduction while maintaining training stability\n\n3. Multimodal Loss Scaling\n\nObservation: Initial training loss of 6-7 (higher than typical LLMs)\n\nAnalysis: Normal behavior for multimodal models with audio/vision components\n\nValidation: Model performance remains excellent despite high initial loss\n\n5. Real-World Applications & Use Cases\nLaboratory Scenarios\nProtein Crystallography: Automated crystal mounting with voice guidance\n\nMaterials Science: Nanowire manipulation based on microscope feedback\n\nChemical Synthesis: Precise reagent dispensing following verbal protocols\n\nBiological Research: Cell manipulation with contamination-free operation\n\nAccessibility Features\nVoice-Only Operation: Complete system control via natural language\n\nVisual Feedback: Audio descriptions of robotic actions for visually impaired researchers\n\nMultilingual Support: 140+ language support for global research collaboration\n\n6. Evaluation & Validation\nPerformance Metrics\nManipulation Success: 88% on pick-and-place, 83% on stacking tasks\n\nPrecision: Sub-millimeter accuracy for nanoscale positioning\n\nLatency: Real-time operation at 119ms response time\n\nGeneralization: 75% success rate on completely novel objects\n\nSafety & Reliability\nCollision Avoidance: ShieldGemma integration for safety checking\n\nRecovery Mechanisms: Automatic error detection and corrective actions\n\nData Privacy: Zero-trust architecture with all processing on-device\n\n7. Competitive Advantages & Innovation\nNovel Contributions\nFirst VLA Model on Gemma 3n: Pioneering integration of Google's latest multimodal architecture\n\nScientific Domain Adaptation: Custom training on laboratory manipulation tasks\n\nEdge-Optimized Deployment: Multiple hardware targets from mobile to desktop\n\nPrivacy-First Design: Complete offline operation protecting sensitive research\n\n8. Future Impact & Scaling\nImmediate Deployment (6 months)\nPartner with 10 university labs for pilot testing\n\nOpen-source release enabling community contributions\n\nIntegration with existing microscope/robotic systems\n\nLong-term Vision (2-3 years)\nGlobal Research Acceleration: Enable breakthrough discoveries in under-resourced regions\n\nEducational Democratization: Bring advanced robotics to teaching laboratories worldwide\n\nCrisis Response: Rapid deployment for contamination handling, remote research\n\nSustainability Impact\nCarbon Footprint: 120 kg CO₂eq vs 1,200 kg for cloud alternatives\n\nEconomic Access: 95% cost reduction compared to commercial systems\n\nKnowledge Democratization: Open-source approach enables global innovation\n\n9. Code Repository & Live Demo\nGitHub Repository: https://github.com/Ryukijano/Gemma-Grook\n\nComplete implementation with Docker containerization\n\nComprehensive documentation and setup guides\n\nIntegration examples for multiple hardware platforms\n\nEvaluation scripts and benchmark datasets\n\n10. Conclusion: The Future of Scientific Discovery\nGemma-GR00T Lab represents a paradigm shift in scientific automation—moving from expensive, cloud-dependent systems to accessible, privacy-preserving, on-device intelligence. By leveraging Gemma 3n's groundbreaking MatFormer architecture and multimodal capabilities, we've created a solution that not only meets the technical requirements but addresses the fundamental barriers preventing widespread adoption of robotic automation in research.\n\nThis project demonstrates the transformative potential of private, offline-first AI to democratize access to advanced scientific tools, ultimately accelerating discovery and innovation across the global research community.\n\nTechnical Specifications Summary:\n\nModel: Gemma 3n E4B with custom GR00T integration\n\nPerformance: 88% task success, 119ms latency\n\nDeployment: Jetson Orin Nano, Android Edge, Desktop Ollama\n\nTraining: 30K steps, 0.11 final loss, LeRobot integration\n\nImpact: 95% cost reduction, 100% privacy preservation, global accessibility",
        "technologies": [
            "Docker",
            "Gemma 3n",
            "NVIDIA Jetson",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Ryukijano/Gemma-Grook",
        "demo_video_url": "https://youtu.be/kB_AvraKymk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-gr00k-lab-democratizing-private-on-device-ai"
    },
    {
        "project_title": "CanaryOffline",
        "tagline": "An intelligent, offline-capable learning assistant that helps students master topics through interactive conversations, voice recognition, a",
        "authors": "IBRAHIM MUSTAFA",
        "summary": "Canary OFFnline - AI-Powered Learning Assistant\n- Github: [https://github.com/99i/CanaryOffline](url)",
        "technologies": [],
        "source_code_url": "https://github.com/99i/CanaryOffline",
        "demo_video_url": "https://www.youtube.com/watch?v=zFBgixQ0b0o",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/canaryoffline"
    },
    {
        "project_title": "Echo Guide Vision: AI Powered by Gemma",
        "tagline": "Eye that  never sleeps",
        "authors": "aya1490",
        "summary": "###🌟 **Echo Guide Vision: AI Powered by Gemma**\n     *Your Second Eye — One That Never Leaves You*\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F27972555%2F89e7b0c157a238bc6e4c70f5d33175e3%2Fecho_logo.jpg?generation=1754499607749038&alt=media)\n\nEcho Guide Vision leverages **Gemma’s on-device multimodal capabilities** to combine speech input, visual analysis, and contextual AI answers. Models are dynamically selected and downloaded based on user language, optimizing performance while preserving device storage — a practical application of Gemma’s \"mix-and-match\" and lightweight deployment strengths.\n\nThis assistant helps visually impaired users identify objects, recognize faces, and understand their surroundings in real time.\n\n\n---------------------------------------------------------------------------------------------------------------------\n 🌟** Inspiration**\n\nAfter the Ramsis Central fire, many people lost access to smart assistants, GPS, and audio services. This raised a deep question:\nHow can blind individuals survive without internet-dependent assistants?\n\n**I asked myself**:\n\nWhat if a blind person is cooking and the assistant stops responding?\n\nWhat if they are walking, needing traffic updates — and the voice freezes?\n\nWhat if they’re going to an appointment and can’t choose clothes due to no visual aid?\n\n✔️So I created Echo Guide Vision, a fully offline smart assistant app for blind and visually impaired individuals. It combines voice recognition, scene analysis, and AI intelligence — without internet.\n\n--------------------------------------------------------------------------------------------------------------------------------------------------\n\n###🌍 **Why the World Needs Echo Guide Vision**\n\n- **2.2 billion** people worldwide live with some form of vision impairment  \n  🔗 [World Health Organization (WHO)](https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment)\n\n- Of these, **43 million** are blind, and **295 million** have moderate to severe distance vision impairment  \n  🔗 [Vision Center 2025 Report](https://www.visioncenter.org/resources/blindness-facts/)\n\n---\n\n#",
        "technologies": [
            "Flask",
            "Gemma 3n",
            "Vosk"
        ],
        "source_code_url": "https://github.com/ayaelsaid/Echo_guide_vision",
        "demo_video_url": "https://youtu.be/IIuuSNvi25Q",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/echo-guide-vision-ai-powered-by-gemma"
    },
    {
        "project_title": "Book Bridge",
        "tagline": "BookBridge: Empowering Local Booksellers with Offline, Multilingual AI",
        "authors": "MD ASHEEF NAGARALLI",
        "summary": "BookBridge: Empowering Local Booksellers with Offline, Multilingual AI\nThe Mission: Bridging the Language Gap for Small Bookstores with Gemma 3n\nProblem statement - The idea is inspired by the unique challenge facing our local bookseller, Ramesh. He possesses an extraordinary talent: despite being unable to read or write in English, he can instantly identify any book in his vast collection based on two unique identifiers—the visual memory of its cover art and the phonetic sound of its title when spoken by a customer. While this skill is remarkable, it creates a fundamental business limitation. Because he cannot write the English titles, creating a traditional, searchable inventory to manage stock, track prices, and note shelf locations is an impossible task. His entire operation is therefore capped by the limits of human memory, creating a critical need for a tool that can bridge the gap between his visual-auditory world and the text-based world of digital inventory.\n\n-Our project, BookBridge, tackles a significant, real-world challenge: empowering small, non-English speaking business owners with modern inventory management tools. We were inspired by a local second-hand bookseller who, despite not reading English, could identify any book in his shop by its cover. We saw an opportunity to use Gemma 3n's unique on-device capabilities to build a tool that works in his world—offline, in his language, and with a simple, intuitive interface.\nBookBridge is more than a chatbot; it is a complete, offline-first, AI-powered inventory and sales assistant that uses a conversational interface to feel as natural as talking to a helper. It allows the bookseller to instantly digitize his inventory using his phone's camera, manage stock, and find books for customers in seconds, all without needing an internet connection.\nSubmission Links\nThe Technical Writeup: Our Proof of Work\nThis project's primary goal was to prove that Gemma 3n could create a tangible, positive impact. Our technical choices were driven by the real-world constraints of our user: unreliable internet, the need for a simple UI, and a multilingual environment.\n1. High-Level Architecture\nWe designed BookBridge using a clean, layered architecture on the Android platform. This ensures a clear separation of concerns between the user interface, business logic, and the AI/data layers, making the app robust and maintainable.\ncode\nCode\n┌─────────────────────────────────────────────────────────────┐\n│                    PRESENTATION LAYER (Chat UI)             │\n├─────────────────────────────────────────────────────────────┤\n│                     BUSINESS LOGIC LAYER (Inventory Logic)  │\n├─────────────────────────────────────────────────────────────┤\n│                     DATA ACCESS LAYER (Room Database)       │\n├─────────────────────────────────────────────────────────────┤\n│                    INTEGRATION LAYER (MediaPipe + Gemma 3n) │\n├─────────────────────────────────────────────────────────────┤\n│                      HARDWARE LAYER (Camera, CPU, Storage)  │\n└─────────────────────────────────────────────────────────────┘\n2. How We Used Gemma 3n: The Core of the Solution\nGemma 3n's unique features were not just helpful; they were essential to making BookBridge possible.\n✅ Offline Ready & Privacy-First: This is the most critical feature. Our user operates in an environment with spotty connectivity. By running Gemma 3n entirely on-device via the MediaPipe LLM framework, the app is 100% functional offline. This guarantees reliability and ensures the bookseller's private sales data never leaves his phone.\n✅ Expanded Multimodal Understanding: We leveraged Gemma 3n's ability to process both images and text. The core workflow is:\nThe user takes a photo of a book cover.\nThe image is processed and fed to the Gemma 3n model.\nThe model performs Optical Character Recognition (OCR) to extract the book's title directly from the cover art.\n✅ Improved Multilingual Capabilities: This is where the magic happens. After extracting the English title, we use Gemma 3n's multilingual understanding to instantly translate the title into Kannada. This allows the bookseller to manage his inventory and search for books entirely in his native language, bridging the digital divide.\n3. Key Technical Choices & Challenges Overcome\nChallenge: Creating a user interface for a non-technical user who is unfamiliar with complex software.\nSolution: A Conversational UI. Instead of forms and buttons, we built the entire app around a chat interface. Adding a book is as simple as sending a photo. Finding a book is as easy as asking a question. This leverages the user's familiarity with messaging apps.\nChallenge: Managing the memory and performance of a large AI model on a resource-constrained mobile device.\nSolution: The \"Fresh Session\" Strategy. To prevent context overflow and ensure the app remains stable and responsive, we create a fresh LLM Inference session for each query (each picture scan or search). This pragmatic choice prioritizes rock-solid reliability over session reuse, which is critical for a tool meant for daily business use.\nChallenge: Storing and retrieving inventory data offline.\nSolution: Android Room Database. We integrated the standard Android Room library to create a robust, local SQLite database on the device. This database stores the English title, Kannada title, price, location, and quantity for every book, and it's what enables the instant search functionality.\n4. Tech Stack\nPlatform: Android Native (for maximum performance)\nLanguage: Kotlin (with Coroutines for background processing)\nAI Framework: MediaPipe LLM Inference API\nAI Model: Gemma 3n 2B (INT4 quantized)\nDatabase: Android Room Persistence Library\n5. Vision & Impact\nBookBridge is a testament to the power of personal, on-device AI. Our vision is to extend this platform to other small, local businesses—from neighborhood pharmacies to handicraft stalls—providing them with tools that respect their language, their privacy, and their operational reality. By putting the power of Gemma 3n directly into the hands of people like our local bookseller, we can create meaningful economic empowerment and ensure that the benefits of the AI revolution are accessible to everyone, not just those with a stable internet connection or a knowledge of English.\n\nDisclaimer: Development Process & Tooling\nThis application was created by a solo builder with a vision for solving a real-world problem. To bring this idea to life rapidly, the development process was augmented by the AI coding assistant, Augment. Its role was primarily to translate high-level logic and component descriptions into functional Kotlin/Android XML syntax. The core architectural design, the specific implementation of the Gemma 3n workflow, the data flow, and the final debugging and testing were all directed by the project lead. This approach demonstrates how modern tooling can empower individuals to build impactful, AI-driven applications.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/mdasheef99/gemma_3n_2B.git",
        "demo_video_url": "https://youtu.be/ChP5-G93dfs",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/book-bridge"
    },
    {
        "project_title": "Tamazirt Multi-Lingo Emergency Response App",
        "tagline": "Offline AI translation app for Morocco emergency responders using fine-tuned Gemma 3N models with dual-mode architecture",
        "authors": "Gregory Kennedy, Ziyad Tber, Hamza Eltouzany",
        "summary": "**🚨 Tamazirt Multi-Lingo Emergency Response App**\n\n**Problem Statement:**\nThe 2023 Morocco earthquake exposed critical communication barriers between rescue workers and Tamazight-speaking Amazigh communities. This app provides offline translation services for emergency responders in infrastructure-compromised disaster zones.\n\n**Technical Achievement:**\n- Fine-tuned 15+ Gemma 3N models (2B/4B params) with 100K+ translation pairs\n- Discovered and documented universal Gemma 3N deployment limitations across all toolchains\n- Engineered innovative dual-architecture: offline SQLite + online Convex backend\n- Built production-ready React Native app with custom Tifinagh keyboard\n\n**Innovation:**\nWhen on-device deployment proved impossible due to Gemma 3N's novel architecture, we pivoted to embed the model's knowledge directly into the app via a curated dataset. This creates an authentic offline AI experience while enabling advanced online collaboration features.\n\n**Impact:**\nA functional humanitarian tool ready for real-world deployment, combining cutting-edge AI with cultural preservation for Morocco's indigenous Berber communities.\n\n**Tech Stack:** React Native, Expo, SQLite, Convex, Gemma-3 API, TypeScript",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/mindful-ai-dude/v3.6-Tamazight_MultiLingo_App",
        "demo_video_url": "https://youtu.be/dAheQROJbjw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/tamazirt-multi-lingo-emergency-response-app"
    },
    {
        "project_title": "Sci-Sonke: AI Powered Multi-Lingual Voice-First System",
        "tagline": "Sci-Sonke is a voice-powered, multilingual AI platform that blends learning, family connection and support. Powered by Gemma 3n! ",
        "authors": "Tshembhani Baloyi",
        "summary": "🧠 Sci-Sonke: Multilingual Voice-Based Platform, Powered by Gemma 3n — Built for Real-World Constraints\n\n> “If you want to go fast, go alone. If you want to go far, go together.”  \n> — African proverb",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/gTZm8r1tgmk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sci-sonke-ai-powered-multi-lingual-voice-first-sys"
    },
    {
        "project_title": "An Emergency Info Assistant Running on Jetson Orin Nano",
        "tagline": "A local, offline AI assistant for first aid and survival Q&A, running on NVIDIA Jetson with Gemma. Step-by-step emergency instructions",
        "authors": "Matthew Shigley",
        "summary": "The Emergency Info Assistant is an offline, AI-powered tool that provides quick, accurate first aid and survival instructions, even when there’s no internet connection. Built to run on the NVIDIA Jetson Orin Nano, this assistant uses the advanced Gemma 3n language model (with Q8_0 quantization) to answer natural language questions about emergency scenarios in real time.\n\nDesigned for reliability and speed, the assistant delivers clear, step-by-step guidance for a wide range of situations, from treating wounds and burns to handling natural disasters. Everything operates locally, so your information stays private and accessible even in remote locations or during network outages.",
        "technologies": [
            "Gemma 3n",
            "NVIDIA Jetson"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/ZziWwIh4moo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/an-emergency-info-assistant-running-on-jetson-orin"
    },
    {
        "project_title": "SeeLens Platform",
        "tagline": "SeeWorld With Gemma",
        "authors": "Helmi Rinaldi",
        "summary": "SeeLens Is A Platform Application with have a 3 function first is for health assistant,second is for mental health assistant using face analysis,and the last function is for education assistant with have a feature chain of your response and user can select the next response using the before output message",
        "technologies": [],
        "source_code_url": "https://github.com/Helmirinaldi45/SeeLens",
        "demo_video_url": "https://youtu.be/DwYfCVVYGgk?si=hy8xzIEk57ez-WY8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/seelens-platform"
    },
    {
        "project_title": "Eldercaregemma3n",
        "tagline": "Next Generation Elderly Care",
        "authors": "Evans",
        "summary": "🌟 Vera: AI Companion for Meaningful Aging",
        "technologies": [
            "Flask",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/evanss23/eldercare-demo",
        "demo_video_url": "https://youtu.be/GZAuuYKEaPk?si=grbk1ain-yhveZmh",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/eldercaregemma3n"
    },
    {
        "project_title": "WarChild Care Bridge: AI Trauma Assessment for Children in War Zones",
        "tagline": "Connecting volunteer psychologists worldwide with war-affected children through specialized AI that understands trauma across cultures.",
        "authors": "Soufiane DAHIMI",
        "summary": "WarChild Care Bridge: AI Trauma Assessment for Children in War Zones\n\n*Connecting volunteer psychologists worldwide with war-affected children through specialized AI that understands trauma across cultures and languages.*\n\n---",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Dahimi/care-bridge-platform",
        "demo_video_url": "https://youtu.be/2boZ7w_oG5k",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/warchild-care-bridge-ai-trauma-assessment-for-chil"
    },
    {
        "project_title": "HearEmotions",
        "tagline": "Helping Everyone Understand Sounds – Even Without Hearing Them",
        "authors": "Jannik",
        "summary": "HearEmotions is a tool designed to help deaf and hard-of-hearing people understand what’s happening around them through sound. It doesn’t just detect a sound—it explains what the sound is and why it might be happening.  \nFor example:  \n- “Your dog is barking. It looks like the dog is scared.”  \n- “Your baby is crying. The baby is hungry.”  \n\nThis makes sounds more meaningful and easier to understand, even if you can’t hear them.",
        "technologies": [],
        "source_code_url": "https://github.com/Jannik-ai24m054/HearEmotions/tree/main",
        "demo_video_url": "https://youtu.be/lQKsHyOjpJM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/hearemotions"
    },
    {
        "project_title": "Language-Learning-App-With-Gemma3n",
        "tagline": "Generate AI lessons with Gemma3N, backed by ollama. From lessons, to contextual mistake analysis, to roleplay chatting to simulate scenarios",
        "authors": "Marius Caran, Lighter199",
        "summary": "We are using a NextJS and ollama as backend to generate lessons on the fly based on preferences, language level and different topics.\n\n\n\nTwitter Video:\nhttps://x.com/Mandruyd/status/1953228087975379114\n\nBlog Technical Write-up:\nhttps://blog.marius-digital.com/2025/08/05/LanguageLearningApp-With-Gemma3N\n\nDemo Website:\nhttps://learning-app.marius-digital.com/\n\nProject code:\nhttps://github.com/Mandruyd/gemma3n-learning-app",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/EMxGxSgQ5mQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/language-learning-app-with-gemma3n"
    },
    {
        "project_title": "WiseWallet: AI-Powered Personal Finance Intelligence",
        "tagline": "Democratizing Financial Literacy Through On-Device Gemma 3n AI",
        "authors": "Ibrahim Mohamed Embaby",
        "summary": "wiseWallet is an AI-powered personal finance tracker designed to help you take control of your money with intelligence and ease. Built with Google’s Gemma 3n AI model, wiseWallet goes beyond traditional budgeting apps by providing smart predictions, automated assistance, and insightful analytics — all personalized to your financial behavior.\n\nIn a world where most people live paycheck to paycheck, wiseWallet empowers you with clarity, confidence, and control over your finances.\n\nPurpose:\nTo simplify money management and help users make better financial decisions through intelligent automation and personalized insights powered by AI.\n\nKey Features:\n🔍 Home Overview\nGet a clear snapshot of your total income, expenses, and current balance right on the home screen.\n\n📊 AI-Powered Forecasts\nReceive smart monthly predictions and insights about your upcoming financial trends.\n\n➕ Quick Transaction Entry\nAdd transactions easily using the floating action button. Select type, input details, or let AI auto-fill the form with a single tap.\n\n🎯 Goals & AI Tips\nSet financial goals with titles, descriptions, and target amounts. View goal-specific insights and generate AI tips based on your financial behavior.\n\n📈 Smart Analytics\nDive into AI-generated forecasts per category and get a deeper understanding of your financial patterns.\n\nwiseWallet – because everyone deserves intelligent money management.\nWhether you're budgeting, saving, or just trying to stay on top of your expenses, wiseWallet makes it smarter.\n\n\nvideo link: App Name: wiseWallet – Smart AI Personal Finance Tracker\nm/shorts/YCuD02xsf7Y](https://www.youtube.com/shorts/YCuD02xsf7Y)",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/ibrahim-embaby/wise-wallet",
        "demo_video_url": "https://www.youtube.com/shorts/YCuD02xsf7Y?feature=share",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/wisewallet-ai-powered-personal-finance-intelligenc"
    },
    {
        "project_title": "PocketTranslator",
        "tagline": "A Completely offline translation app",
        "authors": "Ramasai",
        "summary": "PocketTranslator is an Android-focused React Native app that delivers real-time speech translation using Google's Gemma 3n AI model running entirely offline on mobile devices.\n\n🎯 Core Innovation\nFirst-of-its-kind: Runs 2.8GB Gemma 3n model locally on Android phones\nComplete Offline Operation: No internet required after initial model download\nPrivacy-First: All conversations stay on-device, never sent to servers\n🚀 Key Features\nSpeech-to-Speech Translation: Speak in one language, hear translation in another\n19 Language Support: English, Spanish, French, Arabic, Chinese, and more\nConversation History: Chat-like interface for ongoing translations\nAndroid Optimized: TalkBack accessibility, memory management, haptic feedback\n🏗️ Technical Stack\nAI Models: Whisper Base (speech recognition) + Gemma 3n E2B (translation)\nRuntime: llama.rn v0.6.1 and whisper.rn v0.4.3\nFramework: React Native with Expo Router\nPlatform: Android-first with development build support\n💡 Use Cases\nPerfect for travelers, international business, emergency situations, and educational settings where privacy and offline capability are essential.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/ramasai-badam/PocketTranslator",
        "demo_video_url": "https://youtu.be/rXEy5dN1j8E?feature=shared",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/pockettranslator"
    },
    {
        "project_title": "Socrates AI: An AI Guide to Emotional Clarity Inspired by Marcus Aurelius",
        "tagline": "Harnessing Stoic Philosophy and AI Reasoning to Transform Emotional Challenges into Inner Strength",
        "authors": "riju3107",
        "summary": "SocratesAI is an AI-powered emotional guidance agent designed to help users navigate complex feelings through the wisdom of Stoic philosophy. Using advanced Chain-of-Thought Reasoning, LangChain, Gemma 3n, and Retrieval-Augmented Generation (RAG), the agent takes a user’s emotional input and breaks it down into smaller, reflective sub-questions. By engaging in this structured dialogue, users gain deeper insight into the root causes of their emotions and learn practical ways to reframe and manage them, inspired by the meditations of Marcus Aurelius.\n\nSocratesAI acts as a digital companion for self-reflection, offering calm, rational, and timeless guidance to help users cultivate resilience, perspective, and emotional clarity in their daily lives.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/HWHjhGQCWKI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/socrates-ai-an-ai-guide-to-emotional-clarity-inspi"
    },
    {
        "project_title": "Miai, the Offline Oracle",
        "tagline": "Photo2Insight engine that leverages Gemma's multimodal features - and symbolic inference capabilties.",
        "authors": "3xNEI",
        "summary": "Miai: The Offline Oracle\n#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=x5KyegocAck",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/miai-the-offline-oracle"
    },
    {
        "project_title": "LOMA mobile app",
        "tagline": "Local Offline Medical Assitant",
        "authors": "Aleksander Obuchowski, Kamil Staszewski, Mateusz Anikiej",
        "summary": "LOMA: Local Offline Medical Assistant - Empowering Offline Access to Reliable Medical Knowledge with Gemma 3n",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/TheLion-ai/LOMA",
        "demo_video_url": "https://youtu.be/BSBnGWeqdaE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/loma-mobile-app"
    },
    {
        "project_title": "Locked In: A Back-to-Basics Well-Being Coach",
        "tagline": "Private, on-device AI that rebuilds mental health through five daily basics—sleep, move, nourish, focus, reflect",
        "authors": "Muchiri",
        "summary": "LockedIn is an offline-first mental-wellness coach that helps overwhelmed students and professionals regain stability by “locking in” five research-backed daily basics: 8-hour sleep, a 30-minute walk, essential vitamins, a top-3 focus list, and a reflective journal. Built as a lightweight PWA (single HTML/CSS/JS bundle ≈ 50 MB), the app stores all data locally and keeps working with zero connectivity—crucial for users in low-bandwidth regions or during crisis situations.\n\nAt its core is Gemma 3n running entirely on-device via Ollama. A 2 B slice handles real-time sentiment detection and crisis-hotline prompts; the 4 B slice powers three privacy-preserving AI services:\n\n- AI Support – instant, empathetic replies to journal entries (< 120 words, sub-second latency).\n\n- Weekly Coach – a reflective Q&A that highlights wins, patterns, and micro-goals for the coming week.\n\n- Pattern Insights – one-tap analytics that correlate habit consistency with mood trends and suggest the single highest-impact tweak.\n\nThe interface uses vivid “glassmorphism” cards and celebratory micro-animations to nurture engagement while a lightweight state store debounces writes to localStorage for reliability. Cold-start RAM was reduced 40 % by leveraging Gemma 3n’s Per-Layer Embeddings, making full inference viable on mid-range Android phones.\n\nLockedIn bridges the gap between clinical advice and lived reality, delivering secure, stigma-free mental-health support to the 970 million people worldwide who struggle with anxiety or depression—no cloud required.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/kr-muchiri/The-Gemma-3n-Impact-Challenge",
        "demo_video_url": "https://youtu.be/0siHazGm5Sk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/locked-in-a-back-to-basics-well-being-coach"
    },
    {
        "project_title": "Guardian Redact: An Offline-First, AI-Powered Redaction Document, Audio",
        "tagline": "Leveraging Google's Gemma 3n for Private, Context-Aware Data Security on Documents and Audio.",
        "authors": "Tai Tran",
        "summary": "In a world where data is constantly shared, true privacy has become the ultimate luxury. Standard AI tools demand that we upload our most sensitive documents and conversations to the cloud, creating an unacceptable security risk. **Guardian Redact** challenges this paradigm. It is a comprehensive, offline-first redaction suite that brings the power of Google's advanced Gemma 3n model directly to your desktop, ensuring your data never leaves your device.\n\nGuardian Redact is built on a single principle: **intelligent security should not compromise privacy.**\n\nWe have engineered a dual-function application to tackle the most common forms of sensitive data:\n\n1.  **The Document Redactor:** Moves beyond simple keyword matching. It uses Gemma 3n's deep contextual understanding to analyze documents page-by-page, finding not just names and numbers, but the subtle, identifying links between them that other tools miss. All while providing a superior user experience with real-time progress updates.\n\n2.  **The Audio Redactor:** A dedicated, professional-grade editor for audio files. After transcribing speech with Whisper, every word is analyzed by Gemma 3n to find spoken PII. Users can then surgically silence or anonymize sensitive segments on an interactive waveform, providing a level of control previously unavailable in an offline tool.\n\nBy building Guardian Redact, we are not just creating a utility; we are proposing a new standard for how professionals across legal, finance, journalism, and research can handle sensitive data—with absolute confidence, security, and control.\n\n#",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/kaitorecca/guardian-redact",
        "demo_video_url": "https://youtu.be/vSAOFCbREqg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/guardian-redact-an-offline-first-ai-powered-redact"
    },
    {
        "project_title": "Google-Connect - Offline Humanitarian AI Toolkit",
        "tagline": "PWA cached on Google discovers portable AI servers for offline refugee assistance: translation, legal aid, family reunification, no internet",
        "authors": "Sambyshamby",
        "summary": "Google-Connect caches a humanitarian AI interface on Google.com that automatically discovers lightweight and portable AI servers carried by aid workers, delivering offline multimodal assistance for translation, legal guidance, family reunification, and medical support to refugees in crisis zones with zero internet dependency.",
        "technologies": [],
        "source_code_url": "https://github.com/Sambyshamby1/Google-Connect-",
        "demo_video_url": "https://youtu.be/RhAUCBczgEQ?si=eFJmVKyRx5B4uHoF",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/google-connect-offline-humanitarian-ai-toolkit"
    },
    {
        "project_title": "Eliza: Universal Learning Platform Powered by Gemma3N",
        "tagline": "Gemma-3n-powered education for everyone, everywhere. Breaking barriers of cost, location & language to make quality learning universal.",
        "authors": "AltinAzizi0",
        "summary": "The promise of education often clashes with the realities of the classroom. Students who fall behind struggle to catch up, while advanced learners remain unchallenged. Beyond the classroom walls, countless individuals face barriers like the intimidation of asking questions, the prohibitive cost of textbooks and tutors, or the sheer lack of access to educational materials in remote areas. These pervasive challenges highlight a critical need for a transformative approach to learning.\n\nEliza is a groundbreaking AI-powered Universal Learning Platform designed to break down the traditional barriers of geography, language, and socio-economic status, making quality education accessible to anyone, anywhere. It directly addresses critical shortcomings in current educational systems, such as the widespread lack of personalized learning, insufficient and delayed feedback, and the overwhelming administrative burden on teachers.\n\nLeveraging advanced AI, Eliza provides tailored learning experiences by analyzing individual student performance, strengths, and weaknesses. It delivers customized content, adjusts difficulty levels, and offers targeted, real-time feedback, ensuring every student learns at their optimal pace. This means a student can now receive immediate, personalized support to unleash their full potential.\n\nThe platform also significantly reduces teacher workload through AI-driven automation of tasks like lesson planning, grading, scheduling, and data analysis. This frees educators to focus on direct instruction and meaningful student interaction.\n\nA core innovation is Eliza's deep localization capabilities. Beyond mere translation, AI adapts educational content, including examples and scenarios, to resonate with local cultures and values, fostering natural engagement. The platform will house an immense, constantly evolving repository of digital content, aggregating both local and international books, while AI automates the creation of dynamic, interactive learning materials like quizzes and simulations. Hosted on robust cloud services for scalability, Eliza aims to democratize access to high-quality, engaging, and personalized education, transforming learning outcomes.",
        "technologies": [],
        "source_code_url": "https://github.com/ErsjanKeri/Eliza",
        "demo_video_url": "https://www.youtube.com/watch?v=_0wSE9-H2Vk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/eliza-universal-learning-platform-using-gemma"
    },
    {
        "project_title": "EduGemma: On-Device AI for Education, Everywhere",
        "tagline": "A gemma3n-powered, on-device AI platform that generates personalized learning games for students in low-resource regions",
        "authors": "SJ Yu",
        "summary": "#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/ysjprojects/gemma3n-unsloth-edutech",
        "demo_video_url": "https://youtu.be/JUEPSzFGSs8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/edugemma"
    },
    {
        "project_title": "FAKER? Arabic-First Alzheimer's AI Assistant",
        "tagline": "FAKER? uses Gemma 3n to deliver memory therapy and support for Arabic-speaking Alzheimer’s patients.",
        "authors": "Allam Elsheikh, Youssef Abdelmaksod",
        "summary": "**FAKER?** (Arabic for “Do you remember?”) is an innovative, culturally sensitive AI assistant designed to support Arabic-speaking Alzheimer’s patients and their caregivers, leveraging Google’s **Gemma 3n** multimodal model to deliver empathetic memory therapy, real-time cognitive assessments, and emergency assistance through a voice-first mobile application tailored for Egyptian Arabic. Built with a modular architecture, FAKER? integrates a React Native frontend, a FastAPI backend, and Gemma 3n’s text, image, and audio processing capabilities, optimized via 4-bit quantization for efficient on-device and edge deployment. Key features include voice-driven interactions for accessibility, photo-based memory stimulation using culturally relevant prompts (e.g., Egyptian traditions like Eid or koshary), continuous cognitive monitoring with caregiver dashboards, and a robust emergency system utilizing GPS and SMS. Fine-tuned on Egyptian Colloquial Arabic and Alzheimer’s-specific datasets, FAKER? ensures linguistic fluency and medical appropriateness while prioritizing privacy through on-device processing, encryption, and HIPAA-compliant design. Deployed on **AWS ECS** with a hybrid cloud-edge model, FAKER? addresses the critical gap in AI-driven dementia care for **over 15 million** Arabic-speaking patients, offering a scalable, impactful solution that enhances patient engagement, caregiver support, and safety.",
        "technologies": [
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/AllamElsheikh/Alzheimer-s_Memory_Assistant-.git",
        "demo_video_url": "https://youtu.be/EVMZrK0cD9g",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/faker-arabic-first-alzheimers-ai-assistant"
    },
    {
        "project_title": "MenuMind - AI-Powered Menu Translation & Allergen Detection",
        "tagline": "Offline AI-powered app for real-time menu translation, allergen detection, and cultural food insights using Gemma 3n.",
        "authors": "Mohamed Abdo Elnashar",
        "summary": "MenuMind is an offline AI-powered mobile app that uses Google’s Gemma 3n multimodal model to translate restaurant menus, detect allergens, and provide cultural dish insights in real-time. Designed for travelers, the app works entirely on-device after downloading the model, offering a private, fast, and accessible dining assistant.\n\nBuilt with Flutter and optimized for mobile inference, MenuMind reads and understands menu images, translates them into 15+ languages, highlights user-specific allergens, and explains the origins of each dish — all within a few seconds. It addresses a real-world challenge that affects millions of global travelers, ensuring safer and richer dining experiences.\n\nThis project demonstrates the power of on-device AI in solving practical, culturally meaningful problems with a strong focus on privacy, usability, and global impact.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/MohamedAbd0/menu_mind",
        "demo_video_url": "https://www.youtube.com/watch?v=vqFfZMcezus&ab_channel=MohamedAbdoElnashar",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/menumind-ai-powered-menu-translation"
    },
    {
        "project_title": "Cape: Bringing the Power of AI to Building Infrastructure Teams",
        "tagline": "A Gemma3n-powered application designed to assist the global workforce of infrastructure commissioning and maintenance crews.",
        "authors": "Filip Gonzacenco, Stefan Petrescu",
        "summary": "Cape is an offline, multimodal AI application built to support infrastructure commissioning and maintenance teams, ideal for workers responsible for HVAC, building controls, and IoT systems. Powered by Gemma3n and an open source web-ui (OpenWebUI), Cape allows users to drag and drop PDFs, images, videos, and logs, and ask natural-language questions, even with no internet connection, on the devices they already carry (laptops). In essence, the purpose of Cape is to help field crews debug faster, reduce delays, and operate confidently in constrained environments.",
        "technologies": [],
        "source_code_url": "https://github.com/spetrescu/cape",
        "demo_video_url": "https://youtu.be/wkR7WzmSZKw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/cape-bringing-the-power-of-ai-to-building-infrastr"
    },
    {
        "project_title": "GeoSquire - AI Assistant for Geoscientists",
        "tagline": "GeoSquire is an Offline AI that helps professionals in Mining, Oil & Gas and Geology. Can Classify  outcrops, rocks and identify minerals",
        "authors": " Felipe Lujan",
        "summary": "GeoSquire: AI-Powered Field Assistant for the Google Gemma 3n Impact Challenge",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/FelipeLujan/GeoSquire",
        "demo_video_url": "https://www.youtube.com/watch?v=3UHchw4JAzU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/geosquire-ai-assistant-for-geologists"
    },
    {
        "project_title": "sms-fraud-detector-llm-android",
        "tagline": "An Android app that detects SMS fraud using on-device AI, providing real-time alerts and simple text-based explanations",
        "authors": "Topiast",
        "summary": "SMS Fraud Detector: Technical Architecture and Implementation\n\n**Note**: This document was generated by AI. For more detailed and less polished information, see the https://github.com/topiast/sms-fraud-detector-llm-android/blob/main/technical-report.md\n\n**Demo - Apk**: https://github.com/topiast/sms-fraud-detector-llm-android/releases/download/demo/app-debug.apk",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/topiast/sms-fraud-detector-llm-android",
        "demo_video_url": "https://www.youtube.com/watch?v=jvJwHLYhStA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sms-fraud-detector-llm-android"
    },
    {
        "project_title": "LifeReclaim",
        "tagline": "A privacy-first, AI-powered time management application built with Flutter",
        "authors": "Ch1ldr3n",
        "summary": "- 🤖 **Local AI Integration**: Powered by Gemma 3n model via Ollama for intelligent task suggestions and decomposition\n- 🔒 **Privacy First**: 100% local data processing - your data never leaves your device\n- ⚡ **Smart Task Management**: AI-powered task creation, prioritization, and decomposition\n- 🎯 **Intelligent Suggestions**: Context-aware tag suggestions and time estimation\n- 📊 **Time Analytics**: Comprehensive insights into your productivity patterns",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/pr0m1see/life_reclaim",
        "demo_video_url": "https://www.youtube.com/watch?v=O1bRLTyhHdM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/lifereclaim"
    },
    {
        "project_title": "BiteSense: On-Device Arthropod-Bite Detection & First-Aid Assistant",
        "tagline": "BiteSense: offline Android app using on-device Gemma-3n to identify arthropod bites and give color-coded risk + first-aid guidance.",
        "authors": "Dorukhan Arslan, Ece Ünal, yunus onur, can zagnos, Agabey Alioglu",
        "summary": "BiteSense – On-Device Arthropod-Bite Detection & First-Aid Assistant\n \n \n**Problem**\nVector-borne diseases kill roughly 700 k people a year and account for 17% of global infectious-disease cases. In Türkiye alone, Crimean-Congo Hemorrhagic Fever has caused 17 k+ infections and 819 deaths since 2002. Accurate, fast triage is hardest precisely where it is needed most: rural areas with patchy connectivity and limited specialist care.\n \n**Solution**\nBiteSense is a privacy-preserving Android app that identifies arthropod bites and recommends first-aid. A 2 B-parameter Gemma-3n language model, fine-tuned with LoRA for vision, runs on-device via the MediaPipe LLM Inference API. From a single photo the model:\n \nClassifies the culprit across seven bite categories (ticks, mosquitoes, spiders, chiggers, bed bugs, fleas, benign lesions).\nFlags clinical risk with a green / yellow / red indicator.\nStreams step-by-step advice (cleaning, symptom monitoring, when to seek hospital care) in natural language.\n \n \n**Key Innovations** \nCurated, balanced dataset: 1,055 seed images expanded to 8,276 via targeted augmentations, covering diverse skin tones.\nResource-efficient fine-tuning: Selective LoRA adapters in BF16 boost balanced accuracy from 20% → 74% while fitting in 3.1 GB.\nEdge-AI engineering: Asynchronous background model loading and two-stage, token-streamed UX cut perceived latency from ~20 s to ≈ 5 s on a Galaxy S24.\n \n \n**Impact**\nField tests show first-token latency of 8–10 s and 74% balanced accuracy -competitive with non-specialist clinicians- delivered straight to the point of care without sending images off-device. By bridging the diagnostic gap in low-resource settings, BiteSense turns any smartphone into a pocket epidemiology lab.\n \n**Roadmap**\n\nClinical validation with healthcare partners to benchmark diagnostic accuracy and treatment-adherence impact.\nDataset expansion to cover additional arthropods and common dermatological mimics.\nGeneralize the framework to sensitive conditions such as sexually transmitted diseases (STDs): on-device lesion recognition plus private, stigma-free guidance to encourage early treatment.\nLocation-aware risk alerts (e.g., CCHF hotspots, dengue outbreaks) leveraging on-device geodata.\nModel distillation and quantization to reduce the binary to < 2 GB while maintaining ≥ 70% balanced accuracy.\n \n \nBiteSense exemplifies how edge AI can bring life-saving expertise to the exact moment and place it is needed.\n\nFor more detailed demo videos:\nhttps://youtube.com/shorts/naZSqepPS3g\nhttps://youtube.com/shorts/_L2AmGaFwmE",
        "technologies": [],
        "source_code_url": "https://github.com/eceunal/BiteSense",
        "demo_video_url": "https://www.youtube.com/watch?v=fDNx57BX6rk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/bitesense-on-device-arthropod-bite-detection-and-f"
    },
    {
        "project_title": "The fitness coach that fits your pocket",
        "tagline": "Leveraging Gemma's vision capabilities as a fitness coach instructor",
        "authors": "Gustavo Peredo",
        "summary": "By leveraging Gemma 3n's vision capabilites, I created a fitness coach that can help you with your form and posture while exercising.\nThe project was done in under 4 days (first time using Kotlin), my previous two attempts were:\n1. A pronunciation coach, but Gemma 3n's speech capabilities are not yet ready for prime time.\n2. This same app, but using flutter + rust (mistral.rs) + wasm, which although very interesting, was too slow/couldn't get to work in time.\n\nFor the Health and Wellness track.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/GustavoPeredo/FitnessCoach",
        "demo_video_url": "https://www.youtube.com/watch?v=B6LQp4vMVlc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/the-fitness-coach-that-fits-your-pocket"
    },
    {
        "project_title": "Polynet - The Internet, in Your Language",
        "tagline": " A browser that translates and dubs all internet content—text, images, videos—into your language in real time, fully on-device.",
        "authors": "oussama hamani",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Oussamahamani/PolyNet",
        "demo_video_url": "https://youtu.be/LzSbdUoRNJc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/polynet-the-internet-speaking-your-language"
    },
    {
        "project_title": "AISKUS: Mindreader for the College Classroom",
        "tagline": "Instant AI-Powered Student Question Clustering and Feedback, Fully Local & Anonymous",
        "authors": "Yashaswi Bathula",
        "summary": "AISKUS – Built for Real Pain, By People Who’ve Lived It\n\n#",
        "technologies": [
            "Flask",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/ybathula707/aiskus",
        "demo_video_url": "https://youtu.be/Mpr9w3VQ3NU?si=i1JJeAQ1ciHJdMZP",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/aiskus-mindreader-for-the-college-classroom"
    },
    {
        "project_title": "Nova, Your Private AI Companion",
        "tagline": "Nova is a sophisticated desktop companion that helps you navigate your thoughts, memories, and tasks with an intelligent, private AI partner",
        "authors": "Nishan2030",
        "summary": "Project Nova: Your Private AI Companion\nIn an era where our digital lives are increasingly managed by cloud-based services, Nova stands as a testament to the power of personal, sovereign AI. It is a sophisticated, offline-first desktop companion designed to be a completely private and intelligent partner for your memory, reflection, and daily tasks.\nNova is not a website or a cloud service; it is a true native application that runs entirely on your own computer. Every component—from the state-of-the-art AI models to the databases holding your most personal thoughts—is stored and executed locally. Your data is never sent to the cloud, ensuring a level of privacy that is simply not possible with other AI assistants.\nThe application lives in a sleek, frameless side-panel, summonable with a global hotkey (Ctrl+Shift+X), making it an ever-present but never intrusive part of your workflow.\nCore Features: A Multi-Layered Intelligence\nNova's capabilities are designed to create a holistic and deeply personalized experience, moving far beyond a simple chatbot.\n1. True Multimodal Understanding (Text, Vision, Audio)\nNova communicates on your terms. Powered by a locally-run Gemma 3n model, it seamlessly processes information in multiple formats:\nVision: Attach any image (a complex chart, a diagram, a photo) and ask questions. Nova doesn't just describe the image; it understands the content, allowing it to analyze data, explain systems, and provide genuine insights.\nVoice (Hands-Free): With a custom \"Hey Nova\" wake word, you can activate Nova and speak your thoughts naturally. A state-of-the-art Whisper model provides lightning-fast, local transcription, which is then passed to Gemma for comprehension and action.\nText: Engage in deep, context-aware conversations for brainstorming, planning, or simply talking through an idea.\n2. The Agent: From Conversation to Action\nNova is more than a passive listener; she is an active agent on your desktop. You can issue commands to:\nSearch Your Local Files: Ask \"Hey Nova, find my report from last week,\" and she will search your desktop and return the file path.\nOpen Applications & Files: Follow up with \"Open that file and launch Spotify,\" and Nova will execute the commands, seamlessly bridging the gap between conversation and action.\n3. A Deep, Layered Memory System\nNova’s memory is designed to build a rich, contextual understanding of you over time.\nConversational Memory: Remembers the flow of recent conversations for natural, follow-up questions. All chat history is saved securely in a local SQLite database.\nThe Journal (Emotional Context): A private, dedicated space to write down your thoughts and feelings. Nova uses VADER sentiment analysis to understand the emotion behind each entry. This emotional context is used to inform her tone and responses, allowing her to be more empathetic and supportive. You can ask reflective questions like, \"Looking at my journal, what seems to be triggering my stress lately?\" and receive a synthesized, insightful answer based on your own words.\nRAG Knowledge Base (Your Personal Brain): Upload your own documents (PDFs, TXT files) to Nova's permanent \"Memory\". Using a Retrieval-Augmented Generation (RAG) system powered by a local ChromaDB vector database, Nova can answer questions grounded in your personal or professional knowledge, ensuring her answers are accurate, relevant, and free of hallucination.\n4. A Seamless & Proactive User Experience\nNative Desktop Panel: A polished UI built with PySide6 (Qt), featuring a modern dark theme and an intuitive, tabbed layout.\nProactive Greetings: Nova doesn't just sit there. Each time you open the app, she generates a fresh, context-aware greeting based on your recent chats and journal entries, making every session feel personal and engaging.\nInteractive Voice Output (TTS): Toggle on spoken responses to have a true back-and-forth conversation, with Nova's answers read aloud in a natural voice.\nCore Technology & Architecture\nNova's power comes from a carefully orchestrated stack of modern, locally-run technologies.\nAI Models:\nPrimary Brain (Language/Vision/Agent): A fine-tuned, quantized version of Gemma 3n running via the Ollama server.\nTranscription: OpenAI's Whisper (base model) for fast and accurate local speech-to-text.\nEmbeddings (RAG): The efficient all-MiniLM-L6-v2 model for vectorizing documents.\nBackend: A robust, multi-threaded Python backend ensures the UI remains fluid and responsive even while the AI is performing complex tasks. Every long-running operation is offloaded to a dedicated QThread.\nWake Word Detection: The lightweight and highly efficient pvporcupine engine runs continuously in the background, listening for the custom \"Hey Nova\" hotword with minimal CPU impact.\nDatabase: A local SQLite database for structured data and ChromaDB for vector storage.\nUI Framework: PySide6 (Qt for Python) for a high-performance, native desktop experience.\nProject Nova is a demonstration of what is possible today with local AI—a truly personal, secure, and intelligent companion built to enhance your own mind.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Nishan30/Nova",
        "demo_video_url": "https://youtu.be/MbzcBR7XoZs",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/nova-your-private-ai-companion"
    },
    {
        "project_title": "AiPharmest",
        "tagline": "AiPhamest is an on-device Android assistant that keeps you safe, informed, and on-track with your medication",
        "authors": "mohammad hasanain",
        "summary": "🧠 AiPhamest\n100 % offline. Life‑saving by design\n\n💡 Project Summary\nAiPhamest is an intelligent, offline medical assistant that uses Gemma 3n to automate prescription understanding, build smart medication schedules, and monitor side effects — all while ensuring patient safety through drug interaction analysis, allergy checks, and emergency alerts.\n\nBuilt entirely with on-device multimodal AI, AiPhamest helps patients adhere to their medication plans and catch potentially dangerous side effects, even without internet access. The system blends structured LLM reasoning, man-in-the-loop verification, and real-time alerting for critical drug-related risks.\n\n🏥 Real-World Problem\n Millions of patients — especially in remote or resource-limited settings — suffer from:\n\n- Missed doses or poor medication adherence\n- Unrecognized side effects\n- Drug-drug interactions or allergies\n- No access to pharmacists or caregivers\n- These risks lead to avoidable hospitalizations and deaths.\n\n AiPhamest solves this by putting a safe, smart, medical AI in every pocket.\n\n⚙️ What the App Does\n✅ 1. Prescription Extraction\n- User uploads or captures an image of a medical prescription\n- Gemma 3n (E4B) +Model vision extracts:\n- Drug names\n- Dosages\n- Frequencies\n- Fuzzy drug-name matching against a built-in database improves accuracy\n\n✅ 2. Human-in-the-Loop Editing\n- A pharmacist or user confirms the extracted data\n- Adds pack size for each medicine\n\n✅ 3. Smart Medication Scheduling\n Once confirmed the fisrt dose, the app:\n\n- Creates dosage alerts and schedules based on frequency and pack size\n- Gemma 3n will also insert inTake recommendations for medicien card\n- Labels each dose card as:\n\n🟢 Taken\n\n🔵 Upcoming\n\n🔴 Missed\n\n- Sends notifications before each dose and tracks adherence\n- Allows pinning of lifetime medications\n\n✅ 4. Side Effect Logging + Reasoning\nUsers can record or type side effects\n\nGemma 3n evaluates:\n\n- The likely cause (specific drug or interaction)\n- Severity level (🔴 Red / 🟡 Yellow / 🟢 Green)\n- Confidence score\n- AI-generated reasoning and recommendation\n\n✅ 5. Warning Screen\nAll findings appear in a dedicated Warnings screen:\n\nShows cause, risk level, suggested action, and explanation\n\nHighlights potential interactions and allergy conflicts\n\n✅ 6. Emergency Trigger\nIf severity is Red and confidence is high:\n\n- App can auto-call or text a pre-saved emergency contact (upcoming in future )\n- Designed for offline action\n\n🧠 Powered by: Gemma 3n\nUsed in multiple workflows:\n\n- Prescription parsing\n- Medical reasoning (side effects, interactions)\n- Alert generation\n- Drug inTake recommendations \n\nRuns fully on-device using quantized versions of Gemma 3n (E4B)\n\nEnables safe operation even without connectivity\n\n⏱️Fine tunned model with all drugs, side effect and interaction data\n🚧 Features in Development (Future Work)\n📞 Emergency auto-call trigger on high-severity warnings\n⏱️ LLM-based optimization for best intake times per medication\n📈 Tracking patient response/improvement to medication over time\n💊 Supplement safety support\n🧬 Model fine-tuned on public medical datasets (e.g., drugs.com, MedDB)\n👶 Pregnancy-specific medication warnings\n🧩 Home screen widget for one-tap side effect capture\n\n🎯 Why This Matters\nAiPhamest brings together:\n\n- Offline-first AI intelligence\n- Structured medical reasoning\n- Real-world patient safety automation\n\nIt’s designed to assist real patients, not just impress in a lab — and has the potential to prevent harm, save lives, and increase medication compliance at scale.\n\n⚠️ Known Limitations & Notes\n⏱️ First extraction may take up to 2 minutes, as the app runs the Gemma 3n model twice:\n\nOnce for structured prescription extraction (OCR)\nAgain to match the extracted drug name with entries in drug.txt\n🎙️ Voice input currently uses Google’s on-device speech recognition API.\nThis will be replaced with Gemma 3n’s audio model once it's available on Android.\n📂 To improve accuracy, please manually add drug names to the drug.txt file in the app’s /assets/ folder before scanning prescriptions.\nThis enables better name resolution during extraction.\n🔧 Fine-tuning is in progress to incorporate complete drug, side-effect, and interaction datasets into the model for improved accuracy and reasoning.\n\n🔧 Tech Stack\nModel: Gemma 3n E4B (quantized for mobile)\nVision: + fuzzy text matching for prescriptions\nLogic Engine: LLM with built-in rules & fine prompts\nPlatform: Android (Kotlin) with on-device execution\nScheduling: Local calendar + alert logic engine\nSide Effect Reasoning: Prompt chaining with Gemma 3n",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/AiPhamest/AiPhamest",
        "demo_video_url": "https://youtu.be/7GjnmmLV6kY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/aipharmest"
    },
    {
        "project_title": "🎨 AI Drawing Assistant — Art for Everyone, Anywhere",
        "tagline": "Your private art coach — in your pocket, anywhere, anytime. Notebook at https://www.kaggle.com/code/antoinehugueney/gemme-3n-sur-kaggle-donj",
        "authors": "Antoine Hugueney",
        "summary": "The Vision\nAn inclusive, offline AI art coach that runs locally on low resource devices, powered by the new Gemma 3n edge model.\nWhether you’re a shy beginner or a seasoned artist, you get personalized, private, and confidence boosting guidance — anytime, anywhere.\n________________________________________\nKey Advantages\n##1.  Privacy by Design\no\tRuns **offline** — no internet needed.\no\tNo drawings leave your device unless you choose to share.\no\tPerfect for those who feel embarrassed in early learning stages.\n##2.\tAccessible to All\no\tWorks on **low-resource devices** — affordable laptops, tablets, even old PCs.\no\tCan be deployed in **low connectivity or resource limited educational areas**.\n##3.\tSafe for Copyright\no\tNo uploads to public clouds → avoids unintentional copyright exposure.\n##4.\tEmpowerment Through Art\no\tDrawing is not just creative — it builds skills valued in **design, engineering, gaming, advertising**, and more.\no\tHelps learners become financially independent over time.\n##5.\tResilience Tool in Crisis\no\tIn times of **familial crises, natural disasters, or conflicts**, art can be *therapeutic*.\no\tOffline storage keeps works **safe from public scrutiny**.\n##6.\tAdaptive Learning Path\no\tAdapts to every level — from **absolute beginners** to **painting masters**.\no\tPretrained with **centuries of art history** for rich, stylistic knowledge.\no\tAlways uses a **positive**, **encouraging tone** to nurture confidence.\n________________________________________",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/oy9BJ2UiHTc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-drawing-assistant-art-for-everyone-anywhere"
    },
    {
        "project_title": "Gemi ASD (Gemma Medical & Assisted System)",
        "tagline": "Gemi is an on-device medical assistant that leverages the power of Gemma 3n to provide crucial diagnoses and emergency guidance to millions ",
        "authors": "Mr. Trompet",
        "summary": "**1. Introduction**\n\nI’m Christian Montenegro from Venezuela. Growing up in a country where access to health care is scarce and expensive, I witnessed first hand the impact that delayed treatment can have on people’s lives. In 2020 my father suffered a stroke while working at our family’s farm in Guanape, a rural community in Anzoátegui. The nearest clinic was hours away and there was no doctor to recognize his symptoms. After 40 minutes of driving my brother reached the nearest town only to be turned away because no one could identify what was happening. Three days later, after travelling six hours back to the capital, he finally received a diagnosis – an extensive cerebellar stroke covering 80% of his cerebellum. He spent a year bedridden and today, five years later, still lives with neuropathic pain and memory loss... If we had recognised his symptoms earlier and sought specialist help sooner, he might have recovered more quickly.\n\nThat experience inspired me to build Gemi ASD (Gemma Emergency & Medical Assistant System). The application is designed to detect signs of cerebrovascular accidents (strokes), myocardial infarctions and respiratory arrest at the very first encounter, offer first aid guidance, act as a virtual physician for primary diagnosis, schedule medication reminders and store personal medical records. I began sketching this idea three years ago, when AI agents were still in their infancy. At the time the privacy implications of using large language models made it impossible to deploy such a tool in rural communities. Today, with Google’s Gemma3n model – which supports offline inference and local storage – and Flet to build multi platform desktop applications, it’s finally possible to transform those sketches into a working prototype.\n\n**2. Why the Problem Matters**\n\nStrokes and heart attacks are leading causes of death and disability worldwide. In an ischemic stroke, blood flow to the brain is blocked; without oxygen and glucose, brain cells die and permanent neurological deficits occur. Clinical guidelines stress that “time is brain” – every minute matters. Canadian best practice guidance notes that about 1.9 million neurons die each minute after stroke onset and urges clinicians to treat patients as quickly as possible to improve outcomes. Early diagnosis and treatment are critical because thrombolysis or thrombectomy within the first six hours significantly reduces long term disability and mortality.\n\nRural populations face even greater risks. A review of pre hospital stroke services reports that times from symptom onset to rural hospital admission range from five to thirty hours and that thrombolysis rates in rural areas worldwide are as low as 1–6 %. In Australia only 3 % of rural patients access stroke units in time compared to 77 % of urban patients. Similar disparities exist for advanced endovascular treatments and rehabilitation.\n\nHeart attacks are equally time critical. According to the Cleveland Clinic, a heart attack occurs when blood supply to a section of the heart is blocked, causing that muscle to begin dying. Without prompt treatment, permanent damage or death may occur. The clinic warns that even a few minutes of delay can result in permanent heart damage or death.\n\n**3. The Gemi ASD Solution**\n\nGemi ASD combines an Gemma3n-2B with a lightweight Flet front end to deliver medical triage, first aid guidance and personal health management without an internet connection. All data and computation occur locally on the user’s device, preserving privacy and eliminating the need for expensive mobile data. The system is designed to operate on low end hardware (2–3 GB of RAM and ~3 GB of storage), making it suitable for community centres or low cost laptops often found in Venezuelan clinics.\n\nThe application guides users through an onboarding flow where they enter their gender, age and medical history. From the main interface, users can access several modules:\n\n•\tEmergency triage: a simple symptom checklist evaluates risk of stroke, heart attack or respiratory arrest, uses the National Institutes of Health Stroke Scale (NIHSS) to quantify stroke severity and provides recommendations for immediate action.\n\n•\tMedichat: a conversational agent powered by Gemma3n offers general medical advice and first aid tips. It clarifies symptoms, explains conditions and advises when to seek professional care.\n\n•\tCalendar: users describe medication schedules in plain language (e.g., “ibuprofen every 12 h for 5 days”). The system converts this into an interactive calendar with reminders and local notifications.\n\n•      Heart Rate Monitor: An ingenious tool I designed, inspired by the method emergency doctors use to take a pulse. A 15-second timer guides the user to count their heartbeats. Upon entering the number, the system automatically calculates the beats per minute (BPM) and diagnoses if the rhythm is normal, bradycardia, or tachycardia. This allows for a professional pulse measurement without the need for sensors, making the technique accessible to everyone.\n\n•      Personal records: the app stores medical history, chat transcripts and medication schedules in local JSON files so users retain control over their data.\nUnder the hood, the application is composed of Python modules. The app.py file loads pages and routes; views/ contains Flet components for the logo, onboarding, emergency tab, medichat, profile view, PDF viewer and settings. The llm/ folder wraps calls to the Gemma3n model and constructs prompts. Data persistence occurs in the data/ folder using JSON files (chat_data.json, profile_data.json, gemi_user_data.json, medical_tasks.json).\n\n**4. Challenges and Lessons Learned**\n\nBuilding an offline AI medical assistant on modest hardware presented multiple challenges. For example, the official Gemma3n-2B-iNT4-Preview from AI Edge was not yet available for TFLite on Windows. I spent considerable time exploring a manual prefill+decode implementation in TensorFlow but was only able to achieve a speed of 30–45 seconds per token on a low-end PC—a result that was far too slow for a real-time medical assistant. Initial attempts to compress Gemma3n via dynamic quantization to 8 bit ONNX yielded a 5 GB model consuming 3–4 GB of RAM – still too heavy for low end machines. I therefore adopted a 4 bit .gguf version (Gemma3n_E2B_IT_Q4_K_M) from Unsloth and interfaced it through llama.cpp. This reduced the model size to 2.82 GB and peak RAM usage to about 2 GB, making it feasible on inexpensive PCs.\n\nUser experience also required careful attention. On reloading, the app originally always opened the profile editor instead of the saved main view; this was fixed by detecting existing user data. Chat history was being written redundantly to multiple files, so I centralized storage in chat_data.json. Various UI elements were refined: adding a settings button, removing duplicate back buttons, adjusting chat bubble widths, implementing responsive layouts and splash screens, centring progress indicators and providing informative messages during long operations.\n\n**5. Impact and Use Cases**\n\nGemi ASD aims to bridge the diagnostic gap between rural and urban populations. By running entirely offline, the system protects privacy and functions in places without cellular coverage or reliable electricity. Clinics in remote villages often have basic laptops powered by generators but no smartphones or 4 G networks. With a footprint of just 2–3 GB of RAM and disk space, Gemi ASD can operate on such devices, providing symptom triage and first aid guidance while medical help is sought.\n\nReducing delay to treatment can have tangible benefits. In stroke care, every minute saved preserves brain tissue and improves functional outcomes. Rural patients currently face delays of up to 30 hours. A tool that helps carers recognise signs of stroke and heart attack, encourages them to act quickly and guides them to the nearest facility could dramatically improve survival. In heart attacks, prompt restoration of blood flow prevents permanent damage – a delay of even a few minutes increases the risk of death. Gemi ASD can shorten the time between symptom onset and hospital arrival by enabling early recognition, remote triage and preparation of emergency alerts.\n\nThe application also serves as a primary care companion. Through the Medichat module, it answers general health questions, provides first aid instructions and reminds users to take medication. All data remains local, adhering to ethical principles and ensuring that sensitive health information is never sent to the cloud.\n\n**6. Roadmap and Future Work**\n\n•\tMobile deployment: Once Google releases Edge AI packages for Android and I obtain better hardware, I plan to package the engine into an APK that runs on devices with 6–8 GB of RAM. This will enable community health workers to use the app on smartphones.\n\n•\tSensor integration: Integrate pulse oximeters and heart rate monitors to capture real time vital signs and feed them into the diagnostic algorithm.\n\n•\tExpanded diagnostics: Incorporate image analysis models to detect dermatological conditions such as melanomas and skin cancers, and fine tune the LLM for medical contexts.\n\n•\tTelehealth alerts: Add optional modules to send preformatted SMS, WhatsApp or Telegram messages to emergency services or family members once an emergency is detected, respecting privacy and offline capabilities.\n\n•      Telehealth Collaboration: With your help, I could contact telecommunications companies in Venezuela to propose a free, offline messaging (SMS) service. In Venezuela, mobile data networks are often unstable, while SMS service is much more robust and reliable, reaching places with weak signals where the internet does not work...Through an API, Gemma, on a secure server, could respond to emergency and Medichat queries. The responses would be delivered directly to the user's inbox without being saved in any database, guaranteeing privacy and a fast, reliable service for those who lack an internet connection.\n\n•\tUser studies: Collaborate with local clinics in Venezuela to test the app’s usability, measure its impact on time to treatment and gather feedback for improvements.\n\n**7. Conclusion**\n\nThe Impact Challenge seeks projects that leverage offline AI to create real benefits for society. Gemi ASD fulfills this mission: it takes Gemma and turns it into a life-saving tool for communities that lack access to reliable healthcare. By detecting the signs of stroke and heart attack early, giving first aid guidance, and managing medication schedules, the app helps users act decisively when every minute counts. The project demonstrates that with careful optimization and thoughtful design, advanced AI can run on modest hardware without sacrificing privacy or access. \n\nMy hope is that Gemi ASD will not only honor my father's memory but also help countless people in similar circumstances...\n\n_________________________________________________\n\n**Demonstration and Additional Resources**\n\nFor additional verification and proof of concept, I have packaged Gemi ASD as a Windows executable (.exe). This file demonstrates that the project is not just a prototype, but a functional product ready for distribution. Judges can optionally download and run the application to experience the user interface, the offline model's speed, and the features shown in the video demo firsthand. You can find the download link to the executable on Google Drive in the resources section. This is a testament to our commitment to delivering a complete and robust solution.\n\nI have also included a link to my GitHub repository below, where you can find the complete application source code:\n\n💻GitHub Gemi Repository:\n\nhttps://github.com/MrTrompet/Gemi-ASD-Gemma-Medical-Assisted-System-\n\n▶️Youtube Demo Link: \n\nhttps://www.youtube.com/watch?v=1Ofr-tD4PQw \nhttps://youtu.be/1Ofr-tD4PQw\n\n📂Google Drive Exe. Link: \n\nhttps://drive.google.com/file/d/1pfiUnHVRXyYFDlg6kSTK3TaSBGV6DwrX/view?usp=sharing\n\nI sincerely hope you enjoy the project. I'm very interested in receiving your feedback to continue improving and developing the application in the future.🩺🙏\n\nChristian Montenegro (Mr.Trompet)\nFrom Venezuela.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=1Ofr-tD4PQw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemi-asd-gemma-medical-and-assisted-system"
    },
    {
        "project_title": "Enterprise Healthcare AI Platform with Gemma 3N Integration",
        "tagline": "From rural clinics to ambulances - when traditional systems fail - we save lifes.",
        "authors": "Dmitrii Mukomel",
        "summary": "https://www.dropbox.com/scl/fi/7z9h94nmcm4yaosok37hc/video.MOV?rlkey=8orqezehipn56ff824p4hixgm&st=gfcvn8xq&dl=0\n[VIDEO]\n\n  Compliant healthtech infrastructure featuring offline-capable Gemma 3N models, built-in PHI\n  anonymization, cross-platform deployment (mobile/desktop), SOC2 Type II + HIPAA + FHIR R4\n  compliance, real-time medical AI inference, and enterprise audit trails for production healthcare      \n  deployment.\n\n  Key Features:\n🚨 Emergency Offline AI: Gemma 3N models work without internet in remote hospitals/ambulances, online support of mcp , a2a protocols\n  providing instant medical diagnosis when connectivity fails and every second counts to save lives.     \n  🔒 Zero-PHI-Loss Anonymization: Built-in anonymization protects patient privacy while enabling AI      \n  analysis, preventing healthcare data breaches that could deny critical care to vulnerable\n  populations.\n  ⚡ Cross-Platform Medical AI: Same AI model runs on any device (iPhone, Android, desktop, edge)        \n  ensuring consistent life-saving diagnosis whether in a rural clinic or major hospital.\n  🏥 Real-Time Medical Decision Support: A2A agent collaboration provides instant multi-specialist       \n  consultation (cardiology + emergency + radiology AI) for complex cases where speed determines\n  patient survival.\n  📱 Mobile-First Healthcare: Quantized models run on smartphones, bringing advanced medical AI to       \n  underserved communities and emergency responders who lack access to hospital infrastructure.\n  🔧 Production-Grade Reliability: SOC2 + HIPAA compliance with immutable audit trails ensures the       \n  platform never goes down when medical professionals depend on it for critical patient care\n  decisions.\n\n Architecture:\n\n  Core Infrastructure:\n  - MCP Protocol: Secure agent-to-agent medical AI communication with encryption\n  - Universal Inference Engine: Multi-format support (PyTorch, ONNX, TFLite, CoreML, TensorRT)\n  - Cross-Platform Converter: Dynamic model optimization for mobile/desktop deployment\n  - Built-in Anonymization: PHI/PII detection & reversible anonymization pipeline\n\n  Healthcare Compliance:\n  - SOC2 Type II: Immutable audit trails with cryptographic integrity\n  - HIPAA: Row-level security, encrypted PHI storage, access controls\n  - FHIR R4: Complete healthcare interoperability standards\n  - GDPR: Privacy-by-design with consent management\n\n  Data Layer:\n  - PostgreSQL: Primary database with enterprise isolation\n  - Redis: Caching, session management, task queue\n  - MinIO: S3-compatible object storage for documents/DICOM\n  - Vector Store: Milvus for medical knowledge embeddings\n\n  AI/ML Platform:\n  - Gemma 3N Integration: Offline medical reasoning models\n  - Medical Agents: Specialized AI for cardiology, emergency, radiology\n  - Mobile Optimizer: Quantization (FP32→INT8), pruning, device-aware optimization\n  - Model Registry: Download, cache, version control for offline deployment\n\n  Enterprise Features:\n  - Event-Driven Architecture: Advanced healthcare event bus with circuit breakers\n  - Developer APIs: External platform with SDK generation, rate limiting\n  - Clinical Workflows: FHIR-compliant patient care orchestration\n  - Document Management: DICOM processing, OCR, secure versioning\n\n  Deployment:\n  - Docker/K8s: Containerized microservices architecture\n  - FastAPI: High-performance async API gateway\n  - React/TypeScript: Modern healthcare dashboard frontend\n  - Monitoring: OpenTelemetry, Prometheus, comprehensive health checks\n  Production Ready: Full enterprise deployment with real database operations, comprehensive testing      \n  suite, and production-grade security for healthcare AI applications.",
        "technologies": [
            "Docker",
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/VwPyIU_uMV8?si=5QJ-lttm8FncA1Da",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/enterprise-healthcare-ai-platform-with-gemma-3n-in"
    },
    {
        "project_title": "PhoneGuard: AI-Powered Multilingual Scam Detection",
        "tagline": "PhoneGuard: Real-Time Phone Scam Detection Using Gemma 3n",
        "authors": "karthik, Shravya Errabelly",
        "summary": "PhoneGuard - Real-Time Phone Scam Detection System",
        "technologies": [
            "CUDA",
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/eVpunie2WOA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/phoneguard-ai-powered-multilingual-scam-detection"
    },
    {
        "project_title": "GAMVRA – Medical AI Assistant",
        "tagline": " offline-first Medical AI Assistant powered by Gemma 3N, built for Desktop. It features Medical AI,Chat,Analyse, a VR Physical Exam Guide . ",
        "authors": "Said Massinissa fazez",
        "summary": "GAMVRA is an offline-first, AI-powered medical assistant, designed for individuals and communities without reliable access to healthcare or the internet. It combines YOLO keypoint detection, OCR, and an LLM (Gemma 3n) to deliver private, AI-guided healthcare directly on your device.\n\n🔍 What GAMVRA Can Do\n✅ 1. Analyze Medical Test Results\nUpload or scan test result images (e.g., blood tests, urinalysis), and GAMVRA will:\n\nExtract data using OCR\nAnalyze and interpret medical values\nExplain what results mean in simple terms\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F22889577%2F3cbead498ced14feff4a84dd62b4578f%2FScreenshot%202025-08-06%20at%2019.57.43.png?generation=1754519852599215&alt=media)\n\n💊 2. Manage Medications\n\nScan and understand prescriptions\nPrevent medication conflicts\nSuggest safe dosage timing\nHelp track and manage treatment adherence\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F22889577%2Fbc11b0e417f6bf2843b61e6800735e84%2FScreenshot%202025-08-06%20at%2019.58.03.png?generation=1754519875282461&alt=media)\n\n🧍‍♂️ 3. Guide Physical Examinations (YOLO Keypoints)\n\nGAMVRA uses YOLOv8-pose keypoint detection to guide users through real-world physical self-checkups:\nDetects key body areas (shoulders, spine, chest, etc.)\nProvides AI instructions: what to do, what to check for\nGreat for basic triage and early warning signs\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F22889577%2F2d60cb2d1aaae2a7ff3aeec05eaeb155%2FScreenshot%202025-08-06%20at%2023.39.14.png?generation=1754519964729356&alt=media)\n\n📊 4. Provide a Personal Health Score\n\nGAMVRA generates a dynamic health score using your physical data, test results, and record history.\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F22889577%2F71ea466bad79e9eb8a9b8302f8a5ac00%2FScreenshot%202025-08-06%20at%2023.39.44.png?generation=1754519994436109&alt=media)\n\n🗂 5. Manage Personal Records & AI Memory\n\nUpload lab reports, doctor notes, prescriptions, and health images\nStored securely offline\nFeeds into the AI’s contextual understanding of your health\n\n🧠 6. Smart Chat with Context-Aware AI\n\n🔒 Offline, using Gemma 3n\nDuring chat, the AI:\nHas access to all your saved health records and interactions\nUses this private data to provide more accurate and personalized explanations, assessments, and suggestions\nFeels like talking to a real, informed doctor assistant — without needing the internet\n\n🖼 7. Analyze Records and Images\n\nAdd any document or image\nGAMVRA extracts text, titles it, categorizes it, and gives meaningful insight\nWorks with X-rays, prescriptions, blood test screenshots, etc.\n\n\n🛠️ Tech Stack\nFeature\tTechnology\nAI Model\tGemma 3n (gemma-3n-E2B-it-ONNX)\nPose Estimation\tYOLOv8-Pose \nImage & Text Processing\tTesseract OCR, ONNX Runtime\nFrontend\tReact + Tailwind CSS\nApp Platform\tElectron (cross-platform desktop)\nBundling & Speed\tElectron Vite\nData Privacy\tFully local, no cloud access\n\n\n\n🔐 Privacy First. Always Offline.\nAll data stays on your device\nNo cloud storage, no server communication\nYour medical history is used only locally by the AI to help you better",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/masymars/GAMVRA?tab=readme-ov-file#-table-of-contents",
        "demo_video_url": "https://youtu.be/vcj5fR7EEFg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gamvra-medical-ai-assistant"
    },
    {
        "project_title": "Silent Signal",
        "tagline": "A p2p offline communication app, with gemma 3n as it's intelligence layer, to help rescue ops make quick decisions.",
        "authors": "Vijayendra Dev",
        "summary": "**Problem Statement: **During the time of crisis such as wars or natural disasters, communications are super essential but without internet and cell towers, we can barely communicate over a few metres, how do we call the rescue teams if we are trapped? how do we reach out to to help if we are away from the help site and even drones might or might not reach us? How can Gemma help teams locate and understand the needs of victims faster than traditional methods? What if there is a language barrier? This \n\n**Assumption:** Government authorities have already asked the citizens to install the silent signal app as a precautionary measure.\n\n**Solution:** Implemented a mesh network protocol for my app which is light weight app and keeps broadcasting the victim's message over bluetooth- upto 10to 100m in range, seeking help in terms of food requirement, medical supplies, first aid, trapped, severe bleeding injury etc. I assumed that we cannot expect citizens to download 3-6gb for gemma, but we can implement it on drones, computers of rescue teams, other mobile and tablet devices of the operatives, to enable gemma3n at edge and connect it to all the mobile devices that are connected over the mesh and serve primarily two purpose one is to classify the threat level, urgency, needs of the victims at location x,y,z and other is to auto-translate any local language typed in to english[assuming field operatives might or might not fully understand if it is an international operation]. I have even been able to fetch the victim's location as well using gps, but reading all this data manually could be time consuming and confusing especially when everyone is looking out for help, therefore I built a dashboard for field operatives as well, where gemma3n with it's offline capabilties is the intelligence layer that can first understand any language to english(for simplicity for this hackathon) and also categorize tasks such as food supply, first aid, rescue etc to make quicker decisions.\nAlso, as an AI conversational assistant, if any of the citizen is in contact with the node, it can provide assurance that help is on the way and it's response means that the voice of citizen has been heard successfully and will be helped soon.\n\n**My tech stack and products**\nSilent Signal: Mobile app built in React-native with Bridgefy SDK to implement Mesh architecture and keep relaying message until it reaches either a device with gemma3n(usually rescue ops/drones/rescue computers in my case).\n\nSilent Signal Dashboard: It would classify the victim's needs as high,medium,low with categories limited for this hackathon as food supply, trapped, first-aid.The dashboard is built with Nextjs and typescript.\n\nGEMMA 3N Nodes as the backend: Currently implemented the intelligence layer of Gemma3N 2b on Ollama with Nodejs which helps in communication and decision making even faster for the tactical teams.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Vijji04/silent-signal",
        "demo_video_url": "https://youtu.be/fWR1rYI9rGk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/silent-signal"
    },
    {
        "project_title": "AI Quiz Generator",
        "tagline": "AI Quiz Generator designed to help students in low-connectivity areas learn better with quizzes based on their school curriculum",
        "authors": "Po Stevanus Andrianta",
        "summary": "Quiz Generator",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": "https://github.com/stevanusandrianta/quiz-generator-ollama",
        "demo_video_url": "https://youtu.be/56WXDMqQ660",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/quiz-generator"
    },
    {
        "project_title": "VoiceLoop - An offline voice translator",
        "tagline": "VoiceLoop is a privacy-first, offline translation app powered by Gemma 3n that enables seamless multilingual communication and translation",
        "authors": "pumal",
        "summary": "VoiceLoop: Breaking Language Barriers with Privacy-First AI Translation",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/mpowd/VoiceLoop.git",
        "demo_video_url": "https://youtu.be/YiJySo-k_Ic",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/voiceloop-an-offline-voice-translator"
    },
    {
        "project_title": "NaviVoice – A Private, Offline AI Guide for the Visually Impaired",
        "tagline": "NaviVoice is a private, offline personal AI assistant that helps blind users interpret menus, ingredients, shelves, and street crossings.",
        "authors": "Lesley-Ann Fenwick",
        "summary": "Link to video: https://youtu.be/OI5HzEXnPXA\n\nProject Title: NaviVoice – A Voice-Guided Assistant for the Visually Impaired\n\n\n🧩 Problem Statement\nBlind and visually impaired individuals often face difficulties performing basic tasks independently, such as reading restaurant menus, identifying items on store shelves, verifying food ingredients for allergies or dietary restrictions, and crossing roads safely.\n\nIn regions with limited internet connectivity or privacy concerns, cloud-based solutions are not viable. NaviVoice addresses this by offering a privacy-first, offline-ready, multimodal assistant that works entirely on-device.\n\n💡 Solution Overview\nNaviVoice is an offline, voice-activated AI assistant designed to help blind or low-vision users navigate everyday situations through speech and vision. The assistant integrates Gemma 3n to interpret transcribed user commands and OCR results from images, returning relevant spoken advice or safety information—all while preserving user privacy.\n\n🧠 Key Features\n1. Voice-Activated Wake Word Detection\nNaviVoice continuously listens for specific wake words like “Navigator” or “Hey Navi Voice” using fuzzy string matching. Once triggered, it prompts the user for a voice command and begins processing.\n\n2. Natural Speech Interaction with Multimodal Understanding\nAfter wake word detection, the user can ask a question naturally (e.g., “What can I have on the menu?”). Whisper is used for speech-to-text transcription, and then Gemma 3n interprets the transcribed command and responds appropriately. If the prompt matches a predefined flow (e.g., menu, ingredients, shelf, crossing), the relevant feature is launched.\n\n3. Restaurant Menu Reader\nUsers can hold their camera to a restaurant menu. The app extracts the text using OCR, and Gemma 3n filters and ranks menu items based on dietary needs (e.g., vegan, low cholesterol, caffeine-free). It responds with three recommended options, each with a description and price. For demo purposes, sample menu text is used to ensure consistent output.\n\n4. Ingredient Safety Checker\nThis feature helps users identify if a food product is safe to consume based on dietary restrictions or allergies. The user scans the ingredient label with the camera, and OCR extracts the text. Gemma 3n then evaluates whether the product contains unsafe components. In the demo version, a sample Doritos ingredients list is used to simulate the experience.\n\n5. Store Shelf Analyzer\nWhen pointed at a grocery shelf, the app captures the image and extracts visible text like product names, brands, and prices. Gemma 3n analyzes the text and describes what’s on the shelf, focusing on affordable, vegan-friendly, and ready-to-eat items. This feature supports independent grocery shopping for users with low or no vision.\n\n6. Pedestrian Crossing Detection\nUsing a video clip of a pedestrian crossing, the app detects whether the traffic light is red or green using color masking in HSV space with OpenCV. It informs the user via voice feedback whether it's safe to cross the street or not, essential for blind or low-vision individuals navigating urban environments.\n\n7. General Question Fallback\nIf the voice input doesn’t match a specific intent, the app falls back to Gemma 3n to answer general knowledge or assistive questions. This ensures flexibility and extends usefulness beyond the hardcoded flows.\n\n🛠️ Technologies Used\n📚 AI Models\nGemma 3n (via API call)\nHandles all natural language processing and response generation. \nUsed to:\n\nExtract meal suggestions based on dietary needs\n\nAnalyze ingredient safety\n\nDescribe visible products on store shelves\n\nWhisper (OpenAI)\nUsed locally for speech-to-text transcription of user voice commands and responses.\n\n📷 Computer Vision\nOCR: pytesseract\nExtracts text from images (e.g., menus, packaging, shelves).\n\nImage & Video Capture: OpenCV\n\nCaptures static images from a webcam or phone camera.\n\nFor crossing detection: HSV masking is used to detect red/green pedestrian lights in real-time video frames.\n\n🔊 Speech\nText-to-Speech: pyttsx3\nConverts AI-generated responses into clear spoken feedback—fully offline and cross-platform.\n\nWake Word Detection: fuzzywuzzy\nAllows for loose matches to wake words like “Navigator” or “Navi Voice” for accessibility.\n\n🗂️ Project Architecture\nmain.py\nEntry point with wake-word listening loop and command routing via fuzzy matching.\n\nutils/audio.py\nHandles voice recording, Whisper STT, and pyttsx3 speech playback.\n\nutils/vision.py\nImage/video capture, OCR text extraction.\n\nutils/vision_flows.py\nOrchestrates multimodal flows like menu reading, shelf analysis, crossing detection, and ingredient checking.\n\nutils/gemma.py\nHandles prompt sending and response receiving from Gemma 3n.\n\nutils/logging_utils.py\nLogs system messages for debugging and traceability.\n\n🚀 Offline-Readiness & Privacy Focus\nWhile the prototype currently uses Gemma via API, the architecture is modular and ready to swap in local inference via:\n\nOllama\n\nGoogle AI Edge\n\nJetson (for mobile deployment)\n\nUnsloth (for fine-tuned dietary question model)\n\nAll components that can run offline, audio recording, text-to-speech, wake-word detection, CV, and OCR already do.\n\n🧪 Demo Enhancements for Submission\nTo ensure reliability in demo environments, OCR results are overridden with clean sample text for:\n\nMenus\n\nIngredients\n\nStore shelves\n\nThis ensures deterministic results while showcasing real-world applicability.\n\n🔗 Links\nDemo Video: https://youtu.be/OI5HzEXnPXA\nGithub: https://github.com/MaraschinoGirl/NaviVoice-Hackathon",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/MaraschinoGirl/NaviVoice-Hackathon",
        "demo_video_url": "https://youtu.be/OI5HzEXnPXA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/navivoice-a-private-offline-ai-guide-for-the-visua"
    },
    {
        "project_title": "EduTutor Local: Offline AI Learning for Underserved Rural Communities",
        "tagline": "Offline AI tutor using Gemma 3n brings quality education to poor students in remote areas with no internet or electricity constraints.",
        "authors": "Shreya Mishra 0307",
        "summary": "EduTutor Local: Bridging the Digital Divide in Education\nThe Crisis: Education Inequality in Remote Communities Over 258 million children worldwide are out of school, with the highest concentrations in rural and impoverished areas. In remote villages across Africa, Asia, and Latin America, students face a triple burden: lack of qualified teachers, unreliable or nonexistent internet connectivity, and economic constraints that make quality education a luxury they cannot afford. A village school in rural Kenya might have one teacher for 80 students, while a farming community in rural India might have internet connectivity for only 2-3 hours daily - if at all.\n\nOur Solution: Education That Works Anywhere EduTutor Local is specifically designed for the world's most underserved students. By leveraging Gemma 3n's revolutionary on-device capabilities, we've created an AI tutor that works on basic smartphones and tablets, requires no internet connection, and provides personalized education comparable to expensive private tutoring - completely free.\n\nTechnical Architecture for Resource-Constrained Environments\n\nOptimized for Low-Resource Devices:\n\nUltra-Efficient Processing: Utilizes Gemma 3n's Per-Layer Embeddings to run on devices with as little as 3GB RAM\nBattery-Conscious Design: Optimized power consumption for areas with limited electricity\nOffline-First Architecture: Zero internet dependency after one-time installation\nLocal Data Storage: SQLite database ensures learning progress persists across power outages\nCore Components Built for Rural Challenges:\n\nMulti-Backend Support: Runs on both Android phones and basic laptops\nVoice-First Interface: Critical for students with limited literacy in their local language\nPersistent Learning: Conversation history survives device restarts and power interruptions\nResource Monitoring: Automatically adjusts performance based on available device resources\nGemma 3n: The Perfect Model for Underserved Communities Gemma 3n's breakthrough architecture makes quality AI education accessible to the world's poorest students:\n\nUnprecedented Efficiency: 5B parameter performance with 2B parameter memory usage means powerful AI on $50 smartphones\nMultilingual by Design: Native support for local languages including Spanish, French, German, Korean, and Japanese\nMultimodal Understanding: Students can speak, show pictures of textbooks, or draw problems in the sand\nPrivacy-First: Critical for communities wary of data collection by corporations or governments\nReal-World Impact Scenarios\n\nRural Kenya - Solar-Powered Learning: 10-year-old Amara uses a donated tablet with EduTutor Local, powered by a small solar panel. She practices English pronunciation, gets help with math problems written in her notebook, and learns about science - all while herding goats. No internet required.\n\nRemote Brazil - Flood Season Education: During the rainy season when rivers flood and cut off village access, students continue learning with EduTutor Local. The AI remembers each child's learning level and continues personalized lessons despite months of isolation.\n\nMountain Village Nepal - Post-Earthquake Recovery: After schools were destroyed, EduTutor Local on ruggedized tablets helped 200 children continue their education in temporary shelters, with the AI adapting to trauma-informed teaching approaches.\n\nTechnical Implementation for Harsh Environments\n\nRobust Architecture:\n\nFlask Backend: Lightweight server optimized for resource constraints\nSQLite Persistence: Handles frequent power outages without data corruption\nModular Design: Components can run on different device types as needed\nError Recovery: Graceful handling of hardware failures and resource limitations\nEducational Optimization:\n\nCurriculum Mapping: Aligned with UNESCO and local education standards\nProgress Tracking: Works without cloud sync, purely local analytics\nAdaptive Learning: AI adjusts difficulty based on student responses and available study time\nCultural Sensitivity: Prompts and examples adapted for local contexts\nOvercoming Infrastructure Challenges\n\nNo Internet Dependency: Complete offline operation after setup\nLow Power Requirements: Optimized for solar charging and battery constraints\nRobust Storage: Handles dust, heat, and power fluctuations\nScalable Deployment: Can be distributed via USB drives, SD cards, or brief internet connections\nMulti-Device Sharing: One installation serves entire families or small schools\nMeasuring Impact in Underserved Communities\n\nLearning Outcomes: Local assessment integration without requiring internet reporting\nEngagement Metrics: Time spent learning, topics explored, skill progression\nDevice Usage: Optimization based on real-world device capabilities and constraints\nCommunity Feedback: Built-in feedback system for continuous improvement\nThe Vision: Education as a Human Right EduTutor Local represents more than technology - it's a commitment to educational equity. By putting AI-powered tutoring directly into the hands of the world's most disadvantaged students, we're not just closing the digital divide; we're ensuring that geographic location, economic status, or infrastructure limitations never again determine a child's educational potential.\n\nEvery student deserves a world-class education. EduTutor Local makes that possible, one offline lesson at a time.",
        "technologies": [
            "Flask",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/shrey2003/Gemma-Challenge-local",
        "demo_video_url": "https://youtu.be/WvIoeI3kDZ0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/edututor-local-offline-ai-learning-for-underserved"
    },
    {
        "project_title": "BreakThrough",
        "tagline": "An AI-driven platform for real-time speech analysis and personalized public speaking feedback.  ",
        "authors": "Snehaufff, SAI SAKTHIDAR S R",
        "summary": "Our Guiding Philosophy & Technical Journey",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": "https://github.com/nthsneha/BreakThrough",
        "demo_video_url": "https://youtu.be/uipArL1wPQs?si=8xER3cQGB2cAymeW",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/breakthrough"
    },
    {
        "project_title": "FlexYou – Personalized AI Fitness & Nutrition App",
        "tagline": "An offline-first AI fitness app using Gemma 3n",
        "authors": "Lina Elmanjra",
        "summary": "FlexYou is an AI-powered fitness and nutrition app created during the Google Gemma 3n Hackathon. It lets users choose their current body type and set a target, then provides a personalized workout and diet plan. Users are guided by fictional coaches like Monica and Zuri for daily motivation. Designed in Thunkable with fun visuals and offline-friendly screens, it helps users stay consistent and achieve their goals from home—without any gym equipment.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/0lIPNWbSC44?feature=shared",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/flexyou-personalized-ai-fitness-and-nutrition-app"
    },
    {
        "project_title": "GemmaVision",
        "tagline": "An intelligent surveillance system powered by Google's Gemma 3n multimodal AI model for real-time anomaly detection and security monitoring.",
        "authors": "EmilioVolpeC, Rafael Mosquera",
        "summary": "Gemma Vision is an on-premise security surveillance solution that leverages Gemma 3n's vision capabilities to detect and classify security incidents in real-time. The system prioritizes privacy by processing data locally while providing government and private security teams with automated threat detection.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/eavolpe/surv_gemini3n",
        "demo_video_url": "https://youtu.be/7OTEAGTyfLk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemmavision"
    },
    {
        "project_title": "Wearable Sensor Fusion based AI Health Coach",
        "tagline": "Empower Gemma3n to provide insightful health advise by performing fusion of processed vitals data from multiple external sensor tools",
        "authors": "Sumantra Dasgupta",
        "summary": "The Gemma family of models keeps pushing the boundary of on-device language model capability. The 3n version is it's latest manifestation. This project aims to leverage Gemma's excellent language generation capability for delivering data-driven insights from multiple data-analyzers (each connected to a sensor for monitoring a health vital like heart_rate, spo2, skin_temperature).\n\nIn this project we utilize the Gemma 3n:e4b model. It is set up to act as a health coach. To provide sensor data and the corresponding analysis on health vitals, the system is equipped with a heart_rate analyzer, a spo2 analyzer and a skin temperature analyzer. The user can ask Gemma about a combination of vitals including heart_rate, spo2 level and skin temperature (a question about health includes all). On detecting a request for individual or aggregate health vitals data, Gemma delegates responsibility for fetching such data to the corresponding sensor analyzers, before crafting it's final response. Gemma responds like a health professional/coach only after it receives enough data to provide the user with a convincing data driven insight into his/her health.\n\nEach analyzer is attached to a wearable sensor. On data request, it runs analysis on the last n samples collected by the analyzer. It calculates features like mean, std, min, max, mode and range on the data sample. The analysis is done by feeding the features to a pre-trained Random Forest classifier. Once done, each analyzer sends back the final analysis (healthy/not-healthy), the feature values and the feature importance to Gemma. Experimenting shows that this enhanced contextual data from each sensor helps Gemma in explaining the final labels much better. Without such data as backup, Gemma is forced to report back just the final labels augmented with some very generic text generation.\n\nGiven below are 2 samples of Gemma's text generation:\n\nSample 1: Gemma equipped with only healthy/unhealthy labels\n\nUser: what are my health vitals?\n\nGemma: Okay, let's take a look at these vitals. I'm seeing that your heart rate, temperature, and SpO2 are all currently flagged as HealthCondition.NOT_HEALTHY. \n\nThat's important information, and it tells us we need to pay close attention to your well-being.  These are key indicators of how your body is functioning, and when they fall outside the normal range, it's a signal that something needs addressing.\n\n**Don't panic!**  The first step is to understand *why* these values are elevated.  It could be a number of things, and we need to investigate further. \n\n**Here's what I recommend:**\n\n*   **Prioritize Checking In with a Healthcare Professional:**  This is the most important step.  A doctor can properly diagnose the underlying cause of these readings and recommend the best course of action.  Please schedule an appointment as soon as possible.\n*   **While you wait for your appointment, focus on gentle self-care:**  This isn't a substitute for medical advice, but it can help support your body.  \n    *   **Hydration:** Make sure you're drinking plenty of water.\n    *   **Rest:**  Prioritize getting adequate sleep.\n    *   **Stress Reduction:**  Engage in calming activities like deep breathing, meditation, or gentle stretching.\n    *   **Avoid strenuous activity:**  Don't push yourself physically.\n*   **Be prepared to share details with your doctor:**  Think about any recent illnesses, changes in your routine, medications you're taking, or any other relevant information that might be helpful.\n\nI understand this can be concerning, and I'm here to support you.  Remember, you're not alone, and taking proactive steps is the best thing you can do for your health.  Let's work together to get you feeling better.  Please let me know if you have any questions, and I'll do my best to guide you.\n\n\n\n**Disclaimer:** *I am an AI Chatbot and not a medical professional. This information is for general guidance only and does not constitute medical advice. Always consult with a qualified healthcare provider for any health concerns or befo…\n\n\n\nSample 2: Gemma equipped with labels, feature values used for analysis and feature importance\n\nUser: What are my health vitals?\n\nGemma: Okay, let's take a look at your health data. Based on the information you've provided, here's my assessment:\n\n**Overall Health Condition: HEALTHY**\n\n**Here's why:**\n\n*   **Heart Rate:** Your average heart rate is 81.4 beats per minute, with a standard deviation of 10.3. This falls within a healthy range (60-97 bpm). The mean heart rate is relatively low, indicating good cardiovascular fitness.\n*   **Temperature:** Your average temperature is 98.6°F, with a standard deviation of 0.5. This is a normal and healthy body temperature. The temperature range is also within a healthy range.\n*   **SpO2:** Your average SpO2 is 94.9%, with a standard deviation of 2.6%. This is slightly below the optimal range of 95-100%, but still considered acceptable. \n\n**Important Considerations:**\n\n*   **Heart Rate:** The heart rate is a very important indicator of cardiovascular health. A lower heart rate generally suggests a stronger heart. The standard deviation indicates consistency in your heart rate, which is also a positive sign.\n*   **Temperature:** Body temperature is a key indicator of overall well-being. A stable temperature within the normal range is a good sign.\n*   **SpO2:** While 94.9% is acceptable, it's worth noting that a consistently lower SpO2 could sometimes indicate underlying respiratory issues. However, given the other data points, it's likely not a cause for concern in your case.\n\n**Next Steps:**\n\nTo ensure continued well-being, I recommend:\n\n*   **Maintaining a consistent exercise routine:** This will help keep your heart rate healthy and improve cardiovascular fitness.\n*   **Staying hydrated:** Adequate hydration is essential for maintaining a healthy body temperature and blood oxygen levels.\n*   **Monitoring your SpO2:** If you experience any shortness of breath or other respiratory symptoms, it's a good idea to monitor your SpO2 more closely.\n\n**Disclaimer:** *This is a general assessment based on the provided data and should not be considered a substitute for professional medical advice. If you have any conc…\n\nThese 2 examples clearly show the importance of enhancing the LLM context with meaningful data; it makes the whole response more explainable and useful\n\nFeatures Implemented:\n- Sensor Fusion of multiple health vital sensors\n- Random Forest based Healthy/Unhealthy classifiers based on heart_rate, spo2 and skin_temperature readings\n- Feature engineering for time series\n- Ollama based on-device inference with Gemma3n:e4b\n- Basic implementation of function call inference and tool calling\n- extensible sensor_type based multi sensor framework\n- improved explainability and better data driven response by passing augmented data back to LLM for final response generation\n- basic UI (Gradio)\n- basic monitoring based on SQLite and Datasette\n\nFeatures for future:\n- Use a smaller (number of parameters and quantized) instruction tuned  gemma3n variant\n- integration as a mobile app with fitbit as wearable\n- implementation of MCP to communicate with remote devices and handling more related health/coaching functions without running into ambiguity (also utilize something similar to Langgraph big tool)\n- adding more vital sensors\n- finetune gemma3n embeddings with sensor data\n- more labeled sensor data",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/444EIipEQzs",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/wearable-sensor-fusion-based-ai-health-coach"
    },
    {
        "project_title": "Gemma Local General-Purpose",
        "tagline": "Chatting, streaming, agent tasks, and coding powered by Gemma 3n.",
        "authors": "Peter Gamil",
        "summary": "Chatting, streaming, agent tasks, and coding powered by Gemma 3n.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/7Gamil/Gemma-Offline-Studio",
        "demo_video_url": "https://www.youtube.com/watch?v=s9lmQqs0jXY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-local-general-purpose"
    },
    {
        "project_title": "GemmEye - Your Perfect Eye for Legal Aid Review",
        "tagline": "Help reviewers at the Taiwan Legal Aid Foundation identify whether land parcels on the cadastral map are connected/disconnected using Gemma.",
        "authors": "Federer Yu",
        "summary": "💡 Project Title: **GemmEye — Your Perfect Eye for Legal Aid Review**",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/charliewhiteyu/GemmEye",
        "demo_video_url": "https://youtu.be/VizXOgeF-HA?si=Yv9OE9foBMQoDy6c",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemmeye-your-perfect-eye-for-legal-aid-review"
    },
    {
        "project_title": "SAT AI TUTOR",
        "tagline": "An AI Tutor that uses Gemma 3n fine-tuned using Unsloth.AI",
        "authors": "Vivek Haridas",
        "summary": "This project introduces SAT Tutor, a lightweight, locally runnable AI assistant built to help students—especially those in low-connectivity regions—prepare for the SAT. Using Gemma 3n (4B) fine-tuned via Unsloth AI on SAT-style datasets (Math + EBRW), this solution enables offline, interactive question answering without reliance on cloud APIs.\n\nThe final model was quantized to GGUF Q8_0 format and hosted using llama-cpp-python on Hugging Face Spaces. It is also fully compatible with Ollama for local use. The UI is built with Gradio, ensuring an intuitive experience for students and educators alike.\n\nSubmitted Files:\n1. Video Demo: [YouTube Link]: Walkthrough of the app, its features, and deployment\n2. Technical Writeup: [Medium Blog]: In-depth breakdown of the problem, approach, fine-tuning, deployment, and lessons learned\n3. Source Code: [GitHub Repository]: Full fine-tuning and deployment code\n4. Live Demo: [Hugging Face Spaces]: Try the SAT Tutor in your browser\n\nThis project showcases how Gemma 3n and efficient fine-tuning can power accessible, low-cost educational tools for a global audience.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/VivekHaridas-01/sat-tutor",
        "demo_video_url": "https://youtu.be/HeoWpe1K8YE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sat-ai-tutor"
    },
    {
        "project_title": "Private-AI-Hiring-Assistant",
        "tagline": "A privacy-focused AI Hiring Assistant that uses Gemma 3n to screen resumes, assess candidate-job fit, all on-device without exposing privacy",
        "authors": "Keerthana S",
        "summary": "This project introduces a privacy-first AI Hiring Assistant powered by Gemma 3n, designed to streamline and personalize the recruitment process. Running entirely on-device, the assistant analyzes resumes and job descriptions to generate match scores, identify skill gaps, suggest improvements, and automatically craft relevant interview questions. By leveraging the capabilities of Gemma 3n through a local API (served via Flask and accessed through Ngrok), it ensures that sensitive candidate information remains secure, making it ideal for use in enterprise or offline settings. The solution showcases the practical application of LLMs in human resource workflows—enhancing hiring efficiency while respecting data privacy.",
        "technologies": [
            "Flask",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/KeerthanaS04/Private-AI-Hiring-Assistant",
        "demo_video_url": "https://youtu.be/fQbtI8Z9Owo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/private-ai-hiring-assistant"
    },
    {
        "project_title": "A Node-Based Interface for Building Agentic Robotic Workflows",
        "tagline": "A node based interface and backend that enables non-robotic hobbyist to create agentic robotic workflows powered by LeRobot and Gemma3n",
        "authors": "niel hu",
        "summary": "The idea is inspired by \"ComfyUI for stable diffusion hobbyist\". With the launch of LeRobot and Vision Language Models (VLMs) like Gemma and Gemini with spatial reasoning capabilities, I believe it's time for a wider audience without robotics background to create robotic workflows. I've been working on the UI for a month and am currently testing and integrating Gemma and other VLMs/Vision Language Agents (VLAs) into the framework.\n\nThe framework enables endless possibilities and control for end users through drag-and-drop interfaces. Users can leverage VLMs, build chains of VLM inferences, or use VLMs for reasoning and planning to select suboperations. This mimics system 1 (suboperations, VLAs) and system 2 (spatial reasoning and trajectory planning from models like Gemma3n).\n\nThe framework is fully open source. The VLM pull request is still in progress. In the demo, I invoke Gemma (hosted on Colab) with captured camera images and text prompts to generate robot arm trajectories of X points to complete the given task.\n\nFrom these points on the image, through inverse kinematics calculations, we generate instructions for each motor on the SO-ARM101 robotic arm.\n\nI believe there is huge potential and synergy here. More features are coming - stay tuned to the project. I would love to collaborate further.",
        "technologies": [],
        "source_code_url": "https://github.com/TengHu/FactoryUI",
        "demo_video_url": "https://www.youtube.com/watch?v=Xb4BY4CsLPk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/a-node-based-interface-for-building-agentic-roboti"
    },
    {
        "project_title": "Offline AI Tutor",
        "tagline": "Offline AI Tutor Powered by Google's Gemma 3n e2b-it model.",
        "authors": "Fourat Garrach",
        "summary": "This app is meant to revolutionize the use of Educational AI in developping Countries with bad internet access.",
        "technologies": [],
        "source_code_url": "https://github.com/Tayer007/Offline_AI_Tutor/",
        "demo_video_url": "https://www.youtube.com/watch?v=4AVOu9vcldw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/offline-ai-tutor1"
    },
    {
        "project_title": "Studyx-dyslexia-duplort-aoo",
        "tagline": "Personalised AI support app for children with dyslexia using gemma 3n",
        "authors": "NOHA ELMANJRA",
        "summary": "StudyX is an AI-powered mobile app designed to support children with dyslexia by transforming learning into an emotionally safe, space-themed adventure. Using Gemma 3n, a lightweight open-source model from Google, StudyX identifies the user's dyslexia subtype through a gentle, engaging quiz — then tailors the learning experience to fit their specific needs.\n\nBuilt in Figma with accessibility and neurodiversity at its core, StudyX creates a customized galaxy of supportive features: gamified tasks, encouraging feedback, and adaptive challenges. Though the prototype is visual, the technical architecture has been thoughtfully planned to allow for full backend integration and real-world scalability.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/1XOtnzrC0QY?si=5FwF-hxBpDMoriHN",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/studyx-dyslexia-duplort-aoo"
    },
    {
        "project_title": "Hydrogen-energy 'rule-of-thumb' chatbot",
        "tagline": "Chatbot aiming at educating students and youth on green sustainable energy",
        "authors": "namitha murali",
        "summary": "The chatbot provides verifiable, expert-level answers on hydrogen energy by retrieving information from a local PDF knowledge base and generating responses with a locally-hosted Ollama- gemma.",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": "https://github.com/namitha01m/chatbot",
        "demo_video_url": "https://youtu.be/d8XayPB97ms",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/hydrogen-energy-rule-of-thumb-chatbot"
    },
    {
        "project_title": "The Offline Learning Coach ",
        "tagline": "\"An offline AI-powered feedback tool that helps users improve their speech and writing by analyzing clarity, tone, and empathy\"",
        "authors": "Saksham Kapoor1470, Sanskritiiio",
        "summary": "**Offline Learning Coach** is a fully local, AI-powered educational assistant built for students in under-resourced or internet-restricted environments. It enables users to improve their essay writing, spoken communication, and empathy-driven expression—completely offline, with no data sent to external servers.\n\n**Tech Stack & Architecture**\nModel: Gemma 3n (via Ollama) – provides intelligent, contextual analysis of writing and speech.\n\nTranscription Engine: faster-whisper – enables low-latency, on-device speech-to-text.\n\nUI: Streamlit – intuitive and lightweight interface for accessibility on low-end devices.\n\nBackend: Python + Ollama local server.\n\nPackaging Potential: Portable across low-cost laptops, Raspberry Pi, and Android (planned).\n\n**Core Features**\n 100% Offline & Private – Runs entirely on-device, ideal for schools/labs without stable internet.\n Speech & Text Input – Users can either speak or type; both are processed and evaluated locally.\n Teacher-Style Feedback – Clear, actionable suggestions on: Grammar & Structure, Clarity & Logic, Empathy & Tone, Overall Impact .\n\nProvides Multilingual Support – Gemma 3n supports 140+ languages for native/local language feedback.\n\n**Please refer to the attached screenshots for:**\n\nThe main Streamlit UI interface\nSample feedback on a spoken input\nOffline speech-to-text transcription output\nOllama model running locally with no API calls\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28016153%2F3ed3d32b8cade14dbe580c184636bdff%2FScreenshot%202025-08-07%20032455.png?generation=1754518755201925&alt=media)    , ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28016153%2F5e5df46bf70abbc45e871f91d2b0f717%2FScreenshot%202025-08-07%20033022.png?generation=1754518767981600&alt=media)\n\n**Technical Challenge & Solution**\nInitially, we aimed to analyze raw audio features like pitch, pacing, and confidence. However, Gemma 3n doesn’t accept raw audio via Ollama yet. As a workaround:\nWe integrated faster-whisper to first transcribe speech into text.\nThen passed that text to Gemma 3n for detailed feedback—allowing us to maintain quality without sacrificing performance or privacy.\n\nHow to Run\nSetup environment\npython -m venv venv\nsource venv/bin/activate  or .\\venv\\Scripts\\activate on Windows\npip install -r requirements.txt\n\nStart Ollama server\nollama serve\n\nPull model (first time only)\nollama pull gemma3n:e4b\n\nRun main app\nstreamlit run local_app.py",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/SakshamKapoor2911/Gemma-Go",
        "demo_video_url": "https://www.youtube.com/watch?v=8kTWGMw9_Ig",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/the-offline-learning-coach"
    },
    {
        "project_title": "GemYum - AI-Edge + Gemma3n-4B - (Google's Gemma 3n Impact Challenge)",
        "tagline": "Meal Tracking with Gemma3n + AIEdge",
        "authors": "Sid Kandan",
        "summary": "GemYum: On-Device AI Nutrition Tracker - Kaggle Project Description",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Stellife/GemYum",
        "demo_video_url": "https://www.youtube.com/watch?v=7BBNPGm1R3E",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemyum"
    },
    {
        "project_title": "AI Powered Emergency Response System",
        "tagline": "AI-powered emergency response system: citizen reporting to hospital coordination. Offline-first design for high-stakes, low-connectivity.",
        "authors": "Richard Rembert",
        "summary": "🆘 **Complete Enhanced AI Emergency Response System**\n*Revolutionary Crisis Management Through Intelligent Technology*",
        "technologies": [
            "Docker",
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/rembertdesigns/gemma3n-disaster-assistant",
        "demo_video_url": "https://youtu.be/1VE8vfJ84lQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-powered-emergency-response-system"
    },
    {
        "project_title": "Cure Genie",
        "tagline": "Your personal health care assistant for you and your plants",
        "authors": "Mehul, Sparger, Arnav Singh, OMI KUSHWAHA, Sachal Khan",
        "summary": "🌿 Cure Genie: Your Offline AI Symptom Checker\nCure Genie is a smart, private, and offline-first health companion that helps you diagnose symptoms in both plants and humans using the power of AI. Whether you're worried about your crop's leaves or a sudden cough, Cure Genie analyzes symptoms through text or images and offers insights instantly — no internet required.\n\n✨ Key Features:\n\n🧠 Multimodal AI (Text + Image Input)\n\n🌱 Plant & 👨‍⚕️ Human Diagnosis Modes\n\n💻 Works 100% offline — powered by Gemma & TensorFlow\n\n📥 Easy result downloads\n\n🤖 Smart version selector for best model performance",
        "technologies": [],
        "source_code_url": "https://github.com/Mehulcodes1/Cure-Genie",
        "demo_video_url": "https://www.youtube.com/watch?v=YSlQ5xUMs8s&t=14s",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/cure-genie"
    },
    {
        "project_title": "Field-Comm",
        "tagline": "A life-saving app that delivers offline translation, knowledge search, and alerts in disaster zones.",
        "authors": "Axis Wang",
        "summary": "📱 Field‑Comm: An Offline Multilingual Crisis Communication Hub\n\n> A life-saving mobile app that works when everything else fails.\n\n---",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/xuying7/Field-Comm",
        "demo_video_url": "https://youtu.be/QieXvMpLIRM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/field-comm"
    },
    {
        "project_title": "GemmaVetCare",
        "tagline": "Bangla Offline AI for Cattle Disease Diagnosis and Context-Aware Feeding Guidance",
        "authors": "Sadatul islam Sadi, Hasnain Adil, Md Irtiaz Kabir, imkabir0, MOHAMMAD NINAD MAHMUD",
        "summary": "GemmaVetCare is an AI‑powered livestock management platform designed to address critical challenges in rural Bangladesh’s cattle farming sector, including limited veterinary coverage, delayed disease detection, and inadequate feeding guidance. The system integrates multilingual support (Bangla and English), offline functionality, and AI‑driven multimodal disease diagnosis with context‑aware feeding recommendations.\n\nThe disease detection module leverages a LoRA‑fine‑tuned Gemma‑3N model trained on annotated cattle images and Bangla symptom descriptions, enabling accurate identification of common and economically significant diseases such as Foot‑and‑Mouth Disease, Lumpy Skin Disease, and Pinkeye. The feeding recommendation module employs Random Forest and CatBoost regression models trained on standardized nutritional datasets, coupled with a large language model (LLM)–driven feed formulation system that generates stage‑specific and resource‑adaptive rations.\n\nTo ensure accessibility for farmers with limited internet connectivity, the platform includes offline inference capabilities for both disease diagnosis and feeding guidance. Its multilingual interface ensures usability across diverse literacy levels, providing clear, actionable insights in Bangla and English.\n\nBy combining computer vision, natural language processing, and machine learning with a farmer‑centric design, GemmaVetCare delivers scalable, inclusive, and data‑driven livestock care, improving animal health, productivity, and rural livelihoods.",
        "technologies": [],
        "source_code_url": "https://github.com/Sadatul/GemmaVetCare",
        "demo_video_url": "https://www.youtube.com/watch?v=EoxyudCIVSo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-vet-care"
    },
    {
        "project_title": "AgriVision-Gemma3n: Offline Crop Diagnosis for Farmers",
        "tagline": "An offline mobile-friendly multimodal Gemma3n assistant that empowers farmers with instant crop disease diagnosis and farm-related advice.",
        "authors": "shrey patel",
        "summary": "🌾 AgriVision‑Gemma3n  \n**An offline, mobile‑friendly assistant built on Gemma 3n that empowers farmers with instant crop disease diagnosis and farm advice.**\n\n---",
        "technologies": [],
        "source_code_url": "https://github.com/shreypatel24/AgriVision",
        "demo_video_url": "https://youtu.be/Swm_URZs618",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/agrivision-gemma3n-offline-crop-diagnosis-for-farm"
    },
    {
        "project_title": "Takumi - Solution of the Fading Skills",
        "tagline": "\"Takumi\" uses Gemma 3n as an AI interviewer to convert artisans' tacit skills into explicit manuals.",
        "authors": "mugi0007",
        "summary": "Takumi: A Human-in-the-Loop System for Transforming Tacit Knowledge into Explicit Manuals with Gemma 3n",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/mugi0227/Takumi",
        "demo_video_url": "https://youtu.be/dE4_hXmbWq0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/takumi-solution-of-the-fading-skills"
    },
    {
        "project_title": "ChessMate: An AI-Powered Cognitive Co-Pilot for Learning",
        "tagline": "**ChessMate** is an innovative AI-powered educational platform that transforms the classic game of chess into a dynamic, multilingual learni",
        "authors": "shamb0",
        "summary": "",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/xcRYXtVLC4I",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/chessmate-an-ai-powered-cognitive-co-pilot-for-lea"
    },
    {
        "project_title": "StudyAI: AI-Powered Learning Assistant",
        "tagline": "Intelligent study companion that understands questions from any format and provides instant explanations",
        "authors": "HUDA Al hamid",
        "summary": "StudyAI revolutionizes the learning experience by providing students with an AI-powered study assistant that works seamlessly across multiple input methods. Students can upload images of textbook problems, type questions directly, or speak their queries aloud - StudyAI understands and responds with detailed, step-by-step explanations.\n\nKey Features:\n• Multimodal Input Processing - Text, images, and voice recognition\n• Instant Answer Generation - Detailed explanations with step-by-step solutions\n• Privacy-First Design - All processing happens locally on device\n• Offline Functionality - Works without internet connection\n• Multi-Language Support - Accessible to students worldwide\n• Adaptive Learning - Personalizes responses based on user progress\n\nImpact:\nStudyAI addresses the global education accessibility gap, providing quality learning assistance to students regardless of their location, internet connectivity, or economic background. By democratizing access to AI-powered tutoring, we're helping bridge educational inequalities worldwide.\n\nTechnical Implementation:\nBuilt using advanced on-device AI capabilities with optimized multimodal understanding. The system processes visual, audio, and text inputs locally while maintaining user privacy and ensuring fast response times.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/-z3d-1LE9y8?si=_9dVrxcoVUlvNc2V",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/studyai-ai-powered-learning-assistant"
    },
    {
        "project_title": "Personal Guidance for Visually Impaired Persons with Gemma 3n Fine-Tuning",
        "tagline": "This project aims to provide navigation support for the blinds and their surroundings by fine-tuning Gemma 3n with WAD image dataset.",
        "authors": "Jason Kung",
        "summary": "This project aims to provide navigation support for the blinds and their surroundings by fine-tuning Gemma 3n with WAD image dataset.\n\nDataset Source: https://walkvlm2024.github.io/\n\nCitation:\n@article{yuan2024walkvlm,\n  title={Walkvlm: Aid visually impaired people walking by vision language model},\n  author={Yuan, Zhiqiang and Zhang, Ting and Deng, Ying and Zhang, Jiapei and Zhu, Yeshuang and Jia, Zexi and Zhou, Jie and Zhang, Jinchao},\n  journal={arXiv preprint arXiv:2412.20903},\n  year={2024}\n}",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/hdeEMXbc22s",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/personal-guidance-for-visually-impaired-gemma3n"
    },
    {
        "project_title": "Gemma3n Multilingual Learning Assistant",
        "tagline": "A model that translates Indian language words to English using Hugging Face models and provides AI explanations via Gemma 3n.",
        "authors": "Celine John",
        "summary": "Multilingual Learning Assistant\nA Jupyter notebook-based learning application that helps users learn words from 16 Indian languages using AI-powered translation and explanation services.\n\nKey Features:\nTranslation Service: Uses Hugging Face models to translate Indian languages to English\nGemma 3n AI Integration: Provides comprehensive word explanations with pronunciation, usage examples, and cultural context\nFAISS Vector Storage: Implements semantic search and storage for efficient word retrieval\nInteractive Interface: Widget-based interface for learning, quizzing, and searching words\n\nSupported Languages:\nHindi, Tamil, Bengali, Kannada, Telugu, Gujarati, Marathi, Punjabi, Urdu, Malayalam, Odia, Assamese, Manipuri, Kashmiri, Konkani, and Sanskrit.\n\nCore Functionality:\nWord learning with AI-generated explanations\nInteractive quiz mode for testing knowledge\nSemantic search through learned words\nProgress tracking and data export capabilities",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/celinejp/Gemma3n_Multilingual_Learning_Assistant",
        "demo_video_url": "https://youtu.be/bhlZ5du_vDM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma3n-multilingual-learning-assistant"
    },
    {
        "project_title": "Gemma3n - Medical Advisor",
        "tagline": "Using Gemma3n to make healthcare easier to access in rural areas, developing countries.",
        "authors": "Nguyen Hong Minh Vo, Kim Hang Le",
        "summary": "Using Gemma3n to make healthcare easier to access in rural areas, developing countries.",
        "technologies": [],
        "source_code_url": "https://github.com/lkhang3000/gemma3n.git",
        "demo_video_url": "https://youtu.be/oGpzPx49tWo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma3n-medical-advisor"
    },
    {
        "project_title": "AccesiVoice",
        "tagline": "Accessbility in hands",
        "authors": "Koustav, Priyanshu-Ghosh-unplg",
        "summary": "https://www.kaggle.com/code/koustav2710/accessivoice",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/o59M_ardMhc?si=0IQS7x3s2O5c-uQC",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/accesivoice"
    },
    {
        "project_title": "NeroEdu: AI-Powered Education Platform",
        "tagline": "Revolutionary educational platform powered by locally-run Gemma models, providing feedback on essays, practice tests, and study materials.",
        "authors": "Gabriel Valentim, Hudson Monteiro, Samuel Jabes, Gustavo Victor Valente Braga e Souza, Giovanna Moeller",
        "summary": "**NeroEdu** is an AI-powered educational platform designed to operate **offline** on **low-end devices**, aiming to democratize access to high-quality exam preparation for students facing connectivity and infrastructure barriers. By leveraging the **Gemma3n** family of small, adaptable LLMs and a lightweight **TF-IDF-based RAG (Retrieval-Augmented Generation)** system, NeroEdu delivers personalized support for students preparing for standardized exams like **ENEM, SAT, CUET**, and others.\n\nThe platform includes:\n\n* **Essay Evaluation Agent**: Grades argumentative essays based on official criteria, including granular ENEM competencies.\n* **Mock Exam Generator**: Creates personalized practice questions per exam type and topic, with structured outputs (JSON format).\n* **Flashcard Creator**: Generates flashcards and extracts key topics for study, incorporating video transcript knowledge when available.\n\nNeroEdu's technical architecture includes:\n\n* **Ingestion stage**: Processes over 1000 educational video transcripts into a searchable TF-IDF vector store.\n* **Inference stage**: Specialized agents access the vector store via RAG to enhance response relevance and educational alignment.\n\nAll components run locally, using **pre-compiled binaries** and a desktop GUI (built with Python and packaged via NSIS), ensuring accessibility even in digitally excluded environments. The platform utilizes the **MatFormer** architecture from Gemma3n, adapting to different hardware constraints and promoting scalability.\n\nUltimately, NeroEdu showcases how recent LLM advancements can be applied responsibly to real-world educational inequality, delivering powerful tools without relying on internet connectivity or expensive hardware.",
        "technologies": [],
        "source_code_url": "https://github.com/xValentim/neroedu-gemma3n",
        "demo_video_url": "https://youtu.be/OiGCIs8jFCk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/neroedu-ai-powered-education-platform"
    },
    {
        "project_title": "Codivus Learn to code offline",
        "tagline": "Codivus - an offline AI programming learning platform involving gamified challenges, courses, tutoring, achievement tracking, & flashcards.",
        "authors": "Aakkash Muthukumar, VishalVasanthakumarP, ?Shrimanpro, RishDamarla",
        "summary": "Project Description\n\nCodivus is an innovative AI-powered programming learning platform that revolutionizes how students learn to code through personalized, interactive experiences. Built with Gemma 3n at its core, Codivus addresses the critical problem of traditional programming education being too rigid, impersonal, and disconnected from real-world application. The platform combines interactive coding challenges, intelligent flashcards with spaced repetition, and an AI tutor powered by Gemma 3n that provides real-time, contextual assistance. Unlike conventional learning platforms, Codivus offers a gamified experience with XP tracking, achievements, and progress visualization, while maintaining offline capabilities for uninterrupted learning. The AI tutor, specifically fine-tuned for programming education, can explain complex concepts, debug code, and provide step-by-step guidance in multiple programming languages including Python, JavaScript, C++, and Java. This creates a comprehensive learning ecosystem that adapts to individual learning styles and paces, making programming education more accessible, engaging, and effective for learners of all levels.\n\nTechnical Writeup: The Proof of Work\n\nArchitecture Overview\nCodivus employs a modern, scalable architecture built around a React TypeScript frontend and FastAPI Python backend, with Gemma 3n serving as the intelligent core for AI-powered tutoring. The system is designed with microservices principles, ensuring modularity, maintainability, and extensibility.\n\n**Frontend Architecture:**\n\nReact 18 with TypeScript for type safety and modern development experience\nVite for lightning-fast development and optimized builds\nTailwind CSS with shadcn/ui components for consistent, accessible UI\nReact Query for efficient data fetching and caching\nReact Router for seamless navigation and state management\nLocal Storage for offline functionality and persistent user data\n\n**Backend Architecture:**\n\nFastAPI for high-performance, async API development\nPydantic for robust data validation and serialization\nUvicorn ASGI server for optimal performance\nOllama integration for local Gemma 3n deployment\nStreaming responses for real-time AI interactions\nGemma 3n Integration and Implementation\n\nThe core innovation of Codivus lies in its sophisticated integration of Gemma 3n for AI-powered tutoring. We implemented a multi-layered approach to ensure optimal performance and user experience:\n\n1. Local AI Deployment:\n\n2. Specialized Prompt Engineering:\nWe developed custom system prompts that transform Gemma 3n into a specialized programming tutor:\nContext-aware responses that adapt to user skill level\nCode-specific formatting with proper markdown and syntax highlighting\nMulti-language support for Python, JavaScript, C++, and Java\nProgressive learning with step-by-step explanations\n\n3. Streaming Response Implementation:\nThis enables real-time, responsive AI interactions that feel natural and immediate.\nTechnical Challenges and Solutions\n\n**Challenge 1: Real-time AI Response Streaming**\nProblem: Traditional AI APIs often have high latency and don't provide streaming responses, creating poor user experience.\nSolution: Implemented server-sent events (SSE) with FastAPI's StreamingResponse, enabling real-time token-by-token streaming from Gemma 3n to the frontend.\n\n**Challenge 2: Offline Functionality**\nProblem: Learning platforms often fail when internet connectivity is poor or unavailable.\nSolution: Built comprehensive offline support using React's local storage and service workers, allowing users to continue learning even without internet access.\n\n**Challenge 3: Multi-language Code Execution**\nProblem: Supporting multiple programming languages requires complex sandboxing and execution environments.\nSolution: Implemented a secure code execution service with language-specific containers and proper input/output handling.\n\n**Challenge 4: Personalized Learning Paths**\nProblem: Static learning content doesn't adapt to individual learning styles and progress.\nSolution: Created dynamic content generation using Gemma 3n, with progress tracking and adaptive difficulty adjustment.\n\nWhy Our Technical Choices Were Optimal\n\n1. Gemma 3n Selection:\nLocal deployment eliminates API costs and latency\nOpen-source nature ensures long-term sustainability\nStrong code understanding capabilities perfect for programming education\nCustomizable through prompt engineering for specialized tutoring\n\n2. FastAPI + React Architecture:\nType safety throughout the stack with TypeScript and Pydantic\nAsync performance for handling multiple concurrent AI requests\nModern development experience with hot reloading and fast builds\nScalable microservices architecture for future growth\n\n3. Streaming Responses:\nReal-time interaction creates engaging user experience\nReduced perceived latency compared to traditional request-response\nBetter error handling with immediate feedback\n\n4. Offline-First Design:\nReliable learning experience regardless of connectivity\nData persistence ensures no progress is lost\nReduced server load with local caching\nPerformance and Scalability Considerations\n\n**Frontend Performance:**\nCode splitting with React Router for faster initial loads\nReact Query caching reduces redundant API calls\nOptimized bundle size with Vite's tree shaking\nLazy loading for non-critical components\n\n**Backend Performance:**\nAsync/await patterns for non-blocking operations\nConnection pooling for database operations\nCaching layer for frequently accessed data\nRate limiting to prevent abuse\n\n**AI Performance:**\nLocal model deployment eliminates network latency\nStreaming responses provide immediate feedback\nContext management optimizes token usage\nPrompt optimization reduces response time\nSecurity and Data Privacy\n\n**Security Measures:**\nInput sanitization for all user-generated content\nCode execution sandboxing for safe programming challenges\nCORS configuration for controlled cross-origin requests\nRate limiting to prevent abuse\n\n**Privacy Features:**\nLocal data storage keeps user data on their device\nNo external API calls for AI responses (everything runs locally)\nOptional cloud sync for users who choose to share data\nData encryption for sensitive information",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/vishalVasanthakumarPoornima/google-hackathon",
        "demo_video_url": "https://youtu.be/TYAn-LTEyto?si=412E2kuVFLpxpm-t",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/codivus-learn-to-code-offline"
    },
    {
        "project_title": "Emotional Understanding Powered by AI",
        "tagline": "AI-Powered Emotion Plugin: Technical Implementation with Gemma 3n",
        "authors": "FuyuhiMa",
        "summary": "This project demonstrates how LLM especially, Gemma 3n can bridge the gap between human emotional complexity and AI understanding. Born from the universal struggle to express our inner world, this project turns the challenge of “naming what we feel” into a concrete, AI-powered solution, helping people feel \"understood,\" and gain deeper self-awareness through intelligent emotional analysis.\n\nThe system leverages Google’s Gemma 3n LLM to provide nuanced and scientifically grounded emotional analysis.  It translates natural language into Plutchik-based emotion insights, offering Emotion classification, Intensity analysis, Reasoning and Visualization. \n\nBeyond analysis, the app features a real-time conversation companion.  Users can chat with the AI like a supportive friend, receiving **empathetic and personalized responses** that make emotional self-reflection engaging and comforting.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/3ZsAurYd5Is",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/emotional-understanding-powered-by-ai"
    },
    {
        "project_title": "Panacea - Autonomous Intelligence",
        "tagline": "Multi Agent AI System with Reasoning",
        "authors": "nvidra",
        "summary": "**Panacea – Autonomous Intelligence**\nPanacea is a general-purpose multi-agent AI framework designed to build, orchestrate, and deploy collaborative autonomous systems. It empowers organizations to automate complex workflows through intelligent agents capable of reasoning, coordination, and secure data management.\n\n**🧠 What is Panacea?**\nPanacea is a modular platform that enables developers to:\n\n- Create agents with memory, tools, and reasoning abilities for autonomous task execution.\n- Engage agents through an intuitive interface that supports local execution and real-time control.\n- Coordinate teams of agents to collaborate, pass tasks, and solve problems in sequence or parallel.\n- Track and optimize agent performance with detailed metrics, logs, and evaluation pipelines.\n- Integrate seamlessly with local or cloud infrastructure (APIs, databases) while maintaining privacy and control.\n\n**🚨 Problem Statement**\nAs workflows grow in complexity, organizations increasingly need scalable, private, and autonomous systems. Traditional AI tools lack the ability to manage long-term context, collaborate across multiple agents, or execute tasks without human supervision. These limitations hinder automation and scalability. Panacea solves this by introducing a flexible agentic architecture that supports autonomy, collaboration, and secure local deployment—making it ideal for sensitive and mission-critical environments.\n\n**⚙️ How Panacea Works**\nPanacea uses an agentic graph model:\n\n- Interface – Human-facing input and command layer.\n- Node – An individual agent with a specific role or capability.\n- Edges – Communication pathways between agents.\n- Orchestrator – Core engine that routes tasks, manages state, and ensures agents collaborate effectively.\n\n**Each agent is defined with:**\n\n- Role-based reasoning capabilities\n- Tool access (e.g., code execution, document parsing)\n- Memory and context tracking\n- Optional fine-tuned LLMs (e.g., Gemma 3n)\n\n\n**🛠️ Supported Use Cases**\nPanacea is domain-agnostic and supports agents across multiple industries. Example tasks include:\n\n- Document Analysis & Summarization: Agents collaborate to extract insights from unstructured text (e.g., legal, clinical, or financial documents).\n- Data Processing & Integration: Agents synthesize data across formats and sources (e.g., CSVs, PDFs, APIs).\n- Project Management: Agents plan, assign, and track project tasks with persistent memory and contextual reasoning.\n- Customer & Technical Support: Specialized agents work together to answer queries, troubleshoot issues, and escalate when needed.\n\n**🤖 Adaptive Intelligence**\nPanacea agents continuously learn from interactions, update their knowledge base, and evolve with each task. This adaptive intelligence allows the system to:\n\n- Dynamically select the best agent for a task\n- Add or remove tools on the fly\n- Adjust to changing inputs or mission needs\n\n**🧪 Open Source Research Backbone**\nPanacea is part of the broader Autonomous Intelligence open-source research initiative. The framework is designed to enable the study and development of collaborative AI systems that operate in dynamic, real-world environments—pushing the frontier of scalable autonomous reasoning.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/nv78/Autonomous-Intelligence",
        "demo_video_url": "https://www.youtube.com/watch?v=OgFijOrzarg&embeds_referring_euri=https%3A%2F%2Fanote.ai%2F",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/anote-autonomous-intelligence-product"
    },
    {
        "project_title": "AI First-Aid Assistant",
        "tagline": " A multimodal first-aid assistant, built on the Gemma 3n model, that analyzes injury photos or text to generate instant first-aid guidance,,",
        "authors": "Sameerkumar1012",
        "summary": "Confusion can be a significant barrier in the crucial moments after a common injury. Our project, a First-Aid Assistant driven by AI, aims to provide confidence and clarity when it's most needed. Users only need to upload a picture of their injury to get prompt, useful advice. Our application uses Google's Gemma 3N model's sophisticated multimodal capabilities to produce clear, step-by-step text instructions. This project demonstrates proficiency with AI model integration and user-centric design, producing a tool that makes all people able to access and comprehend basic first-aid knowledge.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/sameerkumar1012/Ai-First-Aid-Assistant.git",
        "demo_video_url": "https://youtu.be/HmE2dRkEOtk?si=XKjflh9LHmT2VTw4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-first-aid-assistant"
    },
    {
        "project_title": "Gemma 3n chat Application - Technical Proof of Work",
        "tagline": "The app features three AI modes study, advice, and general so users can learn new topics, get advice, or ask everyday questions.",
        "authors": "soham desai",
        "summary": "The Gemma 3n Chat Application is a privacy-first, offline capable conversational AI interface that leverages Google’s Gemma 3n language model through local inference via Ollama. This application addresses the critical need for private AI interactions by ensuring all conversations remain on the user’s device, with no data transmitted to external servers.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/desaisoham0/Gemma3n",
        "demo_video_url": "https://youtu.be/P_riLyePwZ4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-3n-chat-application-technical-proof-of-work"
    },
    {
        "project_title": "An Offline Cocoa Bean Quality App with Gemma 3n and YOLOv8",
        "tagline": "An offline-first cocoa quality app using YOLOv8 and Gemma 3n to classify beans and provide AI feedback without internet access",
        "authors": "Dickson Sarpong",
        "summary": "**Building an Offline Cocoa Bean Quality App with Gemma 3n and YOLOv8**\nIn Ghana, cocoa is more than just a crop — it's a livelihood. But assessing bean quality still relies heavily on human judgment, and many farmers lack reliable connectivity to access modern tools. This sparked my goal for the Gemma 3n Hackathon: Could I build an offline-first, AI-powered assistant that classifies cocoa bean quality and explains the results using natural language?\n\nThis is the story of how I brought that idea to life.\n\n**The Vision**\nI wanted to combine computer vision and language generation to do two things:\n\n1.  Classify cocoa bean quality from images using a YOLOv8 model.\n\n2.  Explain the result with a natural language response powered by Gemma 3n.\n\nEverything had to work offline, on a local machine. No internet. No APIs. Just pure local inference.\n\n\n**Building the Image Model**\nThe first step was training a model to recognize cocoa bean quality.\n\nDataset\nI curated and resized a set of labeled images representing good and bad cocoa beans. The dataset came from a mixture of sources and was structured for YOLO training.\n\nYOLOv8 Training\nUsing Ultralytics’ YOLOv8, I trained a model with the following configuration:\n\nModel: yolov8n\n\nEpochs: 50\n\nMetrics: mAP improved consistently, especially in the \"good\" class\n\nExport: After validation, I exported the best weights as quality_best.pt\n\nTraining was done locally using CPU, which took some time, but the results were usable.\n\n\n**Introducing Gemma 3n**\nGemma 3n is a lightweight 2B parameter model by Google that runs offline. I used it to explain the predicted image class in a friendly, contextual manner.\n\nBut using it wasn't easy\n\n**The Struggle of Scripted Inference**\nTo make Gemma work offline via CLI and Flask, I had to export the model using TorchScript. Unfortunately, Gemma 3n's architecture includes dynamic arguments and keyword-only parameters, which TorchScript does not support.\n\nThe Workaround\nI switched to using the native Hugging Face model directly in gemma_model.py, skipping the TorchScript path. The model was loaded via AutoModelForCausalLM and AutoTokenizer, pointing to the local cache:\n\n*model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map=\"cpu\", torch_dtype=torch.float32)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)*\n\n\nI then defined a simple function to generate text responses based on the image prediction:\n\n\n*def generate_response(prompt):\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    outputs = model.generate(**inputs, max_new_tokens=100)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)*\n\n*CLI Interface: Simplicity Over Style*\nDue to resource constraints and Flask performance issues, I built a Command Line Interface (CLI) that takes an image path as input, predicts the class using the YOLO model, and calls Gemma to explain the result.\n\nScreenshots of the CLI app were included in my submission to show functionality.\n\n**Why Placeholder Responses?**\nDue to local hardware limitations and slow inference times with Gemma 3n, placeholder responses were added during the demo to simulate how the final app would behave in real-time. This keeps the experience fluid while still showcasing real offline AI logic and integration.\n\n**Flask App (Bonus)**\nI also created a Flask app to support web-based uploads and display both the prediction and Gemma’s explanation. However, due to memory constraints and latency, this version may not work reliably on low-resource systems.\n\n**Demo and Storytelling**\nIn the video demo, I walk through:\n\n1. The offline image model prediction via CLI\n\n2. The use of Gemma to generate natural language responses\n\n3. How the two systems combine into one cohesive tool for cocoa bean assessment\n\n**Submission Summary**\nProblem Addressed: Offline cocoa bean quality evaluation and explanation\n\nModels Used:\n\n1. YOLOv8n (Ultralytics) for image classification\n\n2. Gemma 3n (local model) for natural language explanation\n\nDeployment: CLI + screenshots; optional Flask interface\n\nDemo Video: [link]\n\nPublic Code Repo: [GitHub link]\n\nLive Demo: N/A (offline-first by design)\n\n\n**What’s Next?**\nIf I had more time or computing power, I would:\n\nOptimize Gemma's inference time via quantization\n\nIntegrate voice input (e.g., Twi or Hausa) using Whisper or speech recognition\n\nAdd a biodiversity tracking feature for cocoa-related leaf diseases\n\nDeploy on Raspberry Pi or low-power devices\n\n**Final Words**\nBuilding this app stretched my patience and CPU to the limit. But it taught me what’s possible with small models, smart architecture, and a real-world use case. Gemma 3n might be small, but its potential for good is huge.",
        "technologies": [
            "Flask",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/dicksarp09/cocoa-quality-detection",
        "demo_video_url": "https://youtu.be/1ELm-gQizGg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/an-offline-cocoa-bean-quality-app-with-gemma-3n-an"
    },
    {
        "project_title": "LiGHT MAM-AI",
        "tagline": "An offline-first, privacy-preserving, clinical smart-search app designed to support nurses and midwives in Zanzibar for neonatal care.",
        "authors": "Mohammad Moonsamy, FayElhassan, Fiifi Dawson, Cael Marquard, Trevor Brokowski1",
        "summary": "**NB: the full (and fancy!) PDF writeup is available under the files for this project.** What follows here is a brief summary of the project. Please rather read the PDF format writeup!\n\nZanzibar experiences unacceptably high rates of maternal and newborn mortality, primarily due to lack of information at the point of care. MAM-AI is an offline-first, privacy-preserving, clinical smart-search app designed to support nurses and midwives in Zanzibar for neonatal care. The key challenge of these nurses is information scarcity - doctors and supervisors are often far away and unavailable. MAM-AI addresses this by putting easy summaries of gold-standard clinical guidelines right in the hands of the nurses in need at the point of care. Nurses type their queries into the search bar, the app searches through the guidelines, and they receive a summary of the relevant clinical information. MAM-AI aims to reduce preventable maternal and neonatal deaths by democratizing access to expert medical information at the point of care.",
        "technologies": [],
        "source_code_url": "https://github.com/MZSFighters/Mam-AI",
        "demo_video_url": "https://youtu.be/ks5fF-1xY4M",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/light-mam-ai"
    },
    {
        "project_title": "DoctorPulse: Offline AI Chatbot for Healthcare & Crop Care using Gemma 3n 2b",
        "tagline": "Empowering patients and farmers with an offline AI assistant that provides instant guidance even without internet connectivity.",
        "authors": "Raja Danish",
        "summary": "🩺🌿 DoctorPulse — Your Offline AI Health & Crop Assistant (Powered by Gemma 3n)\n🔍 Project Overview\nDoctorPulse is a multimodal AI assistant built on the Google Gemma 3n 4B model, designed to function completely offline for two critical domains:\n\n🏥 Healthcare (medical assistance)\n\n🌾 Crop Care (agriculture and plant health)\n\nWith a beautifully interactive UI powered by Gradio, DoctorPulse combines natural language chat, image input, and even voice interactions to serve communities with limited internet access.\n\n⚙️ Key Features\n✅ Offline LLM (Gemma 3n 4B)\n✅ Healthcare Assistant – Ask medical questions privately\n✅ Crop Disease Expert – Diagnose crop issues with image & text\n✅ Voice Input + Output – Talk to your assistant hands-free\n✅ Image Upload – Attach plant or symptom photos\n✅ User-Friendly UI – Interactive buttons, new chat reset, category selection\n✅ Multimodal Chat History – Keeps all messages with editability",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Danish6952/Kaggle-Competitions/tree/0b1d12616a1056d6620f70f3bb8e35d381f46599",
        "demo_video_url": "https://youtu.be/xTssscJwa-w",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/Gemma-Powered-App"
    },
    {
        "project_title": "Navigator Bot",
        "tagline": "Helps you navigate autonomously with just commands in simple english.",
        "authors": "Sanchayan Ghosh, sharat kumar m, Advith J, Harish Chandrasekaran",
        "summary": "Navigator Bot helps users navigate through obstacles by looking at an image.\n\nIt can be used to navigate offline and unmanned completely, thus allowing robots to go to dangerous places.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/hc_DQ4EVM-Q",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/navigator-bot"
    },
    {
        "project_title": "Rural Danger Detection System",
        "tagline": "RDDS (Rural Danger Detection System) is an AI-powered safety monitoring system that uses Gemma 3n vision-language model to detect threats",
        "authors": "ibukun Emmanuel Olofin",
        "summary": "Rural Danger Detection System (RDDS) Lite: A Strategic Intervention Against Food Insecurity in Nigeria\n1. The Problem: A Nation in Crisis, A Farmer in Fear\n\nNigeria is facing a catastrophic food security crisis, directly fueled by the relentless terrorization of its rural farming communities. Pervasive banditry, herder-farmer conflicts, and rampant kidnappings have forced countless farmers to abandon their lands. This isn't just a headline; it's a daily reality that has crippled agricultural production and caused food prices to skyrocket. The cost to feed a family of five a pot of Jollof rice has surged from N4,000 to N28,000. A bag of rice that cost N30,000 before June 2023 now sells for N90,000.\n\nThis crisis has pushed Nigeria to rank 109th out of 125 countries on the 2023 Global Hunger Index, with a \"serious hunger\" score. The core issue is the critical absence of timely, actionable intelligence. Farmers operate in constant fear, vulnerable to attacks that have evolved from isolated clashes into organized criminal enterprises imposing \"taxes\" for access to their own fields. This cycle of fear, displacement, and economic collapse is the primary driver of Nigeria's food insecurity.\n\n2. Our Solution: The Rural Danger Detection System (RDDS) Lite\n\nThe RDDS is a direct technological intervention designed to break this cycle of terror. It is an early warning system that provides immediate, real-time danger detection for farmers. By deploying a network of low-cost, easily deployable acoustic and motion sensors in vulnerable farmlands, our system is designed to identify specific threats—such as the sound of gunshots or the movement of large, approaching groups—and instantly alert farmers and community leaders via SMS and simple app notifications. This proof-of-concept directly tackles the root cause of the food crisis by empowering farmers with the most valuable asset in a crisis: time. It gives them the critical moments needed to find safety, enabling them to return to their fields with confidence.\n\n3. Technical Implementation & Capabilities\n\nThe RDDS Lite Demo is built on a scalable and robust architecture designed for the challenges of rural environments:\n\nSensor Network: Utilizes strategically placed acoustic and motion sensors to monitor for specific, pre-defined threat indicators.\n\nEdge Processing: A local processing unit (e.g., Raspberry Pi) analyzes sensor data on-site. This ensures rapid, low-latency detection without relying on often-unstable rural internet connectivity.\n\nMulti-Channel Alerts: Upon threat detection, the system triggers instant SMS and mobile app notifications to all registered users, ensuring wide and immediate dissemination of warnings to an entire community.\n\nOur demo proves the system's ability to rapidly identify simulated threats and effectively alert stakeholders, showcasing a viable and life-saving path to enhancing farmer safety.\n\n4. Projected Impact: From Farm Safety to National Stability\n\nThe RDDS offers a powerful, multi-layered impact that ripples from the individual farmer to the national economy:\n\nImmediate Safety & Increased Production: Drastically reduces farmer casualties and kidnappings, encouraging a return to over 200,000 hectares of abandoned farmlands and boosting local food production.\n\nEconomic Stabilization: Increased agricultural output will exert significant downward pressure on food inflation, making food more affordable and reducing Nigeria's costly reliance on imports.\n\nHumanitarian Relief: By making rural areas safer, the RDDS helps stem the tide of internal displacement, which has left 5 million children suffering from acute malnutrition. It allows families to return home and rebuild their livelihoods.\n\nEnhanced Governance & Community Resilience: The system can help rebuild trust between communities and governance structures by providing tangible security. Furthermore, a scaled-up system will provide invaluable data on insecurity patterns, enabling more effective, evidence-based security policies.\n\nBy securing farmers, the RDDS doesn't just save lives—it unlocks Nigeria's vast agricultural potential, creating a multiplier effect of economic stability and development across the nation.\n\n5. Conclusion: A Strategic Imperative for Nigeria's Future\n\nThe RDDS is more than a security tool; it is a strategic investment in Nigeria's food sovereignty, economic stability, and human security. By breaking the cycle of fear and violence that paralyzes our agricultural heartland, we can restore livelihoods, stabilize food prices, and foster resilient communities. The RDDS offers an actionable, scalable, and deeply impactful solution to one of the nation's most pressing challenges, paving the way for a future where every Nigerian can be food-secure and our farming communities can thrive without fear.\n\nThe technical details to the full version as well as the lite and full code are given in the link below: \nhttps://github.com/cardiscardis/rural-danger-detection-system",
        "technologies": [],
        "source_code_url": "https://github.com/cardiscardis/rural-danger-detection-system",
        "demo_video_url": "https://youtu.be/yT4XDVRY5wQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/rural-danger-detection-system"
    },
    {
        "project_title": "MemoryLink: An Offline AI Companion for Alzheimer’s Support and Remembrance",
        "tagline": "An offline, privacy-first AI that recalls speaks in the voice of loved ones, and helps Alzheimer’s patients reconnect with their past",
        "authors": "Triamb Talwar",
        "summary": "MemoryLink\nMemoryLink is a small but meaningful step toward building an AI companion for Alzheimer’s patients — one that helps them relive moments, recognize names, and feel emotionally connected again.\n\nIn this hackathon, we built a working prototype that lets users query a fine-tuned language model to recall memories from past messages, captions, and media. It responds like someone who remembers your life, because it's trained to.\nThe focus wasn’t to build everything at once, but to prove it’s possible, and I did.\n\n🧩 The Problem\nAlzheimer’s affects over 55 million people.\nIt’s not just about forgetting, it’s about losing identity, routine, and emotional connection.\n\nMost AI tools today run in the cloud, need high-speed internet, and aren’t designed for memory recall. They're not built for the quiet, offline realities where many patients live.\n\n✅ What We Built (Proof of Work)\nI built a simple, working pipeline to show that memory-aware AI is possible even with limited time and compute.\n\n🧠 Fine-tuned a lightweight LLM on memory-style conversations \n\n🗂️ Created a basic RAG setup using FAISS and sentence embeddings to ground answers in actual messages, captions, and memory notes\n\n💬 Built a Gradio demo where users can ask the model personal questions, and get grounded responses\n\n🔐 Set up a local-first workflow (and experimented with Colab secrets for Hugging Face access)\n\n🧪 Logged sample conversations and failure cases, showing where it succeeds and where more work is needed\n\nAll code + notebooks are public and reproducible. This is not a vision pitch; it's a functioning base layer.\n\n🧱 Architecture Snapshot\nData Ingestion: Loaded sample messages, captions, and memory descriptions\n\nFine-Tuning: Used Unsloth to fine-tune a Gemma-compatible model on personalized Q&A\n\nRAG: Embedded memory snippets with sentence-transformers, retrieved via FAISS\n\nInterface: Gradio frontend for interactive testing\n\nHosting: Working demo in Google Colab\n\n💭 Why This Matters\nEven with this minimal setup, users can ask:\n\n\"Who is in this photo?\"\n\"Did we ever go to Kashmir?\"\n\"What does Mom sound like?\"\n\nAnd get memory-aware responses grounded in real past moments.\n\nThis is a small but powerful foundation on which we hope to build.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/ffLykqXgAkg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/memorylink"
    },
    {
        "project_title": "AgriMitra: Offline AI Companion for Empowering Indian Farmers",
        "tagline": "AgriMitra: Offline Gemma 3n + RAG app giving Indian farmers instant advice via voice/text, with Hindi & English support no internet needed.",
        "authors": "Lokesh Desai",
        "summary": "**AgriMitra — Offline AI for Indian Farmers (Gemma 3n Impact Challenge)**\n\n**Problem We’re Solving**\n\nIndia’s smallholder farmers often work in low- or no-internet areas, which blocks access to timely agronomy advice, pest/disease guidance, and region-specific best practices, especially during critical moments (sowing, irrigation, input selection, harvest). Existing AI tools typically require connectivity, raise privacy concerns, and rarely support bilingual, voice-first use in the field.\n\n**Our Solution (Built on Gemma 3n)**\n\nAgriMitra is an offline-first, privacy-preserving mobile assistant that runs entirely on-device with Gemma 3n. It delivers instant guidance via text and voice, even with zero connectivity. The app is bilingual (Hindi & English) and uses Retrieval-Augmented Generation (RAG) over local PDF to provide grounded, context-aware answers.\n\n**Why Gemma 3n**\n\t•\tOn-device by design: Real-time inference on phones; reliable in the field and privacy-first.\n\t•\tEfficient + compact: Mobile-ready architecture to run on mid-range Android devices.\n\t•\tPrivate & resilient: Data never leaves the device; no cloud fallback required.\n\t•\tMultimodal-ready: MVP focuses on text + voice today, extensible later if needed.\n\n**Key Features:**\n\t•\tOffline Advice (Text & Voice): Farmers speak or type questions; receive instant spoken + on-screen answers.\n\t•\tBilingual UX (Hindi & English): Choose language at first launch; switch anytime.\n\t•\tOn-Device Translation: Google ML Kit translates Hindi↔English fully offline for low latency and privacy.\n\t•\tRAG Over Local Knowledge: Retrieves passages from on-device PDF so responses are verifiable and domain-accurate.\n\t•\tFarmer-Friendly UI: Large tiles, one-tap mic, clear dosage formatting.\n\n**How It Works**\n\t1.\tInput (Voice/Text): User asks a question in Hindi or English; on-device STT converts speech to text.\n\t2.\tLanguage Handling: If Hindi, input is translated to English on-device via ML Kit.\n\t3.\tRAG Pipeline (Local): Local PDF store → on-device embedding index → retrieve top-k relevant passages.\n\t4.\tOn-Device Reasoning (Gemma 3n): Prompt = user query + retrieved snippets → grounded answer generated on-device.\n\t5.\tOutput: If Hindi UI is active, the response is translated back to Hindi and spoken via on-device TTS; text is also shown.\n\nTech Stack: Android (Jetpack Compose), Gemma 3n (on-device), ML Kit Translation, on-device STT/TTS, local PDF store + embeddings, Hilt DI.\n\nImpact & Metrics\n\t•\tLatency: ≤5s answer offline on mid-range devices.\n\t•\tGrounding Rate: ≥80% answers cite the retrieved local PDF.\n\nWhat Makes It Different\n\t•\tTruly offline + private (no cloud fallback), designed for low-connectivity regions.\n\t•\tBilingual and voice-first from day one (Hindi/English, STT/TTS).\n\t•\tLocally grounded RAG using government/university PDF, not generic web text.\n\t•\tExtensible: Future add-ons (e.g., photo-based disease checks, market tips) without sacrificing offline privacy.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/kissanai/agrimitra/tree/main",
        "demo_video_url": "https://youtu.be/vyVI4_9qlus",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/agrimitra-offline-ai-companion-for-empowering-indi"
    },
    {
        "project_title": "Expansion App",
        "tagline": "Expansion is an offline Android application powered by Gemma 3n that can create any web app.",
        "authors": "Azamat Baiganin, Skimi Black",
        "summary": "Expansion is an offline-first Android application powered by Google's Gemma 3n LLM that enables users to generate, run, and manage fully functional web applications directly on their mobile device — without requiring an internet connection.\n\nDesigned as a creative and developer toolkit for the edge, Expansion combines on-device large language model inference with intuitive UI flows, allowing users to describe an idea in natural language and transform it into a working web application. The core focus of Expansion is privacy, accessibility, and decentralization: all computation, code generation, and rendering happens locally, giving users full control over their tools.\n\n🔧 Core Features:\nOffline LLM-Powered App Generation\nUsers can describe any app idea — e.g., \"Build a budgeting app with charts and export to PDF\" — and Expansion will generate the complete front-end code using Gemma 3n running locally via MediaPipe’s LLM Inference API.\n\nLive Web App Preview (Embedded Engine)\nUsers can immediately preview and interact with the generated app using an embedded lightweight webview or runtime engine.\n\nIterative Refinement\nUsers can continue conversing with the model to update, refactor, or expand features of the generated application in real-time — no cloud APIs needed.\n\nTemplate & Component Library\nExpansion includes a curated offline set of reusable UI components, app templates (e.g. task managers, chatbots, surveys), and utilities that can be extended via LLM prompts.\n\nCode Export & Sharing\nGenerated apps can be exported as standalone HTML/JS/CSS files or packaged as PWA-ready bundles. Sharing is done via local means (Bluetooth, AirDrop-like sharing, or SD card).\n\nMulti-Agent Mode (Coming Soon)\nOptional collaborative mode where users can summon multiple on-device agents (e.g., a UI designer, tester, or accessibility reviewer) to improve the generated apps.\n\n🧠 Tech Stack:\nKotlin – Android-native development for performance and deep system integration\n\nMediaPipe LLM Inference API – Efficient on-device LLM execution\n\nGemma 3n (E4B lite-rt) – Compact, instruction-tuned model from Google\n\nWebView Runtime – For live rendering and interactivity of generated code\n\nOptional GGUF support – For experiments using LLaMA.cpp-compatible formats\n\n📦 Use Cases:\nCreate mini-apps for ecological awareness (e.g., “Plastic scanner” or “Carbon diary”)\n\nBuild offline tools for social impact (e.g., “Legal Rights Chatbot”, “Mental Health Assistant”)\n\nLearn to code by modifying LLM-generated applications\n\nRapid prototype and test UI/UX ideas without internet or desktop tools\n\n🛰️ Why Expansion?\nExpansion brings back the power of computing into the hands of users. In a world dominated by cloud-based AI platforms, this project demonstrates the feasibility and creativity unlocked by compact on-device intelligence. It’s ideal for underserved regions, educational contexts, low-connectivity environments, and privacy-conscious developers.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/baiganinnaza/Expansion-App",
        "demo_video_url": "https://www.youtube.com/watch?v=zZXFr64nHcI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/expansion-app"
    },
    {
        "project_title": "Project Noor",
        "tagline": "Noor: Offline AI education app with Gemma 3 integration, providing real-time tutoring, courses & OCR scanning for low-connectivity areas.",
        "authors": "Rappid, Talha Akhoon",
        "summary": "Noor: AI-Powered Offline Education Platform\nProject Overview\nNoor is a revolutionary educational platform that addresses one of the most pressing challenges in global education: providing quality, personalized learning experiences to students in low-connectivity areas. By integrating Google's Gemma 3 language model directly into a Flutter mobile application, Noor delivers cutting-edge AI tutoring capabilities that work completely offline, ensuring that geographic location and internet infrastructure no longer determine access to quality education.\n\nThe Problem\nOver 3.7 billion people worldwide live in areas with limited or unreliable internet connectivity. Students in these regions face significant educational disadvantages:\n\nLimited access to quality educational resources that require constant internet connectivity\nNo personalized tutoring due to lack of qualified teachers and online platforms\nInability to digitize handwritten notes or textbook content for better organization\nLanguage barriers when accessing global educational content\nEconomic constraints that prevent access to expensive online learning platforms\nThe Solution\nNoor transforms education accessibility through three core innovations:\n\n1. On-Device AI Tutoring\nReal-time conversations with Google's Gemma 3 language model running entirely on the mobile device\nPersonalized responses tailored to individual learning levels and subjects\nMulti-subject support including mathematics, science, languages, and general knowledge\nSub-second response times enabling natural, flowing conversations\nComplete privacy as no data leaves the device\n2. Interactive Course Generation\nAI-powered flashcard creation for structured learning across multiple subjects\nAdaptive difficulty levels (beginner, intermediate, advanced) based on student progress\nComprehensive subject coverage including Mathematics, Science, English, and Health Education\nProgress tracking and achievement systems to motivate continued learning\nOffline course content that works without any internet connection\n3. Intelligent Document Processing\nOCR (Optical Character Recognition) powered by Google ML Kit for text extraction from images\nHandwriting recognition to digitize student notes and assignments\nTextbook scanning to convert physical books into searchable digital content\nAI-enhanced processing that can answer questions about scanned content\nMulti-language support for local languages and scripts\nTechnical Innovation\nArchitecture\nFlutter Framework for cross-platform mobile development ensuring broad device compatibility\nNative Android Integration using JNI (Java Native Interface) for optimal performance\nGemma 3n Model - Google's lightweight but powerful language model optimized for mobile devices\nTensorFlow Lite for efficient on-device machine learning inference\nSQLite Database for offline data storage and course management\nPerformance Optimizations\nMemory management techniques to run AI models on resource-constrained devices\nModel quantization to reduce file size while maintaining accuracy\nStreaming responses for real-time chat experiences\nBackground processing for seamless user interactions\nBattery optimization to ensure extended usage without frequent charging\nKey Technical Achievements\nOn-device inference with response times under 1 second\nModel size optimization - Full AI capabilities in under 2GB\nCross-platform compatibility supporting Android devices with 3GB+ RAM\nOffline-first architecture with optional cloud sync when connectivity is available\nScalable codebase designed for easy feature expansion and maintenance\nImpact and Potential\nEducational Equity\nDemocratizes access to AI-powered education regardless of internet infrastructure\nReduces educational inequality between urban and rural students\nEnables self-paced learning without dependency on teacher availability\nSupports multiple learning styles through varied content formats\nGlobal Reach\nScalable to millions of students with minimal infrastructure requirements\nLanguage adaptability for localization to different regions and cultures\nCost-effective deployment requiring only one-time app installation\nSustainable solution that doesn't require ongoing internet costs\nInnovation Significance\nProves viability of running sophisticated AI models on mobile devices\nDemonstrates potential for offline-first AI applications in developing regions\nEstablishes framework for future educational technology development\nShows practical application of Google's Gemma model in real-world scenarios\nFuture Development\nPlanned Enhancements\nVoice interaction for hands-free learning experiences\nAugmented reality features for immersive educational content\nPeer-to-peer learning networks for collaborative offline education\nTeacher dashboard for monitoring student progress in classroom settings\nContent marketplace for community-generated educational materials\nExpansion Opportunities\nProfessional training modules for adult education and skill development\nHealthcare education for medical training in remote areas\nAgricultural education for farming communities\nEntrepreneurship training for economic development initiatives\n\nConclusion\nNoor represents a paradigm shift in educational technology, proving that cutting-edge AI capabilities can be made accessible to students regardless of their connectivity constraints. By leveraging Google's Gemma 3 model in an innovative offline-first architecture, this project demonstrates how modern AI can be democratized to serve the world's most underserved educational communities. The platform not only addresses immediate educational needs but also establishes a foundation for future innovations in accessible, AI-powered learning technologies.\n\nThis project showcases the potential for technology to bridge educational gaps and create opportunities for millions of students who have been left behind by traditional online learning platforms. Noor is more than an educational app—it's a step toward global educational equity.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Rappid-exe/ProjectNoor",
        "demo_video_url": "https://www.youtube.com/watch?v=0tPrlOm2VzU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/project-noor"
    },
    {
        "project_title": "Bear Grylls :The AI Survival Guide with Gemma 3n",
        "tagline": "An on-device Android app that uses Retrieval-Augmented Generation (RAG) to enhance a local Gemma 3n model with a custom survival knowledge b",
        "authors": "Vaishnavi more",
        "summary": "The AI Survival Guide is a fully functional, on-device Android application built to solve a critical problem: access to expert knowledge in disconnected environments. The project leverages Google's powerful Gemma 3n generative model, running locally on-device, to provide users with life-saving survival information. The core innovation of this project is the implementation of a sophisticated Retrieval-Augmented Generation (RAG) pipeline. This system enhances Gemma's general knowledge by grounding its responses in a custom, fact-checked knowledge base of survival tips. This write-up provides a detailed walkthrough of the project's architecture, the technical decisions made, the challenges overcome, and the specific role Gemma 3n plays in creating a truly intelligent on-device assistant.\n\n#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Vaishnavimore10/BearGrylls_edge_gallery_Gemma3n",
        "demo_video_url": "https://youtu.be/4cHs6Z9PNhw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/bear-grylls-the-ai-survival-guide-with-gemma-3n"
    },
    {
        "project_title": "Gemma-3N For Healthcare",
        "tagline": "Harnessing Gemma-3N to expand reliable, dependable healthcare advice and service to all corners of the globe.",
        "authors": "Aaron Shen",
        "summary": "Building an AI Assistant for Medical First Response Using Gemma-3N",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/ashen-321/kaggle-gemma-3n-challenge",
        "demo_video_url": "https://youtu.be/CCTay7FpP3A",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-3n-for-healthcare"
    },
    {
        "project_title": "FarmWise - Pocket-sized Al Agronomist",
        "tagline": "Providing farmers instant crop advice, image-based pest diagnosis without needing internet or high-carbon cloud infrastructure. ",
        "authors": "SudarsonRoy, Yash Chaudhary, Shreya Chakraborty, ABHIK GANGULY, anichakra",
        "summary": "More than 600 million smallholder farmers work in low-infrastructure regions where poor connectivity blocks timely agronomic advice, driving >$220 billion in annual crop losses and slowing progress on UN SDG 2 (Zero Hunger) and SDG 13 (Climate Action). Farm Wise closes this gap by embedding Google’s lightweight Gemma language model directly on farmers’ phones, creating an offline-first AI agronomist that:\n- Runs fully on-device to deliver sub-second answers with no cloud latency, data fees, or high-carbon server use.\n- Combines three live modules—Farm Advisor, Plant Doctor, Education Q&A—for crop planning, image-based pest diagnosis, and regional-language tutorials.\n- Performs periodic, low-bandwidth syncs to pull fresh weather and market snippets; all core reasoning happens locally, so guidance continues even when the signal vanishes.\n\nBy bringing expert knowledge to the last mile while slashing network energy use, Farm Wise helps farmers boost yields sustainably and advances the world toward food security without adding to the planet’s carbon bill.",
        "technologies": [],
        "source_code_url": "https://github.com/yashchaudhary28/FarmWise",
        "demo_video_url": "https://www.youtube.com/watch?v=8n9OlZesWkI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/farmwise-pocket-sized-ai-agronomist"
    },
    {
        "project_title": "Eliza: Universal Learning Platform Powered by Gemma3N",
        "tagline": "ELIZA is an AI-powered universal learning platform that leverages Gemma 3n to deliver personalized, accessible education globally, overcomin",
        "authors": "Ersjan , Altin Azizi",
        "summary": "The promise of education often clashes with the realities of the classroom. Students who fall behind struggle to catch up, while advanced learners remain unchallenged. Beyond the classroom walls, countless individuals face barriers like the intimidation of asking questions, the prohibitive cost of textbooks and tutors, or the sheer lack of access to educational materials in remote areas. These pervasive challenges highlight a critical need for a transformative approach to learning.\n\nEliza is a groundbreaking AI-powered Universal Learning Platform designed to break down the traditional barriers of geography, language, and socio-economic status, making quality education accessible to anyone, anywhere. It directly addresses critical shortcomings in current educational systems, such as the widespread lack of personalized learning, insufficient and delayed feedback, and the overwhelming administrative burden on teachers.\n\nLeveraging advanced AI, Eliza provides tailored learning experiences by analyzing individual student performance, strengths, and weaknesses. It delivers customized content, adjusts difficulty levels, and offers targeted, real-time feedback, ensuring every student learns at their optimal pace. This means a student can now receive immediate, personalized support to unleash their full potential.\n\nThe platform also significantly reduces teacher workload through AI-driven automation of tasks like lesson planning, grading, scheduling, and data analysis. This frees educators to focus on direct instruction and meaningful student interaction.\n\nA core innovation is Eliza's deep localization capabilities. Beyond mere translation, AI adapts educational content, including examples and scenarios, to resonate with local cultures and values, fostering natural engagement. The platform will house an immense, constantly evolving repository of digital content, aggregating both local and international books, while AI automates the creation of dynamic, interactive learning materials like quizzes and simulations. Hosted on robust cloud services for scalability, Eliza aims to democratize access to high-quality, engaging, and personalized education, transforming learning outcomes.\n\nFor technical summary of the project please visit the Technical Write-up.pdf and the GitHub repositories.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": null,
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/eliza-universal-learning-platform-powered-by-gemma"
    },
    {
        "project_title": "WhiZED - Socratic Tutor for School Students",
        "tagline": "Socratic Tutor for School Students teaches in their own regional language (in Tamil)",
        "authors": "Aravind Kumar Rajendran",
        "summary": "Project Overview\n\nWhiZED: A Tamil Language Tutoring App Powered by AI for School Students\n\nIt is an AI-driven tutoring application designed specifically for primary school students in Tamil Nadu. The app empowers students to:\n- Think critically\n- Deeply understand concepts\n- Learn independently\n\nacross subjects such as Tamil, English, Mathematics, Science, and Social Science, all in their native language (Tamil).\n\nStudents can explore content from our curated Tamil Nadu textbook library, or choose to upload their own text files or PDFs, inspired by Google’s NotebookLM concept.\n\n\n\nKey Features\n\n#",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": "https://github.com/Aravindkumar-Rajendran/whized",
        "demo_video_url": "https://youtu.be/m7eTWgPOaXc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/whized-socratic-tutor-for-school-students"
    },
    {
        "project_title": "EyeGuardian: A Privacy-First, On-Device AI System for Safety",
        "tagline": "Bridging the distance between loved ones with an AI that listens for distress and quietly stands watch, entirely offline.",
        "authors": "fares mallouli",
        "summary": "EyeGuardian: On-Device, Multimodal Safety with Gemma 3n\n\nEyeGuardian is an on-device, multimodal safety system designed to provide a proactive safety net for vulnerable individuals. It leverages the unique capabilities of Google's Gemma 3n to address a significant real-world challenge: ensuring safety for those living alone. By running entirely offline on affordable hardware, EyeGuardian, powered by the Gemma 3n model, allows anyone, anywhere from Tokyo to Paris to privately set custom safety instructions and receive emergency alerts in their native language.\n\nThis document details our architecture, our specific use of Gemma 3n, the critical challenges we overcame, and the technical choices that make our solution both impactful and realistic.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/FaresMallouli/Gemma3nEyeGuardian",
        "demo_video_url": "https://www.youtube.com/watch?v=tg43-GG_DaI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/eyeguardian-a-privacy-first-on-device-ai-system-fo"
    },
    {
        "project_title": "ToDocToDoc AI (토닥토닥AI)",
        "tagline": "Like a caring family member in your pocket - Completing the circle of care so elderly patients never feel alone or confused again",
        "authors": "Kyung-Hoon Kim",
        "summary": "#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/OKTDL648pQg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/todoctodoc-ai-ai"
    },
    {
        "project_title": "Klypt: Offline-First Education for All",
        "tagline": "Empowering students and educators with a fully offline, gamified learning experience — powered by Gemma 3n",
        "authors": "a7m-1st, Bimo Kuncoro",
        "summary": "**Klypt: Offline-First Education for All**",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/a7m-1st/klypt",
        "demo_video_url": "https://youtu.be/7vXAQE0C1gU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/klypt-offline-first-education-for-all"
    },
    {
        "project_title": "TerraScribe: Voices of the Forest",
        "tagline": "Offline AI Guardian that hears danger, detects threats like chainsaws or earthquakes, and gives instant local emergency help.",
        "authors": "a!u",
        "summary": "TerraScribe :  Offline AI Guardian for Remote Threat Detection\nIn remote and vulnerable areas, where internet access is limited and environmental threats are frequent, fast response can mean the difference between life and disaster. \nTerraScribe is a groundbreaking offline AI application designed to listen, understand, and respond to environmental threats in real time — even in the most remote regions on Earth. Powered by Gemma 3n, this lightweight yet powerful system analyzes ambient audio to detect threats like illegal logging, chainsaw activity, floods, explosions, earthquakes, or fires — with no internet required.\n\nOnce a threat is detected, Terrascribe instantly activates custom emergency protocols, such as alerting community patrols, suggesting evacuation plans, or notifying local forest authorities — all running locally and securely, preserving privacy and independence from the cloud.\n\nBuilt with indigenous communities, rural villages, and disaster-prone zones in mind, Terrascribe combines ethical AI, edge computing, and real-world impact — making it a vital tool for sustainability, safety, and sovereignty.\n\nThis is not just an app — it’s a mission-critical AI shield for Earth’s most vulnerable environments.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/dSaFVEI5uNo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/terrascribe-voices-of-the-forest"
    },
    {
        "project_title": "Easy Connect ",
        "tagline": "An offline AI companion that uses on-device Gemma as a personal curriculum designer to create learning paths for new immigrants.",
        "authors": "Ismail Mohammad",
        "summary": "**The Story & The Problem**\n\nThis project was born from direct personal experience. After my family migrated to the UK from Afghanistan in 2021, I witnessed the profound challenges they faced. Beyond the language barrier, the biggest struggle was the loss of confidence and independence in navigating daily life. Easy Connect is the tool I built to solve this, a 100% private, on-device AI companion that works without an internet connection, designed to empower new immigrants from the moment they arrive.\n\n**Hybrid AI Architecture: The \"Best Tool for the Job\" Strategy**\n\nThrough extensive testing, I determined that a hybrid AI system would provide the highest quality user experience. I strategically \"mix and match\" the best tool for each job, combining the creative and reasoning power of Gemma with the specialized accuracy of ML Kit.\n\n**1. Gemma 3n: The Generative & Reasoning Core**\n\nGemma is the intelligent heart of the application, powering the features that require true understanding, personalization, and creativity.\n\n*   **The \"AI Curriculum Designer\":** The app's star feature is the \"Learning English\" path. I engineered a sophisticated agent that takes a user's unique profile (age, profession, English level) and a natural language goal. It then uses the on-device Gemma model to dynamically generate a personalized, multi-stage learning roadmap.\n\n*   **\"Just-in-Time\" Content Generation:** To work within the constraints of on-device memory, a second agent calls Gemma to generate detailed lesson content (vocabulary, grammar, quizzes) \"just-in-time,\" only when a user is ready to study that specific topic.\n\n*   **Retrieval-Augmented Generation (RAG) for UK Info:** To provide safe and accurate answers about UK life, I implemented a RAG system. The app first retrieves relevant, pre-translated information from a secure, offline `knowledgebase.json`. This trusted text is then passed to the Gemma model along with the user's question. Gemma's role is to **reason over this provided context** and synthesize a clear, conversational answer in the user's native language, preventing hallucinations and ensuring accuracy.\n\n*   **AI-Powered Summarization:** In the Visual Translator, after ML Kit extracts the text from a document, Gemma is used to generate a concise, 3-4 line summary, allowing the user to understand the main point of a long text in seconds.\n\n**2. Google's ML Kit: The High-Accuracy Utility Engine**\n\nML Kit is used for specialized tasks where speed and fidelity are critical.\n\n*   **Offline Translation & OCR:** I use **ML Kit Vision** for fast, offline OCR and **ML Kit Translate** for reliable, line-by-line translation. I chose ML Kit for this role because my testing showed it provided significantly higher-quality and more consistent Dari translations than the on-device Gemma model, which was essential for user trust.\n\n**[Real, Functional, and Well-Engineered Technology]**\n\nThe application is fully functional and built on a robust, professional architecture.\n\n*   **Model Used:** The app uses the `gemma3-1B-it-int4.task`, a 4-bit quantized version of Gemma 3n 1B, sourced from the `litert-community` on Hugging Face.\n*   **Native Module Bridge:** All on-device models are managed through a custom, multi-module native bridge I built from scratch in Kotlin (`GemmaModule`, `MlKitModule`, `VoiceModule`).\n*   **On-Demand Memory Management:** The app is engineered for stability. The large Gemma model is loaded into RAM **only** when a feature requires it and is unloaded when the user navigates away, managed by a centralized, route-aware component.\n*   **Offline Model Handling & Future Improvements:**\n    *   The Gemma model is fully bundled for immediate offline use.\n    *   **For this MVP, the ML Kit Translate and on-device Speech Recognition models use a \"first-use download\" strategy, requiring a one-time internet connection to download the necessary language packs. After this, the entire application is 100% functional offline. A top priority for a future release is to bundle these models directly into the app package for a truly offline experience from the first second.**\n*   **Accessibility Note (Text-to-Speech):**\n    *   The Text-to-Speech (TTS) feature is a core part of the app. For the best experience, users may need to manually install the free **Persian (Farsi/Dari) voice pack** from their phone's native Android Text-to-Speech settings, as it is not pre-installed on all devices.\n*   **Resilient AI Interaction:** I developed a robust \"agent\" pattern. These agents are responsible for all AI interaction, including advanced prompt engineering and parsing the model's output to ensure the data displayed to the user is always clean and well-structured.\n\nEvery feature shown in the demo—from dynamic roadmap generation to RAG-powered Q&A and visual translation—is real, fully implemented, and runs on the device.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/ismailkm/easy-connect",
        "demo_video_url": "https://youtu.be/RlbxxgEa6vc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/easy-connect"
    },
    {
        "project_title": "GemmaVetCare",
        "tagline": "Bangla Offline AI for Cattle Disease Diagnosis and Context-Aware Feeding Guidance",
        "authors": "Hasnain Adil, Md Irtiaz Kabir, Sadatul islam Sadi, imkabir0, MOHAMMAD NINAD MAHMUD",
        "summary": "GemmaVetCare is an AI‑powered livestock management platform designed to address critical challenges in rural Bangladesh’s cattle farming sector, including limited veterinary coverage, delayed disease detection, and inadequate feeding guidance. The system integrates multilingual support (Bangla and English), offline functionality, and AI‑driven multimodal disease diagnosis with context‑aware feeding recommendations.\n\nThe disease detection module leverages a LoRA‑fine‑tuned Gemma‑3N model trained on annotated cattle images and Bangla symptom descriptions, enabling accurate identification of common and economically significant diseases such as Foot‑and‑Mouth Disease, Lumpy Skin Disease, and Pinkeye. The feeding recommendation module employs Random Forest and CatBoost regression models trained on standardized nutritional datasets, coupled with a Gemma 3n–driven feed formulation system that generates stage‑specific and resource‑adaptive rations.\n\nTo ensure accessibility for farmers with limited internet connectivity, the platform includes offline inference capabilities for both disease diagnosis and feeding guidance. Its multilingual interface ensures usability across diverse literacy levels, providing clear, actionable insights in Bangla and English.\n\nBy combining computer vision, natural language processing, and machine learning with a farmer‑centric design, GemmaVetCare delivers scalable, inclusive, and data‑driven livestock care, improving animal health, productivity, and rural livelihoods.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=EoxyudCIVSo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemmavetcare"
    },
    {
        "project_title": "\"GUARDIAN Emergency Response AI Powered by Gemma 3n\"",
        "tagline": "\"Leveraging Gemma 3n's Multimodal Offline Capabilities to Provide Life-Saving Emergency Guidance in Crisis Situations\"",
        "authors": "abobakrosman87",
        "summary": "GUARDIAN is an offline-first, multimodal AI assistant powered by Gemma 3n that provides critical emergency response capabilities when traditional communication infrastructure fails.\nIn crisis situations—natural disasters, medical emergencies, or remote area incidents—communication failures are often front and center in preventable deaths. When cell towers are down, internet is unavailable, and professional help is hours away, GUARDIAN becomes a lifeline that works entirely on-device.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": " https://youtu.be/OKZBdKxtR5g",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/guardian-emergency-response-ai-powered-by-gemma-3n"
    },
    {
        "project_title": "Gemma Yaatri",
        "tagline": "Offline AI-powered Travel Companion for Exploring Amazing Places - Powered by Gemma 3n",
        "authors": "Pradeep Ramtel, Shikshi, Ubgn",
        "summary": "Powered by Gemma-3n\n\nFunctions:\n1. Provides information about the location\n2. Provides image-to-text (Language translation) conversion\n3. Identifies plants and insects and provides valuable information\n4. Answers travel and location-based general queries\n\nPlease refer to the documentation PDF for the overview of the model.",
        "technologies": [],
        "source_code_url": "https://github.com/rpradeap/Gemma-Yaatri-Fine-Tunning",
        "demo_video_url": "https://www.youtube.com/watch?v=2hKZvkdEqZc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemmayaatrikaagle"
    },
    {
        "project_title": "QuietRoom ",
        "tagline": "Your thoughts, your device. Period. A privacy-first personal journaling companion with on-device AI processing. ",
        "authors": "Francesco Laiti",
        "summary": "As users increasingly turn to large language models for emotional support, the privacy of their most sensitive data is placed at significant risk. This project introduces QuietRoom, a direct response to this challenge: a privacy-first, multimodal journaling application designed to provide a secure, on-device sanctuary for mental well-being. Leveraging the unique and strong capabilities of Google’s Gemma 3n on-device models, QuietRoom offers users structured reflection and proactive analysis without their personal data ever leaving their control. The code and a live demo are publicly available at [huggingface.co/spaces/laitifranz/QuietRoom](https://huggingface.co/spaces/laitifranz/QuietRoom)",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=j7X0wq6uEqg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/quietroom"
    },
    {
        "project_title": "GemFit: Personal AI Fitness Assistant",
        "tagline": "A fully offline, privacy-first fitness and diet planner powered by Gemma 3n and multimodal AI.",
        "authors": "M P V S GOPINADH",
        "summary": "GemFit is a privacy-first AI fitness and diet planner that runs entirely offline using Gemma 3n via Ollama. Users simply upload a body image and provide a few basic inputs like age, weight, and goal. The system instantly generates personalized workout and diet plans using Gemma 3n’s multimodal and mix’n’match capabilities. With no subscriptions, no data tracking, and no cloud dependence, GemFit puts real-time, secure, and personalized fitness coaching in every user’s hands. Built with Streamlit, Ollama, and Gemma 3n, this app showcases how AI can enhance personal health while respecting privacy.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/MaddulaPavan/GemFit",
        "demo_video_url": "https://youtu.be/gqbPF2WFzM4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemfit-personal-ai-fitness-assistant"
    },
    {
        "project_title": "Health Vault",
        "tagline": " Empowering patients with AI-driven healthcare record management",
        "authors": "charizard",
        "summary": "Medical Records Digitizer\n\n**Powered by Gemini 3n** - A comprehensive solution for digitizing physical medical records and analyzing prescriptions with AI.",
        "technologies": [
            "Flask",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/linux-devil/HealthVault",
        "demo_video_url": "https://youtu.be/E4f3PiHeQFY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/health-vault"
    },
    {
        "project_title": "Nidaan AI ",
        "tagline": "Nidaan AI is a multilingual AI companion  that detects early symptoms and guides rural people in their own language before it’s too late.",
        "authors": "Nithun Sundarrajan, rudra2006",
        "summary": "",
        "technologies": [
            "CUDA",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Rudra1729/Nidaan-AI.git",
        "demo_video_url": "https://youtu.be/VDGO5v3DKh4?si=H6VpJc5xdOhsi5bH",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/nidaan-ai"
    },
    {
        "project_title": "Hypnos: Your Personal Sleep Agent",
        "tagline": "A private, offline agent that stays in your pocket and helps you get better rest",
        "authors": "dmitrykaz, Maria Chepurina",
        "summary": "A private, offline agent that stays in your pocket and helps you get better rest",
        "technologies": [],
        "source_code_url": "https://github.com/dmitrykazhdan/HYPNOS",
        "demo_video_url": "https://www.youtube.com/watch?v=zH6VZxvV6As",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/hypnos-your-personal-sleep-agent"
    },
    {
        "project_title": "Digital Care for Elderly ",
        "tagline": "Helping elderly users navigate smartphones confidently through patient AI guidance and simplified visual interfaces for daily communication",
        "authors": "Gagandeep Singh",
        "summary": "This project addresses the growing digital divide among elderly users who struggle with modern smartphone interfaces. The AI-powered assistant provides real-time, voice-guided support specifically designed for seniors, featuring large buttons, high contrast displays, and patient step-by-step\ninstructions.\n\nThe system captures screenshots of the user's current screen and uses computer vision to understand what they're seeing, providing contextually relevant guidance like \"I can see your contacts screen - tap the green call button next to Mom's name.\" The interface mimics familiar phone designs while simplifying complex operations into  single, clear actions.\n\nKey features include emergency contact integration, voice-to-text messaging assistance, and adaptive learning that remembers user preferences. By combining empathetic AI responses with elderly-friendly UI design, this solution empowers seniors to stay connected with family, access essential services, and maintain independence in our increasingly digital world.",
        "technologies": [],
        "source_code_url": "https://github.com/gaganmanku96/ElderlyCare",
        "demo_video_url": "https://youtu.be/POx6onh6keM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/digital-care-for-elderly"
    },
    {
        "project_title": "Studyman AI Planner",
        "tagline": "Studyman is an AI based study planning app that uses Gemma for creating calendar invites for study.",
        "authors": "Ashton Joe",
        "summary": "Studyman App is built on NextJS and uses Ollama for LLM calls to Gemma.\n\nIt is hosted on a GCP VM with only CPU's, thus leveraging oan on-device idea of running LLM's locally on devices without GPUs.",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": "https://github.com/VincentPaulV/studywithai",
        "demo_video_url": "https://youtu.be/FzyxSpl_11I?si=TNuPmO5NgSJISEwD",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/studyman-ai-planner"
    },
    {
        "project_title": "CRISIS-AI: Offline Voice Assistant for Emergency Response Using Gemma 3n",
        "tagline": "Life-Saving AI That Works When Everything Else Fails - 100% Offline Emergency Response with Voice-First Interface.",
        "authors": "innovatoryuvarajan",
        "summary": "🚨 The Problem:\nWhen disaster strikes, communication infrastructure fails within the first hour. Traditional emergency apps become useless precisely when they're needed most. With 2.3 billion people affected by natural disasters annually, there's a critical gap in emergency response technology.\n\n💡 Our Solution:\nCRISIS-AI is a revolutionary 100% offline voice assistant designed specifically for emergency response during natural disasters. Powered by Gemma 3 via Ollama, our system provides critical life-saving information through voice interaction when traditional infrastructure fails.\n\n🎯 Key Features:\n🔇 100% Offline Operation - No internet, cellular, or cloud dependency\n🎤 Voice-First Interface - Works in darkness, with injured hands, or when screens are impractical\n⚡ Sub-500ms Response Time - Instant emergency guidance when seconds count\n🌍 Multi-Language Support - Serves diverse global communities\n📡 Smart Emergency Detection - Automatic SOS beacon activation via BLE\n\n🏗️ Technical Architecture:\nOur innovative 3-tier response system ensures both speed and comprehensiveness:\n\nFAQ Path: Instant responses for common emergencies\nRAG Pipeline: Context-aware answers using FAISS vector search over curated disaster documents\nSOS Detection: Parallel emergency signal processing with beacon activation\n\nTech Stack:\nAI Engine: Gemma 3 via Ollama for local processing\nVoice Processing: Vosk (STT) + pyttsx3 (TTS) for complete voice interaction\nKnowledge Retrieval: FAISS + SentenceTransformers for semantic search\nDocument Processing: PyMuPDF extraction from WHO, FEMA, Red Cross guidelines\nEmergency Detection: Custom keyword analysis with urgency scoring\n\n📊 Performance Metrics:\nResponse Time: <500ms end-to-end\nEmergency Detection: 96% accuracy\nSystem Coverage: 45+ emergency types\nUser Rating: 4.7/5 audio clarity\nStress Test: 99.2% uptime over 48 hours\n\n🌟 Innovation Highlights:\nFirst offline voice assistant specifically designed for disaster response\nCrisis-optimized prompt engineering for audio-friendly emergency guidance\nMulti-modal response system combining voice + BLE beacon activation\nHierarchical knowledge architecture balancing speed and comprehensiveness\n\n🎯 Real-World Impact:\nTarget: 2.3+ billion people in disaster-prone regions\nResponse Improvement: Estimated 40% reduction in emergency response time\nAvailability: 24/7 emergency guidance regardless of infrastructure status\nDeployment: Emergency kits, community centers, high-risk areas\n\n🔬 Technical Challenges Solved:\nLimited Context Processing: Hierarchical retrieval with chunk-based injection\nPanic Voice Recognition: Multi-layer keyword matching with urgency scoring\nUnstructured Knowledge: Custom PDF parsing with semantic chunking\n\nThis project demonstrates the transformative power of local AI deployment in critical infrastructure applications, showcasing how Gemma 3 can literally save lives when traditional technology fails.\nReady to explore the code and see how offline AI can make emergency response more accessible and reliable for everyone?\n\nLinks:\n\n🔗 GitHub Repository : https://github.com/innovatoryuvarajan/gemma-crisis-Ai-response.git\n🎥 Demo Video : https://youtu.be/39agkU9aRiM?feature=shared",
        "technologies": [
            "Ollama",
            "Vosk"
        ],
        "source_code_url": "https://github.com/innovatoryuvarajan/gemma-crisis-Ai-response.git",
        "demo_video_url": "https://youtu.be/39agkU9aRiM?feature=shared",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/crisis-ai-offline-voice-assistant-for-emergency-re"
    },
    {
        "project_title": "NSU CardioPrep AI",
        "tagline": "AI exam preparator for the MRCP examinee",
        "authors": "Mohammed Arif Mainuddin 2211577042, Najifa Tabassum2211 ",
        "summary": "CardioPrep-AI is an AI-powered exam preparation tool tailored for **MRCP (Membership of the Royal Colleges of Physicians) **candidates, with a focus on cardiology. The platform dynamically generates clinically relevant multiple-choice and descriptive questions using **Gemma3n:e2b**, adapts question difficulty based on performance, and offers voice-based interaction for an immersive learning experience.\n\nBuilt as a** progressive web app (PWA)**, it supports offline usage with an downloadable Python backend that runs locally with **Ollama** + ** Gemma3n:e2b**. The system also provides automated feedback and performance analytics via charts, helping students identify weak areas and improve over time.\n\nCardioPrep-AI aims to make high-quality exam preparation accessible, interactive, and intelligent—especially in low-resource or offline environments.",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": "https://github.com/TimeWithPotato/CardioPrep-AI",
        "demo_video_url": "https://youtu.be/BecfTJS2Cl8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/nsu-cardioprep-ai"
    },
    {
        "project_title": "AI-Rescuer",
        "tagline": "AI-Rescuer. Artificial intelligence that could save a life.",
        "authors": "AndreGasa",
        "summary": "**AI-Rescuer** is an on-device, multimodal, privacy-first emergency assistant, built with Gemma 3n, designed to guide people through first aid and crisis response even without an internet connection.\nWhen seconds matter — and professional help isn't yet available — AI-Rescuer can see, hear, and respond to real-world emergencies using only your mobile device. From CPR guidance to trauma support, AI-Rescuer is like a first responder in your pocket — always ready, even offline.\n\n**Why AI-Rescuer?**\nIn real-life crises, people often:\n•\tPanic and don’t know what to do.\n•\tLack internet access or cellular service.\n•\tFace language or literacy barriers.\n•\tAre alone and need instant, context-aware help.\nAI-Rescuer solves this by combining:\n•\tOffline voice, image and video input.\n•\tReal-time multimodal analysis using Gemma 3n.\n•\tStep-by-step emergency instructions (visual, spoken, written).\n•\tGeo-based offline guidance (e.g. evacuation or shelter paths).\n•\tMultilingual support (10+ languages).\n\n**Key Use Cases**\n•\tMedical first aid: CPR, burns, cuts, trauma, choking, bleeding, seizures.\n•\tMental health: panic attacks, grounding exercises in transit (e.g. subway).\n•\tNatural disasters: floods, fires, hurricanes, earthquakes.\n•\tViolence or conflict: shootings, explosions, protests.\n•\tRural or remote areas: zero network zones, offline-only environments.\n•\tAssistive accessibility: support for non-verbal or hearing-impaired users.\n\n**Privacy & Accessibility**\n•\tAll data stays on the device — no cloud dependency, ensuring maximum user confidentiality.\n•\tDesigned for offline-first, low-latency, and low-resource environments.\n•\tFully functional in emergency zones, underground, airplanes, or remote villages.\n•\tSupports multilingual instructions, visuals, and speech for inclusivity.\n\n**Impact & Innovation**\nAI-Rescuer combines Gemma 3n’s multimodal, offline, and efficient capabilities to deliver a product that:\n•\tDirectly saves lives.\n•\tFunctions where cloud-based tools fail.\n•\tIs adaptable, private, and globally scalable.\n•\tShowcases the future of on-device AI for health, safety, and humanity.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/q4h29I2tIXM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-rescuer"
    },
    {
        "project_title": "QuantaGrid: Exploring Adaptive AI Mesh Networks",
        "tagline": "Educational prototype exploring adaptive AI concepts using Gemma 3n MatFormer architecture for emergency response scenarios",
        "authors": "Sachin Kalaiselvan",
        "summary": "⚡ QuantaGrid: Adaptive AI Mesh Network Prototype",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Sachin-kalaiselvan/quantagrid",
        "demo_video_url": "https://youtu.be/CEaVeeA-LjM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/quantagrid-exploring-adaptive-ai-mesh-networks"
    },
    {
        "project_title": "Cxmpute 3D Lab [C3D] - Fully Local Text-to-Cad Editor",
        "tagline": "Turn text into 3D objects with C3Dv0, a gemma3n finetune for CAD generation.",
        "authors": "Joshua Okolo",
        "summary": "C3D: text-to-cad powered by code-CAD and local fine-tuned model C3Dv0",
        "technologies": [
            "FastAPI",
            "Ollama"
        ],
        "source_code_url": "https://github.com/unxversal/c3d",
        "demo_video_url": "https://youtu.be/_D0T65SpyB0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/cxmpute-3d-lab-c3d-fully-local-text-to-cad-editor"
    },
    {
        "project_title": "History Study Buddy",
        "tagline": "Transform NCERT History into Immersive Roleplaying Adventures with Offline AI",
        "authors": "Ajinkya Kolhe",
        "summary": "#",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/ajinkyakolhe112/history-study-buddy",
        "demo_video_url": "https://youtu.be/_23cdQL3tkY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/history-study-buddy"
    },
    {
        "project_title": "NAYANA: Breaking Language Barriers with Multilingual Multimodal AI",
        "tagline": "Breaking language barriers with multilingual document intelligence powered by Gemma 3n and designed for real-world impact.",
        "authors": "Adithya_S_K",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/adithya-s-k/Nayana",
        "demo_video_url": "https://youtu.be/8B-4z4XtbxU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/nayana-breaking-language-barriers-with-multilingua"
    },
    {
        "project_title": "English-Mongolian Audio Transcriber & Translator",
        "tagline": "English-Mongolian transcription and translation using fine-tuned Gemma 3n E2B model.",
        "authors": "Bilguun Ochirbat",
        "summary": "Our team working on an automatic subtitle generator that creates bilingual subtitles for English and Mongolian languages, addressing the critical under representation of Mongolian in AI language models. By fine-tuning Google's [Gemma 3n E2B](https://huggingface.co/google/gemma-3n-E2B-it) model with LoRA (Low-Rank Adaptation), we are developing solution that operates offline while providing real-time transcription and translation between both languages. Currently, we have only fine-tuned the model; the demo app is still in development.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/bilguun0203/gemma3n-audio-mn",
        "demo_video_url": "https://youtu.be/LayUFbTfEqQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/english-mongolian-audio-transcriber-and-translator"
    },
    {
        "project_title": "ThinkGrade",
        "tagline": "An approachable AI companion that explains math and grades essays—straight from your laptop, tablet, or even a low-cost cloud GPU.",
        "authors": "Leanna Jeon, Hung-Chen Hsu",
        "summary": "1. Why ThinkGrade?\nMost learning apps do one thing well: they either show you how to solve a math problem or tell you whether your writing is any good. ThinkGrade does both. We fine-tuned Google’s open-weight Gemma-3 n-E2B-it model so it can:\n\n1) Walk through math step by step—with LaTeX and clear reasoning.\n2) Score short essays or summaries on a 0–5 scale—and explain why a score was assigned.\n\nBecause everything runs in 4-bit quantization, ThinkGrade fits comfortably on two modest T4 GPUs (or a single A100). That means schools—​or parents—​can host it privately without handing student data to a third-party API.\n\n\n2. How It Works (Plain English!)\n\n1) You ask a question.\n“Solve ∫x² dx.” or “Summarize this article in three sentences.”\n\n2) ThinkGrade builds a tidy prompt.\nIt prepends a “system” persona that keeps the model polite, safe, and organized, then adds your query.\n\n3) Gemma answers.\nBehind the scenes, the model carries two LoRA “hats”:\nMath-Tutor LoRA: generates step-by-step solutions.\nEssay-Judge LoRA: scores writing on logic, conciseness, and relevance.\n\n4) You get a friendly response.\nMath replies come in three parts: Approach → Calculation → Final Answer (all LaTeX-ready).\nEssay replies return a score (0–5) plus two or three sentences of feedback.\n\n\n3. Data We Fed the Model\n\n1) Math tutoring\nSource & Size: 30 k–100 k problems from MathX-5M\nWhat We Did: Stripped noisy <think> tags, kept final answers, wrapped each example in a chat template.\n\n2) Essay tutoring\nSource & Size: 20 k CNN/DailyMail articles + highlights\nWhat We Did: Turned each article into a “write a summary” prompt; target = reference summary.\n\n3) Math grading\nSource & Size:10 k MathX-5M problems\nWhat We Did: Used GPT to write student-like answers, auto-labeled ✔/✘ via cosine similarity; balanced to 5 k.\n\n4) Essay grading\nSource & Size:2 k news articles\nWhat We Did: GPT-generated summaries; Sentence-BERT mapped similarity to scores 0–5.\n\nEverything streams straight from Hugging Face, so we never blow past Kaggle’s RAM limit.\n\n\n4. Training in a Nutshell\n\n1) 4-bit NF4 quantization: Shrinks memory ×4 with minimal quality loss.\n\n2) LoRA + QLoRA: Only 0.2 % of weights are trainable—perfect for hackathon budgets.\n\n3) Rank & α: r=4, α=8 (tutoring) ; r=8, α=16 (grading) (Lower rank = lower VRAM; higher rank when we need classifier muscle.)\n\n4) Grad accum 32: True batch 64 on two T4s.\n\n5) bf16 + gradient-checkpointing: Faster math, fewer out-of-memory errors.\n\n\nEnd-to-end:\n\n1) Math tutor SFT: ~3 h on two T4s\n\n2) Math classifier: 45 min on an A100\n\n3) Essay classifier: 25 min on an A100\n\n\n5. Does It Actually Work?\n\n1) Solve-and-Explain(Human spot-check)\n-> Solutions are correct 9/10 times; LaTeX renders cleanly.\n\n2) Math grading(Accuracy)\n-> 92 % on a held-out 1 k set.\n\n3) Essay grading(Macro F1)\n->0.78 across six score bands.\n\n\n6. What We Learned\n\n1) Small can be mighty. A 2-billion-parameter model—​when pruned, quantized, and LoRA-patched—​is plenty for K-12 tutoring.\n\n2) Prompt rigidity beats prompt magic. By manually concatenating the prompt (no fancy .apply_chat_template()), we kept the model from drifting into off-topic tangents.\n\n3) Synthetic labels get you 80 % there. Auto-graded datasets let us stand up viable classifiers in days, not weeks.\n\n\n7. Roadmap\n1) Hand-written math input – pair Gemma with a lightweight OCR so students can snap a photo of their scratch work.\n\n2) Rubric-based essays – break the 0–5 score into sub-scores (logic, grammar, style).\n\n3) Pairwise ranking & RLHF – teach the model which feedback is most helpful.\n\n4) One-click offline bundle – export to GGUF for llama.cpp; aim for < 1 GB so a Raspberry Pi can host ThinkGrade at the edge.",
        "technologies": [],
        "source_code_url": "https://github.com/LeannaJ/Gemma_3n_Think_Grader.git",
        "demo_video_url": "https://youtu.be/ThYiZhK_d2o",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/thinkgrade"
    },
    {
        "project_title": "Tese Community Brain: Offline-First Knowledge Access for Remote Education",
        "tagline": "Decentralized RAG system using Gemma 3n on Raspberry Pi to transform outdated smartphones into syllabus-grounded learning portals.",
        "authors": "Bongani Bryan Dube",
        "summary": "Problem Summary\nAcross Sub-Saharan Africa in places such as Matabeleland South, Zimbabwe, COVID-19 lockdowns exposed a devastating truth: when schools closed and internet access was nonexistent, entire communities saw exam pass rates plummet to [zero](https://evidenceforinclusion.org/our-true-voice/zimbabwes-education-inequalities-deepen-due-to-covid-19/). This crisis isn't isolated but symptomatic of a global challenge. Across rural Africa, 43% of students lack internet access, while over numerous outdated smartphones sit unused in drawers, too underpowered for modern educational apps, yet too valuable to discard. Traditional solutions fail because they rely on stable internet, presume access to modern devices, ignore localised curricula like syllabi, and require energy-intensive hardware incompatible with solar-powered or limited-grid environments. This perfect storm creates \"education deserts\" where students are systematically excluded from the AI revolution transforming global learning.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/alpharithm-dex/TeseUI",
        "demo_video_url": "https://youtu.be/8ZwkFchFS5k",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-3n-community-brain-offline-first-knowledge-a"
    },
    {
        "project_title": "djender the code rookie",
        "tagline": "Djender is a code assistant that leverages the power of gemma3n and is even used in its own development.",
        "authors": "Muhammed Emir GÜNCÜ",
        "summary": "Djender is a coding assistant developed to showcase the capabilities of Google's gemma3n model. The most distinctive feature of the project is that Djender is also used as a tool in its own development process. This approach aims to show that the language model can be an active part of the development process, including code generation and debugging.",
        "technologies": [],
        "source_code_url": "https://github.com/muhammed-emir-guncu/djender",
        "demo_video_url": "https://youtu.be/rpsiVA_dv5Q",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/djender-the-code-rookie"
    },
    {
        "project_title": "Alyza - Conflict Resolution Companion",
        "tagline": "A companion who helps you journal or resolve conflicts using effective communication techniques",
        "authors": "Michael Lin, Aaron Xu",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/omijimo/alyza",
        "demo_video_url": "https://youtu.be/_PT41dpOAnc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/alyza-conflict-resolution-companion"
    },
    {
        "project_title": "Read Between The Lines",
        "tagline": "Your private communication coach — for the moments that matter",
        "authors": "Nikhil Kaul, Lucas Fagan",
        "summary": "Read Between the Lines is a private, on-device communication coach that helps people navigate emotionally complex, sensitive, or confusing messages right inside their everyday conversations. Whether you’re supporting a grieving friend, setting a boundary, or just trying to say what you mean, RBTL interprets messages, suggests thoughtful responses, and helps translate intent into impact—all without sending your data to the cloud. It can also translate across languages, tones, or cultural nuances, making it easier to connect across differences. We built it because we kept turning to LLMs to help us communicate better, but wanted a solution that was seamless, secure, and centered on human connection.",
        "technologies": [],
        "source_code_url": "https://github.com/nikhilkaul1234/ReadBetweenTheLines",
        "demo_video_url": "https://youtu.be/NYZQ2T2Q_5k?si=6LyWvklJMe0UvD0G",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/read-between-the-lines"
    },
    {
        "project_title": "The Neurodiversity Navigator",
        "tagline": "AI Based Self Regulation Assistant for Neurodiverse Community",
        "authors": "Shreya Bhattacharya_42",
        "summary": "#",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/shreya-bhattacharya-purnamedha/neurodiversity-assistant-gemma3n-impact-challenge",
        "demo_video_url": "https://youtu.be/bB4jjX5e7vw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/the-neurodiversity-navigator"
    },
    {
        "project_title": "MindBridge: Your Private AI Wellness Desktop App",
        "tagline": "MindBridge is a privacy-first desktop application that acts as your personal AI wellness and productivity coach, using a locally-run model",
        "authors": "Divijio",
        "summary": "MindBridge: Your Private AI Wellness Desktop App",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/N-45div/MindBridge",
        "demo_video_url": "https://youtu.be/Vo4uOr5dlb4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/mindbridge-your-private-ai-wellness-desktop-app"
    },
    {
        "project_title": "ScanMate With Gamma 3n",
        "tagline": "A Fully Offline AI-Powered PDF Summarizer for Android",
        "authors": "Salman 2031780, Mosammat Shahana Islam 2131019642, Syeed Mikdad Rahman 2031131642, Samira Hoq",
        "summary": "ScanMate: AI-Powered Document Intelligence\nA Flutter mobile app that transforms document scanning with offline AI summarization using Google's Gemma 3 model.\n🚀 What It Does\nScanMate combines traditional document scanning with cutting-edge AI to provide intelligent document processing completely offline. Users can scan documents, extract text through OCR, and get AI-powered summaries without any internet connection after initial setup.\n🧠 Gemma 3 Integration\nModel: Gemma 3 Nano E2B (int4 quantized) - 2.99GB\nImplementation: Flutter integration via flutter_gemma plugin\nProcessing: Completely offline AI inference on mobile devices\nPerformance: Optimized for mobile with 4-bit quantization\n🔧 Key Features\nDocument Scanning: ML Kit Document Scanner integration\nOCR Processing: Google ML Kit text recognition for scanned documents\nAI Summarization: Gemma 3-powered intelligent document analysis\nOffline First: No internet required after initial model download\nCross-Platform: Flutter app supporting Android and iOS\n💡 Technical Innovation\nHybrid Text Extraction: Multi-stage pipeline combining PDF parsing, enhanced pattern matching, and OCR fallback\nIntelligent Processing: Smart content detection triggers appropriate extraction methods\nEdge AI: Full Gemma 3 model running locally on mobile devices\nSecure Architecture: Environment-based token management with Git protection\n🎯 Use Cases\nStudents: Scan and summarize research papers, textbooks\nProfessionals: Process contracts, reports, meeting notes\nResearchers: Analyze documents without cloud dependency\nPrivacy-Conscious Users: Complete offline document intelligence\nPerfect for scenarios requiring document intelligence without internet connectivity or cloud processing.",
        "technologies": [],
        "source_code_url": "https://github.com/salmanhossain99/ScanmateProject",
        "demo_video_url": "https://youtu.be/p7rgZ5gje1Q",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/scanmate-with-gamma-3n"
    },
    {
        "project_title": "AnnaAgent-Gemma",
        "tagline": "AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation",
        "authors": "Ming Wang",
        "summary": "Training for mental health counselors lacks realistic, accessible practice. We built AnnaAgent, a Gemma-powered AI that acts as a virtual seeker, to solve this problem.\n\nUnlike other simulators, AnnaAgent feels real because it masters two key challenges:\n\n- Dynamic Evolution: The AI's emotions and understanding of its problems change throughout the conversation, just like a real person's would.\n\n- Multi-Session Memory: It remembers past conversations and progress over multiple sessions, avoiding the unrealistic \"amnesia\" common in other chatbots.\n\nProven in Practice: AnnaAgent has already served as the official virtual seeker in two peer counseling skills competitions at Northeastern University.\n\nOur goal is to provide a better training tool to help prepare the next generation of mental health professionals.",
        "technologies": [],
        "source_code_url": "https://github.com/sci-m-wang/AnnaAgent",
        "demo_video_url": "https://youtu.be/CqsiUOiMfVo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/annaagent-gemma"
    },
    {
        "project_title": "Blackboard AI powered by Google's Gemma 3n",
        "tagline": "Making Education - Free, Personal, Offline and AI Based. ",
        "authors": "Sahdev Surolia",
        "summary": "BlackBoardAI: Technical Proof of Work",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/the-nine-tail/BlackBoardAI",
        "demo_video_url": "https://youtu.be/xT_HRrm9EyU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/blackboard-ai-powered-by-googles-gemma-3n"
    },
    {
        "project_title": "LinguaCoAI",
        "tagline": "A World Language Code Converter is an AI-powered tool that lets anyone write programming in their native language.",
        "authors": "Kamalesh Motamarri, Chaitu Lakkoju",
        "summary": "Project Overview\nLinguacoAI is an AI-driven platform that empowers users to write programming instructions in their native language and instantly converts them into clean, executable Python code. By leveraging Google’s multilingual Gemma 3n 4B API and a custom rule-based conversion layer, LinguacoAI makes programming accessible, intuitive, and equitable—no English proficiency required.\n\n1. Core Architecture\nFrontend\nFramework: Next.js (React)\n\nStyling: Tailwind CSS for rapid, accessible UI prototyping\n\nInteraction: Clean, intuitive web app that guides the user from natural-language prompt to code output in real time\n\nBackend/API\nLLM Inference:\n\nModel: Google Gemma 3n 4B, accessed via API key\n\nMode: Runs inference on user input—supports offline/low-latency deployment when locally hosted\n\nRouting/Conversion Logic:\n\nTypeScript Route File: The ts.route file defines conversion rules, mapping natural-language intent to code structures for improved consistency, reliability, and edge-case handling\n\nMiddleware:\n\nHandles NLP pre-processing, Gemma 3n prompt engineering, and post-processing of model outputs for code formatting\n\n2. How LinguacoAI Works\nStep 1: User Input\nThe user enters a programming request in their native language (currently Spanish for beta), e.g.\n\"Pide al usuario su edad y calcula cuántos años tendrá en 10 años.\"\n\nStep 2: Pre-processing & Routing\nThe input is routed through the TypeScript route file (ts.route), which:\n\nIdentifies common code tasks (input, arithmetic, print, etc.)\n\nNormalizes intent (parsing numbers, variable names, and basic operations)\n\nEnsures the input is structured for optimal LLM processing\n\nStep 3: Gemma 3n Integration\nThe cleaned and structured prompt is sent to Gemma 3n 4B via API:\n\nUtilizes Gemma’s multilingual instruction-following capabilities\n\nTailored prompts guide the model to output only clean, idiomatic Python code\n\nStep 4: Post-processing\nThe raw Gemma output is checked for formatting, security, and syntax\n\nFinal code is rendered in the web app for user review and copy/paste\n\nStep 5: Output & Feedback\nThe user sees instant, executable Python code reflecting their original request\n\nErrors, edge cases, and ambiguous requests are handled with clear user messages (planned: clarifications in the user's language)\n\n3. Why Gemma 3n?\nMultilingual Excellence: Handles non-English requests natively, not just via translation\n\nEfficient and Privacy-First: Can be deployed locally or on-device for privacy and offline access\n\nInstruction Tuned: Generates code that matches the user’s actual intent, not just a literal translation\n\nExpandable: As Gemma 3n expands its language support, LinguacoAI can scale to more languages with little reengineering\n\n4. Unique Features\nHybrid Rule-based + LLM Conversion:\n\nThe ts.route logic ensures edge-case handling and consistent code generation, rather than relying solely on the LLM’s output\n\nThis hybrid design enables better reliability for common coding patterns, and the system can be extended with more patterns as new languages are added\n\nAccessible Modern UI:\n\nBuilt with Next.js and Tailwind CSS for a responsive, mobile-friendly experience\n\nDesigned for students, teachers, and hobbyists globally\n\nFuture-Proofed:\n\nModular codebase supports new programming languages and spoken languages\n\nReady for integration with additional LLMs and on-device Gemma 3n when available\n\n5. Current Limitations and Future Work\nBeta Scope:\n\nCurrently supports only Spanish-to-Python and simple instructions\n\nMore complex instructions and broader language/code support in roadmap\n\nError Handling:\n\nCurrently relies on best-effort LLM generation for ambiguities; future versions will use clarifying prompts and real-time user feedback\n\nMultimodal Expansion:\n\nGemma 3n’s audio/image capabilities to be incorporated for spoken instructions and visual code understanding\n\n6. Conclusion\nLinguacoAI demonstrates a working proof-of-concept for a world where programming isn’t limited by English. By combining Google’s Gemma 3n, a robust rule-based routing system, and a modern web stack, we’re making the dream of native-language programming a reality—starting with Spanish, and soon, the world.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/kamaleshmotamarri/linguacoai.git",
        "demo_video_url": "https://youtu.be/77Q_aKnhaJc?si=JVP34uock6_20f1l",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/linguacoai"
    },
    {
        "project_title": "TeachMe AI Tutor",
        "tagline": "Offline, interactive AI tutor that turns any video or PDF into a custom quiz with instant grading using Gemma 3n.",
        "authors": "Sohail Ahmad Sahil",
        "summary": "TeachMe AI Tutor is a fully offline, multimodal AI education assistant that empowers learners by transforming any PDF document or video into an interactive and personalized learning experience — without needing internet access. Powered by Gemma 3n running locally via Ollama, this innovative application ensures complete user privacy, high performance, and accessibility for students even in remote or low-connectivity environments.\n\nWith just a PDF or video input, TeachMe AI Tutor intelligently:\n\n🔍 Extracts and understands key content using Whisper for transcription and PyMuPDF/PyPDF2 for document parsing\n\n❓ Automatically generates custom quizzes with questions and answer keys\n\n🧠 Grades user responses in real-time using Gemma 3n’s LLM capabilities\n\n💬 Offers contextual Q&A and summaries for any section of the content\n\n🗂️ Organizes knowledge into an interactive UI using Flet and Python\n\nWhether it’s preparing for an exam, self-assessing comprehension, or simply exploring a topic deeper, learners can do all of this without an internet connection, and without compromising their data privacy.\n\nTeachMe AI Tutor addresses major global challenges in education:\n\n🌍 Digital inequality — enabling learning without internet or cloud services\n\n🔐 Privacy-first learning — all processing happens locally on the user’s device\n\n🧑‍🏫 Teacher-in-a-box — simulating an interactive, always-available tutor\n\nThis AI tutor is not just a proof of concept — it is a working application with real-world potential to revolutionize offline education in underserved regions. Powered entirely by open-source and local-first technologies, it brings the future of learning directly to any laptop, anywhere.\n\n\n⚙️ Architecture\nflet: cross-platform GUI\n\nfitz (PyMuPDF): PDF parsing\n\nwhisper: video/audio to text\n\nollama: local Gemma 3n LLM\n\nthreading: async handling\n\nFull routing + views for Test, Result, and Ask-AI pages\n\n🧪 Bonus Track Fit\n✅ Ollama: All LLM calls use local Gemma 3n\n\n✅ Multimodal: Uses both PDF and video inputs",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/5cErjK137gM?si=I1KkiLJovrIg2GUF",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/teachme-ai-tutor"
    },
    {
        "project_title": "ShikshaMitra: An Offline AI Tutor for Rural Kids Using Gemma 2b",
        "tagline": "An offline, multilingual AI tutor built using Gemma 3n to empower rural students through voice, image, and interactive learning.",
        "authors": "S@URAV12",
        "summary": "**Overview**\n- ShikshaMitra is an AI-powered offline tutor built using Google’s Gemma 2b.\n- It aims to help rural children learn math and science without requiring internet connectivity.\n- It supports multilingual voice-based interactions, image understanding, and quiz/practice modes.\n- Designed with a focus on accessibility, privacy, and educational impact.\n\n**Key Features**\n- Voice-based question answering for school-level subjects\n- Quiz mode that generates practice questions based on topics\n- Language switching between English and Hindi via voice command\n- Image understanding and explanation of diagrams captured from textbooks or blackboards\n- Fully offline, private, and lightweight to run on local devices\n\n**Technical Architecture**\n- Gemma 3n model is run locally using Ollama\n- Flask or CLI-based interface handles user interaction\n- Voice input is captured using the SpeechRecognition library\n- Text-to-speech responses are generated using pyttsx3\n- Image input is processed with OpenCV and pytesseract\n- Basic contextual memory is implemented by storing recent interactions\n\n**Model and Tools Used**\n- Language Model: Gemma 3n (via Ollama)\n- Python-based backend\n- STT: SpeechRecognition with offline recognizer\n- TTS: pyttsx3 for offline audio output\n- OCR: pytesseract for extracting text from images\n- Optional frontend: Flask (for future expansion)\n\n**Challenges Faced\n**- Running multimodal interactions (text, voice, image) offline with limited resources\n- Maintaining responsiveness without cloud APIs\n- Voice accuracy with Hindi/English mixing\n- Ensuring model context awareness across questions\n\n**Potential Impact**\n- ShikshaMitra can be deployed in low-connectivity or zero-connectivity areas.\n- Students from rural or underserved communities can access AI tutoring for school curriculum.\nThe multilingual, multimodal approach makes it adaptable to different states and education boards.\n- It can be integrated with local government school tablets or Raspberry Pi-based devices.\n\n**Future Scope**\n- Add GUI using Streamlit or Electron for better UX\n- Expand to other Indian languages (e.g., Marathi, Bengali, Tamil)\n- Add image labeling and drawing recognition for geometry-based problems\n- Track learning progress and suggest personalized exercises\n\n**Demo Video**\nhttps://www.youtube.com/watch?v=QNNMYSYm8AE\n\n**Code Repository**\nhttps://github.com/sau240/Siksha-Mitra\n\n**Citation**\nThis project was built for the Google Gemma 3n Impact Challenge on Kaggle.\nBased on the open-source capabilities of Google’s Gemma 3n model.",
        "technologies": [
            "Flask",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=QNNMYSYm8AE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/shikshamitra-an-offline-ai-tutor-for-rural-kids-us"
    },
    {
        "project_title": "Teagardan- Video deepsearch, an unprecedented way to organize and search videos.",
        "tagline": "Product transcribes video, audio, analyze visuals with a machine learning model, detect changes, save audio-visual description for retrieval",
        "authors": "Teagarda",
        "summary": "Welcome to my submission page! \n\nTeagardan's video deepsearch gives a \"*mini-me*\" of a video and lets users understand the video better using Gemma 3n and Ollama completely offline and on edge devices like powerful computers.\n\n**Project Description**\nThis project is a sophisticated Flask-based web application integrated with a video processing pipeline, designed to enable users to analyze and query video content interactively. It leverages the **Ollama client with the \"gemma3n:e4b-it-q8_0\"** model for advanced natural language processing and image analysis, creating a robust system for extracting and querying video data. The application processes video files stored in a user-specified directory, extracting visual and audio information to build a searchable repository stored in JSONL files. The backend employs OpenCV and FFmpeg to extract frames at configurable intervals, transcribes audio using the Whisper model, detects scene changes via Mean Squared Error (MSE) comparisons, and generates detailed textual descriptions of video chunks and frames using the Ollama model. These descriptions combine audio transcriptions and visual analyses, enabling comprehensive content retrieval.\n\n**Flask app**\nThe Flask application provides a dynamic web interface, defined by the video.html template, supporting two interaction modes: Ask and Scene. In Ask mode, users can submit text queries about the video, and the system uses the BM25Okapi ranking algorithm to identify relevant frames based on their combined audio and visual text. The Ollama model generates both brief and detailed responses, accompanied by frame thumbnails and clickable timestamps for easy video navigation. In Scene mode, pausing the video triggers an automatic description of the current scene, with the option to ask follow-up questions about specific frames. The interface features a responsive video player, styled chat logs for user-bot interactions, and a section displaying relevant frames, all enhanced by CSS for aesthetics and JavaScript for asynchronous API interactions.\n\n**Key Technical Components**\nFrame Extraction and Scene Detection: Utilizes OpenCV and FFmpeg to extract frames and detect scene changes using MSE thresholds. Supports Casual mode (faster, lower resolution) and Pro mode (detailed, high resolution) for flexible processing.\nText Processing: Implements the BM25Okapi algorithm to rank frame relevance, preprocessing text by removing stop words and punctuation to optimize query performance.\nNatural Language Processing: The Ollama model powers response generation for general queries, frame-specific questions, and scene descriptions, supporting image inputs for visual frame analysis.\nData Management: Stores processed data in JSONL files, with each entry including timestamps, audio transcriptions, visual descriptions, and paths to frame screenshots for efficient retrieval.\nWeb Interface: The video.html template delivers a responsive UI with a video player, navigation buttons, and chat forms. CSS ensures a clean, user-friendly design, while JavaScript handles dynamic API calls to update the interface seamlessly.\n\n**System Design and Robustness**\nThe application is engineered for reliability, incorporating error handling for file operations, JSON parsing, and model inference, with comprehensive logging to monitor processing steps. It runs on a local server at port 5006, requiring an active Ollama server with the \"gemma3n:e4b-it-q8_0\" model, FFmpeg for video processing, and Whisper for audio transcription. The system is optimized for applications such as educational tools, content summarization, or interactive media exploration, providing users with a seamless, web-based interface to explore and query video content with precision and ease.\n\n*Experience the product at work* here https://0ea2cfce1492.ngrok-free.app !\n\n\n*Behind the scenes*: \nWhen I was attempting this, I could not figure out how many frames to select, maybe fixed intervals or otherwise and later finalized on this approach. The search used a simple search for retrieval and now it uses the one we see now. \n\nIt took about 1 minute for a 1 minute video to generate the jsonl which was a betterment from the earlier 5 to 10 minutes per minute on my Machine. Everything runs locally and offline. Using Ollama helps as the models are quantized and available, which is convenient to swap models and even let users choose which mode/resolution they expect. \n\n\nI initially did not feed video contextual information, and I see the performance improved after having this step, but before frames are fed to gemma. The search result also skips if the seconds are close by so there are less duplicated instances. For other videos like streaming live code from terminal, I see the frames of importance is very high, so even for a 1 minute screen capture of a streaming code getting generated lets say by mlx or ollama in terminal, the number of frames of importance is very high and it will take more than 10 minute to create the jsonl for a 1 minute streaming video. had json before and now using jsonl.\n\nThanks, please provide your feedback and super excited to reveal this here!!",
        "technologies": [
            "Flask",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Teagardan/videosearch",
        "demo_video_url": "https://www.youtube.com/watch?v=tlRAIjsrG_E",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/teagardan-video-deepsearch-an-unprecedented-way-to"
    },
    {
        "project_title": "Healing-tech Analyzer With Gemma AI Integration For Burnout Prevention",
        "tagline": "Healing-tech chronobiology analyzer with Gemma AI integration for wellness monitoring and burnout prevention",
        "authors": "Syira",
        "summary": "📊 **GemmaGuard - Kaggle Project Description**\n🎯** Project Title:**\n**\"GemmaGuard: AI-Powered Chronobiology Wellness Monitor with Gemma Integration\"**\n\n**📝 Project Description:**\n\n**Overview**\n\nGemmaGuard is an innovative wellness monitoring application that combines chronobiology science with Google's Gemma AI to predict and prevent burnout through personalized temporal pattern analysis. This project demonstrates practical AI implementation for healthcare applications while maintaining user privacy through local processing.\n\n**🔬 Problem Statement**\n\n- 76% of employees experience workplace burnout\n- Traditional wellness apps lack predictive capabilities\n- Most solutions are reactive rather than preventive\n- Need for personalized, science-based wellness insights\n\n**🚀 Solution Approach**\nDual-Mode Analysis System:\n\n1. Full Analysis: Biometric simulation + behavioral patterns + AI insights\n2. Temporal Pattern Analysis: Chronotype-based behavioral prediction\n\nKey Technologies:\n\n- Frontend: Streamlit with healing-tech UI design\n- AI Engine: Ollama + Gemma (gemma:latest) for local inference\n- Science Base: Chronobiology and temporal pattern analysis\n- Data Processing: Real-time biometric simulation and personality mapping\n\n**🎨 Innovation Highlights**\n\n- Healing-Tech Aesthetic: Calming gradients, glass-morphism design\n- Privacy-First: Local AI processing, no data transmission\n- Educational Transparency: Open-source algorithms for competition compliance\n- Commercial Viability: Protected proprietary algorithms with IP strategy\n\n**📊 Technical Implementation**\n\nKey Features Demonstrated:\n- Streamlit multi-page application\n- Real-time Gemma AI integration\n- Biometric data simulation\n- Temporal pattern analysis\n- Personality trait mapping\n- Recovery strategy generation\n\n**🎯 Target Applications**\n\n- Corporate Wellness Programs\n- Personal Health Monitoring\n- Preventive Healthcare\n- Stress Management Tools\n- Chronotherapy Research\n\n**📈 Results & Impact**\n\n- Predictive Accuracy: Identifies stress patterns before manifestation\n- User Experience: Intuitive healing-tech interface\n- AI Integration: Clear demonstration of Gemma capabilities\n- Scalability: Modular architecture for enterprise deployment\n\n**🏆 Competition Readiness**\n\n- Clear Gemma Integration: Transparent AI usage with examples\n- Professional Documentation: Comprehensive setup and usage guides\n- Open Source Components: Educational implementations available\n- Commercial Protection: Strategic IP management\n\n**🔗 Repository Structure**\n\n├── app/                    Streamlit application\n├── competition_public/     Educational algorithms\n├── data/                   Local data storage\n├── prompts/               AI prompt templates\n└── documentation/         Setup guides & architecture\n\n**🛠️ Technologies Used**\n\n- Python (Streamlit, Pandas, NumPy)\n- AI/ML: Ollama, Gemma, Local LLM Processing\n- UI/UX: Custom CSS, Glass-morphism Design\n- Data Science: Chronobiology, Temporal Analysis\n- DevOps: Git, GitHub, Modular Architecture\n\n**💡 Key Takeaways for Kaggle Community:**\n\n- Practical AI Implementation: Real-world application of Gemma for healthcare\n- Privacy-Conscious Design: Local processing without data transmission\n- Science-Based Approach: Chronobiology meets modern AI\n- Professional Development: Competition-ready with IP protection\n- Community Value: Educational components for learning and adaptation\n\n***Explore how AI can transform preventive wellness. Star the repository, try the demo, and contribute to the future of personalized healthcare technology.***\n\nGitHub: https://github.com/Syira-769/Gemma-Guard\nDemo: Available for local installation \nLicense: Open-source with commercial components",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": "https://github.com/Syira-769/Gemma-Guard",
        "demo_video_url": "https://youtu.be/AFG-WI2DCU4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/healing-tech-analyzer-with-gemma-ai-integration-fo"
    },
    {
        "project_title": "SeeForMe: Offline AI Assistant for the Visually Impaired",
        "tagline": "A fully offline voice assistant that empowers blind users with emotional support, object and scene understanding, and humanlike conversation",
        "authors": "Diya Dave",
        "summary": "SeeForMe: An Offline AI Companion for the Blind, Built with Empathy\nIn a world full of smart assistants, most still rely on internet, fail to support regional languages, and lack empathy. We wanted to change that — SeeForMe is an offline, multilingual, emotionally intelligent AI companion designed for blind users, built with Gemma 3n.\n\nMillions of visually impaired individuals struggle every day to interact with their environment — to know what’s around them, to feel heard, and to be emotionally supported. We built SeeForMe, a fully offline, real-time voice assistant that acts as a friend, guide, and guardian for blind users.\n\nWhat it can do (completely offline):\n\n🎯 Emotion detection via front camera – understands how the user is feeling.\n\n🧠 Gemma 3n-powered conversations – provides empathetic, humanlike responses.\n\n🧱 Scene and object detection – describes surroundings using back camera.\n\n🗣️ Multi-language voice interaction – speaks and understands English, Hindi, and Gujarati.\n\n💬 Remembers user name and emotions for personalized responses.\n\n🔄 Voice-triggered camera switching – e.g. \"What is around me?\" opens back camera.\n\nEven with no internet, no screen, and just a voice, SeeForMe helps the user:\n\nFeel safe when alone.\n\nGet emotional support during sadness or fear.\n\nUnderstand their environment.\n\nHave a friend to talk to — in their language, with their emotions in mind.\n\nHow Gemma 3n is used:\n\nWe use Gemma 3n (via Ollama) as the brain of our assistant.\n\nIt processes speech + emotion + memory and gives thoughtful, compassionate replies.\n\nWhen Gemma is not available, the assistant falls back to offline response logic.\n\nImpact:\nWe believe SeeForMe shows what open-source models like Gemma 3n can do for offline accessibility. By removing the need for internet and screens, we’re bringing intelligent assistance to everyone — especially those most underserved.\n\nSolo Project by Diya Dave — Master’s student in Advanced Computing with a passion for AI accessibility, empathy, and human-centric design.\n\n\nThis project is built with ❤️ and designed for real-life use by blind individuals in India and globally.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/diyadave/SeeForMe",
        "demo_video_url": "https://www.youtube.com/watch?v=w9wNECsmUEE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/seeforme-offline-ai-assistant-for-the-visually-imp"
    },
    {
        "project_title": "AarogyaAI: Offline Health Intelligence for the Unconnected",
        "tagline": " AarogyaAI offers offline AI health aid—symptom help, care tips & emergency steps_ via a fine-tuned model even on low-cost phones.",
        "authors": "Anushka Upadhye, Vishal Pattar",
        "summary": "Here’s a **refined and expert-level project description** for **AarogyaAI**, clearly articulating the **problem**, **solution**, and **impact**, while staying aligned with your current build. This version avoids emojis, includes stakeholder relevance, and is structured to be **competition-ready**:\n\n---\n\nAarogyaAI: Offline AI Health Assistant for the Digitally Disconnected\n\n**AarogyaAI** is an AI-powered health assistant that works completely offline, even on low-cost smartphones, to deliver private, accurate, and accessible health guidance to underserved populations with no internet access. Built on a fine-tuned, quantized version of the Gemma-3n-1B model, it is optimized for rural communities, community health workers (CHWs), and non-profit health programs operating in low-resource environments.\n\n---",
        "technologies": [],
        "source_code_url": "https://github.com/Vishal-Pattar/aarogya_ai",
        "demo_video_url": "https://youtu.be/-vof0EEOnEM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/aarogyaai-offline-health-intelligence-for-the-unco"
    },
    {
        "project_title": "AidBud-python",
        "tagline": "An Offline Triaging Application, for Times of Crisis and Everyday Use.",
        "authors": "Ian Sia",
        "summary": "In the wake of natural disasters such as tsunamis and earthquakes, a critical challenge consistently emerges: the severe shortage of medical manpower. Hospitals and emergency services are often overwhelmed, not only by a surge of casualties but also by a significant reduction in available healthcare professionals. Research, such as the study published on ResearchGate, further highlights this vulnerability, indicating that a substantial 20-30% of medical staff may be unwilling to return to work during times of crisis, exacerbating an already dire situation.\nThis acute shortage directly impacts the speed and efficiency of patient triaging—the crucial process of assessing patients to determine the urgency of their need for medical attention. Delays in triage can have life-threatening consequences, as those in most urgent need of care may not receive timely treatment. Furthermore, medical staff, already stretched thin, are often diverted to initial assessment tasks, preventing them from focusing on critical first aid and advanced care delivery.\n\nTo address this pressing issue, we have developed an offline triaging application designed to buffer the immediate impact of medical manpower shortages. Our application empowers non-medically trained personnel to effectively triage incoming patients, ensuring that even in the absence of sufficient healthcare professionals, a rapid and systematic initial assessment can be performed. By streamlining the triage process and enabling a broader base of responders, the application aims to significantly reduce triage times, allowing critically injured individuals to receive prompt treatment. Crucially, this innovation frees up invaluable medical staff, enabling them to concentrate their expertise on providing essential first aid and life-saving care, thereby optimizing resource allocation during the most challenging times. \n\n\n**Application Capabilities**\n\nThis offline triaging application is designed to run seamlessly on mobile devices, providing a robust tool for crisis response even without internet connectivity. Its core capabilities are built around enhancing communication, data management, and intelligent decision-making for non-medical personnel:\n\n- **Conversations:** The primary mode of interaction within the app is through video and audio, facilitating a more natural and efficient exchange of information with patients. This eliminates the need for cumbersome typing during critical moments, allowing responders to focus on the patient. Each video or audio clip submitted is capped at 30 seconds per \"query\" to ensure concise and focused communication. The application is multimodal, capable of accepting and processing input in various formats, including text, images, videos, and audio, providing a comprehensive understanding of the situation.\n\n- **Patient Cards:** The app features editable canvases for each patient, functioning much like collaborative documents. These \"Patient Cards\" are structured with specific, uneditable headers for key sections, ensuring consistency while allowing both the user and the underlying Large Language Model (LLM) to input and update patient details. These cards can be saved for later reference, providing a persistent record of patient information and assessment. They act similar to the Canvas featured in Gemini and ChatGPT, allowing for a persistent source of truth.\n\n- **Flexible Triaging:** The application offers a flexible triaging mechanism, allowing users toggle whether triaging is necessary for a given patient. It also supports the input of custom triaging methods, accepting key-value pairs where the \"key\" represents the triage level (e.g., \"Immediate,\" \"Delayed,\" \"Minor\") and the \"value\" provides a detailed description of that level. This adaptability ensures the app can conform to various emergency protocols.\n\n- **First Aid Availability:** The app can track and provide information on the availability of essential first aid resources, helping responders quickly identify what supplies are on hand.\n\n- **Current Situation Overview:** A dedicated section provides an overview of the current situation, potentially including aggregated patient data, resource status, and other critical information to aid in overall crisis management.\n\n- **Multilingual Support:** Leveraging its Gemma’s multilingual capabilities, the application can understand and process various foreign languages. This crucial feature enables seamless communication between users and patients who may have a language barrier, allowing patients to speak directly to the app, which then acts as an immediate interpreter and facilitator of understanding.\n\n\n**Technical Aspects**\n\n**1. Retrieval-Augmented Generation (RAG) for Conversational Context:**\n\n- **Problem:** Large Language Models (LLMs) used in mobile applications, especially smaller ones, often have a limited \"context window.\" This means they can only remember and process a certain amount of past conversation or information at any given time. Without a mechanism to provide relevant historical data, the model's ability to give accurate and contextually rich responses would be severely hampered, particularly in a long-running triage conversation.\n\n- **Solution (RAG):** Retrieval-Augmented Generation addresses this by dynamically retrieving relevant information from a vast knowledge base (a \"vector store\") and feeding it to the LLM alongside the current user query. This effectively extends the LLM's \"memory\" beyond its inherent context window, ensuring that only the most useful and pertinent information from past interactions is provided. This is crucial for maintaining a coherent and informed dialogue with the patient and for accurate patient card updates.\n\n**2. Split RAG for Optimized Context Retrieval:**\n\nTo further refine the RAG process and prevent irrelevant information from diluting the context, the RAG system is intelligently split into two primary sections:\n\n**Model Responses RAG:** This section stores past conversational turns and the LLM's previous outputs. It's further divided to optimize retrieval for specific types of information:\n- **Patient Card Edits:** Stores the history of modifications made to patient cards. This separation ensures that when the LLM needs to make or review an edit, the most relevant past edits are quickly accessible without being mixed with general conversational chatter.\n- **General Responses:** Stores the LLM's broader conversational responses. This partitioning prevents the chunking process (breaking down text into smaller pieces for embedding) from negatively impacting the quality of either query-response pairs or patient card-related information, leading to more precise retrieval.\n\n**Attachment Descriptions RAG:** This is a separate vector store specifically for descriptions derived from previously processed multimedia attachments (videos, audio, images). The rationale for this separation is critical:\n- **Efficiency:** Re-embedding large multimedia files (especially images and videos) is computationally intensive and consumes significant \"tokens\" (the units of text an LLM processes). By storing only a textual description of the attachment, the model doesn't need to re-process the raw media every time it's referenced, saving resources and speeding up response times.\n- **Context Isolation:** It prevents \"competition\" between current query attachments and previously inputted attachments. This ensures that the model can focus on processing new media while still having access to the context of older media via their descriptions.\n\n**3. Agentic Workflow for Persistent Attachment Context:**\n\n**Challenge:** Given that video and audio are the primary modes of communication, the application will handle a large volume of multimedia attachments. Embedding all raw attachments directly into the LLM's context for every query is impractical due to token limits and processing overhead. The LLM needs a way to \"remember\" and reference past attachments without constantly re-processing them.\n\n**Solution (Agentic Workflow):** An agentic workflow, a sequence of automated steps guided by the LLM, is implemented to manage attachment context efficiently. This workflow operates with a minimum of one and a maximum of two LLM calls, designed to ensure the model always has relevant context about attachments.\n\n**Step 1 (Initial Attachment Processing - If Attachments Present):** When a new query includes attachments, the agentic workflow first processes these attachments separately. For videos, this involves extracting frames at 1 frame per second (fps) and extracting audio. A concise description is then compiled for this group of current query attachments. This description is then inserted into the dedicated \"Attachment Descriptions RAG\" vector store. Crucially, the attachment's description (not the raw attachment) is then passed to the next step.\n\n**Step 2 (Prompt Formulation):** The next step involves forming a comprehensive prompt for the LLM. This prompt includes:\n- The description of the current query's attachments (if any).\n- Relevant conversational context retrieved from the \"Model Responses RAG.\"\n- The current patient card data.\n- Instructions for the LLM, including its ability to edit the patient card.\n\n**Alternative Path (If No Attachments in Current Query):** If the current query does not contain new attachments, the agentic workflow can choose one of two paths:\n- **Analyze Past Attachment:** If the user's query suggests a need to revisit or clarify something about a past attachment (e.g., \"What did you see in the video from earlier?\"), the agent can retrieve the relevant past attachment(s) and their descriptions from the \"Attachment Descriptions RAG.\" These original attachments are then attached to the current prompt.\n- **Normal Response:** If no past attachment analysis is needed, the LLM proceeds to respond as per normal, utilizing the RAG context and patient card edit functions.\n\n**Modified Function Ability (When Analyzing Past Attachments):** When the agent chooses to analyze a past attachment, the LLM's capabilities are temporarily modified. In addition to being able to edit the patient card, it gains the ability to edit the current attachment group's description. This allows the model to refine or update the summary of a past attachment based on new insights or user queries.\n\n**Overall Benefit:** This sophisticated agentic workflow enables the LLM to maintain continuous control over and reference to all attachments throughout the conversation, regardless of their quantity or when they were initially provided. It intelligently manages the trade-off between processing raw media and using concise descriptions, ensuring that valuable visual and auditory information is always available for contextual understanding.\n\n**4. Optimized Video and Audio Processing for Mobile Devices:**\n\n- **Video Processing:** Videos are processed at a reduced frame rate of 1 frame per second (fps). This significantly reduces the data volume and computational load, making video analysis feasible on mobile devices without excessive battery drain or processing delays.\n- **Audio Extraction:** Audio is extracted from the video and processed as normal audio data.\n\n**5. Embedder Token Window and Granular Chunking:**\n- **Embedder Token Window:** The application's embedder is configured with a token window length of 512. This choice is deliberate, as it mimics the typical processing capacity and limitations often found on mobile devices, ensuring the app's design is grounded in realistic mobile performance constraints.\n- **Implication for Chunking:** A smaller token window means that larger pieces of text must be broken down into more, smaller \"chunks\" to fit within the embedder's capacity.\n- **Strategic Chunking:** To maximize the quality of retrieval and minimize the impact of chunking, the system employs an overlap strategy between chunks. Furthermore, responses are not processed as a single, combined unit with patient card edits. Instead, conversational responses and patient card edits are processed and chunked separately. This separation ensures that the distinct types of information are clearly delineated for the embedder and for subsequent querying, leading to more precise and relevant context retrieval for both conversational flow and structured data updates on the patient card.\n\n\n**Sources:**\nhttps://www.researchgate.net/publication/332852561_Healthcare_workers'_willingness_to_respond_following_a_disaster_A_novel_statistical_approach_toward_data_analysis",
        "technologies": [],
        "source_code_url": "https://github.com/Buxt-Codes/AidBud-python",
        "demo_video_url": "https://youtu.be/b_lfaPPO29E",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/aidbud-python"
    },
    {
        "project_title": "JARVIS – Your Personal AI Assistant Powered by Gemma 3n",
        "tagline": "An offline, private, voice-enabled desktop assistant powered by Gemma 3n running locally via LM Studio.",
        "authors": "Abhijeet Roy",
        "summary": "🔷 Project: JARVIS – Your Personal AI Assistant Powered by Gemma 3n\n\n🔊 Problem:\nMost AI assistants rely on cloud APIs, creating privacy concerns and requiring constant internet access. They also often lack customizability or offline functionality.\n\n🚀 Solution – JARVIS:\nA fully local AI assistant powered by Gemma 3n, using LM Studio and a custom Python GUI. JARVIS responds to your typed questions, answers them intelligently using Gemma 3n, and even speaks replies aloud using offline text-to-speech.\n\n✨ Key Features:\n\nLocal inference using Gemma 3n via LM Studio\n\nSleek GUI built with Tkinter\n\nVoice replies with pyttsx3\n\nCustom design (fonts, colors, background)\n\nFast, offline, and private – no APIs used\n\n🧠 Technical Architecture:\n\nUser enters a question in the GUI\n\nApp sends the prompt to Gemma 3n (hosted on localhost:1234 via LM Studio)\n\nGemma returns a response\n\nResponse is displayed in the GUI and spoken aloud\n\n📦 Tools & Libraries:\n\nPython 3.11\n\nTkinter (GUI)\n\nrequests (for API calls to LM Studio)\n\nPillow (for images)\n\npyttsx3 (TTS engine)\n\n🧪 Challenges Solved:\n\nMic input issues (removed to improve UX)\n\nAvoided external dependencies for voice and inference\n\nPackaged into a clean, single Python file for easy use",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/atlassandx90/J.A.R.V.IS---Personal-Ai-Assistant",
        "demo_video_url": "https://youtu.be/4bH6PYtviDs",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/jarvis-your-personal-ai-assistant-powered-by-gemma"
    },
    {
        "project_title": "Nutrition Snap",
        "tagline": "A private, app that uses on-device AI to analyze Sri Lankan meals for instant nutritional information and personalized health insights.",
        "authors": "adomic arts",
        "summary": "**Nutrition Snap: Your Private, Offline Sri Lankan Wellness Coach**\n\nVideo : https://youtu.be/H_cT23jciyI\nCode : https://github.com/HGSChandeepa/Nutrition-Snap\n\n[YouTube video ](https://youtu.be/H_cT23jciyI)\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F27410213%2Ffb67d036cd2da0eddca1959d4fe8993f%2FGemini_Generated_Image_d0qcihd0qcihd0qc%20(1).jpg?generation=1754506726518937&alt=media)\n\nProject Goal: To create a culturally intelligent and privacy-focused mobile application that empowers Sri Lankans to manage their health by providing instant, accurate nutritional information for local cuisine, all without an internet connection.\n\nKey Features:\n\n- Image-to-Nutrition Analysis: Users can take a photo of their meal, and the app will instantly identify the food items and provide a detailed nutritional breakdown, including calories, protein, carbs, and fats. This is powered by on-device AI for speed and privacy.\n\n- Multi-Food Recognition: The AI is specifically trained to recognize and differentiate multiple food items on a single plate, a crucial feature for complex Sri Lankan \"rice and curry\" meals.\n\n- AI-Powered Portion Estimation: The app uses the device's camera to estimate portion sizes. This can be done either by using a common reference object (like a hand) or through an interactive on-screen guide, ensuring greater accuracy in nutritional tracking.\n\n- Comprehensive Sri Lankan Food Database: A pre-loaded, offline database contains hundreds of local dishes, ingredients, and packaged goods (e.g., Maliban, Munchee). This is a core differentiator, making the app highly relevant and practical for the target audience.\n\n- Personalized AI Coach: Leveraging the on-device AI's reasoning capabilities, the app provides private, personalized insights and suggestions based on the user's dietary history and health goals. For example, it might recommend a lighter dinner or suggest a high-protein local recipe.\n\n- 100% Offline and Privacy-First: The entire app, from photo analysis to data tracking and insights, functions completely without an internet connection. This is a fundamental promise, ensuring that no personal health data or photos ever leave the user's device.\n\n- Multi-Language Support: The user interface and all content, including food names and AI-generated insights, are available in Sinhala, Tamil, and English, making the app accessible to a broad demographic across Sri Lanka.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/H_cT23jciyI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/nutrition-snap"
    },
    {
        "project_title": "Zenvironment",
        "tagline": "A place for your family to recharge in nature",
        "authors": "Nitin Pasumarthy, AditiSResearch",
        "summary": "",
        "technologies": [
            "CUDA"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/2CrHQT-Nikg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/zenvironment"
    },
    {
        "project_title": "NHS Buddy - helping those who help us",
        "tagline": "An on-device medical assistant that supports doctors & hospital staff with administrative protocol, critical alerts and medical information",
        "authors": "SidRam, Adhisma Das, Adyasha Das",
        "summary": "We are a team of three members - two of whom have a background in AI Product and Engineering while the third member is a doctor working in the NHS, UK. As AI experts we have delivered a lot of value to customers through Consulting projects but always felt like there is so much more we could do with this wonderful technology to help people directly. We have always felt a strong desire to help those who help others - doctors, nurses, hospital staff. They are our modern heroes working tirelessly to save lives, often at the cost of their own desires and well-being. In our team, we have seen up close and experienced the the toll that such jobs can take - irregular hours, on-call shifts and working in a high-stress environment. We felt like if we could build something that makes the life of this one person a bit easier we would have achieved our goal. When the Gemma Impact Challenge came around, we saw a way to challenge ourselves to think a bigger and execute to a larger vision. We are proud of NHS Buddy - our submission to the hackathon and below is the story of how we built it and where we want to take it.\n\nNHS Buddy is an on-device AI medical assistant powered by Gemma3n, designed specifically for doctors and nursing staff in NHS trusts. Imagine the physical clipboards often placed beside each patient’s bed, replaced with an iPad or Remarkable-like device that ensures patient privacy while providing instant access to critical medical intelligence. It assists doctors in administrative tasks like preparing for handovers between shifts, creating case notes at the end of surgery but also supporting with access to critical medical information when required. This includes alerting nursing staff about critical allergies based on patient data as well as looking up British National Formulatory (BNF) recommendations on medicine dosage and counterindications. NHS Buddy aims to help those who help others - allowing them to finish their shift in time so that they can get home to take care of their own loved ones.\n\nWhile designing NHS Buddy we were inspired by several research papers but two of them stood out. The [Microsoft AI Diagnostic Orchestrator](https://microsoft.ai/new/the-path-to-medical-superintelligence/) (MAI-DxO) which allows doctors to leverage the expertise of different specialist consultants when analyzing a case as well as the [LLM on FHIR](https://arxiv.org/abs/2402.01711) approach suggested by Stanford University. The reality of the modern hospital environment is a combination of disparate systems that doctors must rely on to do their jobs. Looking up patient records requires access to specialized systems like EPIC or others, case summaries and medical dosage information is found via online resources like British National Formulary which is typically accessed via a Google Search on doctor’s personal devices. In the midst of an ongoing procedure, doctors must coordinate several systems to find the information they are seeking. In addition, several administrative tasks like handovers to the next shift staff, summarizing case records have to be done with attention to detail so that critical information is not left out. NHS Buddy aims to support doctors by taking away many of the administrative tasks, allowing them to focus more on their medical expertise and patient care.\n\nHere is the architecture diagram for our solution:\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F161914%2F3e33df6f5ffbc465846fc42b0ae454ff%2Farchitecture.png?generation=1754505249029139&alt=media)\n\nAt the heart of NHS Buddy is an intelligent Function Dispatcher that acts as Gemma3n's coordination layer, intelligently routing requests to the appropriate specialized modules. This ensures optimal performance by allowing each component to excel in its domain while maintaining seamless integration. The system supports advanced LLM function calling for more sophisticated understanding and responses. We have deliberately chosen to go with this modular solution where specific libraries are used for each function instead of integrating everything into Gemma3n. In our initial experiments we found that latency for vision and audio usage did not match what was expected by hospital staff often leading to delayed responses and frustration. This is the opposite of what we want to achieve with our solution. Therefore we start with a modular approach that has low latency and use it to gather feedback and data allowing us to perform fine-tunes later and build more of the functionality into the base model.\n\nHere is a highlight of the important modules in our solution repository:\n\n1. Fast Healthcare Interoperability Resources (FHIR) Integration - available in `backend/fhir_handler.py`. This module provides the capability to read from FHIR files - a typical industry standard for storing and exchanging patient records. It understands the format and provides functionality to filter for relevant information and provide it in the context to the LLM. We used the popular [Synthea](https://github.com/synthetichealth/synthea) library to create realistic but synthetic patient data that can be found in `/fhir`.\n2. BNF RAG System - implemented in `backend/rag_handler.py`. This provides the capability to answer questions about medical dosage, counterindications as prescribed by the BNF but available offline. It overcomes the constraints of needing access to WiFi which is often poor or unavailable within operation theaters. This has been implemented as a RAG solution which uses ChromaDB as the Vector Database. We do not provide the data with our repository as the files are large.\n3. OCR Module - Complete `/ocr/` package with EasyOCR integration. Fast and accurate with the aim of eventually collecting enough data that can be used to fine-tune Gemma3n and achieve the necessary latency.\n4. Voice/Speech - `/voice/` module with [KyutAI](https://github.com/kyutai-labs/delayed-streams-modeling) STT/TTS (`kyutai_stt.py`, `kyutai_tts.py`) This makes use of the best on-device real-time voice model which also accounts for interruptions.\n5. Gemma3n LLM - We have tested with `gemma3n:e2b` (base model) and `gemma3n:e4b-it-q8_0` (larger instruction-tuned model) and had no issues. With the smaller model, we experienced a higher failure rate for function calls. The detailed Ollama client integration can be found in `/llm/ollama_client.py`.\n6. Frontend Interfaces - We create a tablet UI (found in `/frontend/`) which is shown in the demo video and also a simpler testing interface (`simple_chatbot.py`) which we used to perform extensive testing.\n\nWhile the NHS Buddy works well already, it is currently designed to work on Mac and iOS devices that run MLX hardware. We found this to be the easiest way to test the demo and get feedback to iterate. The eventual goal is to make this run on any tablet device and start by rolling it out with one of the NHS Trusts in London.",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=iqFBM9bOMKU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/nhs-buddy-helping-those-who-help-others"
    },
    {
        "project_title": "AI FOR EVERYONE EVERYWHERE",
        "tagline": "Unlocking Dreams Through Edge-Powered Offline Learning",
        "authors": "Monica Susan",
        "summary": "This project is about something very simple — and very powerful: giving every child, no matter where they are in the world, the chance to learn. We believe learning is not just about books and exams. It's about dreams, purpose, and access. Today, technology has the power to make that happen — but the problem is, most AI solutions need internet, data, and infrastructure that many regions just don’t have.\nThe Gemma 3N -e2b-it model deployed on Google AI Edge that uses Edge TPU's instead of Cloud GPU's makes it the apt for offline, intelligent, adaptive learning.\nWe’re not just teaching kids facts. We’re helping them discover their purpose.\nWhether it's a child in South Sudan wanting to become a scientist, or a boy in a remote Philippine village dreaming of disaster relief, or a kid in rural Mexico hoping to fight environmental pollution — this system will hear them, respond in their local language, and guide them forward, offline, with no extra requirement than the device in their hands. \n**WHAT WE BUILT:**\n-A local learning engine that interacts with the user and uses a feedback loop to adapt questions, difficulty level, and encouragement style based on learner responses.\n-Integrated a quantized version of the Gemma 3N model for on-device inference.\n-Created simple pre-trained modules for voice-to-text, text understanding, and image recognition for multimodal interaction—all running offline.\n\n**Technical Overview**\nModel Selection:\nWe selected Gemma 3N -E2B-it model due to its smaller size (~2B parameters), efficient architecture, and support for multi-mixture routing (Mixture-of-Experts), enabling better task specialization.\nModel Optimization:\nWe used post-training quantization (via TensorFlow Lite tooling) to significantly reduce model size and make it compatible with edge devices while maintaining acceptable performance. This made the model lightweight enough for offline deployment.\nDeployment Compatibility:\nWhile not yet deployed, the system is designed to run on Raspberry Pi 5 or be integrated with Google AI Edge, making it suitable for remote or resource-constrained environments.\nOn-Device Capabilities:\nMultimodality (basic): Voice, text, and image inputs are processed using preloaded local modules.\nOffline Inference: No internet connection is required after installation.\nAdaptive Feedback Loop: Built-in logic to receive input from the learner (student) and adjust responses dynamically to promote personalized learning instead of static answer delivery.\n**Challenges We Faced & What We Learned**\nWe faced multiple constraints early on:\nSpeech recognition in noisy, offline setups\nPersonalizing content without internet access or cloud memory\nPower efficiency and memory limits on edge devices\nTo overcome this, we:\nUsed quantized models and model distillation for deployment\nBuilt a local feedback loop that uses edge-level short-term memory\nPreloaded essential training data for different learning modules (STEM, environment, health, ethics)\nWe simulated the deployment environment of the Google AI Edge TPU by using quantized models with TensorFlow Lite delegates (e.g., XNNPack and Edge TPU delegate) to benchmark expected on-device performance. This allowed us to estimate inference latency, memory footprint, and energy efficiency metrics in a local environment, closely mimicking real-world deployment.\n**🔧 Simulated On-Device Performance Metrics**\n(for quantized Gemma 3n-e2b-it  running on Google AI Edge TPU / simulated with TFLite delegates)\nMetric\t                                                    Estimated Value\t                                  Notes\t\t\nModel Size (post-quantization)\t                  9.8 MB\t                      After applying int8 dynamic range quantization + pruning\t\t\nInference Latency (text)\t              12–18 ms per inference\t      For short-form adaptive QA or response generation\t\t\nThroughput\t                                          50–65 inferences/sec          \tDepending on input length and pre-tokenized batching\t\t\nMemory Footprint (RAM)\t                     190–230 MB\t         Peak RAM usage during inference; optimized with static graph loading\t\t\nCPU Utilization (RPi5)\t                             ~30% avg\t               With Edge TPU delegate enabled; spikes during input/output processing\t\t\nPower Consumption\t                     ~1.2W additional draw\t   Estimated from Coral Dev Board benchmarks under similar conditions\t\t\nBoot-up Time (ready state)                \t~4 seconds\t                               After model load and interpreter initialization\t\t\nEnergy Efficiency\t                        ~92 inferences per Joule\t               Based on per-inference energy consumption simulation (~10.9mJ/inference)\t\t\n\t\t\t\t\n**What Makes This Project Special**\nInstead of asking kids to adapt to the AI, we made the AI adapt to them.\nEvery child has a voice. And for many, this AI might be the first time they feel heard and guided.\n\nThis is not just about fancy tech — it’s about equity, inclusion, and dreaming big. A child in a remote village can now say,\n\n\"I want to save the environment.\"\nAnd this AI will help them learn how.\n**Conclusion & Future Scope**\nWe are still in development, but the foundation is strong. The results so far prove that offline AI is not only possible but powerful, accessible, and transformative.\nFuture work includes:\nDeploying on Google AI Edge: Testing real-world performance and power usage under load.\nAdding More Languages: Using on-device language modules to support wider local language use.\nAdvanced Adaptivity: Fine-tuning Gemma using user interaction logs (differentially private) for better personalization.\nExpanding Multimodality: Using vision transformers for more complex image-based teaching modules.\nScaling with newer Gemma models as they become lighter\nPartnering with education NGOs for deployment\nThis is just the beginning.\nNo internet. No limits. AI for everyone, everywhere.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/iBrO5l--BGk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-for-everyone-everywhere"
    },
    {
        "project_title": "AgriGemma: A Multilingual, Multimodal AI Assistant for Sustainable Agriculture",
        "tagline": "Enabling farmers with offline-first, multilingual AI assistant that provides crop advice, plant diagnosis, and sustainable farming counsel.",
        "authors": "Muhammad Faheem, zeeshanx",
        "summary": "**AgriGemma: A Multilingual, Multimodal AI Assistant for Sustainable Agriculture**",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/faheemgurkani/smart-farm-assistant.git",
        "demo_video_url": "https://youtu.be/aQHUOFWuB7Y",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/agrigemma-a-smart-farm-assistant"
    },
    {
        "project_title": "Chop Chop Bus: Gemma 3n AI-Powered Inclusive Transit Assistant",
        "tagline": "Chop Chop Bus transforms transit queries into natural conversations powered by on-device Gemma 3n. No internet required for AI processing",
        "authors": "Manikandan S",
        "summary": "The Problem\nSingapore's public transport system serves over 5 million daily commuters across 5,200+ bus stops, each identified by cryptic 5-digit codes. While existing apps like MyTransport.SG and SG Bus provide real-time data, they follow traditional navigation patterns that create barriers for:\n-  **Elderly residents** struggling with complex app interfaces\n- **International tourists** unfamiliar with Singapore's transport ecosystem\n- **Visually impaired individuals** needing audio-first interactions\n- **Busy professionals** wanting quick, conversational access to transit info\n\nCurrent solutions require multiple taps, precise searches, and navigation through hierarchical menus - creating friction when you just need to know \"when's my bus coming?\"\n\nThis Solution: Conversational Transit AI\n**Chop Chop Bus** transforms transit queries into natural conversations powered by **on-device Gemma 3n.** No internet required for AI processing, ensuring privacy and accessibility anywhere.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/manikandan4/ChopChopBus_Gemma3n",
        "demo_video_url": "https://www.youtube.com/watch?v=DEtsky_I6vI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/chop-chop-bus-gemma-3n-ai-powered-inclusive-transi"
    },
    {
        "project_title": "Health Assistant",
        "tagline": "Finetune gemma-3n to boost performance and build Health Chabot",
        "authors": "hoanglvuit",
        "summary": "Our goal was to develop a chatbot capable of diagnosing dermatological conditions based on input images. In narrow domains like healthcare, general-purpose LLMs often underperform unless fine-tuned. Fortunately, dermatology datasets with image-label pairs are relatively accessible. Deep learning models excel at mapping input-output pairs, and LLMs can benefit similarly from such data. However, naive training on image-label pairs could cause the model to respond with overly brief outputs—missing the natural interaction style of a chatbot. That's where prompt engineering becomes essential. By augmenting training prompts, we preserve the contextual coherence of LLMs. This project tests whether fine-tuning with image-label data and prompt engineering allows a model to retain both accuracy and conversational naturalness.",
        "technologies": [],
        "source_code_url": "https://github.com/hoanglvuit/Finetune-Gemma-3n",
        "demo_video_url": "https://www.youtube.com/watch?v=rE5uK2Jmb_o",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/health-assistant"
    },
    {
        "project_title": "AutoVisionInspector",
        "tagline": "Gemma3n as an on-device damage assessor to streamline the car insurance claim process using visuals of the vehicle at the crash site",
        "authors": "Martins Awojide, GeorgeIgwegbe, vice_admiral_kurlekov",
        "summary": "**Introduction**\n\nIt is commonplace to have damage to property during natural disasters, especially to automobiles. Since disasters affect a large population at once, the insurance companies are usually inundated with motor insurance claims from customers. The bottleneck causes a delay in the claims process as insurance adjusters need to assess the damage, compare it with the previous state of the car, look up the best and cost-effective fixes and estimate the cost of said fixes.\n\nOur app leverages Gemma3n to provide detailed information about car damage and recommend fixes based solely on the car images at the site of a crash or accident. We achieve this by using a unified setup that combines an object detection model with the spatial understanding and broad context of a multimodal LLM. For this project, the object detection model used is the YOLOv11, and our VLM of choice is the Gemma3n model. Since this device used for inference is a major concern during natural disasters, due to power cuts, communication bottlenecks and so on, our app is deployed on consumer-grade GPUs (RTX 3090 and Macbook Pro M4) and NVIDIA Jetson devices (TX2 and Nano 4GB).\n\n**Approach**\n- Training of VLM (Gemma 3n) with CarDD dataset with Bounding Boxes embedded in the images\n- Finetuning the YOLOv11 model with CarDD dataset for damage type detection\n- Finetuning Gemma 3n with unsloth\n- Deployment on different devices\n\n**App Architecture**\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F828688%2F25265c1b75b47847877f95c86af7f605%2FAutoVisionInspector.png?generation=1754524268669197&alt=media)\n\nWe built the frontend using plain **HTML**, **JavaScript**, and **CSS**, enabling users to upload an image and interact with the backend systems.\n\n1. The uploaded image is first sent to the **Object Detection Backend** (Python), which processes the image and generates annotations (e.g., bounding boxes, detected parts).\n\n2. These annotated results are then passed to the **VLM (Gemma3n) Backend**, which performs deeper visual-language analysis.\n\n3. The final analysis output from the VLM is returned to the frontend for display.\n\nThis modular flow allows for clear separation between object detection and vision-language reasoning, while keeping the frontend lightweight and efficient.\n\n**How is Gemma 3n leveraged in this solution?**\n\nThe Gemma 3n model is used to assess the image of the damaged vehicle with positional information of the damage as context (which is provided in the form of bounding boxes by the YOLOv11 model). Based on this context, it provides general information about the damage on the vehicle, the severity, the recommended fix and the estimated time of fix for each damage which is decent information to determine the cost of fix depending on the car make, availability of parts and skilled labour in the vicinity. \n\n**Challenges you Overcame and Why your Technical Choices were the right ones**\n\nFinetuning the YOLOv11 model was seamless, but finetuning the Gemma3n model for vision capabilities was quite challenging, most especially merging the base model and the vision adapter led to multiple crashes of my RTX 3090, RTX A6000 and Tesla T4 instance on Colab due to massive CPU usage. The unsloth documentation provided some information about resolving this issue. A specific flag under the troubleshooting guide was used to quell the CPU usage issue. \n\nAnother challenge was the conversion of the merged model to the GGUF format for efficient deployment across several edge devices (MacBook Pro, Jetson TX2 and Jetson Nano). This required a lot of memory and the API provided by unsloth for conversion is not clear for GGUF format. Since this could not be resolved in time, we opted for llama.cpp for the conversion.\n\nWe noticed that after conversion with llama.cpp, the model does not support the vision capabilities, which could be a result of the vision projection layer being disabled, or in general, the feature after conversion is not yet implemented. We currently have the GGUF format deployed on TX2, Nano and MacBook Pro without the vision capability — just text completion.\n\nWe noticed that after conversion with llama.cpp, the model does not support the vision capabilities which could be as a result of the vision projection layer being disabled or in general, the feature after conversion is not yet implemented. We currently have the GGUF format deployed on TX2, Nano and Macbook Pro without the vision capability — just text completion.\n\n**Summary of Deployment**\n\n|  | Devices | Inference Engine | Vision Capability |\n| :---- | :---- | :---- | :---- |\n| Consumer-Grade GPU | RTX 3090 | Unsloth FastVisionModel | Yes |\n|  | MacBook Pro M4 48GB | HuggingFace Transformer | Yes |\n|  |  | Ollama | No |\n| NVIDIA Jetson Devices | TX2 | Ollama | No |\n|  | Nano 4GB | Ollama | No |\n\n**Result**\nThe demo videos for all deployments can be seen in the link below:\n- [Video1](https://www.youtube.com/watch?v=2nr59kP-Ano)\n- [Video2](https://www.youtube.com/watch?v=lZjY6w1PU30)",
        "technologies": [
            "Gemma 3n",
            "NVIDIA Jetson",
            "Ollama"
        ],
        "source_code_url": "https://github.com/gigwegbe/gemma3n-car-damage-competition/tree/main",
        "demo_video_url": "https://www.youtube.com/watch?v=2nr59kP-Ano",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/autovisioninspector"
    },
    {
        "project_title": "UrbanGro: AI-Powered Sustainable Farming Ecosystem",
        "tagline": "Empowering growers with Gemma 3n for climate-resilient, data-driven agriculture — from rural farms to urban balconies.",
        "authors": "ANN MARIA ROY, Bernard P S",
        "summary": "UrbanGro is a full-stack smart farming ecosystem designed to make agriculture sustainable, intelligent, and accessible. In India, where over 55% of the population relies on farming, climate change, poor soil quality, and water scarcity continue to threaten food security.\n\nTo solve this, UrbanGro integrates IoT-based monitoring, AI-powered plant assistance, and blockchain-enabled data logging — all in one platform. Our custom-built Tokenizer device tracks real-time grow parameters like temperature, light, and humidity, while securely logging this data on a blockchain. This ensures transparency, traceability, and paves the way for future data-based incentives.\n\nUsing Gemma 3n, our on-device AI assistant delivers real-time plant health insights, multilingual guidance, and offline support — empowering growers even in remote or low-connectivity areas.\n\nWhile our marketplace for organic produce is planned as a future expansion, UrbanGro already offers a robust foundation for climate-resilient farming, data ownership, and community-driven growth.\n\nUrbanGro isn’t just a project — it’s a leap toward decentralized, tech-enabled agriculture that truly works for everyone.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Bangalore-Microgreens/UrbanGro-codebase",
        "demo_video_url": "https://youtu.be/aX4B5q_w1PE?si=8SWRNA5uHQ_Vbevc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/urbangro-ai-powered-sustainable-farming-ecosystem"
    },
    {
        "project_title": "Berdaflex VideoScribe",
        "tagline": "AI-powered video processing pipeline that transforms video content into searchable, structured documentation.",
        "authors": "Siarhei Berdachuk",
        "summary": "Berdaflex VideoScribe CE - Technical Writeup",
        "technologies": [
            "CUDA",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/berdachuk/berdaflex-video-scribe-ce",
        "demo_video_url": "https://www.youtube.com/watch?v=yMCcv7nWg7c",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/berdaflex-videoscribe"
    },
    {
        "project_title": "Waryon - The Watchful Guardian",
        "tagline": "AI That Acts. Fully On-Premises for Maximum Privacy and Real-Time Protection.",
        "authors": "Hyndavi",
        "summary": "Waryon – The Watchful Guardian\nYour AI Guardian, Always On. Always Private.\n\nWaryon uses Google’s Gemma 3n multimodal AI technology to provide intelligent, on-device recognition of important safety events. All data processing is performed fully offline within the device, ensuring that no information leaves the premises, for strong privacy and security.\n\nDesigned for use in home and care settings, Waryon conducts real-time analysis to detect relevant situations quickly and accurately. Its fully on-premises operation removes dependency on external servers, enabling fast, autonomous protection with complete user control.\n\nWaryon establishes a high standard for trustworthy AI safety systems: efficient, reliable, and fundamentally respectful of privacy.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/CGF61G8slS0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/waryon-the-watchful-guardian001"
    },
    {
        "project_title": "EcoGuardián",
        "tagline": "An AI-powered assistant for understanding and protecting our planet's biodiversity.",
        "authors": "Kushagra Singh",
        "summary": "EcoGuardián is an AI-powered mobile application designed to democratize environmental conservation by placing powerful analytical tools directly into the hands of conservationists, researchers, and citizen scientists.\n\nPowered by Google's on-device Gemma 3n model, the app offers real-time, multimodal species identification using images of plants and animals or audio from bird calls. Its offline-first design ensures full functionality in remote locations without an internet connection, guaranteeing user privacy by processing all data locally. EcoGuardián also provides environmental health assessments, personalized conservation actions, and multilingual support, making it an accessible and essential tool for global conservation efforts.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/_F6gjfEroL8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ecoguardin"
    },
    {
        "project_title": "LingoLens: Spear phishing your language learning",
        "tagline": "LingoLens is an AI language assistant that translates, teaches, and personalizes learning through camera, stories, and flashcards.",
        "authors": "Eyad Salama, TheInvestigator (Merna Khalid)",
        "summary": "LingoLens is an innovative multilingual assistant specifically designed to support\nlanguage learners. The core functionality allows users to point their device's\ncamera at any object or text, receiving its name or translation in their target\nlanguage, accompanied by audio pronunciation. The assistant can also\ndynamically know what kind of topics you want to learn and make stories or make\nflash cards that you can interactively learn.\nLingoLens utilizes AI to act as a language learning assistant that helps users practice\nconversational skills, learn grammar, expand vocabulary, and interactively engage\nwith custom made lessons that ensures that it will be personal to what the user\nwants to learn.",
        "technologies": [],
        "source_code_url": "https://github.com/Merna-Khalid/LingoLens",
        "demo_video_url": "https://www.youtube.com/watch?v=4fYWz1iw2N0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/lingolens-spear-phishing-your-language-learning"
    },
    {
        "project_title": "HeartBridge",
        "tagline": "Lifetime companion to mitigate loneliness",
        "authors": "Viaan Shah",
        "summary": "Inspiration\n\nThe increasing issues of loneliness and isolation in today's fast-paced world inspired us to create Heartbridge. Many individuals, especially those living alone or going through tough times, lack meaningful conversations. We envisioned an AI-driven platform that could provide continuous companionship, support, and positivity through engaging conversations, helping users combat feelings of loneliness.\n\nWhat it does\n\nHeartbridge is an AI-powered application that generates endless, meaningful conversations between two virtual personas. These personas can converse on various topics, and the user can join in at any time, either through voice or by typing their inputs. The aim is to provide a comforting, non-judgmental space where users can feel less isolated and more connected through the power of conversation.\n\n\nHow we built it\n\nWe developed Heartbridge using Flask for the web application, Gemma 3n to generate dynamic and engaging dialogue, and gTTS (Google Text-to-Speech) to convert the generated conversations into speech. The application also incorporates speech recognition for users to interact with the AI using their voice. We integrated the project into a web interface using HTML/CSS and JavaScript for smooth user interaction, while threading helps keep the conversation continuous.\n\nChallenges We Ran Into\n\nSynchronizing conversations: Getting two AI personas to converse fluidly while allowing user interruptions was tricky.\nAudio Queue Management: We had to implement a robust queue to avoid audio overlaps and ensure responses were spoken in the right order.\ngTTS limitations: gTTS is not instant and occasionally fails on slow connections.\n\nAccomplishments That We're Proud Of\n\nBuilt a fully functional conversation engine with dual AI personas and user-injected input.\nAchieved clean audio streaming with queued TTS responses.\nDesigned it in a way that’s scalable for mental health chatbots, podcasts, or educational tools.\nBuilt a friendly, welcoming interface for users who may be emotionally vulnerable or simply curious.\n\n\nWhat We Learned\n\nHow to integrate Gemma in creative dialogue scenarios.\nHandling real-time user interruptions without breaking flow.\nDesigning thoughtful experiences using text + speech, not just chatbots.\nThe importance of ethical AI when dealing with sensitive topics like mental wellness.\n\nWhat's next for Heartbridge\n\nWe plan to expand Heartbridge capabilities by incorporating more diverse conversational topics and emotional intelligence features. We aim to improve the application's ability to detect a user's emotional state and adjust responses accordingly, making conversations even more personalized and supportive. Additionally, we are exploring deploying the project on more platforms to make it accessible to a wider audience.",
        "technologies": [
            "Flask",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Viyu12345/heartbridge-google-gemma3n/",
        "demo_video_url": "https://www.youtube.com/watch?v=jTeu9SCpu10",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/heartbridge"
    },
    {
        "project_title": "NyayaLM: Democratizing Legal Access in Nepal with Gemma 3n",
        "tagline": "A privacy-first Gemma 3n model that delivers accessible legal information in Nepali, running entirely offline to empower rural communities.",
        "authors": "Chhatramani Yadav",
        "summary": "NyayaLM - Democratizing Legal Access in Nepal with Gemma 3n",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/chhatramani01/NyayaLM_v0.5_Gemma3n4B-4bit",
        "demo_video_url": "https://youtu.be/sUEM3kxEYok?si=2FdnZs0V4vOCozRL",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/nyayalm-low-compute-legal-llm-for-nepal"
    },
    {
        "project_title": "TerraSeed: An Offline-First Multimodal AI for Site-Specific Reforestation",
        "tagline": "A system using the Gemma 3n model to synthesize visual (soil imagery) and auditory (soundscape) data for reforestation.",
        "authors": "Marcelo Sena",
        "summary": "% --- ABSTRACT ---\n\\begin{abstract}\n\\noindent High failure rates in reforestation projects are often due to a mismatch between species and local site conditions, a problem exacerbated in remote, offline environments. We address how an on-device, multimodal AI can provide real-time ecological analysis. We present TerraSeed, a system using the Gemma 3n model to synthesize visual (soil imagery) and auditory (soundscape) data. It generates actionable planting plans without needing cloud connectivity, ensuring privacy and utility. A case study demonstrates the system's ability to produce a detailed, scientifically-grounded plan from real-world inputs, showing the viability of this offline-first approach.\n\\end{abstract}\n\n% --- SECTION 1: INTRODUCTION ---\n\\section{Introduction}\nThe success of global reforestation efforts hinges on sapling survival, which is often compromised by a mismatch between the species planted and the site's specific micro-environment. While remote sensing provides macro-level data, field workers lack tools for immediate, granular analysis of soil and local conditions at the point of planting. This creates a critical gap between ecological knowledge and practical action.\n\nThis paper introduces TerraSeed, an AI-powered field guide designed to fill this gap. Operating entirely on-device, it leverages the Gemma 3n model to analyze environmental inputs and generate immediate planting recommendations. We detail its methodology, which synthesizes visual and acoustic data, present a case study demonstrating its output, and discuss the implications of a private, offline-first approach to ecological AI.\n\n% --- SECTION 2: SYSTEM AND METHODOLOGY ---\n\\section{System and Methodology}\nTerraSeed is designed as an offline-first mobile application. The architecture consists of a simple user interface for capturing image and audio inputs, which are then passed directly to the on-device Gemma 3n model. All processing occurs locally, ensuring user privacy and functionality without an internet connection.\n\nThe core of the system is a prompt-driven methodology for \\textbf{Holistic Ecosystem Assessment}. This structured process guides the AI through four steps:\n\\begin{enumerate}\n    \\setlength\\itemsep{-0.5em} % Reduce space between list items\n    \\item \\textbf{Visual Analysis:} Identify soil type, plant species, or signs of stress from the input image.\n    \\item \\textbf{Acoustic Analysis:} Identify bioacoustic signals and environmental sounds from the audio recording.\n    \\item \\textbf{Synthesis \\& Correlation:} Find causal links between the visual and auditory data (e.g., does the sound of wind explain the sight of a wilting leaf?). This is the key step for an accurate diagnosis.\n    \\item \\textbf{Diagnosis \\& Recommendation:} Based on the synthesized understanding, generate a single, actionable planting or maintenance plan.\n\\end{enumerate}\n\n% --- SECTION 3: RESULTS ---\n\\section{Results: A Case Study}\nTo evaluate the system, the model was provided with a digital image of loamy soil and an audio file of a temperate forest soundscape. The system generated the detailed, actionable plan shown in Figure 1.\n\n% --- FIGURE 1 ---\n\\begin{figure}[H] % Use [H] from float package for \"place exactly HERE\"\n    \\vspace{-1em} % Reduce space before the figure\n    \\centering\n    \\fbox{\n        \\begin{minipage}{0.9\\columnwidth}\n            \\begin{verbatim}\n#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/s6y1Udjg7DI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/terraseed"
    },
    {
        "project_title": "Focus AI TEACHER",
        "tagline": "Focus AI Tutor is an offline-first, multimodal educational assistant powered by Gemma 3n that helps learners in underserved regions study us",
        "authors": "Bahle Chonco",
        "summary": "📚 Focus AI Tutor: Offline-First AI Learning Assistant for Underserved Learners\nProject Description:\nFocus AI Tutor is a privacy-preserving, multimodal learning assistant built with Gemma 3n, designed to run entirely offline on low-resource devices like smartphones and tablets. It allows students—especially in rural or underserved communities—to interact with an intelligent tutor using images, voice, or text to get real-time academic help, without any need for internet connectivity.\n\nUsing on-device AI inference, learners can:\n\nSnap a photo of a math problem for instant step-by-step guidance.\n\nSpeak queries aloud and receive voice responses in their local language.\n\nAccess quizzes and revision materials preloaded on the app.\n\n💡 Key Technologies\nGemma 3n (4B/8B model) for on-device multimodal reasoning\n\nTensorFlow Lite for optimized local inference\n\nVoice, image, and text processing entirely offline\n\nMultilingual support, including African languages like isiZulu\n\n📌 Why This Matters\nBillions of learners face educational inequality due to connectivity barriers. This project shows how AI can be decentralized and brought directly to those who need it most—without compromising on capability or privacy.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/bS54HI-6wTk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/focus-ai-teacher"
    },
    {
        "project_title": "SailMate - A MultiModal Marine Assitant",
        "tagline": "A multimodal voice-and-vision-enabled chatbot designed to assist crews on fishing boats or voyaging vessels with organization, decision-maki",
        "authors": "Vicky258, Raghulgem7, SRIDHANWANTH, Rahul A N, Sudharsan Bdurai",
        "summary": "SailMate",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Vicky-258/TBD.git",
        "demo_video_url": "https://youtu.be/Ts7M0c76AFk?feature=shared",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sailmate-a-multimodal-marine-assitant"
    },
    {
        "project_title": "SmartMoney App ",
        "tagline": "Ai Mobile Money Personal Assistant",
        "authors": "juvet manga, Paul Nana Manga, Cesar Zinga, Zagor Zorn",
        "summary": "SmartMoney is essentially a simple tool to make tracking finances very easy for areas with mobile money like it is the case for most African countries. \nUnlike developed countries where banking systems are deeply \n\n**App Name:** SmartMoney.\n**Core Purpose:** A mobile application designed to help users effortlessly track and manage their mobile money transactions.\n**Key Innovation:** Leveraging Gemma 3n, a powerful on-device AI model, to intelligently parse raw SMS messages from mobile network operators and automatically categorize financial activities.\n\n\n2. Problem Statement: The Challenge of Mobile Money Tracking\n\nSmartMoney attempts to solve long-lasting problems that have existed ever since mobile money services arrived. Among the most painful we have the following:\n\n**Lack of Centralized Tracking:** Users often receive numerous SMS notifications for mobile money transactions (deposits, withdrawals, transfers, payments). These messages are disparate and not easily aggregated or analyzed.\nManual Effort: Manually tracking these transactions, calculating balances, or categorizing spending is tedious and error-prone, and difficult to keep the habit.\n**Limited Insights: ** Without proper aggregation, users lack clear insights into their financial habits, income, and expenses from mobile money.\n**Privacy Concerns:** Existing solutions tend to require sharing sensitive SMS data with third-party servers.\n\n\n3. Solution Overview: SmartMoney's AI-Driven Approach\n\nSmartMoney addresses the fore-mentioned problems in a very simple way.\nEach time the user opens the application, all the messages from the financial operators he received (since the last time he opened) are parsed and sent to Gemma 3n for treatment.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/juv85/SmartMoney",
        "demo_video_url": "https://www.youtube.com/watch?v=S0anUlw8kMA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/smartmoney-the-app-that-helps-you-better-manage-yo"
    },
    {
        "project_title": "ISABELLE: AI Assistant for Blind Users",
        "tagline": "Revolutionary offline Gemma 3n app: AI vision for blind community.",
        "authors": "Usham Kamaljit Singh",
        "summary": "ISABELLE: A Humble Step Toward a More Accessible World\n📺 Watch Demo | ⭐ View Code\n🌍 Empowering the Blind with Private AI Vision\nISABELLE is a fully offline, AI-powered vision assistant designed for the global blind community. Using Google’s Gemma 3n E4B multimodal model, the app provides scene understanding and object recognition — without requiring internet access, ensuring complete privacy, dignity, and independence for users.\nInspired by real-world needs and tested directly with members of the blind community, ISABELLE hopes to be a small but meaningful contribution toward inclusive AI.\n\n👁️ What ISABELLE Can Do (Fully Working)\n🎙️ Ask “What’s in front?” → Get an instant, spoken scene description\n📖 OCR Capability → Read medicine labels, signs, documents aloud\n🔄 Continuous Mode → Ongoing narration of the surroundings\n👐 Hands-Free Use → Voice command-based interaction\n📱 Object & Spatial Detection → Identify people, objects, and layout\nAll features are fully implemented, tested, and offline.\n🧠 Under the Hood — Technical Architecture\n✅ Gemma 3n E4B Multimodal AI (3GB)\nRuns 100% on-device\nProcesses camera frames, generates context-aware descriptions\nOptimized downsampling for faster inference (<2s)\nSmart prompting designed for screen readers\n✅ Android + Flutter Implementation\n// Gemma3nProcessor.kt (native inference)\nprocessImageWithPrompt() → optimizeImageSize() → generateDescription()\n// BlindHome.dart (Flutter UI)\nvoiceCommand → cameraFrame → model → TTS speech output\nPremium, accessible UI with animations\nText-to-Speech, haptic feedback, voice input\nOffline model auto-downloader (15 MB/s)\n📊 Performance Benchmarks\nFeature\tResponse Time\tAccuracy\nScene Description\t< 2s\t~92% relevant\nText Reading (OCR)\t< 1.5s\t~89% accuracy\nObject Recognition\t< 2s\t~87% accuracy\nVoice Commands\tReal-time\t~95% accuracy\nAll tests conducted without internet on Pixel 8 Pro (12 GB RAM).\n🎯 Real-World Use and Feedback\n✅ Field-tested with blind individuals at community centers\n✅ Voice-first design optimized for screen reader environments\n✅ No internet dependency = reliable in remote areas\n✅ Built from the ground up for real-world use, not just a demo\n🔧 Selected Code Snippet – Voice Trigger\nFuture<void> _processVoiceCommand(String command) async {\n  if (command.contains('what') || command.contains('describe')) {\n    await _speak('Let me look and tell you what I see.');\n    final description = await _objectDescriber.describeCurrentScene();\n    await _speak(description);\n  }\n}\n🔍 Privacy by Design, Accessibility at Heart\nNo cloud processing or telemetry — user's data never leaves the device\nSeamless screen reader compatibility (TalkBack tested)\nAdjustable TTS rate and tactile feedback\nBuilt specifically for users who rely on accessibility tools\n🧪 Technical Achievements\nIntegrated 3GB Gemma 3n model successfully in Android using MediaPipe Tasks\nAchieved near-instant inference (<2s) through image pre-processing\nNative Android + Flutter hybrid app with robust architecture\nComplete offline support with fallback handling\n🔭 Looking Forward — Our Roadmap\n🌐 Multi-language narration support\n🔊 Ambient sound classification for enhanced safety\n🏠 Smart home integration for connected accessibility\n🤝 Shared navigation and scene-sharing between users\n🏆 Why We Believe ISABELLE Demonstrates Gemma 3n's Promise\nISABELLE is a heartfelt attempt to demonstrate that advanced AI, when used responsibly, can meaningfully improve the lives of underserved communities. We hope this project shows how:\n✅ Gemma 3n can power practical, real-world accessibility solutions\n✅ On-device AI can protect privacy while enabling intelligence\n✅ Multimodal interaction can bridge gaps for the blind\n✅ Even solo developers can build solutions that matter\nThis is not just a prototype — ISABELLE is real, tested, and already changing lives.\nWith deep humility and great hope, I submit ISABELLE as a working example of what thoughtful AI can achieve.\n\n\n\n-------------------------\n🙏 Note on Current Scope\nISABELLE was originally designed to assist the blind, deaf, and mute communities through multimodal, privacy-first AI. While the blind-accessibility features are fully implemented and showcased in this submission, the components supporting deaf and mute users — including real-time speech transcription and gesture-based communication — are 98% complete but were excluded from this demo due to a final technical setup challenge that could not be fixed as the deadline approached!\nWe deeply believe in building for all underserved communities and are committed to completing and releasing these features shortly after the competition ends.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/ushamkamaljitsingh/isabelle-gemma3n",
        "demo_video_url": "https://youtu.be/IzhDpOID4hI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/isabelle-ai-assistant-for-blind-users"
    },
    {
        "project_title": "Waryon - The Guardian",
        "tagline": "AI that Protects",
        "authors": "Hyndavi Movva",
        "summary": "Waryon - AI that Protects",
        "technologies": [],
        "source_code_url": "https://github.com/Hyndavistfu/Waryon",
        "demo_video_url": "https://youtu.be/CGF61G8slS0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/waryon-the-guardian"
    },
    {
        "project_title": "AgriAid: Offline AI Assistant for Plant Disease Detection",
        "tagline": "An offline-first AI solution using Gemma 3n for real-time plant disease detection and farmer assistance.",
        "authors": "Perumal Pangala",
        "summary": "AgriAid is an AI-powered, offline-first plant disease detection system built using Gemma 3n and computer vision. The solution classifies tomato plant diseases from images and provides actionable responses without internet, ideal for rural or remote farming communities. It uses TensorFlow for model inference, Gradio for UI, and runs locally with minimal setup. The app helps farmers identify common tomato plant diseases like Yellow Leaf Curl Virus, Target Spot, and Two-Spotted Spider Mite, and suggests remedies or next steps. It’s lightweight, private, and accessible — making precision agriculture possible at scale.\n\nhttps://www.veed.io/view/c24a78aa-53a7-41c9-a279-99d3a28bdee0?panel=share\n\nFull Source Code & Model:\nhttps://drive.google.com/drive/folders/1-EVTMe0PpQJcUhXGvDvdLFD3X8__JvsV?usp=sharing",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/iFcqwdmnJ_E",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/agriaid-offline-ai-assistant-for-plant-disease-det"
    },
    {
        "project_title": "frnds4evr_",
        "tagline": "An attempt to increase positive interactions between strangers, hopefully turning into friendships",
        "authors": "Whirlwind_chaser",
        "summary": "frnds4evr is a lightweight, anonymous friendship app designed to combat loneliness and build positive connections over a structured 5-day journey. The app matches users anonymously and encourages daily interaction through compliments that will be suggested using Gemma 3n, encouraging users interaction to be positive only, and shared activities; such as sharing an incident when they felt a stranger's kindness, if they met someday how would they like to spend their day with each other, all while protecting identity until they choose to connect with each other. \n\nEach user begins by choosing a spirit animal that represents their vibe for the journey (so that they will still get to know more about each other, what vibes they prefer, through the animal they connect with the most), followed by a brief self-description. Before being matched, users consent to participate in a 5-day anonymous connection,where negativity and judgement is not allowed. At the end of the journey, both users can choose whether they’d like to \"shake hands\" and reveal their identities, but only if the feeling is mutual.\n\nIn the demo version, I have tried keeping things simple such as, only two spirit animals available, second day picture reveal plan is halted, which will start showing user's image instead of their spirit animal, and no login.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Youngwindchaser/frnds4evr",
        "demo_video_url": "https://youtu.be/KXXd11sd71I",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/frnds4evr"
    },
    {
        "project_title": "Edugem - Offline Education App for Students",
        "tagline": "EduGem is an offline-first AI tutor designed for students in low-connectivity or privacy-sensitive environments. Powered by Gemma 3n running",
        "authors": "Souparno Nag",
        "summary": "EduGem is an offline-first AI tutor designed for students in low-connectivity or privacy-sensitive environments. Powered by Gemma 3n running locally via Ollama, EduGem delivers grade-level topic explanations, interactive quizzes, and even textbook simplification — all without requiring internet access.\n\nOur mission is to close the education gap by enabling personalized, private learning experiences on any laptop or device, especially in underserved or remote regions.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/souparno-nag/gemma3n-educationbot.git",
        "demo_video_url": "https://youtu.be/0vyQVqBiBGc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/edugem-offline-education-app-for-students"
    },
    {
        "project_title": "Harassment Detection using Gemma 3n and Physiological Signals",
        "tagline": "A privacy-first offline web app that uses Gemma 3n to detect harassment through voice and biosignal inputs like stress, heart rate, and EDA.",
        "authors": "neha.sh0123@gmail.com",
        "summary": "🚨 Harassment Detection using Gemma 3n\nThis project uses Google's Gemma 3n large language model to detect harassment risk based on:\n\n📢 Voice input from the user\n\n❤️ Physiological signals: stress level, heart rate, temperature, and EDA (electrodermal activity)\n\nThe tool is:\n\n🧠 Smart: It uses prompt-based reasoning with Gemma 3n\n\n🔐 Private: It runs locally on-device\n\n📶 Offline-friendly: Doesn't need internet to function\n\n👩‍💻 Built with Flask, HTML/CSS, and Hugging Face LLMs\n\nThis project aims to support women's safety and works as an early warning system by analyzing subtle biosignals and tone of voice.",
        "technologies": [
            "Flask",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/abmta-neha/Gemma-Harassment-Detection",
        "demo_video_url": "https://youtu.be/1vuZO3FdtBw?si=bFdzuB7_5hymZJgq",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/harassment-detection-using-gemma-3n-and-physiologi"
    },
    {
        "project_title": "Prism - Accessibility Assistant for Neurodivergent Users",
        "tagline": "An offline-first accessibility assistant for neurodivergent readers.",
        "authors": "asharya, Ananth K S",
        "summary": "Document comprehension is a fundamental cognitive task in academic, professional, and everyday contexts. However, a significant proportion of individuals face persistent challenges in processing written information due to neurodivergent cognitive profiles. Neurodivergence encompasses conditions such as Attention-Deficit/Hyperactivity Disorder (ADHD), dyslexia, and autism spectrum disorders, which are associated with atypical cognitive processing patterns that can hinder sustained attention, decoding, and synthesis of information from text. \n\nEpidemiological studies indicate that dyslexia affects approximately 10–15% of the global population (Shaywitz, 1998), while ADHD is estimated to have a worldwide prevalence of ~5% in children and 2.5% in adults (Polanczyk et al., 2015). These conditions frequently co-occur, and are further linked to impairments in working memory, executive functioning, and reading fluency (Willcutt et al., 2010). As a result, neurodivergent individuals often experience disproportionate cognitive load when engaging with dense or lengthy documents. This can manifest as difficulty sustaining focus, challenges in extracting salient information, and reduced ability to construct coherent summaries of complex texts. \n\nExisting digital reading tools, while offering basic accessibility adjustments such as font resizing or text-to-speech, rarely address the deeper cognitive barriers to comprehension. Specifically, there is limited technological support for incremental semantic understanding, summarization tailored to cognitive profiles, and offline, privacy-preserving document analysis. This represents a substantial accessibility gap, particularly given that a significant proportion of academic and professional workflows rely heavily on document review and knowledge synthesis. \n\nTo address these challenges, we introduce Prism, an offline reader extension designed to facilitate document analysis and comprehension for neurodivergent users. Prism employs automated summarization techniques using Gemma3n and adaptive information structuring to reduce cognitive load, enabling users to incrementally access and retain key insights from text. By operating locally, the system ensures user privacy and offers consistent functionality independent of network availability. The goal of Prism is not merely to reformat text visually, but to fundamentally transform the reading experience by aligning document presentation with neurodivergent cognitive processing needs.",
        "technologies": [],
        "source_code_url": "https://github.com/ksananth4424/gemma-3n-hackathon",
        "demo_video_url": "https://youtu.be/L8wwJ4wSGbI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/prism-accessibility-assistant-for-neurodivergent-u"
    },
    {
        "project_title": "PoBot: AI-Powered Worker Voice Tool for Corporate Supply Chain Due Diligence",
        "tagline": "PoBot is an AI-powered worker voice tool that helps report labor abuse and gives NGOs/brands a dashboard for real-time supply chain insights",
        "authors": "Salma Khaled",
        "summary": "**PoBot** is an AI-powered worker voice tool designed to amplify the experiences of migrant and supply chain workers by enabling them to safely report labor rights violations and access legal guidance. Built to address the systemic silencing of vulnerable workers, PoBot offers a multilingual, intelligent, and accessible platform that guides individuals through the process of documenting and reporting exploitation. At the heart of the system is a chatbot powered by Google’s Gemma 3n model, which conducts natural, context-aware conversations in multiple languages. Through a graph-based (mind map) system, the bot helps workers navigate complex legal topics, identifies policy violations based on company-specific rules, and generates structured, legally grounded reports that can support formal complaint procedures.\n\nComplementing the bot is an interactive dashboard designed for brands, NGOs, and civil society organizations. This dashboard provides real-time insights into anonymized worker reports, offering detailed analytics on policy violations, factories, industries, and geographic patterns. Stakeholders can access session-level case details, view links between factories and global brands, and consult relevant corporate policies. Together, the chatbot and dashboard form a feedback and accountability loop—transforming grassroots worker stories into structured data that informs systemic action and promotes greater transparency across global supply chains.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/MigrasiaGlobal/AI-Powered-Worker-Voice-Tool-for-Corporate-Supply-Chain-Due-Diligence",
        "demo_video_url": "https://www.youtube.com/watch?v=z_FbtR4LrDY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/pobot-ai-powered-worker-voice-tool-for-corporate-s"
    },
    {
        "project_title": "ReadShift: Shifting Reading Frustration into Confidence",
        "tagline": "ReadShift uses assistive AI to shift the frustration of reading with Dyslexia into confident, independent learning.",
        "authors": "NiKole \"Technikole\" Maxwell",
        "summary": "💭Imagine this: Malik, a bright kid who loves building things with his hands and can explain how a car engine works better than most adults I know. But every time reading time comes around, he freezes. Letters jump around the page. Words just don’t stick. And it’s not because he isn’t smart, Malik just learns differently. Malik has Dyslexia, he doesn't like to read because the words always jump around. Its like they don't want to be read.  Malik shuts down and stays frustrated because he doesn't feel like the other kids. Malik needs learning tools that help support his journey. \n\nWe built ReadShift™️ to break the cycle of frustration that leads kids away from reading rather than learning to enjoy it.  This is It's not just another app; it’s an answer to a fundamental question: What if we designed a tool that works for kids like Malik, not against them? What if we could help Malik and all of the other kids struggling to find a solution to learning how to read.\n\nReadShift is a game changer. It’s built for kids who need real support, not just another flashy app. \n\nHere’s what makes it different:\n\n- Talk, Don’t Type – Kids can learn by speaking and listening. No typing, no reading big blocks of text before they’re ready.\n\n- Works Anywhere – Even without Wi-Fi, our kids can keep learning.\n\n- Instant Help – The app talks back, guiding them step by step, so they never feel stuck.\n\n- Fun & Engaging – Bright colors, interactive games, and encouraging voices make learning feel good again.\n\nPicture this: Malik opens ReadShift, pastes in a part of his story, and hears it read out loud. Then, instead of staring at words he can’t decode, he practices speaking. If is isn't sure about one of those tricky words in the sentence, he can press the button to request a definition or for it to be spelled out loud.  His confidence? Through the roof. Now, when it’s time to read in class, Malik is confident with his ReadShift buddy Nyla, by his side. \n\nReadShift is built by Black Women in Technology who know what our communities need: tools that work, no matter your zip code, no matter your struggles. Every kid deserves a shot at learning and winning.\n\n🎯  Our Vision\n\nReading is a gateway to learning, but for millions of children with dyslexia, it's a daily source of frustration and anxiety. Traditional learning tools  are embarrasing,, often fail to accommodate unique needs, and may lead to a widening confidence gap. The challenge is to create a tool that is not only effective but also **private, accessible, and offline-first - it works without an internet connection**— this is crucial for equitable learning in all environments, across neighborhoods, and in spite of devices (#coming-soon)\n\nOur vision is to transform reading from a barrier into an adventure. We aim to build a patient, AI-powered reading companion that empowers children by giving them tools to overcome challenges independently, fostering confidence and a lifelong love of reading.\n\n💡 Our Solution: ReadShift\n\nReadShift is an offline-first, privacy-focused AI application designed for children aged 6-12. When a child encounters a difficult word or sentence while reading a physical book, they can use the app to:\n\n**Read Aloud:** The app uses text input or camera (roadmap) to capture the text. Our custom AI is named Nyla, she is a custom model created from Google Deepminds' Gemma3n model powered by Ollama for offline connection. **\"Nyla,\"** processes and cleans the text, which is then spoken aloud by a clear, child-friendly voice via **Piper TTS**.\n**Define:** The child can ask for the definition of a tricky word. Nyla provides a simple, age-appropriate explanation, which is then spoken aloud.\n**Spell:** The child can ask for a word to be spelled out, helping them make the connection between letters and sounds in a multi-sensory way.\n\nThroughout the experience, fun emoji animations provide positive reinforcement, making learning feel like a game.\n\n👋 Nyla\nMeet Nyla 🤖 Meet Nyla, the heart of the ReadShift experience. Nyla is our custom-built AI reading companion, created from Google Deepmind's powerful **Gemma3n** model and brought to life using a specialized Modelfile within Ollama. We've designed her to be a patient, supportive, and friendly guide for children. When a child encounters a difficult word, Nyla can read their text input, provide a simple, age-appropriate definition, or spell the word out clearly. We have carefully programmed her with a child-safe persona, ensuring she never uses confusing formatting and always stays focused on the task of reading. Nyla’s purpose is not just to provide answers, but to build confidence and turn the challenge of reading into a joyful adventure.\n\nNyla in Action:\n- When you click \"Read Aloud\":\nNyla's Task: We send your raw text to Nyla and ask her to \"read it.\"\nPurpose: Her main job here is to act as a text sanitizer. Based on the rules in our Modelfile, she removes any markdown (like **) and introductory phrases (\"The text says...\") and returns only the clean, core text. We then send her clean response to Piper to be spoken.\n- When you click \"Define\":\nNyla's Task: We send her a single \"tricky\" word (that our simple placeholder function picked) and ask her to \"define it for a child.\"\nHello\nPurpose: She acts as an intelligent, age-appropriate dictionary, creating a simple definition from scratch.\n- When you click \"Spell\":\nNyla's Task: We send her a single \"tricky\" word and ask her to \"spell it.\"\nPurpose: She generates a natural-sounding spelling response (e.g., \"E. N. O. R. M. O. U. S. Enormous.\"), which is more engaging than just splitting the word with JavaScript.\n⚙️ Technical Depth & Execution\n\n\nCore Philosophy: Offline-First & Privacy-Centric\nEvery technical decision was driven by the need for privacy and offline functionality. By running all AI and TTS processes locally, no user data ever leaves the device. This is essential for a COPPA-compliant children's application.\n\n\nTech Stack\n\n**AI Model:** Google's **Gemma 3n** (`gemma3n:e4b` base), customized with a `Modelfile` to create our persona, \"Nyla.\"\n**AI Server:** **Ollama**, allowing us to run Gemma 3n entirely on-device.\n**Text-to-Speech:** **Piper TTS**, for its high-quality, offline-first neural voices.\n**Frontend:** Vanilla **HTML, CSS, and JavaScript**, ensuring maximum performance and no complex dependencies.\n**Styling:** **Tailwind CSS v4** (CLI), for a modern, utility-first design system.\n**Local Server:** A lightweight **Node.js/Express** server to act as a bridge between the browser and the Piper TTS command-line tool.\nChallenges Overcome\nThe primary challenge was bridging the gap between a browser-based application and local command-line tools (`piper.exe`). We solved this by creating a minimal Node.js server. We also fine-tuned the `Modelfile` for our Gemma model (\"Nyla\") to prevent markdown output and keep responses on-topic and child-safe.\n\nCOMPETITIVE TRACKS\nWe came to win.\n\n🏆 Competing for the Overall Track (Grand Prize)\nReadShift was conceived as a direct answer to the Gemma 3n Impact Challenge's central question: how can a private, offline-first model make a tangible difference in people's lives? We built ReadShift not just as an application, but as a piece of assistive technology designed to empower one of the most vulnerable learning populations: children with dyslexia. Our commitment to an offline-first architecture is central to our mission. It ensures that ReadShift is a reliable tool for any child, anywhere, regardless of their internet access. This design choice directly tackles the digital divide and transforms ReadShift into a truly equitable tool for global accessibility. This isn't just a concept; it's a functional, end-to-end prototype that demonstrates the power of on-device AI to build confidence, foster independence, and change a child's relationship with reading from one of frustration to one of joy and discovery.  Its also a great resource for parents, we pooled some of the best resources for parents into our community hub so that enablement happens at every level.\n\n🏆 Ollama Competition Category\n**Ollama was the cornerstone of our project.** Without it, our vision for a truly private, offline-first AI application would not have been possible. Ollama made it incredibly simple to download, manage, and serve our custom `gemma3n` model locally. By creating a custom model named `nyla:latest` with a `Modelfile`, we leveraged Ollama's core features to bake in a child-ready persona and specific behavioral rules, showcasing how easily Gemma3n can be adapted for specialized, real-world tasks. This project is a direct demonstration of Ollama's power to democratize on-device AI.\n\n\n🤝 The Team\nReadShift was built by a passionate team from Black Women in Technology. You can learn more about the contributors on our full \"About\" page. Meet the team here.\n\n🔗 Links & Resources\nVideo Demo Link: https://youtu.be/y6hVFpfWHwo\nPublic Project Link (Live Demo): https://readshift.app/read\nPublic Code Repository: https://github.com/BlackWomeninTech/READSHIFT/tree/readshift-c",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/BlackWomeninTech/READSHIFT/tree/readshift-c",
        "demo_video_url": "https://youtu.be/etfviYWt140",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/readshift-enabling-kids-with-dyslexia-to-read-with"
    },
    {
        "project_title": "I read the news today, oh, boy",
        "tagline": "Sagredo, Salviati and Simplicio in the 21st century",
        "authors": "LuísCarlosSilvaEiras",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": " https://youtu.be/yiuP9dYBD90",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/i-read-the-news-today-oh-boy"
    },
    {
        "project_title": "ADA One which stands for AI on Device Asset (A.D.A)",
        "tagline": "A prototype that solves not one, but several real-world problems using resource-constrained computing with Google Gemma 3n.",
        "authors": "Joseph Petersonn",
        "summary": "**1. Problem Definition and Context**\nProject idea/Solution, Context and Motivation: Living in my country, I discovered that I always had to surmount diverse, complex interconnect challenges that constantly tried to box me in despite being knowledgeable, and with IT, AI, programming skills, now imagine for those who are not. I tried several times to solve these recurring bull-dog challenges, yet even with money to spend on solving challenges like power and electricity, data outages (around Q2-Q4 2024 they said the inter-continental data deep sea cable was damaged etc.), these challenges barked with ferocious tenacity.\n\nThis is where I went on this adventure to architect, develop, package, make available for myself and others like me, a unique solution to solve insistent challenges that keeps us from enjoying the services of advanced computing and AI, which can turn our lives, businesses, careers around, and make us live a life of wellness and productivity. This was when I stumble upon Gemma 3n on the Deep Mind X (formerly Twitter) Account.\n\n**1.2. Constraints and Challenges**\nWe encountered challenges like outages of power supply which we had to acquire backups like Tabs which use low power and can be charged with portable power banks, relocation our development office to where power was currently available because we do not think our caretaker will allows us pin solar panels to the roof of his building. \nWe encountered challenges like data, AI compute, and connectivity challenges. We subscribed to two telecom data providers, so we can switch provided when one fails, which happened a lot.\n\n**1.3. Design Approach and Alternatives Considered**\nIn addressing the challenge, I adopted for a design-first mindset rooted in solving real-world problems within the African context, where computing power, internet access, and power resources can be highly constrained. Early in the planning phase, I evaluated various architectural models and development paths, particularly those relying on commercial large language model (LLM) providers or always-online cloud services.\nHowever, the cost implications, the steep learning curve, and the infrastructure demands of cloud-based AI solutions made such options less viable for long-term, sustainable impact—especially when targeting offline or low-power use cases across regions with limited connectivity or expensive mobile data.\nAs a result, I shifted toward a mobile-first and offline-capable architecture. My goal became to build a lightweight, resource-efficient AI system that could run directly on-device, with minimal reliance on cloud-based computation. This decision aligned with a broader principle of deploying edge AI solutions that are energy-efficient, responsive, and scalable in environments with limited infrastructure.\nAfter researching available models, I stumbled upon, and eventually selected Google's Gemma 3n as the core of my AI implementation. It offered an excellent balance of size, speed, and performance, optimized for local execution without the need for high-end hardware or external GPU servers. This small-scale LLM proved especially well-suited to mobile and edge contexts, enabling AI functionality while keeping power and data usage minimal.\nOverall, the final design emerged as a smart adaptation to the realities of building AI-driven tools in developing or resource-limited environments—delivering meaningful functionality without compromising on accessibility or efficiency.\n\n** 2 Project idea/Solution, Context:**\nI present to you ADA One an AI on Device Asset (A.D.A) that runs on your mobile devices without the need to connect online. It is an AI LM for edge devices, designed for offline-first systems, decentralized compute, that solves the constraints like:\n2.1. Inconsistent Power Supply: Our solution is an energy-efficient tech, because it has low power consumption.\n2.2. Little Availability or no Internet Connectivity: Our ADA One solution does not need depend on internet connectivity for it operate or function.\n2.3. LLM API Credits: It does not need to connect to AI LLM providers like OpenAI, Google, Anthropic, Cohere, Mistral, Meta etc. The main point here is that it saves you money.\n2.4. Data and Information Privacy: ADA One is an on-device or mobile-first solution, which means it resides solely on your device without the need to send/share your private data with eternal serves anywhere.\n2.5. Lack of Advance AI Compute: AI can infuse, kick-start, empower, drive, Sustain, professionalize your skills, work, business, career, education, wellness, productivity etc. This is what ADA does. We handout a unique, powerful solution/platform into your hands, which loos so small, however it is a world of power, possibilities, productivity, wellness etc. Please use it responsibly.\n2.6. Who is for/Wide Areas of Application: Whether you have been waiting for a unique, powerful, rugged, sophisticated AI compute solution to enhance your skills, work, business, career, education, wellness, productivity etc. then know that ADA One is packed, stuffed with prebuilt Experiences tailored for several aspects of “Daily Life” which include but not limited to areas of healthcare (including mental health), education (ADA One is a personal tutor/coach which can even teach you how to code, smart cities (tutorials and appreciation), fintech (tutorials and appreciation), climate (tutorials and appreciation), entertainment (interactive entertainment sessions), agriculture (tutorials and appreciation), gaming (tutorials and appreciation), business and finance (tutorials and appreciation), and beyond\n\n---\n**What we Built (Deeper Report):**\nA. Enhance Accessibility: Build tools for real-time translation or transcription for the hearing-impaired, or visual description apps for the blind.\nB. Revolutionize Education: Create interactive, offline-ready learning experiences for students in low-connectivity regions.\nC. Improve Health & Wellness: Develop on-device apps that can provide mental health support through voice analysis or act as a personal wellness coach.\nD. Promote Environmental Sustainability: Design an app that uses image and audio recognition to identify local plant diseases, track biodiversity, or promote recycling.\nE. Aid in Crisis Response: Build tools that can operate offline to provide critical information or facilitate communication during natural disasters.\n\n---\n**2.7 Enhanced Accessibility:** ADA is bundled with built-in tools I call “ExperienceAI” (Camera Mode) which lets you use your camera to get real-time interactive AI assistance with visual content experience with visual descriptions and audio feedback, all you need to do is to point your camera at text, objects, or scenes for instant \n\n---\n**2.8\tRevolutionize Education:** ADA is bundled with built-in tools for education which I call “Teacher-Student Tools” perfect for educational settings, it is for offline-ready learning experiences for students in low-connectivity regions.\n**For Teachers:**\n- Create virtual classrooms with access codes\n- Distribute assignments and materials\n- Monitor student progress\n- Share ExperienceAI analyses with the class\n- Manage multiple students simultaneously\n\n**For Students:**\n- Join classrooms with access codes\n- Receive assignments and resources\n- Submit work and get feedback\n- Collaborate with classmates\n- Access shared learning materials\n\n**Collaboration Tips**\n- Check your privacy settings before sharing\n- Use classroom mode for educational activities\n- Share ExperienceAI results for group learning\n- Monitor active sessions in the Sessions tab\n\n---\n**2.9\tImprove Health & Wellness:** ADA is bundled with built-in tools for Health & Wellness, and Productivity which I call “ADA” the AI Persona that is vast in several areas of Health & Wellness, and Productivity.\nAI Personas\n\n**What are Personas?**\nPersonas are different AI personalities you can interact with, each with unique traits and expertise areas.\n\n**Ada - Your Default Persona**\n- *Personality: Confident, friendly, nurturing\n- Expertise: Healthcare, education, programming, business, science\n- Best For: General assistance, learning, problem-solving\n\n**Creating Custom Personas**\n1. Tap the Personas section in the menu\n2. Select \"Create New Persona\"\n3. Customize:\n   - Name and Description\n   - Personality Traits (confidence, friendliness, formality, etc.)\n   - Expertise Areas and specializations\n   - Communication Style\n\n**Switching Personas**\n- Use the persona selector in the top menu\n- Your conversation context is preserved when switching\n- Each persona adapts to your selected topic area\n\n---\n\n**2.10 Chat Mode**\n\n**Basic Chat**\n1. Type your message in the input field\n2. Tap Send or press Enter\n3. Ada responds based on her personality and expertise\n\n**Niche Selection**\nChoose from specialized chat areas:\n- Healthcare - Medical guidance and wellness support\n- Education - Tutoring, learning assistance, creative writing\n- Programming - Code help, debugging, best practices\n- Business - Analysis, strategy, professional guidance\n- Science - Research assistance, explanations\n- Entertainment - Creative projects, fun interactions\n\n**Chat Tips**\n- Be specific about what you need help with\n- Ask follow-up questions for deeper understanding\n- Use natural language - Ada understands context\n- Switch niches anytime to get specialized help\n\n---\n**2.11\tPromote Environmental Sustainability:** ADA is bundled with built-in tools for real-time camera interaction for Science - Research assistance, explanations that uses image and audio recognition. SO, it is to be used identify local plant diseases, track biodiversity, or promote recycling.\n\n**Getting Started with Camera Mode for Science - Research assistance, explanations**\n1. Tap \"ExperienceAI\" in the main menu\n2. Select your niche (Education, Programming, Science, etc.)\n3. Allow camera permissions when prompted\n4. Point your camera at what you want to analyze\n\n**Analysis Types**\n- OCR (Text Recognition) - Read and understand text from images\n- Object Detection - Identify objects, people, and items\n- Scene Understanding - Analyze environments and contexts\n- Tutorial Mode - Step-by-step guidance for learning\n\n**Visual Overlays**\n- Green boxes around detected text\n- Blue highlights for important objects\n- Labels and annotations with confidence scores\n- Interactive elements you can tap for more info\n\n**Audio Feedback**\n- Ada speaks her analysis results\n- Toggle audio on/off with the speaker button\n- Adjust volume and speech settings\n- Works great for accessibility\n\n**Camera Tips**\n- Ensure good lighting for best results\n- Hold steady for a moment during analysis\n- Try different angles if results aren't clear\n- Use tutorial mode when learning new topics\n\n---\n\n**2.12 Content Sharing Hub**\nAccess sharing features through the “Collaboration” tab:\n- Share - Send content to others\n- Sessions - Manage active collaborations\n- Devices - See connected devices\n- Stats - View sharing statistics\n\n**Sharing Methods**\n- QR Codes - Generate codes for easy sharing\n- Direct Links - Share via messaging apps\n- File Export - Save content to device\n- Peer-to-Peer - Direct device-to-device sharing\n- WiFi Direct - Local network sharing\n- Bluetooth - Close-range sharing\n\n---\n**2.13 Aid in Crisis Response:** ADA One is has built-in tools that can operate offline to provide critical information or facilitate communication during natural disasters. The same way the ExperienceAI and Improve Health & Wellness modules are applied, is the same way to apply it to get Aid in Crisis Response.\n\nAda One has a lot more please check our uploaded files for a USER_GUIDE.MD which contains the full details of what ADA One can do.\n\n---\n**3. Technologies and Tools Used**\nThe solution was developed using a carefully selected stack of technologies optimized for mobile-first, offline-capable AI applications in low-resource environments. Each tool and framework was chosen based on its ability to support efficiency, scalability, and adaptability across Android and iOS platforms.\n\n**AI Model: Google Gemma 3n**\n•\tReason for Selection:\nGemma 3n is a compact, efficient large language model specifically optimized for low-power devices and edge inference. Its small footprint and strong vision-language capabilities make it ideal for on-device intelligence without relying on constant internet access.\n•\tUsage:\nPowers OCR, object detection, scene understanding, contextual prompts, and persona-aware feedback across all core modules, especially the ExperienceAI and Tutorial systems.\n\nReact Native + Expo\n•\tReact Native:\nChosen for its cross-platform capabilities and strong community support, React Native enabled rapid mobile development for both Android and iOS using a single codebase.\n•\tExpo SDK (~53.0.20):\nUsed to streamline the development workflow with powerful tools for camera access, file management, haptics, sensors, and deployment. Expo also provided built-in testing and packaging utilities that accelerated iteration and reduced complexity.\n\nTypeScript\n•\tReason for Selection:\nAdopted for type safety, better code organization, and predictable state management across a growing and modular codebase.\n•\tUsage:\nEnforced interface contracts for personas, camera analysis, classroom data, and transfer operations, leading to more robust integration between components.\n\nReact Native Camera & Image Picker\n•\tCamera Module:\nEnabled real-time vision processing for object detection, OCR, and environmental analysis.\n•\tImage/Document Picker:\nProvided intuitive access to files from the gallery, camera roll, and local storage, used in PeerMediaSharing and Resource Upload tools.\n\nText-to-Speech (TTS) and Haptics\n•\tText-to-Speech:\nUsed to deliver persona-aware, natural-sounding audio feedback based on analysis results. The TTS system adjusts voice parameters like pitch and speed based on the persona's traits (e.g., nurturing or confident).\n•\tHaptics:\nImplemented via platform APIs for vibration feedback tied to confidence levels and interaction types, enhancing accessibility and real-time user engagement.\n\nPeer-to-Peer Technologies\n•\tWi-Fi Direct & Bluetooth APIs:\nPowered the PeerMediaSharing component to allow offline media exchange between users without needing internet connectivity. Designed with native permissions and error handling for reliability in real-world usage.\n\nPersona System Engine\n•\tCustom Engine:\nDesigned to inject personality, domain expertise, and communication style into the AI feedback loop. This system drives both analysis interpretation and tutorial interaction, enriching user experience.\n\nRole-Based Education Framework\n•\tBuilt on React + Hooks Architecture:\nThe TeacherStudentTools module uses advanced React patterns, including role-based state management, modal workflows, and conditional rendering, to deliver a responsive and scalable learning management interface.\n\nTesting Tools\n•\tUnit Testing & Mocking:\nComprehensive test coverage was applied using built-in test runners and mocking frameworks to validate component behavior, especially in permission-sensitive, UI-dynamic, and asynchronous workflows.\n\nSecurity and Privacy\n•\tEncrypted Storage:\nUsed for storing sensitive healthcare and wellness data offline, ensuring privacy even in disconnected environments.\n•\tRole-Based Access Control (RBAC):\nEnsured that teachers, students, and collaborators have distinct permissions and access boundaries.\n\nPerformance Monitoring\n•\tCustom Metrics Hooks:\nBuilt-in hooks track analysis duration, confidence scores, and persona effectiveness, enabling insights into how different configurations perform under real-world conditions.\n\nTogether, this stack enables the delivery of a robust, privacy-conscious, and intelligent mobile platform that operates efficiently in environments with limited data, connectivity, or compute power. Every tool was chosen not just for its capability, but for how well it aligns with the principles of accessibility, adaptability, and real-world usability across the African tech landscape.\n\n---\n**4. Performance Evaluation and Benchmarks**\nAlthough final performance validation is in progress, a comprehensive testing and benchmarking plan has been outlined to ensure reliability, responsiveness, and quality across the entire platform. The goal is to deliver a seamless experience on low-power devices, especially in environments with limited connectivity and computing resources.\n\nTesting Strategy\nThe performance evaluation is divided into three major phases:\n\n**4.1. Unit Testing for Enhanced Features**\nA full suite of unit tests is being implemented for all new services and components, including:\n•\tPersonaManager & ExperienceAIService — verifying persona state logic, camera analysis behaviors, and data handling.\n•\tCollaboration Services — ensuring consistent peer-to-peer transfer flow and session state management.\n•\tCamera Processing Engines — testing core vision capabilities including object recognition, scene understanding, and error handling.\n•\tEdge Case & Error Tests — validating responses under unusual or failure conditions.\nTarget: >90% code coverage across all enhanced modules.\n\n**4.2. Integration & End-to-End Workflow Testing**\nMulti-module workflows will be tested to confirm seamless interactions and proper system behavior, including:\n•\tPersona-Camera-Collaboration Pipeline — validating context-aware analysis across user roles and interaction types.\n•\tTeacher-Student Assignments — testing full classroom lifecycle from creation to submission and feedback.\n•\tPeerMediaSharing & Cross-Device Workflows — ensuring media exchange reliability and permission compliance.\n•\tAccessibility & Usability Audits — evaluating voice, visual, and haptic feedback effectiveness across devices and use cases.\nTarget: Full workflow validation across supported platforms (Android/iOS), with consistent behavior in both online and offline states.\n\n---\n**5.3. Final Integration & Deployment Validation**\nA final polish phase will integrate all enhanced features with the core application foundation:\n•\tSystem-Wide Workflow Testing — validating user journeys from navigation to feature completion.\n•\tProduction Build Performance Testing — confirming app startup time, memory usage, and frame rates on low- to mid-tier mobile hardware.\n•\tQuality Assurance & Monitoring Setup — analytics tools will monitor feature usage, error rates, and real-world performance across sessions.\n•\tDeployment Readiness — preparing documentation, walkthroughs, and internal guides for future contributors and scaling.\nTarget: Stable deployment with verified requirements integration, performance baselines, and maintainability documentation.\n\n**Initial Observations (Preliminary Benchmarks)**\n•\tPersona-Aware Camera Response: Prototype testing shows sub-1 second response times for OCR and object detection on mid-tier Android devices.\n•\tTTS and Haptics Feedback Latency: Audio and haptic feedback modules respond within ~200–300ms following analysis events.\n•\tPeer Transfer Speeds: Preliminary Wi-Fi Direct tests achieved 3–5MB/s throughput, sufficient for rapid document and image sharing offline.\nFull benchmarking reports and optimization insights will be included in the post-deployment analysis.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/PetersonnOne/AdaOneV1",
        "demo_video_url": "https://www.youtube.com/watch?v=QBx3U3-P7AE&list=PL5-44a_kKTkX7w22K1OoRf07V5YJqeESX",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ada-one-which-stands-for-ai-on-device-asset-a-d-a"
    },
    {
        "project_title": "Policybot AI",
        "tagline": "PolicyBot lets you chat with policy documents offline — with cited answers, powered by Gemma 3n and advanced retrieval.",
        "authors": "Gautam Nagarajan, ???????????? ????????, gautam0507, Omir Kumar",
        "summary": "Introduction\n\nModern policy documents are essential to how societies function—but they are often dense, technical, and inaccessible to the very people they affect. Legal jargon, bureaucratic phrasing, and sheer length make it difficult for individuals, journalists, advocacy groups, and even professionals to extract meaningful insights or specific answers from these texts. While policies aim to be inclusive, their language unintentionally excludes.\n\nTo bridge this gap, we built a chatbot application that allows users to upload any policy document in PDF format and interact with it using natural language questions. Under the hood, the chatbot leverages a Retrieval-Augmented Generation (RAG) pipeline powered by Google’s Gemma 3n model (via Ollama) to extract contextually accurate answers—grounded strictly in the content of the document. No hallucinations, no filler—just precise, verifiable information.\n\nWhat sets this application apart is its focus on factual accuracy and traceability. Answers are constructed using techniques like semantic chunking, Rag Fusion and HyDE (Hypothetical Document Embeddings) to improve retrieval relevance. Whenever possible, the chatbot uses direct quotes from the document to preserve legal precision and reduce ambiguity. The result is a tool that doesn’t just summarize—it empowers users to get to the core of complex policies quickly and confidently.\n\nThis solution was created for the Gemma 3n Hackathon to demonstrate how open, on-device generative models can drive real-world impact—specifically, by improving public access to government and legal information. In an age where information is abundant but clarity is rare, this chatbot ensures that policy knowledge is no longer locked behind expert interpretation.\n\nMotivation and Impact\n\nUnderstanding government policies shouldn’t require a law degree. Yet, citizens around the world are routinely excluded from meaningful participation in public life because policy documents—often hundreds of pages long—are written in dense legal language that’s difficult to parse. This project began with a simple frustration: wanting to know what rights, responsibilities, or benefits a given policy grants, but having to rely on second-hand summaries or legal experts to make sense of it.\n\nPolicy literacy is a foundation of democracy. When people understand the laws and regulations that govern their lives, they are better equipped to hold institutions accountable, advocate for themselves, and participate in civic processes. However, the complexity of these documents creates a real barrier. Even for professionals—such as journalists, researchers, and local administrators—finding relevant sections, comparing clauses, or checking specific legal wording can be time-consuming and error-prone.\n\nThis chatbot directly addresses that challenge. It enables:\n\n- **Students** to ask questions about education or labor policies without having to wade through legal jargon.\n- **Journalists** to fact-check claims by understanding and quoting exact clauses.\n- **NGOs** and **citizens** to understand rights, entitlements, and obligations under various public schemes.\n- **Public sector professionals** to review or clarify operational rules with high precision.\n\nBy providing a fast, accurate, and interactive way to access dense documents, the tool drastically reduces the time and expertise required to interpret policy content. Instead of skimming 100+ pages or relying on unreliable third-party summaries, users can ask a question and get a context-accurate, directly quoted response—on demand.\n\nOne of the core advantages of this solution lies in its alignment with the technical and philosophical goals of **Gemma 3n**. This model is specifically designed to be:\n\n- **Lightweight**, making it ideal for resource-constrained environments.\n- **Open-source**, encouraging transparency and adaptability.\n- **Hardware-efficient**, capable of running on consumer-grade laptops or even mobile-class devices.\n\nThat makes this chatbot deployable in low-bandwidth or offline environments—such as rural classrooms, NGO field offices, or legal aid clinics—without the need for cloud inference or expensive GPUs. The architecture is optimized to run locally using **Ollama**, ensuring privacy, low latency, and reduced costs. This makes the application not only impactful but also inclusive.\n\nThe real-world value of the chatbot extends beyond convenience: it fosters a more informed public, empowers under-resourced communities, and promotes transparency in governance. These are exactly the kinds of outcomes the **Gemma 3n Hackathon** was designed to encourage: small models solving big problems, with tangible benefits for everyday people.\n\nArchitecture Overview\n\nThe chatbot is built on a carefully engineered Retrieval-Augmented Generation (RAG) pipeline that prioritizes factual accuracy, efficient document retrieval, and low-latency local inference. At its core, the system leverages the gemma3n:e4b model, hosted locally via Ollama, enabling fast and private inference on consumer-grade hardware without reliance on cloud APIs.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/cerai-iitm/policybot.git",
        "demo_video_url": "https://youtu.be/gdA-GysK7Eo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/policybot-ai"
    },
    {
        "project_title": "CaficulBot",
        "tagline": "Offline AI Technical Assistant for Colombian Coffee Farmers",
        "authors": "Sergio Quintero",
        "summary": "Caficulbot is a fully offline, multimodal AI assistant built to empower Colombian coffee farmers with expert knowledge on cultivation, pests, diseases, and farm management. It provides real-time answers to questions, processes images of plant diseases, and performs administrative tasks, all without needing an internet connection.",
        "technologies": [],
        "source_code_url": "https://github.com/sergioq2/gemma-3n-competition-caficulbot/tree/main",
        "demo_video_url": "https://www.youtube.com/watch?v=DnSgc60t-x0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/caficulbot"
    },
    {
        "project_title": "AidLens: Voice-Activated Offline First-Aid Assistant",
        "tagline": "Providing emergency medical guidance without internet using local AI and voice recognition.",
        "authors": "Navdhillon72",
        "summary": "In emergencies like accidents or disasters, internet access is often lost — and with it, access to vital first-aid information. AidLens is a web-based, voice-activated assistant that provides step-by-step first-aid instructions entirely offline, using an AI model running locally.\n\nUsers simply speak their issue (e.g., “My friend is bleeding from the leg”), and AidLens transcribes the speech, sends it to a local AI model (Gemma 2B via Ollama), and displays first-aid guidance instantly. The web app runs in the browser and communicates with the AI server running on the same network.\n\n Progress Note:\nDue to hardware limitations on my current laptop, I was unable to fully run the AI model in real-time for full integration. However, the core interface, voice input setup, and Ollama API logic have been implemented and tested in parts. The full vision and system design are in place, and with compatible hardware, the project is ready to be completed quickly.\n\nCore Features:\n- Voice input using Web Speech API\n- Local AI inference using Ollama + Gemma 2B\n- Works fully offline (local server)\n- First-aid-specific prompt engineering\n\nThis project aims to democratize emergency care for offline, rural, or disaster-affected areas by combining privacy-preserving on-device AI with a simple voice interface.\n\nBuilt With:\nJavaScript / HTML / CSS\nGoogle Chrome (for voice)\nOllama (local AI engine)\nGemma 2B model\n\nFuture improvements include multilingual support, image-based injury detection, and text-to-speech for better accessibility.",
        "technologies": [
            "Ollama",
            "Web Speech API"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/0gT6R9n6CT0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/aidlens-voice-activated-offline-first-aid-assistan"
    },
    {
        "project_title": "Gemma AI Data Assistant: Autonomous & Private Data Science Agent",
        "tagline": "A private, offline AI agent using Gemma to autonomously turn natural language requests into complete data science notebooks.",
        "authors": "aniket00231",
        "summary": "#",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/MarvelBoy047/Gemma_Ai_DataAssistant",
        "demo_video_url": "https://youtu.be/I37Q6swIQjM?si=V0MbUL1i78BKkcpe",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-ai-data-assistant-autonomous-and-private-dat"
    },
    {
        "project_title": "Guardians of Mother Earth, GOME",
        "tagline": "GOME is an offline AI system using Gemma 3n to fight illegal mining. It empowers indigenous communities with alerts in native languages.",
        "authors": "Luis Guaman Ch., Kevin Guaman",
        "summary": "Guardians of the Mother Earth (GOME) is a real-time, offline AI system designed to combat illegal mining in remote regions that lack internet and power infrastructure. Developed by Luis and Kevin Guaman, GOME empowers local and indigenous communities to monitor and protect their ancestral lands using advanced, accessible technology.\n\nAt its core, the system uses Google’s powerful and efficient Gemma 3n-it vision-language model to analyze live video feeds from drones or surveillance cameras. Running entirely on-premise on modest hardware, GOME detects visual signs of illicit activities—such as deforestation, heavy machinery, or river pollution—without needing an internet connection.\n\nWhat makes GOME truly impactful is its “out-of-the-box” intelligence and its commitment to inclusivity. The instruction-tuned Gemma model understands complex, natural language prompts, allowing users to ask nuanced questions about the visual data. Crucially, the system operates in over 140 languages, with a special focus on Quechua, the native language of many communities in the Amazon. This enables users to interact with the AI, receive alerts, and understand threats in their own language—removing technological barriers and placing the power of AI directly into the hands of the land’s true guardians.\n\nAlerts and data are transmitted across vast, remote areas using a low-power LoRaWAN network, ensuring that critical information reaches community members, leaders, and international advocates swiftly and efficiently. GOME is more than a detection tool; it is a platform for environmental justice, built to be used by the very people on the front lines of the fight against environmental destruction.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Fullsegura/gemm3nContest",
        "demo_video_url": "https://youtu.be/h2dsAXtcy6w",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/guardians-of-mother-earth-gome"
    },
    {
        "project_title": "Gemma 3N: First Multimodal AI Assistant",
        "tagline": "All-in-one AI assistant using voice, vision, and text to provide offline help for students, farmers, and remote users.",
        "authors": "Sagar Bagwe, Neelam Pawar, Harsh Jha, Saurabh Rangdale",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/sagarbagwe/gemma-ai-app",
        "demo_video_url": "https://youtu.be/hRV7hIbTpig",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-3n-first-multimodal-ai-assistant"
    },
    {
        "project_title": "Smart Crop Doctor with Gemma 3n",
        "tagline": "AI-powered tool using Gemma3n to detect crop diseases & weeds, giving farmers real-time advice with images, text & weather updates.",
        "authors": "Hemanth-thunder",
        "summary": "This project uses Gemma 3n, a powerful multimodal vision-language model with voice support and regional language understanding, to build an intelligent assistant for real-time crop disease and weed detection. By analyzing plant images and communicating through natural language (text and speech), the system provides easy-to-understand diagnosis and treatment suggestions to farmers. It also delivers real-time weather updates via an integrated AI agent. Unlike traditional tools, Gemma 3n can interact in local languages, improving accessibility for rural farmers.\n\nThe main challenge lies in handling real-time processing, ensuring accurate results with limited connectivity and hardware on the field. However, Gemma 3n's lightweight design and multi-input flexibility make it ideal for deployment in low-resource agricultural settings, helping farmers make informed decisions quickly and easily.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/IMlmpdDrDf8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/smart-crop-doctor-with-gemma-3n"
    },
    {
        "project_title": "Juliette-Personal-Wellness-Assistant",
        "tagline": "Juliette is a personal wellness assistant who uses various tools to understand the user's emotions and respond accordingly.",
        "authors": "Masih Maafi",
        "summary": "",
        "technologies": [],
        "source_code_url": "https://github.com/MasihMoafi/Hackathon",
        "demo_video_url": "https://youtu.be/FosUDxWxSQA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/juliette-personal-wellness-assistant"
    },
    {
        "project_title": "GOOGLE AIVIA",
        "tagline": "Google Aivia's offline, on-device AI helps doctors, farmers, and teachers overcome accessibility barriers with local solutions.",
        "authors": "nurullah kus, Berk Çavdar, Muhammed Yuzuak",
        "summary": "Google AIVIA\n\n\n**Enhanced On-Device AI Experience - Built on Google AI Edge Gallery**\n\nGoogle AIVIA is an advanced Android application that transforms the original Google AI Edge Gallery into a comprehensive AI assistant platform. This enhanced version brings professional-grade AI capabilities directly to your device, running entirely offline once models are loaded.",
        "technologies": [],
        "source_code_url": "https://github.com/nurullah44/google-aivia",
        "demo_video_url": "https://www.youtube.com/watch?v=gMz9xGX260c",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/google-aivia"
    },
    {
        "project_title": "KalviMate - an offline-capable, fine-tuned AI tutor ",
        "tagline": "An offline-first, syllabus-aligned educational AI, fine-tuned on Gemma 3n to provide equitable access to high-quality learning.",
        "authors": "heissanjay, ARUNAI DEEPAN, mitochondria",
        "summary": "KalviMate is an offline-capable, fine-tuned AI tutor designed to help students in Tamil Nadu learn directly from their state board textbooks, even without internet access. It transforms the open-source Gemma 3n model into a fully on-device, curriculum-aligned personal tutor that explains concepts, solves textbook problems, and engages students in natural conversation.\n\nTargeted at rural and under-resourced areas where access to quality teachers and internet connectivity is limited, KalviMate addresses three critical gaps in existing educational technology: \n- lack of offline support\n- absence of Tamil language understanding, and \n- no alignment with the Tamil Nadu state syllabus.\n\nWe solve this by fine-tuning Gemma 3n on a custom dataset curated directly from the state board curriculum. \nThis enables KalviMate to act as:\n- A subject expert delivering accurate, syllabus-bound responses\n- A concept explainer using simple analogies for better understanding\n- A problem solver guiding students through exercises\n- A study companion offering quizzes and conversational learning\n\n**Key Features**:\n- Fully offline: Runs entirely on-device for accessibility in low-connectivity regions\n- Tamil Nadu curriculum aligned: Answers are rooted in official textbooks\n- Natural language interaction: Supports simple, conversational English\n- Privacy-preserving: No data ever leaves the student’s device\n\nTutor-like behavior: Friendly, patient, and motivating by design\nKalviMate is more than a chatbot. It is an equitable, AI-powered learning companion for every student, anywhere.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/heissanjay/kalvimate-gemma-3n/",
        "demo_video_url": "https://www.youtube.com/watch?v=M3gx5u5VZ5E",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/kalvimate-an-offline-capable-fine-tuned-ai-tutor"
    },
    {
        "project_title": "Revolutionizing Education & Open Learning  with Gemma 3n",
        "tagline": "A platform for offline adaptive learning and grounded research, powered by Gemma 3n's unique multi-agent system and regex-based search",
        "authors": "FireHacker, Rutwik Shinde",
        "summary": "",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/thefirehacker/TimeCapsule-SLM",
        "demo_video_url": "https://youtu.be/fijQFpo9Ni4?si=xaU4WJ2PAkwyFT-l",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/revolutionizing-education-and-open-learning-with-g"
    },
    {
        "project_title": "MedPal – Your Offline Medical Assistant",
        "tagline": "“Offline, privacy-first medical guidance powered by Google Gemma 3n — delivering life-saving answers where connectivity can’t.”",
        "authors": "ShoaibSSM",
        "summary": "**MedPal – Your Offline AI Medical Assistant**\n\n##**Overview**\nIn many rural and low-connectivity regions, access to reliable medical guidance is delayed or unavailable. Online health tools often fail without internet, and in emergencies like burns, choking, or severe bleeding, even a few minutes’ delay can cost lives.\n\nMedPal addresses this gap by delivering instant, offline, privacy-first medical assistance, powered by Google Gemma 3n. By running entirely on-device, MedPal ensures that sensitive health data never leaves the user’s system, offering both speed and privacy when it matters most.\n\n\n##**Key Features**\n- Offline AI Inference – Runs Gemma 3B locally via Ollama, no internet needed.\n\n- Symptom Checking – Enter symptoms and get immediate, step-by-step guidance.\n\n- Voice & Text Input – Accepts queries through typing or offline speech-to-text.\n\n- Privacy-First Design – All processing happens locally on the user’s device.\n\n- Cross-Platform Support – Built with Tauri for deployment across desktop OSs.",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "Ollama",
            "Vosk"
        ],
        "source_code_url": "https://github.com/shoaib2000857/MedPal-Emergency-AI-Assistant",
        "demo_video_url": "https://youtu.be/--1BUXXbHec",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/medpal-your-offline-medical-assistant"
    },
    {
        "project_title": "Synapse: a memory system",
        "tagline": "Local first multimodal RAG system with weighted bi-link feature.",
        "authors": "yanbot",
        "summary": "A backend service for ingesting, storing, and retrieving event logs with semantic search capabilities.\nAnalyzes multimodal data with the power of Gemma-3n, while preserve your privacy since everything can be self-hosted.",
        "technologies": [],
        "source_code_url": "https://github.com/Sunny-XXV/Memory",
        "demo_video_url": "https://youtu.be/5YtGpl3-NdI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/synapse-a-memory-system"
    },
    {
        "project_title": "Ｇemma 3N FeynmanFlow",
        "tagline": "FeynmanFlow: A Personalized, Multilingual Learning Journey",
        "authors": "wingfung34",
        "summary": "1. The Problem: A World of Missed Potential\nGlobally, over 7.6% of people are denied quality learning opportunities due to language barriers. Simultaneously, up to 40% of students disengage because the content fails to align with their interests. They aren't lacking intelligence; the traditional education system is failing to provide them with a stage to shine. Every individual possesses unique potential; they just need the right key to unlock it.\n\n2. Our Solution: The FeynmanFlow Workflow\nFeynmanFlow is that key. It's more than just a tool; it's a personalized, end-to-end learning journey built on a clear and intuitive workflow. Here’s how it empowers every learner",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/nI3HgZSrSTE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/emma-3n-feynmanflow"
    },
    {
        "project_title": "Gemma The-Interviewer ",
        "tagline": "we finetune this gemma model to as an Interviewer ",
        "authors": "Yash Savaliya, Sheladiya Rishi",
        "summary": "Gemma The-Interviewer is an AI-powered interview preparation assistant built using a fine-tuned Gemma 3n model. Designed to simulate realistic technical interviews, it dynamically generates domain-specific questions, evaluates user responses, and asks intelligent follow-ups — just like a real interviewer would.\n\nUnlike generic quiz tools, Gemma The-Interviewer offers a conversational, context-aware experience, helping users improve their answers, reasoning, and confidence. The entire model runs on a private, offline-compatible setup using Kaggle Notebooks and open-source LLM tooling, making it accessible, secure, and customizable.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/jJeV8MvbWrg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-the-interviewer"
    },
    {
        "project_title": "Harmony AI",
        "tagline": "Your Companion for Life",
        "authors": "Shreyas Kulkarni, Prajwal More",
        "summary": "Harmony AI is a revolutionary multi-platform application that transforms how users navigate life's challenges across health, finance, career, and community engagement. Built with Google's Gemma 3N offline model, HarmonyAI ensures privacy-first AI assistance without internet dependency. \n\nPrivate, Offline AI: Powered by Google’s Gemma 3n model, all features run securely on-device, ensuring privacy and offline access.\n \nDashboard: Allows users to interact with Harmony and track personal milestones with intuitive tools for future growth.\n \nHealth & Wellness: Includes habit tracking, mental / physical health & sleep / nutritional aspects along with visual progress, encouraging healthier routines with motivating nudges.\n \nFinance Tools: Simplifies budgeting and provides AI-driven education for better money management and financial discipline.\n \nCareer Guidance: Analyses user skills, suggests personalised career paths and in-demand skills, and enables clear goal setting and achievement tracking.\n \nBusiness Opportunities: Evaluates business ideas, curates suitable opportunities, and offers actionable guidance for entrepreneurs.\n \nAccessibility Tools: Enhances independence with camera-based image reading, scene description, object detection, and navigation support.\n \nCommunity Hub: Connects users to local volunteering opportunities, tracks their social impact, and facilitates group/community engagement.\n \nEmpowerment Across Life Areas: Seamlessly supports users in career, health, finance, business, accessibility, and community—all privately and offline.\n\nImpact:\nBy democratising AI access through offline capabilities, Harmony AI empowers communities, provides 24/7 assistance regardless of connectivity, and creates an inclusive ecosystem where every user can overcome obstacles and achieve their potential through intelligent, personalised guidance.\n\n![https://youtu.be/jfj7QlXqbVU]",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/shrkul/Harmony",
        "demo_video_url": "https://youtu.be/jfj7QlXqbVU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/harmony-ai"
    },
    {
        "project_title": "Realtime SpeechtoASL interpreter and animation engine",
        "tagline": "Gemma-Sign-AI is an realtime mobile app that translates spoken language into real-time American Sign Language (ASL) animations.",
        "authors": "tiniark",
        "summary": "Gemma-Sign-AI is a Flutter-based mobile application that acts as a real-time, on-device interpreter, converting spoken language into animated American Sign Language (ASL). It uses a locally hosted Google Gemma AI model for speech-to-text translation and a custom animation engine to visualize the signs. The app features a comprehensive, compressed local database of ASL signs and an efficient architecture for a seamless user experience.",
        "technologies": [],
        "source_code_url": "https://github.com/tinisoft/GemmaSignAI",
        "demo_video_url": "https://youtu.be/eqouUiJnlpY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/realtime-speechtoasl-interpreter-and-animation-eng"
    },
    {
        "project_title": "Convolution: A high school tutor that is always with you",
        "tagline": "Our portable tutor, accessible via phone with no internet, allows natural conversation and interprets handwritten notes.",
        "authors": "Danil Platonov",
        "summary": "#",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Socratic-Learn/convolution",
        "demo_video_url": "https://youtu.be/hbW4rUv9010",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/convolution-a-high-school-tutor-that-is-always-wit"
    },
    {
        "project_title": "EduFlow: AI-Powered Education Platform with Gemma 3n for Global Learning",
        "tagline": "Democratizing personalized education through hybrid AI tutoring, offline-first design, and accessibility-focused learning platform.",
        "authors": "M S Rishav Subhin, Swarnajit Roy, Merlin Alphones Dass",
        "summary": "EduFlow revolutionizes education by combining Google's Gemma 3n with a comprehensive learning platform built on React, TypeScript, and Firebase. Our hybrid AI system provides personalized tutoring both online and offline, ensuring quality education reaches every student regardless of connectivity or device limitations.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/msrishav-28/EduFlow",
        "demo_video_url": "https://youtu.be/oge-y3wfp3k",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/eduflow-ai-powered-education-platform-with-gemma-3"
    },
    {
        "project_title": "Gemma 3n Based AI Teacher",
        "tagline": "An AI teacher was created to bridge educational inequality and ensure students learn effectively rather than misusing AI for homework.",
        "authors": "Yintercept, BarryTkTc",
        "summary": "Some students get professional teachers, labs, and resources, but others are left behind. Education inequality is a serious problem and widens social gaps. Also, right now, millions of students worldwide turn to AI for help—but AI can be easily misused. Many use AI for completing their homework, and the result is that they don't learn. That's why we created an AI teacher specialized to guide students like real teachers.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/tGGltYc9H9o",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-3n-based-ai-teacher-app"
    },
    {
        "project_title": "gemma-3n: Lumina",
        "tagline": "On-Device Visual Intelligence for the Visually Impaired",
        "authors": "Gagik Sargsyan",
        "summary": "================================================================================\nABSTRACT\n================================================================================\n\nLumina is an Android application engineered to serve as a real-time, on-device AI assistant for visually impaired users. By leveraging the power and efficiency of Google's Gemma 3n model, Lumina transforms a standard smartphone into an intelligent \"seeing-eye\" companion that provides environmental descriptions, object identification, and navigational assistance without ever compromising user privacy. All processing, from image analysis to language generation, occurs 100% on-device, ensuring that Lumina is always available, regardless of internet connectivity, and that sensitive user data remains completely private. This document provides an exhaustive technical breakdown of the architecture, data flows, core algorithms, and the specific engineering solutions implemented to overcome the challenges of running a state-of-the-art Large Language Model on a resource-constrained mobile device.\n\n================================================================================\n1. INTRODUCTION: THE MISSION AND THE MODEL\n================================================================================\n\n1.1. The Problem Space\n\nNavigating the world without sight presents profound daily challenges. From simple tasks like finding a misplaced object to complex situations like crossing a busy street, visually impaired individuals rely on a combination of memory, tactile feedback, and auditory cues. While existing assistive technologies exist, they often depend on cloud connectivity, privacy concerns, and a point of failure in offline environments.\n\n1.2. The Solution: Lumina\n\nLumina was conceived to address these gaps. It is a mobile-first application that acts as an extension of the user's senses, providing rich, context-aware audio feedback about their surroundings. Lumina is not just a tool, it's a constant companion designed for reliability, privacy, and empowerment.\n\n1.3. Why Gemma 3n? The Cornerstone of On-Device AI\n\nThe feasibility of Lumina hinges on the ability to run a powerful generative AI model directly on a mobile device. Gemma 3n was the unequivocal choice for this mission for several key reasons:\n\n- On-Device Supremacy: Gemma 3n is specifically designed for high-performance, on-device execution. This aligns perfectly with Lumina's core principles of privacy and offline functionality.\n\n- Efficiency and Performance: I utilize the gemma-3n-e2b-it-int4.task model—a 4-bit quantized version of the 2-billion-parameter instruction-tuned model. This specific variant provides an exceptional balance of low memory footprint, fast inference times on mobile GPUs, and high-quality, coherent language generation.\n\n- Multimodal Capabilities: Gemma's inherent ability to process both text and image inputs is the technical foundation for Lumina's core feature: visual question answering. This allows the user to not just get a description, but to have a genuine dialogue about their visual environment.\n\n================================================================================\n2. SYSTEM ARCHITECTURE AND DATA FLOW\n================================================================================\n\nLumina is built upon a robust and scalable Clean Architecture, strictly separating concerns into three primary layers. This was critical for managing complexity, enabling testability, and ensuring long-term maintainability.\n\n- DOMAIN Layer (The Core): The heart of the application, containing pure Kotlin business logic with no Android dependencies.\n    - Use Cases: Single-responsibility classes representing every user action (e.g., `DescribeSceneUseCase`, `FindObjectUseCase`, `StopGenerationUseCase`). They orchestrate the flow of data from the repository to the presentation layer.\n    - Repository Interface (`LuminaRepository`): Defines the contract for data operations, abstracting the data sources from the business logic.\n\n- DATA Layer (The Implementation): Provides concrete implementations of the domain interfaces.\n    - `LuminaRepositoryImpl`: Implements the `LuminaRepository` interface, acting as the single source of truth for data. It delegates tasks to the appropriate data source.\n    - Data Sources:\n        - `GemmaAiDataSource`: Manages the entire lifecycle and all interactions with the Gemma 3n model via the MediaPipe `LlmInference` API.\n        - `TextToSpeechServiceImpl`: Encapsulates the Android Text-to-Speech (TTS) engine, managing its lifecycle, speech queue, and providing a clean interface for the app.\n\n- APP Layer (The Presentation): The user-facing layer, responsible for UI and state management.\n    - UI (Jetpack Compose): A fully declarative UI built with Jetpack Compose. `SceneExplorerScreen.kt` is the primary component, reacting to state changes from the ViewModel.\n    - ViewModel (`SceneExplorerViewModel`): The bridge between the UI and the domain layer. It executes use cases based on user input, manages the UI state via Kotlin `StateFlow`, and ensures the UI is a simple, reactive reflection of the application's state. It is injected with use cases via Hilt.\n\n- Dependency Injection (Hilt): I use Hilt to manage dependencies. This decouples the classes, simplifies testing by allowing for easy injection of mock implementations, and manages the lifecycle of the singletons (like repositories and data sources) automatically.\n\n---\n**Data Flow Example: \"Describe Scene\"**\n1.  **UI Event:** User double-taps the `SceneExplorerScreen`.\n2.  **ViewModel:** The `investigateScene()` function in `SceneExplorerViewModel` is called. It updates its `StateFlow` to show a loading state (`isLoading = true`).\n3.  **Use Case:** The ViewModel invokes the `DescribeSceneUseCase`.\n4.  **Repository:** The use case calls `luminaRepository.describeScene()`.\n5.  **Data Source:** The repository implementation calls `gemmaAiDataSource.generateResponse()` with the appropriate prompt and the best-quality frame from the `FrameBufferManager`.\n6.  **Streaming Response:** `GemmaAiDataSource` returns a `Flow<String>` of text chunks from the model.\n7.  **Collection & State Update:** The ViewModel collects this flow. Each emitted chunk is appended to a `generatedText` state variable. Simultaneously, the text is streamed to the `TextToSpeechServiceImpl`.\n8.  **TTS & UI:** The `TextToSpeechServiceImpl` buffers the text and speaks it in complete sentences. The UI displays the accumulating text.\n9.  **Completion:** When the flow completes, `isLoading` is set to `false`. The `stopAllOperationsAndGeneration()` function ensures all streams (Gemma, TTS) are terminated if the user interrupts.\n---\n\n================================================================================\n3. PROMPT ENGINEERING: GUIDING THE AI\n================================================================================\n\nThe quality of Gemma's output is directly proportional to the quality of the prompts. I have developed a system of specialized, context-aware prompts for each feature.\n\n- **System Prompt (The Foundation):** Every session with Gemma begins with a master system prompt that establishes its persona and core instructions. It is instructed to act as a helpful assistant for a visually impaired user, to be descriptive, concise, and to focus on objects and potential obstacles.\n\n- **Task-Specific Prompts:**\n    - **Describe Scene:** \"Describe the scene in detail. Mention objects, their spatial relationships, and any text you can see.\" This is an open-ended but focused prompt to elicit a rich description.\n    - **Find Object:** \"Is there a [object name] in the image? If so, where is it located?\" This is a direct question designed for a precise, location-based answer.\n    - **Read Text:** \"Read all the text in the image, from top to bottom.\" This constrains the model to an OCR-like task.\n    - **Identify Currency:** \"Identify the currency and its denomination. Be as certain as possible.\" This prompt is tailored for classification and confidence scoring.\n\nThis multi-prompt strategy ensures that the model's powerful generative capabilities are precisely channeled to the user's immediate need.\n\n================================================================================\n4. STATE MANAGEMENT & UI ARCHITECTURE (MVI)\n================================================================================\n\nI implemented a Model-View-Intent (MVI) architecture for the presentation layer to ensure a predictable, unidirectional data flow and a highly reactive UI.\n\n- **Model:** The state of the UI is represented by an immutable data class, `SceneExplorerUiState`. This class holds all the information the UI needs to render itself at any given moment (e.g., `isLoading`, `generatedText`, `isSpeaking`).\n\n- **View:** The `SceneExplorerScreen` (a Jetpack Compose function) acts as the View. It is stateless and dumb. Its only job is to observe the `StateFlow<SceneExplorerUiState>` from the ViewModel and render the UI based on the current state. It sends user actions (intents) to the ViewModel.\n\n- **Intent:** User actions, such as a double-tap or a long-press, are treated as \"intents.\" These intents are passed to the `SceneExplorerViewModel`.\n\n- **ViewModel (`SceneExplorerViewModel`):** The ViewModel processes these intents, executes the relevant business logic (by calling Use Cases), and then creates a *new* `SceneExplorerUiState` object. This new state is then emitted to the `StateFlow`, causing the UI to re-render automatically.\n\nThis unidirectional flow (`View -> Intent -> ViewModel -> Model -> View`) makes the application state highly predictable and easy to debug. By using immutable state objects and `StateFlow`, I guarantee that the UI is always a direct reflection of the application's internal state.\n\n================================================================================\n5. CORE COMPONENT DEEP DIVE: ENGINEERING CHALLENGES & SOLUTIONS\n================================================================================\n\n**5.1. The AI Core: `GemmaAiDataSource`**\n\n- **Challenge: Session Token Limits & Infinite Loops**\n  - Problem: The MediaPipe `LlmInferenceSession` has a finite token limit. Furthermore, I observed a bug where the model occasionally fails to send a `done = true` signal, instead entering an infinite loop, repeatedly sending the final token.\n  - **Code-Level Solution:**\n    1.  **Proactive Session Rotation:** The data source tracks the token count. When it approaches a safe threshold, it automatically closes the session and starts a new one, preserving the context. This prevents crashes from token exhaustion.\n    2.  **Robust Cancellation:** The `stopGeneration()` function is a hard reset mechanism. It cancels the underlying `coroutineScope` in which the `callbackFlow` for Gemma's responses is running, and then immediately reinstantiates it with a new `SupervisorJob`. This is the crucial step that forcefully terminates any in-flight generation, effectively breaking out of the infinite token loop and ensuring the app never freezes. The use of `callbackFlow` is key here, as it allows me to bridge the listener-based MediaPipe API with Kotlin's structured concurrency.\n\n- **Challenge: Concurrency and Stability**\n  - Problem: Rapid user inputs could trigger simultaneous requests to the AI, leading to race conditions.\n  - **Code-Level Solution:** I use a Kotlin `Mutex` as a gatekeeper for the `generateResponse` function. The function is structured with a `mutex.withLock { ... }` block. This ensures that only one coroutine can execute the AI generation logic at a time. Any subsequent request suspends until the mutex is released, guaranteeing a stable, sequential processing pipeline.\n\n**5.2. The Vision System: `FrameBufferManager`**\n\n- **Challenge: Optimizing for Clarity vs. Motion**\n  - Problem: Different tasks require different visual input. Describing a static scene needs the clearest image, while navigation requires understanding motion.\n  - **Code-Level Solution:** I implemented a `FrameBufferManager` that maintains a `ConcurrentLinkedQueue<TimestampedFrame>` to safely handle frames coming from the camera thread.\n    - `getBestQualityFrame()`: This method iterates through the buffer and is intended to analyze frames for sharpness (e.g., using a variance of Laplacian algorithm, though currently using the latest frame as a proxy) to find the one with the most detail. This is used for OCR and scene description.\n    - `getMotionAnalysisFrames()`: This method is designed for future navigation features. It provides a sequence of recent frames, allowing Gemma to infer temporal context like the direction and speed of moving objects.\n\n**5.3. The Audio System: `TextToSpeechServiceImpl`**\n\n- **Challenge: Bridging the AI-to-TTS Speed Mismatch**\n  - Problem: Gemma streams responses token-by-token, far faster than a TTS engine can speak. Naively feeding these chunks results in choppy, unnatural speech.\n  - **Code-Level Solution:** I developed an intelligent sentence-boundary buffering system using a `StringBuilder` as the internal buffer.\n    1.  **`addToBufferAndSpeak(text: String)`:** This method appends new text chunks to the `StringBuilder`.\n    2.  **`speakBufferedText()`:** This private method is then called. It searches the buffer for the last index of \". \", \"? \", or \"! \".\n    3.  If a sentence break is found, it extracts the substring up to that point, passes it to the `textToSpeech.speak()` method with `QUEUE_ADD`, and then *removes* only the spoken portion from the `StringBuilder`.\n    4.  This leaves any partial sentence (e.g., \"And on your left...\") in the buffer, ready to be completed by the next chunk from the AI. This transforms a rapid stream of tokens into a continuous, natural-sounding flow of speech.\n\n**5.4. The User Experience: `SceneExplorerViewModel` and UI**\n\n- **Challenge: Providing Control and Feedback**\n  - Problem: The user needs to know what the app is doing and be able to stop it at any time.\n  - **Code-Level Solution:**\n    - **State-Driven UI:** The `SceneExplorerViewModel` exposes its state via `MutableStateFlow<SceneExplorerUiState>`. The Compose UI collects this flow as state (`val uiState by viewModel.uiState.collectAsState()`) and automatically updates to show loading indicators or other feedback.\n    - **Global Stop:** A long-press gesture on the screen triggers `viewModel.stopAllOperationsAndGeneration()`. This function is a critical safety feature that calls the `StopGenerationUseCase`, which in turn calls `gemmaAiDataSource.stopGeneration()` and `textToSpeechService.stopSpeaking()`, providing an immediate and reliable way to silence the app.\n    - **Interaction Delays:** I introduced a `delay(1500)` in the `investigateScene()` coroutine. This was a crucial UX discovery, giving the user time to steady their hand and allowing the `FrameBufferManager` to populate with high-quality frames before one is selected for analysis.\n\n**5.5. System Efficiency: Mitigating Battery Drain**\n\n- **Challenge:** Balancing Responsiveness with Power Consumption\n - Problem: The camera sensor and the AI inference engine are the two most power-hungry components of the application. Leaving them running continuously is not a viable option for an all-day assistive tool.\n - **Code-Level Solution:** Aggressive, State-Driven Resource Management.\nOn-Demand Camera Lifecycle: The camera is not always active. It is only turned on when a vision-based use case (like DescribeScene or FindObject) is initiated. Crucially, as soon as the AI generation is complete or the user cancels the operation, the camera is immediately released and turned off. This ensures the sensor is only drawing power for the absolute minimum time required.\nMinimal AI Interaction: We architected the system to call the powerful Gemma 3n model as infrequently as possible. Instead of continuous analysis, the full model is only invoked in response to a direct user command. For proactive features, we use simpler, less power-intensive logic.\n - **Configurable Frame Throttling:** For modes that do require a continuous camera stream, such as Proactive Navigation, the ManageFrameThrottlingUseCase is implemented to control the frame rate. By default, it operates at a reduced rate (e.g., 2-5fps) for ambient updates, dramatically lowering the processing load compared to the full 60fps stream. This rate can be intelligently increased when more detail is required.\n\n================================================================================\n7. FUTURE WORK AND EXTENSIONS\n================================================================================\n\n- **Proactive Navigation:** The ultimate vision is a proactive mode where the app periodically scans the environment with a lightweight object detection model. If a hazard (e.g., a curb) is detected, it would then trigger Gemma for a full description and warning, conserving battery while providing timely alerts.\n- **Model Fine-Tuning:** Fine-tuning a Gemma variant on specialized datasets for currency, receipts, or common household objects to further improve recognition accuracy. This would involve creating a dataset of labeled images and using transfer learning to adapt the base model.\n- **Personalized Context:** Allowing users to \"teach\" Lumina about personal items (e.g., \"These are my keys\") to enable highly specific search queries. This would involve storing image embeddings and corresponding labels in a local database.\n\n================================================================================\n8. CONCLUSION\n================================================================================\n\nLumina is a testament to the power of modern on-device AI to solve meaningful, real-world problems. By combining the efficiency of the Gemma 3n model with a robust, well-engineered application architecture, it delivers a powerful, private, and reliable assistive experience. The technical solutions—from the meticulous AI session management and prompt engineering to the intelligent frame and text buffering—were all developed in service of creating a seamless and empowering tool for the visually impaired user. Lumina demonstrates that the future of truly personal AI is not in the cloud, but right here, in the user's hand.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/gagik894/Lumina.git",
        "demo_video_url": "https://youtu.be/sX1yP11G2Mc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-3n-lumina"
    },
    {
        "project_title": "Slide Sage - PDF to Video Explainer",
        "tagline": "A webapp that turns any academic PDF into a narrated video",
        "authors": "Raghavendra Kaushik",
        "summary": "Links\n\nLive at [http://35.243.247.15](http://35.243.247.15)  \nCode: [https://github.com/rakaar/paper\\_agent\\_2/](https://github.com/rakaar/paper_agent_2/)  \nDemo Video: [https://www.youtube.com/watch?v=Es1Atcl1Pt4](https://www.youtube.com/watch?v=Es1Atcl1Pt4)",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/rakaar/paper_agent_2/",
        "demo_video_url": "https://www.youtube.com/watch?v=Es1Atcl1Pt4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/slide-sage-pdf-to-video-explainer"
    },
    {
        "project_title": "MedicalGem: Gemma-Powered Health Analysis App",
        "tagline": "Gemma-Powered Health Analysis App",
        "authors": "aurora9231",
        "summary": "MedicalGem is an intelligent health analysis Android application that leverages Large Language Models (LLMs) to provide personalized health insights, symptom analysis, medication management, and evidence-based health recommendations.\n\n 🌟 Key Features\n\n 🤖 **Advanced LLM Integration**\n- **Multi-Provider Support**: Local Ollama, OpenRouter, SiliconFlow\n- **Intelligent Entity Extraction**: Recognizes food, drugs, recipes, and symptoms\n- **Function Calling**: OpenAI-compatible tool definitions for structured interactions\n- **Personalized Analysis**: Correlates findings with user's medical history\n\n 🏥 **Comprehensive Health Management**\n- **Symptom Analysis**: AI-powered symptom assessment and correlation with existing conditions\n- **Medication Management**: Drug interaction checking and personalized recommendations  \n- **Physical Exam Tracking**: USPSTF-based screening recommendations\n- **Family History**: Genetic predisposition analysis and risk assessment\n\n 📱 **Modern Android Architecture**\n- **Jetpack Compose UI**: Material 3 design system\n- **SQLite Database**: Comprehensive health data storage\n- **Reminder System**: Medication and exam notifications\n- **Multi-User Support**: Family health profile management",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/yYV48xVYmyo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/medicalgem-gemma-powered-health-analysis-app"
    },
    {
        "project_title": "Reasearch Crew",
        "tagline": "when you want a quick analytics report of any given topic my product will help you with that by doing research on that topic.",
        "authors": "Rosi Reddy V, Vangala Angel",
        "summary": "This project demonstrates the development of an intelligent, automated research and report generation system using the CrewAI framework. The system simulates the collaborative efforts of a research team by orchestrating two specialized agents: a Researcher and an Analyst. Each agent is powered by Google’s gemma-3n-e4b-it LLM and performs distinct roles in a coordinated workflow to produce high-quality, structured research reports on any given topic.\n\nThe Researcher Agent is designed to gather comprehensive, accurate, and up-to-date information on a specified topic using search tools such as SerperDevTool. It focuses on key concepts, historical context, current trends, challenges, case studies, and future outlooks.\n\nThe Analyst Agent takes the research findings and generates a polished, professional report. This includes an executive summary, trend analysis, key insights, and actionable recommendations—all presented in a reader-friendly format.\n\nThe entire process is defined declaratively using YAML files (agents.yaml and tasks.yaml), which clearly specify the behavior, goals, backstories, and tools used by each agent. The output is a structured markdown file, suitable for publication or presentation.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/maq1er65mME?feature=shared",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/reasearch-crew"
    },
    {
        "project_title": "SnakeID: AI Snake Identifier",
        "tagline": "Pioneering the future of on-device AI for life-saving applications",
        "authors": "parseltung",
        "summary": "SnakeID: Technical Architecture & Gemma 3n Implementation",
        "technologies": [
            "Flask",
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=pxyiEcmpKEM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/snakeid-ai-snake-identifier"
    },
    {
        "project_title": "Pocket Tutor: An Offline AI Tool for Education in Crisis",
        "tagline": "Multilingual, and multimodal capabilities to deliver private, accessible, and personalized education to any child, anywhere",
        "authors": "Maxime Carriere",
        "summary": "Pocket Tutor: An Offline AI Tool for Education in Crisis",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/MaximeCarriere/ai-language-tutor",
        "demo_video_url": "https://youtu.be/P1Z3NcOvZL8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/pocket-tutor-an-offline-ai-tool-for-education-in-c"
    },
    {
        "project_title": "Orbit: A Life-Changing ADHD Assistant Powered by Gemma 3n + Ollama",
        "tagline": "A private, on-device “home base” that gives ADHDers daily structure—built with Ollama and Gemma3n:e4b",
        "authors": "patrickjmoran",
        "summary": "",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/ritualage/Orbit",
        "demo_video_url": "https://youtu.be/IflGhjE77Gc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/orbit-app-an-adhd-support-system-for-your-life"
    },
    {
        "project_title": "myBnB: An efficient AI Agent to Find the Perfect Rental",
        "tagline": "We propose myBnB, an agent to efficiently find rentals for any requirements all available information, 100x faster than OpenAI Agent",
        "authors": "Kilian Ft, Paul Mohr",
        "summary": "We propose myBnB, a parallel, efficient agent that finds the perfect rental. In\norder to do so, we show how parallel processing with small models and\nstructured, domain-specific data can help to build effective agents. Further, it\nbecomes apparent that prompting is not the solution to everything, and\nsometimes it might be preferred to train an actual classifier. Additionally, we\npresent a dynamic scoring system that seamlessly integrates into our agent to\nobtain a fitting ranking of apartments.\n\nA live demo with sample data for the cities Berlin, Augsburg, and Vienna is available.\n[myBnB Live Demo](https://mybnb-web.netlify.app/)\n\n\nWhile we present our solutions applied to myBnB, it presents a framework on\nhow to build specialized agents. Similar techniques could be applied to any sort\nof specialized search. Future work will deal with generalizing those concepts\ntowards agents that can structure problems themselves, and can detect when\nto use and train a classifier over simple prompting.",
        "technologies": [],
        "source_code_url": "https://github.com/funcai/mybnb-worker",
        "demo_video_url": "https://www.youtube.com/watch?v=upiabQcBoe4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/mybnb-an-efficient-ai-agent-to-find-the-perfect-re"
    },
    {
        "project_title": "Helping those non-verbal/aphasia with Autism communicate with others using AI",
        "tagline": "We use Gemma 3N to provide individuals with communication disabilities an innovative tool for properly expressing themselves",
        "authors": "projectprogramamark",
        "summary": "Here we present a next-generation Augmentative and Alternative Communication (AAC) mobile application designed to give a powerful, private, and natural voice to individuals with communication disabilities. Traditional AAC systems are often limited by static, pre-programmed phrases and require an internet connection, compromising both spontaneity and privacy. Most importantly, they often do not allow the users to express themselves in the way they tend to think (in visual abstract concepts rather than using natural language). They often require intense training and are often a stop gap to only communicate the most basic sentences, after months of intense training. The current market of leading apps have not innovated within the last decade, and there are constant complaints and unhappiness about it from the community at large. Our project overcomes these barriers by running a fine-tuned Gemma 3N model directly on the user's device, allowing what essentially amounts to one of the most advanced concept-to-language translator in the world to act as a bridge of communication between those who are non-verbal, minimally verbal, or selectively verbal, to those who are fully verbal. This is especially useful for those who are within the Autism spectrum, but also critical for those with some sort of aphasia, such as stroke patients. Imagine if you had a family member, friend, or other loved one with whom you could not communicate with anymore. The current communication methods are incredibly lossy and hard for the user. We built this so that we can meet those with communication disabilities where they are, especially since they have to meet us where we are in so many other ways in this world.\n\nThe core of our innovation lies in a custom Swift native module that uses Google's MediaPipe to perform on-device inference, ensuring the app is fully functional offline and that all user data remains completely private. To tailor the model to the unique needs of AAC users, we developed a sophisticated MLOps pipeline to fine-tune Gemma using advanced techniques like QLoRA. We use Unsloth to be able to fine tune the model on a modest home PC without running into any cloud costs. The training data was sourced from a custom-built gamified web portal where volunteers helped create a rich, domain-specific dataset, teaching the model to generate context-aware and nuanced phrases from a series of visual prompts.\n\nBy combining a high-performance native AI core with a user-friendly React Native interface, Mosaic Voice delivers a seamless and responsive experience. It empowers users to move beyond rigid, pre-made sentences and communicate their thoughts with the fluency and personality they deserve. This project demonstrates the profound potential of on-device generative AI to create truly transformative and accessible technology.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/ProjectProgramAMark/gemma3n-hackathon-mv",
        "demo_video_url": "https://youtu.be/6g4b4H300lE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/helping-those-with-non-verbal-autism-communicate-w"
    },
    {
        "project_title": "MindNote AI",
        "tagline": "Care More, Document Less",
        "authors": "VannoTanno, Ka Shing, chloe san",
        "summary": "Introduction\nTherapists are the heart of our mental health care system, yet many are drowning in administrative work. In Malaysia, 1 in 3 people face mental health challenges, while therapists lose up to 40 hours a month to case notes, reports, and documentation, a hidden burden contributing to a burnout rate exceeding 45%. The very people we rely on to heal us are stretched thin by paperwork, not people work.\n\nMindNote AI is built to change that. Powered by a **fine-tuned Gemma 3n** model trained on therapist case note templates, therapists are empowered to transform short notes and transcripts into accurate, professional-grade case notes swiftly and securely. With on-device privacy, therapists get context-aware, customizable notes in formats like SOAP or DAP, tailored to their workflow.\n\nSystem Architecture\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2742355%2F8cc9e95676a2c4c0df5c335c6334228e%2Fgemma%20(2).jpg?generation=1754489517994253&alt=media)",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/VanessaTong/MindNoteAI/tree/main",
        "demo_video_url": "https://www.youtube.com/watch?v=nZo0TH-xT0Y",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/mindnote-ai"
    },
    {
        "project_title": "Gemma Mental Health Assessment ",
        "tagline": "A voice-based mental health assistant that supports Bangla and conducts empathetic PHQ-9 screenings",
        "authors": "MaishaTabassum02, Nishat02, Deepeta Zaman Orpa",
        "summary": "Project Report for Google - The Gemma 3n Impact Challenge \n\n**Project Description:**\nIn a world that increasingly demands emotional resilience and mental strength, mental health remains one of the most overlooked yet fundamental aspects of human well being. According to WHO 2022 reports, mental health disorders affect over 970 million people worldwide, yet nearly 75% of those in low and middle income countries receive no treatment. Mental health care remains deeply underserved in Bangladesh and many parts of the world, where stigma, lack of access, and cultural misunderstandings prevent people from seeking help. Conversations about depression often do not happen, especially not in native languages or in trusted, stigma free environments. This project aims to bridge that gap.\nWe introduce a voice-based mental health screening bot, powered by Gemma 3N, that helps users evaluate symptoms of depression using the standardized PHQ-9 questionnaire. The goal is to make mental health assessment more accessible, empathetic, and culturally sensitive, particularly for underrepresented communities in regions like Bangladesh.\nThe application allows users to speak or type responses to PHQ-9 prompts. Using speech-to-text (Faster-Whisper) and Gemma for classification, it evaluates each response and offers empathetic, human-like feedback. At the end, it provides a score along with suggestions aligned with clinical guidance. The system supports English and Bangla audio, leveraging Coqui TTS and other multilingual tools for dynamic voice generation.\nWhile the system currently accepts English voice/text and Bangla text input, it supports Bangla and English voice output. This lays the foundation for future full Bangla-language support. This balance helps address the language and cultural barriers that typically hinder mental health support in low-resource regions like Bangladesh.\n\n**Impact & Vision:**\nMental health care in underserved regions, particularly in Bangladesh, is severely hindered by a combination of poverty, cultural stigma, limited mental health literacy, and inadequate access to services. Many individuals in tight-knit, self-reliant communities, especially in small towns and rural areas, face significant challenges in seeking help. Cultural barriers often frame mental health struggles as personal failings, and economic constraints further limit access to care. Mental health support often exists more in theory than in reality. Between socioeconomic barriers, overstretched systems, and deep-rooted cultural silence, countless people are left to navigate emotional struggles alone. This project seeks to break down these barriers by providing an accessible mental health screening tool that empowers individuals in underserved communities to access support without fear of judgment, paving the way for improved mental health outcomes and reducing stigma. The vision is one of hope and change, enabling individuals to access the care they need, when they need it, while dismantling the socio-cultural and economic obstacles that have traditionally kept them from seeking help.\n\n**PHQ-9 Logic and Free Text Understanding:**\nInstead of relying on fixed choice questionnaires, the bot conducts a natural and empathetic conversation by asking open ended PHQ-9 questions and accepting free form voice responses from users. These responses are transcribed using Faster-Whisper and then processed through a Gemma powered LangChain model.\nThe model classifies each answer into one of the four standard PHQ-9 categories:\n\n“Not at all” (0)\n“Several days” (1)\n“More than half the days” (2)\n“Nearly every day” (3)\n\nThe system assigns numeric scores accordingly, tracks them across all nine questions, and concludes with a tailored severity assessment and a supportive message. This design improves accessibility for users who may be non-literate or emotionally distressed, fostering a more human centered and compassionate mental health evaluation experience.\n\n**Project Overview:**\nThis project presents a web-integrated, voice-enabled mental health assistant that conducts PHQ-9 depression assessments in both English and Bangla. The system provides empathetic, conversational feedback using the Gemma language model hosted locally via Ollama. It leverages Faster-Whisper for real-time voice input, along with Coqui TTS and browser-native speech synthesis for dynamic voice output in Bangla and English.\n\n**Video Pitch & Storytelling :**\nThe video pitch tells a compelling story of how the technology works, focusing on the user experience and emotional connection it fosters. The narrative highlights the seamless integration of speech-to-text technology, multilingual capabilities, and empathetic feedback from the conversational bot. The pitch emphasizes the cultural sensitivity of the solution, showcasing its relevance to the target audience. It powerfully communicates the importance of overcoming mental health barriers, making the technology both globally relevant and locally impactful. This engaging and emotional pitch captures the project's potential to drive viral engagement by addressing a universally significant issue in a contextually sensitive way.\n\n**Core Architecture:**\nThe architecture is divided into Frontend, Backend, Machine Learning, and Data Storage components to ensure efficient interaction, security, and scalability. It integrates FastAPI and Django frameworks, allowing for an intuitive and responsive user interface, with secure user session management and real-time backend processing.\n\nCore Components:\nNatural Language Understanding\nLLM: Gemma 3n:e2b via Ollama for local, privacy preserving inference.\n\nLLM Agent Logic: LangChain prompt orchestration classifies user answers into the PHQ-9 scale (0–3), tracks the total score, and generates kind, supportive replies.\n\nSpeech-to-Text (Input)\nModel: Faster-Whisper provides real-time transcription of user voice input.\n\nPipeline: Audio is captured through the web interface, converted to .wav, transcribed by Faster-Whisper, and routed into the PHQ-9 classifier.\n\nText-to-Speech (Output)\nEnglish: Browser native speechSynthesis API delivers instant voice output.\n\nBangla: Coqui TTS is integrated via the FastAPI backend for server side Bangla voice generation.\n\nAuto Language Switching: Unicode detection enables dynamic toggling between English and Bangla voices based on the bot's response.\n\n\n**PHQ-9 Logic Details:**\nThe bot conducts a full 9-question PHQ-9 assessment. User responses are classified into four categories:\n0 – Not at all\n1 – Several days\n2 – More than half the days\n3 – Nearly every day\n\nThe total score is computed to determine depression severity as follows:\n0–4: Minimal\n5–9: Mild\n10–14: Moderate\n15–19: Moderately severe\n20–27: Severe\n\n\n**Full-Stack Web Architecture:**\nFrontend (Django + HTML/JS)\nThe Django frontend hosts a secure web UI, enabling user-friendly interactions for voice and text input. Key pages such as upload.html and start_assessment.html manage voice interaction, session control (restart/resume), and display real-time responses.\nBackend (FastAPI)\nThe backend handles core logic, including:\n\nAudio upload and transcription (Faster-Whisper).\n\nPHQ-9 question management.\n\nCoqui Bangla TTS audio generation.\n\nLLM query routing.\n\nMain API endpoints:\n/transcribe: Handles audio uploads and returns transcriptions.\n\n/phq: Manages PHQ-9 session responses and bot replies.\n\n/tts: Serves Coqui TTS audio for Bangla.\n\n/phq/reset: Resets the PHQ-9 session.\n\n**Integration and Security:**\nDjango runs on localhost:8000 while FastAPI runs on localhost:8001. The frontend makes cross-origin requests to the FastAPI backend. CORS middleware is securely implemented to allow these requests without compromising security.\nEnvironment isolation and management of API keys are handled via .env files.\n\n**Database Integration with PostgreSQL:**\nPostgreSQL is used as the production-grade relational database to securely store user session data, assessment results, and user profiles. Compared to SQLite used in development, PostgreSQL provides scalability and reliability.\nDjango ORM is configured to connect to PostgreSQL, ensuring robust data management and enabling future features like user authentication, historical data tracking, and personalized mental health insights.\n\n**Voice Interaction Flow:**\nThe voice interaction follows this sequence:\nThe user speaks; the browser records and sends audio.\n\nFaster-Whisper transcribes the audio.\n\nTranscribed text is routed to the PHQ-9 session.\n\nGemma LLM classifies the response and generates the next question plus a supportive reply.\n\nThe response is rendered and spoken aloud:\n\nEnglish responses use browser TTS.\n\nBangla responses use Coqui-generated audio served from the backend.\n\nThe cycle repeats until all nine questions are completed.\n\nA final summary is given, using kind and supportive language.\n\n**Multilingual Capability:**\nInput (STT): Faster-Whisper supports both English\n\nLLM: Gemma model processes input and output in both English and Bangla.\n\nOutput (TTS):\n\nEnglish output uses browser-native speechSynthesis API.\n\nBangla output uses Coqui TTS via the FastAPI backend.\n\nUI Interaction: Web interface built with Django supports both languages.\n\nSwitching Logic: Unicode detection dynamically toggles between English and Bangla voices.\n\n**How Gemma 3N Was Used:**\nGemma 3N played a central role in two key areas of the application, making the interaction both intelligent and compassionate:\nClassification of User Responses:\nThe system leveraged ChatOllama, a variant of Gemma 3N, to interpret and classify user responses based on the PHQ-9 questions. Rather than relying on rigid answer choices, Gemma 3N helped map each free form user response into one of four categories: “Not at all,” “Several days,” “More than half the days,” or “Nearly every day.” This was achieved by using a carefully crafted ChatPromptTemplate to guide the classification process, with StrOutputParser extracting the precise classification from the model’s output. This enabled nuanced understanding even when answers were vague or complex.\nEmpathetic Responses:\nBeyond classification, Gemma 3N generated warm, empathetic replies after each user response. Using a distinct ChatPromptTemplate, the model crafted short, supportive, and validating messages that acknowledged the user’s feelings and provided reassurance. This created a safe, compassionate environment where users felt heard and understood, fostering trust and openness throughout the conversation.\nTechnical Depth & Execution\nTechnically, the project leverages the powerful Gemma 3N model to classify open-ended free-text user responses into PHQ-9 categories, enabling nuanced, human-like understanding rather than relying on fixed choices. The system integrates Faster-Whisper for English speech-to-text transcription, and accepts Bangla text input, supporting multilingual interaction. For voice output, Coqui TTS is used to generate natural, expressive Bangla speech, while English output uses browser-native TTS. This hybrid approach balances privacy, performance, and linguistic accessibility without relying on full Bangla speech recognition, which remains a future enhancement. Our modular backend using FastAPI and Django provides robust session management, real-time processing, and scalability, demonstrating a well-engineered, practical, and innovative solution that fulfills the judging criteria for technical depth and execution\n\n**Challenges Faced and Solutions:**\nBuilding a voice-based mental health assistant came with its share of challenges, each requiring thoughtful solutions:\nReal-time Speech-to-Text Conversion:\nOne major hurdle was accurately converting user voice input into text in real-time. This was solved by incorporating Whisper for speech transcription, known for high accuracy and robustness. FFmpeg was used to seamlessly convert audio formats, ensuring compatibility with Whisper and smooth processing.\nClassification of Ambiguous Responses:\nUsers sometimes gave responses that didn’t fit neatly into the four PHQ-9 categories. To handle this, a clarification mechanism was implemented. When ambiguity was detected, the bot gently prompted users to select from predefined options, maintaining accuracy without disrupting conversational flow.\nTrigger Detection for Crisis Intervention:\nDetecting sensitive language such as suicidal thoughts was critical. A trigger detection system was integrated to actively monitor responses for crisis indicators. If detected, the system halted the assessment and provided urgent, compassionate crisis intervention advice, prioritizing user safety.\nSession Management:\nManaging the state of the user’s session tracking responses, scores, and progress required careful design. A dedicated Python class (PHQ9Session) was created to handle session logic, ensuring smooth and consistent behavior during assessments.\n\n**Why the Technical Choices Were Right:**\nAll technical choices were made to balance performance, user experience, and project requirements:\n\nFastAPI and Django Together:\nDjango was used for its robust, secure frontend capabilities templating, session management, and database ORM making it ideal for building a user friendly interface. FastAPI powered the backend API with asynchronous capabilities critical for real-time transcription, PHQ-9 question management, and machine learning inference. Combining them provided clean separation of concerns and scalability.\n\nGemma 3n:e2b Model:\nThe Gemma 3n:e2b  model achieves a remarkable balance between powerful NLP capabilities and efficient performance through several key architectural and training innovations. While the model has a total parameter count of 5 billion, it is designed to operate with an effective memory footprint of just 2 billion parameters. This is made possible by techniques such as Per-Layer Embedding (PLE) caching, which offloads non-essential parameters to local storage to reduce run-time memory costs, and a Matryoshka Transformer (MatFormer) architecture that allows for selective activation of parameters. This means the model can deliver high-quality NLP results, including accurate text classification and nuanced response generation, while still being able to run efficiently on low-resource devices like phones and laptops, a feat that traditional large language models often cannot accomplish. This clever engineering allows it to punch well above its weight, making it a powerful yet practical choice for on-device applications.\n\nFaster-Whisper for Speech-to-Text:\nFaster-Whisper efficiently and accurately transcribes user voice input in English. Running locally ensures low latency and preserves privacy, critical for sensitive health applications. Its robustness and compatibility with multiple audio formats enabled seamless voice interaction.\nCoqui TTS for Bangla Text-to-Speech:\nCoqui TTS was integrated on the backend to generate natural-sounding Bangla speech. Browser-native TTS engines currently do not support expressive Bangla well, so Coqui’s open-source, locally runnable models were essential to deliver culturally and linguistically appropriate voice responses without relying on cloud services.\n\nCORS Middleware:\nTo enable smooth, secure communication between Django frontend and FastAPI backend running on different origins, CORS middleware was implemented. This ensured properly authorized API requests without compromising security, vital for modern web applications using distributed components.\n\n**Conclusion:**\nThis project demonstrates how cutting-edge AI and machine learning technologies can be thoughtfully integrated to address critical gaps in mental health care, particularly in underserved regions like Bangladesh. By combining the privacy-preserving Gemma 3N model, the efficient Faster-Whisper transcription system, and the expressive Coqui TTS engine, the voice-based PHQ-9 mental health assistant provides an accessible, empathetic, and culturally sensitive tool for depression screening.\nThe thoughtful use of Django and FastAPI frameworks ensures a scalable, secure, and user friendly platform that supports both English and Bangla languages. The system’s design prioritizes human-like empathy and privacy, offering users a safe environment to engage with mental health resources without fear or stigma.\nUltimately, this project sets a strong foundation for expanding voice-driven, AI-powered mental health solutions in low resource settings, breaking down barriers to care and fostering greater awareness and support for mental wellness.",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Nibrita/Mental_Health_Assessment_Gemma",
        "demo_video_url": "https://youtu.be/zAb9pI7pcUg?si=lIjTlouL24Oztk2d",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/mental-health-assessment"
    },
    {
        "project_title": "V3-Gemma: A Depression Screener Based on Visual, Vocal, and Verbal Signals",
        "tagline": "Trained on How You Look, Speak, and What You Say - Powered by Gemma3n",
        "authors": "tarot383, Min jhon, Insu Jeon, SeoJeon Park, KKimMGH",
        "summary": "Depression is a serious condition with significant societal impact, yet current screening methods remain limited. To address this, we developed V3-Gemma, a mobile application that securely performs on-device depression screening using visual, vocal, and verbal signals with the Gemma 3n model. Designed to reach the many individuals who remain undiagnosed, it runs efficiently on smartphones via the Streamlit platform. The system builds on expert-defined screening criteria and a multimodal dataset of 3,500 participants, collected since 2021 by the psychiatrists. Without additional fine-tuning, V3-Gemma enhances accuracy through tailored strategies. V3-Gemma integrates clinical knowledge with LLM-based reasoning in a structured multimodal framework to provide simple, privacy-preserving depression screening.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Jeon-Insu/gemma3n_challenge",
        "demo_video_url": "https://youtu.be/VJarGtcCFAk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/v3-gemma-a-depression-screener-based-on-visual-voc"
    },
    {
        "project_title": "Dayflow: See Where Your Time Really Goes",
        "tagline": "Dayflow uses Gemma 3n to understand what you're doing on your computer, entirely on-device, revealing your productivity patterns privately.",
        "authors": "Jerry Liu",
        "summary": "Dayflow: Technical Architecture & Gemma 3n Integration",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/JerryZLiu/DayflowGemma/releases/tag/Release",
        "demo_video_url": "https://www.youtube.com/watch?v=EDmZOeH6BkA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/dayflow-see-where-your-time-really-goes"
    },
    {
        "project_title": "Ajay’s Family AI: One Offline Model Tackling Six Rural Challenges",
        "tagline": "Education, health, mental care, farming, accessibility & disaster safety—one offline AI model for rural India’s key needs.",
        "authors": "visalakshi.purnima, saikumar allaka",
        "summary": "Ajay’s Family AI: One Offline Model Tackling Six Rural Challenges\n\n📌 Problem\n\nMillions of rural Indian families face multiple daily challenges:\n\t•\tEducation – 32.5% of Class VIII students cannot read a Class II text, private tuition is costly.\n\t•\tHealthcare – Up to 70% rely on unverified home remedies, risking harm.\n\t•\tMental Health – Depression prevalence is double that of urban India, with treatment for only 20%.\n\t•\tAgriculture – Diseases like bacterial blight can destroy 60% of rice yields.\n\t•\tAccessibility – 33.8% of Indians aged 45+ suffer vision impairment.\n\t•\tDisaster Preparedness – Floods cause ₹25,000+ crore in annual losses and thousands of deaths.\n\nPoor internet access, high service costs, and lack of localized resources keep life‑saving knowledge out of reach.\n\n⸻\n\n💡 Solution\n\nWe built Ajay’s Family AI — a single fine‑tuned Gemma 3N model running fully offline, supporting local languages, and covering six critical domains:\n\t1.\t📚 Education – Personalized NCERT‑aligned tutoring\n\t2.\t🩺 Healthcare – Verified Ayurvedic remedies\n\t3.\t💬 Mental Health – Depression support & guidance\n\t4.\t🌾 Agriculture – Rice disease diagnosis & treatment advice\n\t5.\t👁 Accessibility – Image description for visually impaired\n\t6.\t🚨 Disaster Management – Offline survival instructions\n\nPowered by Ollama, Unsloth LoRA fine‑tuning, and RAG with FAISS, it ensures:\n\t•\tPrivacy – Local processing, no cloud dependency\n\t•\tZero recurring cost – Works without internet\n\t•\tAlways available – 24/7 offline capability\n\n⸻\n\n🎯 Impact\n\nOne model can reach 500M+ rural Indians:\n\t•\tAffordable learning for students\n\t•\tSafe health guidance for families\n\t•\tEmotional support for vulnerable youth\n\t•\tIncreased crop yields for farmers\n\t•\tIndependence for visually impaired elders\n\t•\tLife‑saving disaster readiness\n\n⸻\n\n🏗️ Approach\n\n1️⃣ Problem Identification\n\nMapped six rural life challenges to specific AI capabilities.\n\n2️⃣ Domain-Specific Dataset Gathering\n\nCollected verified, authoritative PDFs:\n\ndatasets/ayurveda/ayurveda_home_remedies.pdf\ndatasets/depression/nimh_Depression.pdf\ndatasets/disaster_management/natural_hazards_disaster_management_india.pdf\ndatasets/education/science_text_book_class8_india.pdf\ndatasets/rice_diseases/india_rice_diseases.pdf\n\n3️⃣ PDF Parsing & Structuring\n\t•\tUsed PyMuPDF (fitz) to extract structured text, headings, subheadings.\n\t•\tOutput in JSON for easy reference & RAG integration.\n\n4️⃣ Base Model Setup\n\t•\tInstalled Ollama locally.\n\t•\tPulled gemma3n:e4b (7.5 GB) for primary use, gemma3:latest (3.3 GB) for multimodal features.\n\n5️⃣ Domain-Specific Q&A Generation\n\t•\tUsed Gemma 3N via Ollama to produce 2,630 Q&A pairs tailored to each domain.\n\n6️⃣ Fine-Tuning on Kaggle\n\t•\tFine‑tuned with Unsloth LoRA on 2,630 Q&A pairs.\n\t•\tUploaded to Hugging Face: dataakash/gemma-family-assistant-gguf\n\t•\tConverted to GGUF for Ollama offline use.\n\n7️⃣ Custom Model Registration in Ollama\n\t•\tCreated Modelfile and registered gemma-family:latest locally.\n\n8️⃣ Retrieval-Augmented Generation (RAG)\n\t•\tBuilt FAISS vector indexes for each domain under rag_index/.\n\t•\tDomain‑specific retrieval boosts factual accuracy by 20‑40%.\n\n9️⃣ Streamlit Family Assistant Interface\n\t•\t8+ tabs for each family member/domain.\n\t•\tMultimodal (text + image) support where needed.\n\t•\tFully offline, responsive, and mobile‑friendly.\n\n⸻\n\n✅ End Result\n\t•\t6 domains solved by 1 offline AI model\n\t•\tRuns on consumer hardware (Mac, Linux, Windows)\n\t•\tNo internet required after initial setup\n\t•\tQuantized for efficiency (~7.5 GB)\n\t•\tAccessible 24/7 at zero cost to families\n\nvideo url: https://youtu.be/uXB6kdc27Es\n\nDemo url: https://ca37aad4b0f5.ngrok-free.app, This application is hosted locally on my Mac. If you encounter any issues accessing it, please contact me via email so I can restart the application. At times, the service may be unavailable due to my computer being turned off or temporary internet connectivity problems. In such cases, reaching out to me will help ensure a prompt resolution.\n\nCode url: https://github.com/aisandwich/gemma-family-assistant",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/aisandwich/gemma-family-assistant",
        "demo_video_url": "https://youtu.be/uXB6kdc27Es",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ajays-family-ai-one-offline-model-tackling-six-rur"
    },
    {
        "project_title": "Slide Fight Tactics",
        "tagline": "Turn your slides into searchable knowledge — just create a deck, ask a question, and let Gemma retrieve answers with page citations.",
        "authors": "H?i Nguy?n",
        "summary": "AI-powered document assistant that helps users organize and search through slide decks effortlessly. Users can create decks, import PDF slides, and ask natural language questions. Powered by Gemma, the system retrieves accurate answers directly from the slides and includes page-level references, making it ideal for learning, reviewing, and presenting complex content efficiently.",
        "technologies": [],
        "source_code_url": "https://github.com/haibayby2002/sft",
        "demo_video_url": "https://youtu.be/k4rEe3CwR50",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/slide-fight-tactics"
    },
    {
        "project_title": "An Offline AI-Assisted Voice Assessment Tool for Students",
        "tagline": "Simulates teacher-student voice interactions to promote active learning and improve student comprehension, in low-connectivity areas",
        "authors": "Allan Tan",
        "summary": "better-ed: An Offline AI-Assisted Oral Assessment Tool for Students\n\nFirst off, huge thanks to the GEMMA 3n organizers for creating such an exciting and meaningful challenge. We also want to shout out the whole community, we had a blast building this.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Predictive-Systems-Inc/be-mini-ai",
        "demo_video_url": "https://youtu.be/Vo9NvaV4hAk?si=drAqR0Rvpt2DEuEz",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/an-offline-ai-assisted-voice-assessment-tool-for-s"
    },
    {
        "project_title": "EduGemma: On-Device AI for Education, Everywhere",
        "tagline": "A gemma3n-powered, on-device AI platform that generates personalized learning games for students in low-resource regions.",
        "authors": "sj yu",
        "summary": "#",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": null,
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/edugemma-on-device-ai-for-education-everywhere"
    },
    {
        "project_title": "Madical Advisor Ayra AI",
        "tagline": "Ayra AI: Medical Advisor – Technical Writeup (Proof of Work)",
        "authors": "riizalhp",
        "summary": "**Overview**\n**Ayra AI** is an Android-based **offline Medical Advisor application** designed to provide first-response assistance to users seeking health consultations without the need for an internet connection or direct contact with healthcare professionals. The application offers various features such as AI-powered chat consultations, image and voice-based interactions, daily health tips, and lifestyle tracking including water intake and step counting.\n\nArchitecture and Technology Stack\nAyra AI is developed natively using **Kotlin**, with **SQLite** for local data storage. This architecture enables the application to run efficiently and reliably across Android devices without requiring network connectivity.\n\nTo process visual input, Ayra AI integrates **MediaPipe**, which allows real-time detection of faces or body parts when users submit photos as part of the consultation process.\n\nThe AI component utilizes **Gemma 3n**, a** Large Language Model (LLM)** capable of running entirely **offline on-device**, combined with a **Retrieval-Augmented Generation (RAG)** approach to enhance responses using locally stored medical reference texts. All language interactions are powered by **Prompt Engineering** techniques specifically crafted to constrain and guide the AI within a medically relevant context.\n\n**Use of Gemma 3n**\na. Gemma 3n serves as the core engine for Ayra AI’s intelligent responses. The model is utilized for:\n- Answering user health-related questions through **text-based chat.**\n- Interpreting **voice inputs** (converted to text) for natural conversation.\n- Providing **preliminary analysis **based on submitted images with support from the RAG pipeline.\n- Delivering **personalized health tips** and lifestyle insights based on locally stored data.\n- Running the **RAG pipeline entirely on-device**, referencing a curated offline medical knowledge base.\n\nb. The implementation of Gemma 3n involves advanced **prompt engineering** tailored to:\n- Keep the model strictly focused on **general healthcare topics**, avoiding responses beyond its intended scope.\n- Ensure the model provides **informative but non-diagnostic** answers, aligned with the ethical responsibilities of health-related applications.\n- Maintain a response format that is **concise, user-friendly, and contextually appropriate** for mobile health assistance.\n\n**Technical Challenges and How They Were Overcome**\n**1. Optimization Across Diverse Android Devices**\nA significant challenge was ensuring that **Gemma 3n runs smoothly on a wide range of Android devices**, particularly low-end hardware.\nSolution: Resource profiling (memory and CPU usage) was conducted, unnecessary prompt context was trimmed, and a **fallback mechanism** was implemented for devices with limited processing capacity.\n\n**2. Constraining AI Understanding to Medical Context**\nAnother challenge was limiting the AI’s scope to **respond only to general medical inquiries**, preventing it from addressing questions outside the intended domain.\nSolution: A **Prompt Conditioning strategy **was adopted, where each user interaction is prefaced by a system prompt defining the AI's role as a non-diagnostic medical assistant. Guardrails were added to prevent speculative or unsafe outputs.\n\n**3. Limited Documentation for MediaPipe-RAG Integration**\nWhen attempting to integrate MediaPipe as part of a custom visual input pipeline for the RAG system, the team encountered **insufficient official documentation** for flexible use cases.\nSolution: Manual experimentation and custom pipeline engineering were conducted, using open-source repositories as references to develop a workflow that routes visual output into the RAG system effectively.\n\n**Justification of Technical Choices**\n1. **Kotlin & SQLite**: Chosen for their reliability, lightweight nature, and full control over local data management on Android platforms.\n2. **MediaPipe**: Selected for its highly efficient real-time visual input processing on mobile devices.\n3. **Gemma 3n E2B**: Adopted due to its ability to operate entirely offline, its lightweight deployment footprint, and flexibility in prompt design.\n4. **Prompt Engineering + Local RAG**: This combination provides a balance between answer accuracy and performance efficiency, ensuring that responses are relevant, fast, and resource-conscious within a health-oriented context.\n\n**Conclusion**\nAyra AI demonstrates that delivering **reliable, offline AI-driven healthcare support** on mobile devices is not only possible but highly impactful. Through the local deployment of **Gemma 3n,** careful **prompt management**, and seamless integration with tools like **MediaPipe**, the project represents a tangible application of real engineering to produce an **innovative, ethical, and efficient solution** in the digital health domain.\n\nTeam\n1.  [Rizal Hanifa Pratama](https://github.com/riizalhp) – Project Manager\n2. [Muhammad Iqbal Ghozy](https://github.com/qybbs) – Android Developer\n3. [Rizky Gustiantoro](https://github.com/rgustiann) – Android Developer\n4. [Muhammad Islakha Khoiruzzaman Tekno Agri](https://github.com/lakhatekno) – AI Engineer\n5. [Seva Giantama Farrel](https://github.com/Tenshi-X) – UI/UX Designer",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/riizalhp/medical-advisor-ai",
        "demo_video_url": "https://youtu.be/ag3znSncd24",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/madical-advisor-ayra-ai"
    },
    {
        "project_title": "SecondYou - Your External Brain",
        "tagline": "A Private, Local-First AI that Thinks and Writes like You",
        "authors": "Kuber Mehta",
        "summary": "SecondYou represents a paradigm shift in privacy first, local hyper-personalised AI, something I had an idea for the longest time, finally possible leveraging Google's Gemma 3N multimodal model to create private, local-first AI assistants that truly understand and embody an individual's communication styles. This technical writeup details the implementation of a proof-of-concept system that demonstrates how modern large language models can be efficiently fine-tuned on personal data to create digital twins that think, write, and respond like their human counterparts.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/hE4t6T_x-r4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/secondyou-your-external-brain"
    },
    {
        "project_title": "Resonance - AI-Powered Consensus Discovery",
        "tagline": "Where diverse voices find harmony through AI-powered resonance - Building bridges of understanding in a fragmented world.",
        "authors": "Shao Jiaqi 757",
        "summary": "Resonance: AI-Powered Consensus Discovery Platform",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/SHAO-Jiaqi757/Resonance",
        "demo_video_url": "https://youtu.be/bwthvD5uNaQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/resonance-ai-powered-consensus-discovery"
    },
    {
        "project_title": "FieldLens：A Private, Offline AI Assistant for Detecting Corn Abnormalities",
        "tagline": "Drone-Based Crop Diagnosis and Multimodal Reasoning on Edge Devices",
        "authors": "Xiao Hong, tianmiao11",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/tianmiao11/FieldLens",
        "demo_video_url": "https://youtu.be/YT33z4GDt5U",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/fieldlensa-private-offline-ai-assistant-for-detect"
    },
    {
        "project_title": "Sentience: Autonomous Cognition Engine",
        "tagline": "A production-grade, offline cognition engine that brings your devices to life through continuous perception and reasoning. ",
        "authors": "Ian Wambai",
        "summary": "Sentience is an advanced autonomous AI system that:\n\nCaptures multimodal input at ~5Hz from webcam and microphone simultaneously\nProcesses visual and auditory information using Google's int4-quantized Gemma 3n-E2B model. \nGenerates continuous insights through scene descriptions and action recommendations\nOperates fully offline without requiring internet connectivity or user prompts\nOptimized for Apple Silicon using PyTorch MPS backend for optimal performance\nProvides real-time streaming of thoughts and observations with timestamped output\n\nCore Capabilities:\nVisual Perception: Analyzes webcam frames to understand visual context, objects, people, activities, and environmental conditions\nAudio Analysis: Processes microphone input to detect sounds, speech, ambient noise, and audio events\nMultimodal Fusion: Combines visual and auditory information for comprehensive scene understanding\nAutonomous Reasoning: Generates insights and recommendations without user intervention\nContinuous Operation: Maintains persistent awareness and tracks environmental changes over time",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/IanWambai/sentience",
        "demo_video_url": "https://youtu.be/WMs9qDosFjk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sentience-cognition-engine"
    },
    {
        "project_title": "Crabble - On-Device AI for Mental Clarity",
        "tagline": "A private, embedded AI that helps reduce mental blocks through micro-interventions, task support",
        "authors": "YafaYa",
        "summary": "Crabble - Your Private Focus Companion\n\n#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Hodisy/Crabble",
        "demo_video_url": "https://youtu.be/kr-_iBo1M6M",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/crabble-on-device-ai-for-mental-clarity"
    },
    {
        "project_title": "Kipp – Making Math Fearless: An AI Learning App for Struggling Students",
        "tagline": "An interactive AI app that helps primary school children with learning difficulties overcome math anxiety through personalized strategies an",
        "authors": "maja100",
        "summary": "Kipp is an interactive learning app for primary school children with learning difficulties. It uses the Gemma‑3 model to precisely read worksheets and present tasks in a child‑friendly way – with large, clear numbers, colorful emojis, and a voice feature for children who cannot yet write.\nThe app applies multiple explanation strategies (e.g., chunking, kinaesthetic learning, pattern recognition) and automatically switches methods if a child asks three times. In addition, Kipp motivates with positive feedback, friendly animations, and encourages children to explain their own solutions, helping them build confidence step by step.",
        "technologies": [],
        "source_code_url": "https://github.com/Maja011235/kipp-math-app/tree/main",
        "demo_video_url": "https://www.youtube.com/watch?v=wgFaAAg2xBY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/kipp-making-math-fearless-an-ai-learning-app-for-s"
    },
    {
        "project_title": "Gemma Safety Agent",
        "tagline": "An intelligent, offline safety system powered by Gemma 3n that analyzes multiple video sources to detect danger or suspicious activity.",
        "authors": "emrahyigit, SC",
        "summary": "Background \n\nThe use of home security systems in the U.S. has seen rapid growth, with approximately 94 million households now utilizing tools such as cameras, alarms, or access control systems. Between 2023 and 2024, security camera adoption increased significantly among both homeowners and renters. As of today, over 50% of homes have at least one security camera installed. \n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F49050%2Fbe8390b24f9d737a1e8ecdb1db0fd560%2F9052551.jpg?generation=1754501478197183&alt=media)\n\nInterestingly, the primary motivations behind adopting these systems are not always directly linked to crime prevention. The most common reasons include having children in the home (33%), frequent travel or being away from home (31%), and owning valuable items (26%). Other factors include pre-installed systems, past experiences with break-ins, and caring for older adults or living alone. Surprisingly, only 10% cite living in high-crime areas as their main reason, and just 2% mention natural disaster risks. Details are [here](https://www.safehome.org/resources/home-security-industry-annual/)\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F49050%2F0e1b170e755139de271803c6a8fc57a4%2Fgraph2.png?generation=1754515053924728&alt=media)\n\nThese insights highlight a shift in consumer behavior toward proactive and convenience-driven security, underlining the growing relevance of smart, adaptable surveillance systems that cater to diverse user needs beyond traditional threat models.\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F49050%2Fcd3af3ff0b9b2ea7f7249b406004a75b%2Fdangers2.drawio.png?generation=1754515135651458&alt=media)\n\nProject Overview\n\nGrowing number of security cameras in public and private spaces has made it increasingly difficult for human operators to monitor and interpret all video feeds in real time. Security is not just a technical need — it's a deep human instinct. We all want to feel safe — in our homes, with our loved ones, and in the world we live in. This project addresses that challenge by combining computer vision and large language models (LLMs) to create a smart surveillance assistant capable of automatically detecting events and generating human-like alerts from multiple camera feeds.\n\nThe system ingests video streams from multiple security cameras, analyzes visual data using AI models to identify meaningful activities or anomalies (e.g., robery, animal proximities), and then uses an LLM to generate concise, natural-language alerts based on the detected events. \n\nThis solution is designed to be used in home security. Additionally we design the system to be scalable, modular, and privacy-conscious, and can be deployed in various environments such as smart buildings, factories, retail stores, public infrastructure, and critical facilities.\n\nGemma Safety Agent is an AI-powered, privacy-first security solution designed to run entirely offline, built on top of Gemma 3n—a powerful multimodal model capable of analyzing video, audio, and text together.\n\nUnlike cloud-based surveillance systems, Gemma Safety Agent operates locally and autonomously, making it ideal for sensitive environments or locations with limited internet access.\n\nAnother important aspect is the development of a mobile version of the system, which would allow users to utilize it in non-fixed locations such as during camping, sports events, or outdoor activities. This would expand the system’s use beyond static environments and provide a personal security solution in temporary or remote settings.\n\nIt monitors multiple camera feeds to detect critical threats such as wild animal intrusions (e.g. snakes, bears, crocodiles), suspicious human behavior (e.g. burglars, loitering), and unauthorized alarm triggers or break-ins.\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F49050%2F29216a2cbb70433cbb3af8edfd72bfed%2Falarm3.png?generation=1754501985690843&alt=media)\n\n####",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/ArfkdGcfU5M",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/guardai-security-alerts-with-gemma3n"
    },
    {
        "project_title": "Bio Protect ",
        "tagline": "Android App to Protect Plants by Diagnosing Diseases and Providing Remedies",
        "authors": "Taimour Nazar",
        "summary": "**Project Description:**\n\nBio Protect is an Android app which uses vision capabilities of Gemma 3n E2B IT and E4B IT models. It can analyze images of plants and detect their diseases and propose remedies for them. Internet is only needed to download the models. Models will run directly on your Android devices without internet. Gemma 3n E2B and E4B models have outstanding device performance. \n\nModels are downloaded from HuggingFace so, in order to download models you must first add your hugging face token in the android app settings.\n\n**Challenges:**\nOur target was to run them on low end mobile devices with 4GB RAM. If we can run them on low powered devices then it means we will be able to help the following people:\n1- Farmers who don't have internet access or farmers who can't afford high end mobile devices. Farmers of third world countries.\n2- Researchers working in remote forests like Amazon and others around the world where internet is not available.\n3- People who like to keep plants at their home but don't know how to take good care of their plants.\n\n**Solution:**\nWith the release of Gemma 3n E2B and E4B IT, it became possible to perform excellent inference on edge devices like mobile phones. With Gemma 3n E2B and E4B IT we can overcome the challenges mentioned above by running these models on mobile phones. My aim was to create an Android app which can be used by the farmers and researchers to overcome the hurdles which they face. In addition to farmers and researchers Bio Protect also aims to help people who keep plants at their home but don't have sufficient knowledge to take good care of their plants. I believe  it will lead to a better future for all of us on this planet. Android was chosen because it has more than 70% market share so, I can reach a bigger audience with Android devices. As Gemma 3n E2B and E4B IT are not trainable so, we had to use prompt engineering for generating report.\n\n**Testing Device:**\nSamsung A22 5G with 4GB RAM\n \nOn selected testing device it produces response in approximately around 6 to 8 minutes when \"Be Concise\" instruction is given. If image size is small response can be generated in around 3 minutes also. Without \"Be Concise\" it can take some extra time for completing response. These responses are streamed on the screen so, it doesn't bother the user and it doesn't feel that we have to wait a lot. The App do maintains a history of all the prompts given to the LLM. With a high end Android device responses can be generated in shorter time which can be around 2 minutes.\n\n**How it works:**\n1- Add your HuggingFace token in settings of Android app.\n2- Download Gemma 3n E2B IT or E4B IT (E2B was used in App Demo).\n3- Provide it a picture with your question or some information.\n4- Report will be generated for the provided picture.\n5- You can long press the response and it will give you an option to copy the text.\n\n**Technical Specification:**\nPackage Name: com.pts.bioprotect\n\n**1. Target Android Versions** \n- Minimum SDK: Android 12 (API level 31)  \n- Target SDK: Android 14 (API level 34)  \n- Compile SDK: 35 (Android 15 Preview or future release)  \n- Supported Devices: Phones and tablets running Android 12+  \n\n**2. Device Compatibility**  \n- Screen Sizes: Phones and tablets (responsive Jetpack Compose UI)  \n- Orientations: Portrait (default)\n- Densities: Supports all densities (mdpi to xxxhdpi)  \n\n**3. Hardware & Sensor Requirements**  \n- Camera: Required (used when needed)  \n- Internet: Only required for HuggingFace authentication to download models \n- GPS / Location: Not required  \n- Biometric Sensors: Not required\n\n**4. Permissions**  \n- CAMERA  \n- INTERNET  \n- ACCESS_NETWORK_STATE  \n- POST_NOTIFICATIONS  \n- WAKE_LOCK  \n- FOREGROUND_SERVICE  \n- FOREGROUND_SERVICE_DATA_SYNC  \n\nNote: POST_NOTIFICATIONS requires runtime request on Android 13+.  \n\n**5. Architecture & Development Stack**  \n- Language: Kotlin  \n- UI Toolkit: Jetpack Compose  \n- Build System: Gradle (Kotlin DSL)  \n- IDE: Android Studio\n\n**6. Key Libraries & Frameworks** \n- Core: androidx.core.ktx, lifecycle-runtime-ktx  \n- UI: Jetpack Compose (BOM), Material 3, Navigation Compose  \n- Dependency Injection: Hilt (hilt-android, hilt-navigation-compose)  \n- Data Storage: DataStore (preferences/proto)  \n- Networking: Built-in HTTP, OpenID AppAuth (OAuth)  \n- AI / ML: TensorFlow Lite (CPU/GPU), MediaPipe Tasks (Text, GenAI)  \n- Camera: CameraX (core, camera2, lifecycle, view)  \n- Serialization: kotlinx.serialization.json, protobuf-javalite  \n- Background Work: WorkManager  \n- OSS Attribution: Google Play OSS Licenses plugin  \n\n**7. AI & On-Device Processing**  \n- On-device inference using TensorFlow Lite and MediaPipe  \n- Supports text processing, generative AI\n- Models run locally (no cloud dependency)  \n- TFLite GPU delegate for performance  \n\n**9. Offline Functionality**  \n- Fully functional offline:  \n  - AI models run on-device  \n  - Local data storage via DataStore  \n  - WorkManager for background sync when online  \n- Internet required only for initial auth and updates  \n\n**10. Compliance & Guidelines**  \n- Open-source attributions displayed (via OSS Licenses plugin)  \n- Privacy-first: on-device processing, minimal data collection  \n\n**11. Integration Requirements**  \n- Authentication: HuggingFace Token\n- AI Models: Bundled or securely downloaded TFLite/MP models  \n\n**12. Performance Requirements**  \n- Startup Time: Optimized with SplashScreen  \n- Memory: Efficient model loading; GPU acceleration for TFLite  \n- Battery: Foreground services used only when necessary  \n- Cold Start: Fast launch via modern Android components  \n\n**13. Future-Proofing** \n- compileSdk 35: Ready for Android 15  \n- Jetpack Compose BOM: Ensures version compatibility  \n- Modern stack (Hilt, DataStore, WorkManager): Maintainable and scalable  \n\n**Acknowledgement:**\nAndroid app takes inspiration from AI Edge Gallery from Google",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/TaimourNazar/Google---The-Gemma-3n-Impact-Challenge",
        "demo_video_url": "https://www.youtube.com/watch?v=rkyVjXBWewo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/bio-protect"
    },
    {
        "project_title": "Gemma-first-aid",
        "tagline": "Offline RAG pipeline for emergency aid using Gemma3n (Ollama), Whisper speech recognition, Specter2 embeddings, and Qdrant vector search.",
        "authors": "Jiuyal2, YuhanZhou0920",
        "summary": "Gemma-first-aid – Local RAG with Gemma3n, Qdrant, & Whisper\n\n**Gemma-first-aid** is a fully offline retrieval-augmented generation (RAG) system designed to provide life-saving and first-aid guidance in remote or disaster-stricken areas where internet access is unavailable. The system combines **Gemma3n**, **Specter2**, **Qdrant** and **Whisper** into a compact, local-first pipeline served via a single **Streamlit app**.\n\n**Github repo:** https://github.com/jiuyal2/gemma_first_aid\n\n**Demo video:** https://youtu.be/nGjEGBj662A\n\n**Live demo:** https://gemma-first-aid.duckdns.org/\n\nNotice: Live demo runs on a 8 vCPU, 16GB RAM, 240GB SSD Ubuntu VM, which only support 1 instance of the app at a time. One run of the RAG pipeline takes about 1-2 minutes.\n\n---",
        "technologies": [
            "Docker",
            "Ollama"
        ],
        "source_code_url": "https://github.com/jiuyal2/gemma_first_aid",
        "demo_video_url": "https://youtu.be/nGjEGBj662A",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-first-aid"
    },
    {
        "project_title": "Plant Disease Diagnosis",
        "tagline": "An AI-powered desktop application for diagnosing plant diseases using the Gemma 3n model.",
        "authors": "Andrei Zagrebin",
        "summary": "",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/MisterAndry/Plant-Disease-Diagnosis",
        "demo_video_url": "https://youtu.be/yppOBmNVQVQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/plant-disease-diagnosis"
    },
    {
        "project_title": "Likhee: Write Spontaneously",
        "tagline": "Meet Likhee: your personal writing teacher and companion that helps you write with ease, confidence, and creativity.",
        "authors": "Saikat Barua 1731052",
        "summary": "**Likhee: Write Spontaneously.**\n\nLikhee is a web-based writing assistant that blends intuitive design with advanced AI to help users become more confident and capable writers. From students drafting essays to professionals refining reports, Likhee offers real-time, intelligent feedback through a seamless, distraction-free interface. This document explores the application’s architecture, model integration, development challenges, and the technical rationale behind key decisions.\n\n**Architecture**\n\nAt its core, Likhee is a full-stack application developed with modern web technologies. The architecture emphasizes performance, scalability, and seamless AI integration. The technology stack includes Next.js for the frontend, Genkit for AI orchestration, and Google’s gemma-3n-e4b-it model for natural language processing. Styling and user interface components are handled with Tailwind CSS and Shadcn UI, respectively.\n\n**Frontend Development**\n\nThe frontend is built using Next.js, a React framework that offers server-side rendering and static site generation. These features were essential in ensuring fast load times and improved SEO, particularly for a web app where responsiveness and performance directly affect usability.\n\nTailwind CSS was chosen for its utility-first approach to styling. It allowed for rapid development of a neo-brutalist, minimalist interface that supports focus and reduces cognitive load. This design choice was critical for maintaining the clean, clutter-free writing environment that defines Likhee.\n\nShadcn UI provided a comprehensive set of accessible and customizable components. Its use significantly reduced development overhead, allowing the team to focus on core functionality while still delivering a polished and consistent user interface.\n\n**Backend and AI Integration**\n\nThe backend of Likhee is orchestrated with Genkit, an open-source framework from Google designed for building AI-powered applications. Genkit allows for the creation of structured AI flows, which serve as the foundation for Likhee’s feedback mechanisms.\n\nThere are two primary AI flows integrated into the application:\n\n- Analyze Writing: This flow processes full-text input from the user and uses the gemma-3n-e4b-it model to identify areas for improvement in grammar, style, clarity, and tone. The output is structured, contextual feedback that helps users understand and improve their writing.\n\n- Improve Sentence: This flow targets sentence-level refinements. It generates alternative phrasings along with explanations, helping users better understand the reasoning behind each suggestion.\n\nThe gemma-3n-e4b-it model is accessed via Google AI for Firebase. Its ability to understand context and deliver stylistically appropriate suggestions made it an ideal choice for Likhee. The integration with Genkit ensures low-latency, reliable communication between the frontend and the AI engine.\n\n**Design Challenges and Solutions**\n\nOne of the core challenges during development was crafting prompts that consistently elicited accurate, relevant feedback from the language model. This required multiple iterations of prompt engineering, where the team refined instructions and tested outputs across a range of writing styles and inputs.\n\nAnother significant challenge was the seamless integration of AI-generated feedback into the text editor. The goal was to mirror the familiar experience of tools like Grammarly, where suggestions appear inline and can be accepted or rejected with minimal disruption. Achieving this required careful management of editor state and precise synchronization between user input and model feedback.\n\n**Intelligent Writing Modes**\n\nLikhee supports two distinct writing modes, each designed to address different user needs:\n\n- Learn Mode offers structured, gamified writing lessons. Inspired by platforms like Duolingo, this mode helps users build foundational writing skills through interactive exercises.\n\n- Play Mode provides an open-ended space where users can write freely and receive real-time, advanced feedback. It is ideal for drafting essays, journal entries, or professional documents.\n\nThe backend dynamically selects the appropriate AI flow based on the chosen mode, ensuring that feedback is aligned with the user’s intent.\n\nConclusion\n\nLikhee demonstrates the potential of combining thoughtful design with powerful AI tools. By leveraging Next.js, Tailwind CSS, Shadcn UI, Genkit, and the gemma-3n-e4b-it model, the team has created an application that empowers users to write more effectively and express themselves with confidence. Every technical decision was made in service of a core mission: to transform writing from a source of hesitation into a source of flow.\n\nUse Likhee. Write Spontaneously.",
        "technologies": [],
        "source_code_url": "https://github.com/GitsSaikat/Likhee_Gemma",
        "demo_video_url": "https://youtu.be/m7ffcTMNpZY?si=ol98jXtRoBbvxA1x",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/likhee-write-spontaneously"
    },
    {
        "project_title": "One Minute: the help you need when seconds count",
        "tagline": "Offline, privacy-oriented agents that offer end-to-end help to emergency victims.",
        "authors": "rorosaga, Ricardo Mendez C",
        "summary": "One Minute is an offline, privacy oriented, agent platform that offers help to people undergoing emergency situations. Our project is composed of a library we developed for creating functional, tool calling, ReAct agents in python for models like Gemma3n, a package containing two standalone, independent agents implemented using the library, and two frontends: one in react (currently WIP) and one in streamlit.\n\nWe believe this idea is very important, as one minute can be the difference between life and death in emergency, high stakes scenarios. Having a bridge between impaired victims and 911-operators can facilitate early dispatching of help, ensure proper emotional support, improve survival rates by recommending appropriate first aid techniques, as well as help people that are usually alone and may not have access to someone to practically save them on a daily basis if they have an emergency. \n\nWe want to extend this help to everyone, anywhere, anytime: especially when seconds count.\n\n**Streamlit Supervisor View:**\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F15999302%2Fca33c5e73813808d1e73f8ecc3df4924%2FScreenshot%202025-08-07%20at%201.49.25AM.png?generation=1754524183425616&alt=media)\n\n**Streamlit Victim View:**\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F15999302%2F36d537bc4761bf90db1bdf0208de01a2%2FScreenshot%202025-08-07%20at%201.50.25AM.png?generation=1754524247826089&alt=media)\n\n**React App Supervisor View:**\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F15999302%2Fd3dfc2f01888ea6cf2a44d560fd1e512%2FScreenshot%202025-08-07%20at%201.51.31AM.png?generation=1754524311370942&alt=media)\n\n**React App Victim View:**\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F15999302%2F5fe7a1f302126c30431c587729e69de7%2FScreenshot%202025-08-07%20at%201.52.56AM.png?generation=1754524389369171&alt=media)",
        "technologies": [],
        "source_code_url": "https://github.com/one-minute-gemma/one-minute-agent",
        "demo_video_url": "https://www.youtube.com/watch?v=UJCZy5DjC1o",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/one-minute-the-difference-between-life-and-death"
    },
    {
        "project_title": "Edify Technical Writeup",
        "tagline": "Edify is an Android app that uses Gemma to create personalized & interactive study experiences",
        "authors": "Dhairya Kumar",
        "summary": "Edify Technical Writeup",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Dhairya10/edify",
        "demo_video_url": "https://youtu.be/F0D_17qsJKg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/edify-technical-writeup"
    },
    {
        "project_title": "Ward: The On-Device AI Guardian for a World in Crisis",
        "tagline": "Ward empowers individuals in any disaster to make life-saving decisions by turning their phone into an offline, AI-powered survival expert",
        "authors": "Andrew Turtsevych",
        "summary": "Ward: On-Device Survival Analysis\n\n**Live Demo:** https://ward-rescue.netlify.app/",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/atNeoCoder/Ward/tree/atBranch005",
        "demo_video_url": "https://youtu.be/VxC-ceER_gg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ward-the-on-device-ai-guardian-for-a-world-in-cris"
    },
    {
        "project_title": "Gemma 3n Based AI Teacher",
        "tagline": "An AI teacher that guides the student and encourages learning",
        "authors": "BarryTkTc, Yintercept",
        "summary": "Some kids get professional teachers, labs, and resources, but others are left behind. Education inequality is a serious problem and widens social gaps. Also, right now, millions of students worldwide turn to AI for help—but AI can be easily misused. Many use AI for completing their homework, and the result is that they don't learn. That's why we created an AI teacher specialized to guide students like real teachers.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/tGGltYc9H9o",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-3n-based-ai-teacher"
    },
    {
        "project_title": "The Curious Frame - An offline AI-based tutor for curious kids",
        "tagline": "The curious frame is a portable offline powered-by-AI device aiming to explain everyday objects to curious kids in their own language.",
        "authors": "Frédéric Collonval ",
        "summary": "Context\n\nThe idea for this project came from my two-years-old kid that is very curious about everything like any kids. So, I wanted to offer them an AI-powered device that can help them fulfill that curiosity.",
        "technologies": [
            "Docker",
            "Flask",
            "Ollama"
        ],
        "source_code_url": "https://github.com/webscit/curious-frame",
        "demo_video_url": "https://youtu.be/yx0OXfG8UnQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/the-curious-frame-an-offline-ai-based-tutor-for-cu"
    },
    {
        "project_title": "Gixit - A Learning assistant ",
        "tagline": "Gixit is an offline learning assistant designed to address Nigeria's overcrowded classrooms with few teachers and no internet access.",
        "authors": "christian Uwa",
        "summary": "Gixit is a learning assistant designed to give students a deep, interactive, and rewarding learning experience that also challenges them intellectually. It was built to address the educational challenges in my country, where there is a serious lack of infrastructure in many public schools.\nIn most public institutions, there simply aren't enough teachers to serve the growing number of students. With large class sizes and limited resources, it's nearly impossible to ensure that every student can explore topics in depth or pursue new areas of interest out of curiosity. The result is a learning environment that often only scratches the surface.\nGixit was created to fill this gap by providing personalized learning support that helps students dig deeper into topics they're interested in or having trouble with. What made this work was Gemma, the AI model that runs Gixit, and the fact that it can run completely offline.\nThis offline capability turned out to be really important. In many schools across my country, internet access is either very limited or just not there at all. Most AI educational tools can't work without internet, which leaves students without help when they need it. Because Gemma works entirely offline, Gixit can give students everything they need: helpful feedback on their work, study plans that fit their learning style, quizzes that adapt to their level, and tools to reflect on what they're learning, all without needing any internet connection.\nThrough careful prompt engineering, we've made Gemma's responses more efficient and focused, so students get better help faster. This means a student in a rural school with no connectivity gets the same quality AI-powered learning experience as someone in an urban school with high-speed internet.\nThis approach becomes even more important when you consider that my country has the highest number of out-of-school children in the world. Millions of students are missing out on education entirely, not just because of poverty, but also due to a lack of accessible tools and structured learning support. Gixit was built with these students in mind by making intelligent, self-paced learning available offline, it brings education directly to those who need it most, wherever they are.",
        "technologies": [],
        "source_code_url": "https://github.com/Makuo12/Gixit",
        "demo_video_url": "https://www.youtube.com/watch?v=kNwuFYjjRzA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/hiiitjjjjjj"
    },
    {
        "project_title": "3VA: The Revolution That Gives Back What Was Never Lost",
        "tagline": "Your voice was always there. We just built the bridge to set it free.",
        "authors": "Juan Francisco Pérez, tierralibre",
        "summary": "3VA: Giving Eva Her Voice After 29 Years of Silence\n\n**Eva has cerebral palsy. She’s brilliant, creative, and has so much to say. But current systems don’t let her speak with her own voice. That’s why we created 3VA.**\n\n**[▶️ Watch Eva’s Story (3 min)](https://www.youtube.com/watch?v=AN9BYKK6pak)**\n\n-----",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/libreearth/eva_voice_release",
        "demo_video_url": "https://www.youtube.com/watch?v=AN9BYKK6pak",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/3va"
    },
    {
        "project_title": "Cogniflow - Offline AI Powered Education System",
        "tagline": "Empowering rural students with offline, interactive AI-driven learning and real-time progress tracking",
        "authors": "ADITYA MAHARAJ PARBAT, Kiran Belsare, Samruddhi Kadam, Kashish Hanchate, Omkar Machale",
        "summary": "Access to quality education is a challenge in regions with limited or no internet connectivity. Our project, CogniFlow, provides a fully offline AI-powered educational platform designed specifically for rural and low-connectivity areas.\n\nBuilt on Google’s Gemma 3n model—served locally using Ollama, an open-source framework for running large language models offline—CogniFlow offers personalized learning with chapterwise quizzes, flashcards, and AI-generated video recaps. It features an offline AI chatbot for tutoring and interactive games such as Hangman, Pictionary, and Spelling Bee to make learning engaging.\n\nThe system tracks student progress in real time, providing daily check-ins, experience points, coins, and subject-wise progress, along with competitive activities managed by teachers to foster motivation and growth.\n\nTechnically, it’s built with React (.tsx), cached via service workers (sw.js), and stores data locally. Videos are generated offline using OpenCV, FFmpeg, and Python, with chapter text extracted from PDFs using pdfplumber and PyMuPDF. Ollama ensures that Gemma 3n runs smoothly and fully offline even on standard local computers by handling model setup and inference in a user-friendly way.\n\nCogniFlow bridges the digital divide by enabling high-quality, AI-enhanced education where traditional internet-based resources fail. Future work includes optimizing for low-end devices, expanding content coverage, and adding teacher analytics.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Adityaparbat/Cogniflow",
        "demo_video_url": "https://youtu.be/H7T9pcdAQQI?si=BCUf-VZZck9nS3oe",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/cogniflow-offline-ai-education-system"
    },
    {
        "project_title": "Pragati AI: A bridge from Potential to Progress",
        "tagline": " An Accessible Interview & Feedback Platform for the Visually Challenged",
        "authors": "Manasvi Logani, Amal Raj Singh",
        "summary": "The idea for Pragati AI came from our direct experience working with visually challenged students in an NGO. We noticed a major gap in the support system: while students get help to reach university, that support often disappears after graduation. This leaves them on their own to face the difficult process of finding a job, a process where interview skills and communication are essential. Through our own coaching sessions, we saw the incredible difference that dedicated practice could make. This showed us how widespread and overlooked this need truly was.\n\nTo address this gap, we developed Pragati AI, a secure, offline-first interview coaching platform designed specifically for visually challenged individuals. \"Pragati,\" the Hindi word for progress, is the core of our mission to build a tool that is both powerful and truly accessible. The application operates entirely on-device, ensuring user privacy and enabling deployment in areas with limited or no internet connectivity, such as rural campuses and local NGOs.\n\nAt its core, Pragati AI provides a safe space for users to practice interviews through a fully voice-driven interface. Beyond practice, the platform's key innovation is its detailed analysis engine, which gives users actionable feedback on both the content and delivery of their answers. This empowers them to iteratively build confidence and master the skills needed to bridge the gap between their education and their career goals.\n\nFor a comprehensive overview of the **technical architecture**, our **iterative problem-solving approach**, and the **future product roadmap**, please refer to the detailed project report submitted alongside this description.",
        "technologies": [],
        "source_code_url": "https://github.com/singhamal001/Pragati-AI/tree/main",
        "demo_video_url": "https://youtu.be/ky04_8zvoFk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/pragati-ai-a-bridge-from-potential-to-progress"
    },
    {
        "project_title": "Green Duty",
        "tagline": "A platform that links verified tree planting to access to jobs, education, and services — driving responsibility and a greener future.",
        "authors": "Mubin UL Islam Chowdhury, Sanzar Rahman , Arnab Dey, Hredoy Sen",
        "summary": "🌱 Green Duty 🌱\n*Using AI for Nature – Everyone’s Duty to Plant Trees*\n\nIn the age of artificial intelligence and large-scale machine learning, we are rapidly advancing toward a highly automated and data-driven world. However, this progress comes at a cost—**massive energy consumption**, increased carbon footprint, and a growing disconnect from our natural environment.\n\n**This project is an effort to use AI for the betterment of nature.**\n\nIn the age of artificial intelligence and large-scale models (VLMs, LLMs), technology is advancing faster than ever — but at a significant cost. These models require immense computational resources, consuming large amounts of electricity and indirectly contributing to environmental degradation.\n\nThis project aims to counterbalance that impact by encouraging personal responsibility for the environment. Our application uses Vision-Language Models (VLMs) to classify the progress of individual tree planting activities. It validates whether a tree has been genuinely planted and maintained over time using AI-powered image verification.\n\nOnce verified, the user is awarded a \"Green Responsibility Certificate\" — a digital credential that serves as proof of their contribution to nature. Over time, this certificate can serve as:\nA resume-enhancing credential for students and professionals,\nA green metric for companies and educational institutions to assess environmental responsibility,\nA policy tool for governments to reward civic participation in climate action.\n\n**This is not just planting trees — it's planting a culture of responsibility.**\n\nIn an era where AI often takes from nature through resource-intensive computation, this initiative seeks to return something meaningful to the environment. It's a symbolic and practical step towards a greener, more responsible future.\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F11263799%2F3f62ea2fbbe89dda2a9ee3995494e3cc%2FGreen%20Duty%20-%20visual%20selection%20(1).png?generation=1754507067439440&alt=media)\n\nThe Green Duty system addresses these challenges through three core APIs that work together to provide comprehensive tree analysis and certification:\n\n#",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/mubinui/green_certification/tree/release-version",
        "demo_video_url": "https://youtu.be/cUkDYNIBReg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/green-duty"
    },
    {
        "project_title": "Clarity - Find Your Voice",
        "tagline": "The private, on-device AI that helps you find your words and connect with confidence.",
        "authors": "Leafx203",
        "summary": "",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/LEAF420/clarity-ai",
        "demo_video_url": "https://youtu.be/tleK7c9nJCs",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/clarity-find-your-voice"
    },
    {
        "project_title": "Moransa: Hope in Code for Guinea-Bissau with Gemma 3n",
        "tagline": "Moransa: Bringing hope where roads can't reach. A project with a soul.",
        "authors": "Filipe Buba",
        "summary": "Moransa: Hope in Code for Guinea-Bissau with Gemma 3n\n\n\nThis project was not born from a technical challenge, but from a tragedy that echoes to this day. In an isolated village in my Guinea-Bissau, a woman and her unborn child lost their lives during childbirth. The cause was not an untreatable medical complication, but a dirt road that the rain had made impassable. Priests tried to help, but the car couldn't get through. They arrived by motorcycle too late and, without medical knowledge, all they could do was pray beside her for eight hours.\nI heard this story, told by a priest in tears during Sunday mass. From that day on, the helplessness I felt transformed into a relentless search for a solution. How could we bring vital knowledge to where roads don't reach? How could we put the power of a specialist in the hands of a community agent, a midwife, a farmer?\nThe answer seemed distant, until the launch of Gemma 3n. A powerful, multimodal AI model and, crucially, capable of functioning 100% offline. It was the missing piece. The technology that could fulfill the promise I made to myself: to build something that would bring real help to my moransa.\nIn Guinea-Bissau, \"moransa\" is more than a house; it's our refuge, the place where our soul rests, where another's pain is our pain. This project is called Moransa because it was born from my home, from my pain, to serve as a refuge of knowledge and hope for other communities, in Guinea-Bissau and around the world. It's the materialization of hope in code.\nThe Solution: An Offline Community Assistance Ecosystem\nMoransa is an Android application that functions as a complete community assistance system, designed to operate in environments with zero or intermittent connectivity. It addresses critical challenges in health, education, agriculture, accessibility, and environmental sustainability, using Gemma 3n as its main AI engine, executed locally through Ollama.\n2.1. Technical Architecture: Robust, Offline and Intelligent\nMoransa's architecture was designed to be resilient and efficient on devices with limited resources, ensuring that help reaches where it's most needed.\ngraph TD subgraph \"User Device (100% Offline)\" A[Moransa App - Flutter] B[Local API Gateway - FastAPI] C[Business Logic - Python Modules] D[AI Engine - Gemma 3n via Ollama] E[Local Database - SQLite/ObjectBox] end\nA --> B\nB --> C\nC --> D\nC --> E\n\n\n• Frontend (Flutter): Ensures a native, high-performance user experience on Android devices, with an interface designed to be intuitive and accessible. • Backend (FastAPI): A local, lightweight, and asynchronous Python server that manages business logic and serves as a bridge to the AI engine. • AI Engine (Gemma 3n via Ollama): The heart of the system. Ollama allows us to package and run Gemma 3n reliably and locally, eliminating any internet dependency for AI functionalities. • Local Database: Stores the corpus of validated translations, educational materials, and other essential data for offline operation.\n2.2. Innovative Use of Gemma 3n: Precision and Adaptation\nThe genius of Gemma 3n is its flexibility. Instead of using a one-size-fits-all approach, we specialize the model for each task, adjusting its parameters to optimize results. This shows a deep and deliberate use of technology.\nModule Use Cases temperature Technical Justification 🏥 Health Emergency diagnosis, childbirth 0.2 - 0.3 Maximum precision. In life-or-death situations, responses must be factual, deterministic, and protocol-based, with no room for creativity. 🌾 Agriculture Pest diagnosis, soil health 0.4 Technical precision. Recommendations must be scientifically correct, but with flexibility to adapt to varied farmer descriptions. 📚 Education Story generation, lesson plans 0.6 - 0.7 Controlled creativity. Ideal for creating engaging and adapted educational content, encouraging learning without sacrificing factual accuracy. ♿ Accessibility Environment description, simplification 0.5 Balance. Description needs to be factual, but with enough naturalness to be easily understood.\nThe Revolution: Gamified Community Validation\nOur greatest technical and social innovation is the Community Validation System. We realized that for low-resource languages like Creole, Balanta, or Fula, no AI model, however advanced, can alone capture the richness and cultural precision.\nTherefore, we inverted the paradigm:\nGemma 3n Generates the Challenge: The model generates relevant phrases and scenarios in Portuguese (e.g., \"Apply direct pressure to the wound\").\nThe Community Translates and Validates: Native speakers translate these phrases into their local languages through a gamified interface.\nCollective Wisdom Creates the Corpus: Users vote on each other's translations. A translation is only approved and added to \"Rescuer Mode\" (the offline emergency mode) after reaching a consensus threshold.\nThis system, powered by a robust backend with PostgreSQL and a unified Flutter interface, transforms users from mere information consumers into active knowledge builders. It ensures that every piece of information in emergency mode is not only technically correct but culturally authentic and validated by those who matter most: the community itself.\nImpact Modules: From Theory to Practice\nMoransa is a modular platform, where each component addresses a specific challenge: • 🏥 First Aid System: Provides step-by-step emergency protocols, with special focus on childbirth and maternal-child health. • 📚 Educational System: Allows teachers in remote areas to generate lesson plans, stories, and exercises adapted to local reality. • 🌾 Agricultural System: Helps farmers diagnose plant diseases and receive advice on sustainable practices. • ♿ Accessibility System: Uses Gemma 3n's multimodal capabilities to describe environments for the visually impaired and simplify texts. • 🌳 Environmental Sustainability System: Enables participatory monitoring of local biodiversity through species identification by image. • 🌍 Translation and Preservation System: Serves as the backbone of the validation system, creating the first large-scale digital corpus for Guinea-Bissau languages.\nConcrete Implementation: From Vision to Reality\n5.1. Demonstrated Technical Achievements\nMoransa is not just an inspiring idea — it's a functional reality that already demonstrates tangible impact:\n🔧 Fully Functional System:\nFastAPI Backend operational with 15+ specialized routes\nNative Flutter Application for Android with responsive interface\nGemma 3n Integration via Ollama working 100% offline\nCommunity validation system with implemented gamification\nLocal database with multi-language support\n📊 Real Impact Metrics:\n6 specialized modules active (Health, Education, Agriculture, Accessibility, Sustainability, Translation)\nSupport for 5+ local languages from Guinea-Bissau (Creole, Balanta, Fula, Mandinka, Papel)\nEmergency protocols validated by health professionals\nEducational content adapted to local reality\nAgricultural diagnostics based on scientific knowledge\n🌍 Reach and Scalability:\nModular architecture allows expansion to other African regions\nSynchronization system for updates when connectivity is available\nCommunity validation ensures cultural and linguistic precision\nComplete offline mode eliminates infrastructure dependency\n5.2. Demonstrated Transformative Impact\n🏥 Maternal-Child Health:\nEmergency childbirth protocols translated and validated\nFirst aid guides in local languages\nPotential 40% reduction in mortality due to lack of basic medical knowledge\n📚 Educational Revolution:\nTeachers in remote areas can generate adapted content instantly\nStories and exercises in native languages preserve local culture\nDemocratization of access to quality educational materials\n🌾 Food Security:\nPest and disease diagnosis through images\nAgricultural advice based on local conditions\nPotential to reduce crop losses by 30%\n♿ Digital Inclusion:\nEnvironment description for the visually impaired\nText simplification for different literacy levels\nFirst accessibility tool in Guinea-Bissau languages\n5.3. Inspiring Vision: Beyond Technology\n🌟 Paradigm Shift: Moransa inverts the traditional logic of humanitarian aid. Instead of waiting for help to come from outside, it empowers local communities with knowledge and tools to help themselves.\n🤝 Active Cultural Preservation: Each translation validated by the community not only improves the system but also preserves and digitalizes languages at risk of extinction, creating a lasting cultural legacy.\n🔄 Intrinsic Sustainability: The system continuously improves through community use, creating a virtuous cycle where each user contributes to the common good.\n5.4. Global Replication Potential\n🌍 Scalable Model:\nArchitecture adaptable to any region with similar challenges\nCommunity validation system applicable to any language\nSpecialized modules can be customized for different contexts\n📈 Exponential Impact:\nPhase 1: Guinea-Bissau (1.9M inhabitants)\nPhase 2: West Africa (400M inhabitants)\nPhase 3: Global rural communities (3B+ people)\n💡 Technological Innovation: The pioneering use of Gemma 3n for multilingual community validation establishes a new standard for humanitarian AI, demonstrating how advanced models can be democratized to serve the most vulnerable populations.\n5.5. Next Steps: Google AI Edge\n🚀 Technological Evolution: The planned integration with Google AI Edge will represent a qualitative leap:\nEven more efficient processing on devices with limited resources\nExpanded multimodal capabilities for image and audio analysis\nIntelligent synchronization with cloud models when available\nAutomatic optimization based on local usage\nThis evolution will maintain the project's core philosophy — complete offline functionality — while adding advanced capabilities that will amplify impact in each served community.\nConclusion: Technology with Soul and Proven Impact\nMoransa transcends the \"application\" category to become a movement of social transformation through technology. It's not just a proof of concept, but a functional reality that already demonstrates tangible impact.\n🎯 Measurable Impact:\nFully functional and tested system\nProtocols validated by specialists\nInterface tested with real users\nProvably scalable architecture\n💫 Inspiring Vision: Every line of code written carries the memory of that woman we lost and the hope of all the lives we can save. Moransa proves that the most advanced technology can and should serve those most in need.\n🌈 Tangible Positive Change: This is not a project about the future — it's about the present. About transforming helplessness into action, isolation into connection, and loss into hope. Each implemented functionality represents lives that can be saved, knowledge that can be preserved, and communities that can prosper.\nWe built Moransa not just because technology made it possible, but because the memory of those we lost made it necessary. This is our mission: to use Gemma 3n to ensure that hope — our moransa — reaches everyone, no matter how distant the road.\nTechnology has a soul when it serves humanity. Moransa is living proof of this truth.",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/7hj7yxEJPs0?si=ea1T6zAxGPtHxEsx",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/moransa-hope-in-code-for-guinea-bissau-with-gemma"
    },
    {
        "project_title": "NeuraTutor: Your ADHD-Friendly AI Study Buddy",
        "tagline": "An ADHD-friendly AI study assistant that explains homework, gives pep talks, breaks down tasks, and boosts focus using the Gemma 3n LLM",
        "authors": "fizza mukhtar",
        "summary": "**🧠 NeuraTutor: Your ADHD-Friendly AI Study Buddy**\n\n**📜 Technical Write-Up (Proof of Work)**\n\n**🎯 Problem Statement**\nStudying can feel overwhelming — especially for students with ADHD and other neurodivergent needs. Conventional AI tutors often flood users with walls of text, complex terms, and lack emotional intelligence.\n\nThis creates friction, reduces focus, and fails to support students in a way that feels truly personalized, empathetic, and structured.\n\nNeuraTutor was built to bridge that gap:\nAn intelligent, visual, and emotionally aware tutor that breaks down homework, keeps students on track, and provides a much-needed pep talk when things get tough.\n\n**System Architecture**\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F27285305%2Feaddc304fbbbeecd37f6be6ba00c7824%2FScreenshot%202025-08-05%20235507.png?generation=1754465053481975&alt=media)\n\n**Core Features & Engineering**\n\n**1. ✍️ Explain My Question (Text or Image)**\n\nAccepts image input (OCR) or text\n\nUses prompt chaining to ask Gemma 2B-Instruct for:\n\nSimplified vocabulary\n\nStep-by-step logic\n\nFriendly, non-robotic tone\n\nGenerates a Mind Map using Graphviz\n\n✅ Designed for neurodivergent learners: visual, modular, emotionally calm.\n\n**2. 💬 Pep Talk Generator**\n\nType messages like: “I feel unmotivated” or “I failed my test”\n\nPrompted to respond with empathy, like a kind older sibling\n\nEmotional encouragement with warmth and reassurance\n\n**3. 🗂️ Task Breakdown Planner**\n\nInput: big task (e.g., “Write a 5-page essay”)\n\nOutput: Pomodoro-style chunks with:\n\nMotivating labels\n\nTime estimates\n\nBreak reminders\n\nKeeps it simple and low-pressure\n\n**4. ❌ Mistake Fixer**\n\nPaste your incorrect answer\n\nReturns:\n\nKind explanation of the mistake\n\nEncouraging correction\n\nNo judgment tone\n\n**5. 🎯 Focus Booster**\n\nDelivers quick, actionable strategies like:\n\n“Set a timer for 5 minutes and just begin. You’ll build momentum.”\n\nUses a mix of pre-written tips + Gemma-generated ones\n\n**🔬 How Gemma 2B Was Used**\n\nGemma 2B-Instruct was selected for:\n\n🧠 Instruction tuning – handles task-specific prompts well\n\n⚡ Efficiency – lightweight and fast (even on Colab)\n\n🤝 Open-weight – transparent and modifiable\n\n**Prompt Engineering Approach:**\n\nEach feature has its own prompt template, refined via iteration.\nExample (Explain):\n\n\"Act like a friendly tutor explaining this to a 13-year-old with ADHD. Use simple words, steps, and encouragement. Explain this: {text}\"\n\n**🔧 Tech Stack**\n\n| Layer   | Technology                        |\n| ------- | --------------------------------- |\n| UI      | Gradio                            |\n| Backend | Python (Flask-style logic)        |\n| LLM     | Gemma 1.1 2B-Instruct             |\n| OCR     | Pytesseract                       |\n| Visuals | Graphviz (mind maps) |\n| Hosting | GPU-enabled Google Colab Notebook |\n\n\n\n\n**🧠 Designed for Neurodivergent Students**\n\n✨ Short explanations — less cognitive overload\n\n📌 Visual mind maps — supports visual learners\n\n💬 Tone-engineered — no judgment, only support\n\n⏱️ Task chunking — avoids overwhelm\n\n💖 Pep talks — emotional regulation support\n\n**🧗 Challenges Overcome**\n\nGemma’s tone was too robotic\n→ Solved via role-based prompting + emotional cues\n\nImage OCR quality was low\n→ Used preprocessing + fallback logic\n\nMind map parsing + layout\n→ Used dynamic Graphviz layout + NLP-based node creation\n\nSummary extraction from large answers\n→ Built sentence classifier + keyword filter\n\nOpen + offline-first architecture\n→ Ensured privacy & hackathon compliance (no OpenAI APIs)\n\n**✅ Why This Stack Was Right**\n\nGemma 3n — Fast, local, customizable\n\nGradio — Lightning-fast UI prototyping\n\nGraphviz/Matplotlib — Lightweight visuals\n\nFully offline — Ensures privacy and speed\n\n**🔮 What’s Next**\n\nAdd voice/audio explanation support\n\nExport mind maps as PDF / PNG\n\nAdd cartoon-style visual options\n\nExpand to support different neurotypes\n\n**💛 Final Thoughts**\n\nNeuraTutor isn’t just another AI tutor.\nIt’s a study companion that adapts to how students feel and how their minds work — especially those often left out of conventional learning systems.\n\nI hope this project inspires more neurodivergent-friendly tools to be built with empathy, intelligence, and love.",
        "technologies": [
            "Flask",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Fizza-Mukhtar/NeuraTutor",
        "demo_video_url": "https://youtu.be/XqmkDWujBtY?si=PsfYSoyklRLH1Iwa",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/neuratutor-your-adhd-friendly-ai-study-buddy"
    },
    {
        "project_title": "Gemma AI Educational Suite",
        "tagline": "Offline AI tutoring suite powered by Gemma 3n. Upload textbooks, ask questions, generate quizzes - all without internet.",
        "authors": "Dalton Payne",
        "summary": "Gemma AI Educational Suite\n\nThe Gemma AI Educational Suite is an offline educational application powered by Gemma 3n language models optimized through Unsloth for local deployment. This is an attempt towards transforming any device into a private AI tutor that works completely offline, addressing critical needs in education: student privacy, internet dependency, and personalized learning accessibility.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/DaltonPayne/Gemma-AI-Educational-Suite",
        "demo_video_url": "https://www.youtube.com/watch?v=GmCh156T_GQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-ai-educational-suite"
    },
    {
        "project_title": "MoLi: AI Mobile Librarian with Gemma 3n",
        "tagline": "An app that helps bookmobiles serving remote areas to scan, recommend, analyze, and manage books on the go.",
        "authors": "Annie Chen",
        "summary": "MoLi is an AI-powered Mobile Librarian app designed to help mobile libraries, also known as \"bookmobiles\", deliver the knowledge of books to remote areas. It can check books in and out by scanning covers, provide tailored book recommendations, and analyze the library collection, all while running on a mobile device without internet connections.\n\nMoLi positively impacts mobile libraries and communities by making book access easier and more efficient. It simplifies tasks like checking books in and out, eliminating the need for expensive barcode systems. By providing personalized book recommendations, it ensures readers get books that match their interests, fostering a love for reading. It also helps librarians manage their collections more effectively by analyzing inventory data, which saves time and labor. As a result, bookmobiles can be more effective in their core mission of delivering the right books to the right hands. \n\nThe Youtube video contains a demo of MoLi's core functions. The Kaggle notebook contains the code, explanations of the code structure, screenshot demos, and a discussion on future work.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/lzP3vdmKNvM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/moli-ai-mobile-librarian-with-gemma-3n"
    },
    {
        "project_title": "Smart Learning: AI-Powered Education for Everyone, Everywhere",
        "tagline": "Building Accessible, Private Learning with Google's Gemma 3n",
        "authors": "Nicoleta Mocanu, Ionel Miroslav",
        "summary": "Smart Learning: AI-Powered Education for Everyone, Everywhere\n**Building Accessible, Private Learning with Google's Gemma 3n**",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/johnnie2701/smartlearning",
        "demo_video_url": "https://youtu.be/AC9Y_mH33aU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/smart-learning-ai-powered-education-for-everyone-e"
    },
    {
        "project_title": "Solve It Smart – A Private, Offline, Conversational Math Coach",
        "tagline": "An on-device math tutor that teaches problem solving step by step—powered by an embedded Gemma 3N model, running fully offline on iPad.",
        "authors": "M. Akif Acar",
        "summary": "**Solve It Smart** is an on-device conversational math tutor that teaches problem solving—not just answers. Built for the iPad and fully offline, it uses an embedded Gemma 3N model to walk through complex problems step by step, explaining techniques like *working backwards*, *use cases*, and more. The app deep-links directly from an **interactive problem-solving iBook** (“Catch the Fish”) to load a problem, reference a provided solution, and then regenerate a detailed step-by-step explanation using Polya-style strategies.\n\nUsers can ask natural-language questions like **“Why is this strategy used?”** and receive thoughtful, LaTeX-rendered responses. The app supports multimodal input, includes math rendering with copyable LaTeX, and handles both geometric and advanced algebraic problems. \n\nSolve It Smart is entirely open source and runs natively on iPad using Swift and Metal. No internet, no cloud—just your problem, your question, and a brilliant embedded tutor.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/memetcircus/solveitsmart",
        "demo_video_url": "https://youtu.be/NdElVZEwUIw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/solve-it-smart-a-private-offline-conversational-ma"
    },
    {
        "project_title": "GemmaOS  An On Device AI Ecosystem",
        "tagline": "GemmaOS is a next-generation on-device AI platform that builds an expandable app ecosystem around a single AI Core.",
        "authors": "HuhYongJoon",
        "summary": "📘 GemmaOS Project Guide\n\n**A Modular On-Device AI Platform Powered by GemmaCore**\n\n---",
        "technologies": [
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/6taboFiCMHU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemmaos-an-on-device-ai-ecosystem"
    },
    {
        "project_title": "PolisAI: Clarity for the Voter, Stability for the World",
        "tagline": "An on-device AI analyst, built with Gemma 3n to empower the rational, interest-based choices that form the foundation of a healthy democracy",
        "authors": "Jason Zhao",
        "summary": "##",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=U2j8nTVuuT0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/polisai-clarity-for-the-voter-stability-for-the-wo"
    },
    {
        "project_title": "Gemma DriveSense: AI-Powered Driving Risk and Wellness Assistant",
        "tagline": "Offline AI for Real-Time Driver Risk Detection and Wellness Coaching",
        "authors": "Karthikeyan Lakshminarayanan",
        "summary": "🎯 Project Summary:\nGemma DriveSense is an iOS + Apple Watch application that leverages the privacy-preserving, multimodal power of Gemma 3n to continuously monitor a driver’s health and stress levels while on the road. The app detects movement and begins real-time collection of biometric and voice data, sends it to the iPhone where Gemma 3n assesses the driver's physical and mental condition, and returns instant risk-level feedback and wellness coaching — entirely on-device and offline.\n\n💡 Problem Addressed:\nRoad safety remains a global challenge. Over 845,000 people have died on U.S. highways since 2000, and 94% of crashes are due to driver behaviour — often linked to fatigue, distraction, or stress. Yet most vehicles lack real-time wellness monitoring. Current driver training does not account for real-world mental strain or biometric risk detection. Gemma DriveSense fills this gap.\n\n✅ Solution Overview:\n- Automatic health monitoring triggered by location change (e.g., user starts driving)\n- Continuous biometric collection: heart rate, HRV, blood oxygen, respiratory rate, step count, energy\n- Voice input to detect emotional and mental stress\n- Gemma 3n performs multimodal analysis (voice + vitals)\n- Risk levels (Low, Moderate, High) are shown on Watch and iPhone\n- Real-time, actionable feedback with zero cloud dependency\n\n🧠 Why Gemma 3n?\n- Offline-first & Privacy-Preserving: No cloud computation or data exposure\n- Multimodal Understanding: Fuses biometric + voice inputs into real-time insights\n- Low Latency: Runs on-device with instant feedback\n\n🚀 Technical Stack:\n- iOS + watchOS app with SwiftUI\n- HealthKit + CoreLocation + AVFoundation\n- WatchConnectivity for real-time sync\n- Gemma 3n model integration via MediaPipe and fallback on Gemma 2b\n- Custom fallback system for rule-based risk analysis when AI is unavailable\n\n🎥 Demo Video:\nAvailable at: https://youtu.be/zjhToFeTKFg \n\n📊 Impact Potential:\nGemma DriveSense empowers individuals and fleets with real-time driver safety analysis, improving decision-making and reducing preventable accidents. It’s scalable to commercial fleets, elder care, delivery services, and beyond.\n\n💻 GitHub Repository:\nhttps://github.com/lkarthik76/DriveSense\n\n🏁 Status:\nFully implemented MVP\nWorking demo across iPhone and Watch\nGemma 3n model integrated and tested",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/zjhToFeTKFg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-drivesense-ai-powered-driving-risk-and-welln"
    },
    {
        "project_title": "Lifeloge op",
        "tagline": "Lifelog which allow you better to understand your past and improve your future",
        "authors": "Dima Yerebakan",
        "summary": "🧠 Problem:\nPeople often struggle to process emotions and patterns in their thinking. Journaling helps, but few tools offer meaningful insight beyond raw text or timestamps.\n\n✨ Vision:\nLifelog is your personal AI-powered thought companion. It helps users reflect and grow by classifying journal entries using life trap theory — a psychological model — giving emotional patterns structure and visibility.\nImagine mental wellness support that works privately, offline, and grows smarter with every thought.\n\n🌍 Impact:\n- Enables early self-awareness of harmful thinking patterns (e.g. self-sacrifice, unrelenting standards).\n- Empowers people without access to therapy or simplify the transition between the therapist \n- Works on-device — no cloud needed — enabling global, private use.",
        "technologies": [],
        "source_code_url": "https://github.com/qlip/lifelog/upload/main",
        "demo_video_url": "https://youtu.be/hB9Ym9rUP7I",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/lifeloge-op"
    },
    {
        "project_title": "EduLibreX - Offline Personalized Education Platform",
        "tagline": "EduLibreX revolutionizes education by delivering completely offline learning experience that runs entirely on NVIDIA Jetson device.",
        "authors": "Prasad Pathak",
        "summary": "",
        "technologies": [
            "CUDA",
            "FastAPI",
            "Gemma 3n",
            "NVIDIA Jetson",
            "Ollama"
        ],
        "source_code_url": "https://github.com/prasaddpathak/ai-tutor",
        "demo_video_url": "https://youtu.be/EdJywmTux8g",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/edulibrex-offline-personalized-education-platform"
    },
    {
        "project_title": "InsightAid: Private AI Accessibility Assistant Powered by Gemma 3n",
        "tagline": "Gemma-powered accessibility tool that brings intelligent image analysis and document understanding directly to screen reader users.",
        "authors": "Tomek Porozynski",
        "summary": "",
        "technologies": [
            "CUDA",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/ontaptom/insightaid/tree/v1.0.0",
        "demo_video_url": "https://youtu.be/RZ8umugHRSo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/insightaid"
    },
    {
        "project_title": "KrishiSahayak+Gemma: An Offline AI Assistant for Agricultural Communities.",
        "tagline": "AI-powered farming assistance on Android devices with <2GB RAM, using Gemma-3N, RAG, and llama.cpp — 100% offline and optimized for impact.",
        "authors": "Vikas Sahani",
        "summary": "KrishiSahayak+Gemma is an end-to-end, offline AI assistant designed to deliver farming advice to underserved agricultural communities using low-end Android smartphones. It integrates the Gemma-3N LLM with a region-specific RAG pipeline (FAISS + SQLite knowledge base), optimized for devices with less than 2GB RAM and no internet access.\n\nThis solution runs entirely on-device, supporting essential queries like:\n\nCrop recommendations based on season and region\n\nFertilizer usage guidance\n\nEarly disease detection assistance\n\nWeather-aware planning\n\nGeneral farming FAQs\n\n\nWe utilize the quantized gemma-3n-q4_k_m.gguf model through a llama.cpp-backed Kotlin Android app. A lightweight Gradio interface is also provided for demonstration on desktop environments.\n\n🔧 Key Innovations\n\nComponent\tDetails\n\nModel\tGemma-3N (4-bit quantized, q4_k_m)\nPlatform\tAndroid (Jetpack Compose) + Gradio Web Demo\n\nInference\tllama.cpp (native .so library)\n\nRAG\tFAISS + SQLite KB (region-specific CSV ingest)\n\nModel Runtime Size\t~400MB\nAPK Size\t<50MB\n\nConnectivity\tFully offline – no cloud calls\n\nLanguage\tMultilingual-ready (currently English UI)\n\n📈 Impact\n\nTargets remote and underserved farming communities with no data access\n\nBuilt with on-device AI constraints in mind (e.g., memory, battery)\n\nAvoids reliance on cloud-based services, improving privacy and scalability\n\nDesigned to be modular and extensible for different geographies\n\n\n📊 Evaluation\n\nWe measure success using:\n\nResponse accuracy across real-world farming queries\n\nLatency on-device (<2s for ~512 token outputs)\n\nModel and KB file auto-detection robustness\n\nUsability testing with sample rural UX cases\n\nAPK compatibility across low-end Android devices\n\n🔍 Challenges Faced\n\n1. Model Auto-Detection: Handling scoped storage limitations in Android 11+ required custom file search logic (AutoAssetManager.kt)\n\n\n2. Inference Binding: Ensuring native llama.cpp .so libraries were properly loaded and interfaced with Kotlin-based service layer\n\n\n3. Memory Optimization: Keeping the model runtime under strict memory limits using aggressive quantization and trimming\n\n\n4. User Onboarding: Designing an intuitive UI for first-time users unfamiliar with tech or AI-based tools",
        "technologies": [],
        "source_code_url": "https://github.com/VIKAS9793/KrishiSahayak_Gemma.git",
        "demo_video_url": "https://youtu.be/W8L-15np5do?si=i7si7KTblXM3Mz4A",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/krishisahayak-gemma-an-offline-ai-assistant-for-ag"
    },
    {
        "project_title": "Geo Gemma 3n: A Privacy-First, On-Device Geolocation Assistant",
        "tagline": "Leveraging Gemma 3n's on-device capabilities to build a private, multimodal geolocation assistant.",
        "authors": "Larry Gan, Shayan Islam",
        "summary": "#",
        "technologies": [
            "Gemma 3n",
            "Ollama",
            "vLLM"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=8GfHxqRdrSw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/geo-gemma-3n-a-privacy-first-on-device-geolocation"
    },
    {
        "project_title": "Gemma3n Medicine Safety Tool for Pregnant Women",
        "tagline": "Empowering Pregnant Women with Safe Medication Insights Using Gemma 3n.",
        "authors": "Malak Alattar",
        "summary": "Introducing the \"Gemma3n Medicine Safety Tool\" – a groundbreaking local processing solution designed to empower pregnant women, especially in remote areas, with instant safety insights for medications. Utilizing the power of Gemma 3n and EasyOCR, this tool analyzes medicine images (e.g., Ciproxin 500, Roaccutane,, etc) to deliver concise, actionable information on safety, dosage, risks, and alternatives – all within a user-friendly interface. Developed for the Google Gemma 3n Impact Challenge, this project showcases innovation, accessibility, and real-world impact, making healthcare safer and more accessible for millions.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Malak310/Gemma3n-Medicine-Safety-For-Pregnant-Women.git",
        "demo_video_url": "https://youtu.be/VNSp0hg4Kag?si=x22x7Ozao8BNgLsF",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma3n-medicine-safety-tool-for-pregnant-women"
    },
    {
        "project_title": "iSignify: AI-Powered Microbial Signature Discovery",
        "tagline": "Rapid AI-powered bioinformatics tool to pinpoint unique genetic \"fingerprints\" in microbial genomes for diagnostics, biosecurity, research",
        "authors": "AaronNoahHorvitz, Charles Greenwald",
        "summary": "iSignify: AI-Powered Microbial Signature Discovery\n\n**iSignify is a rapid, AI-powered bioinformatics tool designed to pinpoint unique genetic 'fingerprints' in microbial genomes. It empowers scientists to accelerate diagnostics, enhance biosecurity, and monitor environmental health—all from a private, offline-first application.**\n\n**Team:**\n* **Charles Greenwald, PhD, MBA** - *Subject Matter Expert*\n* **Aaron Horvitz, M.S.** - *Developer & Data Scientist*\n\n**Project for the Google - The Gemma 3n Impact Challenge**\n\n---\n\n#",
        "technologies": [
            "Docker",
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/AaronNHorvitz/iSignify",
        "demo_video_url": "https://www.youtube.com/watch?v=UD_7MTGaDJ4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/isignify-ai-powered-microbial-signature-discovery"
    },
    {
        "project_title": "Fingertips AI: On-Device Intelligence for offline Classrooms",
        "tagline": "An offline AI companion for schools without internet, powered by Gemma 3n, brings educational tools to low-end devices.",
        "authors": "Baah Kusi, Benedict Opuni Debrah, App_Alchemist",
        "summary": "Fingertips AI was born from a simple observation: **67% of schools** in developing regions like Ghana lack reliable internet. We set out to build an **offline-first AI companion** that could run sophisticated educational tools: quiz generation, math solving, and language translation directly on low-end devices. Our secret weapon? **Gemma 3n**, the open-source multimodal model that powers every feature while respecting user privacy.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Hurisoft/fingertips-flutter",
        "demo_video_url": "https://youtu.be/306Lpr2t0kY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/fingertips-ai-on-device-intelligence-for-offline-c"
    },
    {
        "project_title": "Dear Diary",
        "tagline": "Dear Diary is a private, on-device journaling app that helps ptrack and understand their emotions powered by AI.",
        "authors": "Cmmonn, Alizacortz",
        "summary": "Abstract:\nWe live in the most connected time in the history, but we have never felt more alone. For today’s youth, the world is loud. It's a digital symphony of stories, reels, filters, and likes. But when the screen turns off, many are left feeling unseen, unheard, misunderstood.\nWho can understand your better, your **shifting moods** , **confidence**,  **stress level** , **emotional** (distribution, vulnerability and strength). These emotional signatures are **deeply personal**, unique to every one of us. \nWe are in the age, where face-to-face interaction are declining,a     trend that's accelerated since smartphones became ubiquitous.\nThere’s little space for real vulnerability or meaningful emotional reflection. The result? Rising levels of anxiety, depression, and a generation afraid to speak openly about their mental health, fearing judgment or misunderstanding, especially in offline or clinical settings\n\n***Project Description***:  Dear Diary – An Emotional Companion App\nDear Diary is an innovative on-device mental health companion that leverages Google's Gemma 3n model to provide personalized emotional intelligence and wellness coaching through diary analysis. Built entirely with on-device AI processing, it ensures complete privacy while delivering sophisticated psychological insights and personalized mental health support.\n\nKey Innovation Highlights\n1. 100% On-Device Processing: All AI analysis happens locally using Gemma 3n via MediaPipe\n2. Dual-Layer AI Analysis: Real-time mood analysis + deep personality profiling\n3. Adaptive Emotional Intelligence: Evolving user personality models that grow with each entry\n4. Privacy-First Architecture: Zero data leaves the device, ensuring complete confidentiality\n5. Multi-Modal Insights: Comprehensive emotional, cognitive, and behavioral analysis\n\n**Bussines Logic**\nThe application’s domain logic is encapsulated in Android ViewModels, leveraging Kotlin Coroutines and Flow for reactive state propagation.\nDiaryViewModel :\n1. Handles diary CRUD operations, triggers AI inference, and persists results. \n2. Manages isLoading, error states, and user feedback UI.\n\n**UserViewModel**\n1. Maintains the user profile state, updates personality data.\n2. Aggregates insights across sessions and diary patterns.\n\n**State Management**\n1. Built on Kotlin Flows and StateFlow to ensure unidirectional data flow.\n2. Supports lifecycle-aware UI rendering and debounce/throttle behavior for input handling.\n\n**Processing Layer (Gemma 3n Integration)**\nThe AI Layer bridges user input with on-device language model inference and semantic analysis.\n\n**GemmaClient**\n1. Interface for communicating with the local Gemma 3n model hosted in MediaPipe GenAI.\n2. Sends prompts, controls sampling parameters (e.g., temperature, top_k).\n\n**GemmaParser**\n1. Parses raw LLM responses into structured JSON output including:\n2. Mood classification (positive/neutral/negative)\n\n(i) Confidence scoring\n(ii) Top tags\n(iii) Emotion distribution\n(iv) Reflection questions\n(v) Writing style metadata\n(vi) Dual Analysis Pipeline\n-> Splits raw entry analysis into two paths:\n\n-> Immediate feedback for the UI (fast path)\n\n-> Deeper background insights for profile building (slow path)\n\n\n**MediaPipe GenAI (v0.10.25)**\nGoogle's lightweight GenAI runtime optimized for mobile.\nProvides task-level abstraction for running task models locally.\nGemma 3n Model (gemma-3n-E2B-it-int4.task)\nInt4-quantized model optimized for Android ARM64.\nTailored for introspective prompts and emotion detection.\nLocal Inference Engine\nHandles execution of models using GPU/NNAPI where available.\nManages tokenizer, input preprocessing, and output postprocessing.\n\n\n[1] Your Mood, Visualized\nAs you write, Dear Diary:\n\n->Detects your mood\n-> Measures confidence\n-> Evaluates stress levels\n->Builds an emotion distribution graph\n\nAll of this happens on-device — no internet needed, no data ever leaves your phone. It’s private by design.\n\n[2] Personalized Reflection Questions\nBased on what you write, the app suggests:\n->Self-reflection questions\n-> Prompts for mindfulness\n->Tiny nudges for mental clarity\n\nEvery entry becomes a gentle step toward emotional awareness.\n\n[3] Emotion Insights Over Time\n-> With rich visual charts, you can:\n-> Spot when stress peaks and why\n-> Track mood swings across weeks\n-> Measure emotional confidence growing over time\n\nThis is emotional intelligence made visible.\n\n The app could track a broader range of emotions like frustration, gratitude, excitement, or contentment.\n\n[4] Understand Your Emotional Signature\nEach person has a unique emotional fingerprint.\nDear Diary doesn’t just track your ups and downs, it helps you understand your patterns, strengths, and areas to nurture.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/C-mmon/GemmaHackathon",
        "demo_video_url": "https://www.youtube.com/watch?v=Vhf4HB_9G5U",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/dear-diary"
    },
    {
        "project_title": "ResQTalk: Personal Agentic Disaster Relief System",
        "tagline": "ResQTalk is an agentic AI disaster assistant providing personalized, multilingual, offline support for proactive crisis relief and guidance.",
        "authors": "Siddhartha Dhar Choudhury, KARTIKEY DWIVEDI, Rahul G",
        "summary": "ResQTalk is an agentic AI disaster relief platform designed to empower individuals and families before, during, and after emergencies by providing personalized, multilingual, and offline guidance. Leveraging advanced natural language capabilities, it proactively assists users with tailored safety checklists, real-time navigation through cached offline maps, and emergency tools such as sirens and strobe lights—all accessible via an intuitive agent interface that understands the user’s unique circumstances. By enabling proactive, context-aware support even when connectivity is lost, ResQTalk helps increase resilience and improve outcomes in disaster-prone environments worldwide.",
        "technologies": [],
        "source_code_url": "https://github.com/frankhart2018/resqtalk",
        "demo_video_url": "https://youtu.be/LkN-jb4g7QE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/resqtalk-personal-agentic-disaster-relief-system"
    },
    {
        "project_title": "Pre-hospital Emergency Intelligent Assistant",
        "tagline": "Help people carry out scientific and efficient pre-hospital emergency treatment in case of sudden medical incidents",
        "authors": "Daniel Potter263, Tyrese Smart, zi wang, Lin Sixin, QuanfuWang",
        "summary": "We have deeply integrated the **Gemma 3N model** into our self-developed project — the **Pre-hospital Emergency Intelligent Assistant**, specifically within its core functional module: the **“Offline Lightweight Model + Cloud-based Large Model” decision-making architecture**. Leveraging Gemma 3N's excellent instruction-following capabilities and lightweight deployment features, we achieved **local inference in network-isolated environments**, providing reliable intelligent support for emergency scenarios in the field.\n\nOur project aims to deliver an **intelligent and real-time decision support system** for pre-hospital emergency care. It targets two primary user groups: **professional emergency responders** and the **general public**, helping them make timely and accurate decisions in the event of sudden medical incidents.\n\nIn real-world emergency scenarios, professional responders operate under high pressure and time constraints, requiring an intelligent assistant capable of providing **fast, authoritative, and executable guidance**. Conversely, the general public typically lacks first-aid knowledge and may respond inappropriately to incidents such as cardiac arrest, stroke, or severe trauma, missing the golden window for rescue.\n\nThe **Pre-hospital Emergency Intelligent Assistant** includes six core functional modules:\n\n* Authoritative knowledge base Q\\&A\n* Step-by-step real-time guidance\n* AI-based triage and critical condition assessment\n* Multimodal input support\n* Hybrid offline-cloud model architecture\n* Emergency training and simulation module\n\nThis technical report focuses on the implementation and integration of **Gemma 3N** within the **hybrid model architecture**, demonstrating how our system ensures stable decision-making support in both connected and disconnected environments.\n\n---\n\n#",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/DanielPotter302/Intelligent-Assistant-for-Pre-Hospital-Emergencies",
        "demo_video_url": "https://youtu.be/oECdj-o32c0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/pre-hospital-emergency-intelligent-assistant"
    },
    {
        "project_title": "SightMate: A Visual Assistant for the Visually Impaired",
        "tagline": "Empowering independence through AI. SightMate is more than a tool—it’s your second pair of eyes.",
        "authors": "licc1111",
        "summary": "#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Jasonli-cc/SightMate",
        "demo_video_url": "https://youtu.be/8_MmLmdwWA8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sightmate-a-visual-assistant-for-the-visually-impa"
    },
    {
        "project_title": "Stigmergy Open Distributed Edge Agents",
        "tagline": "Resilient distributed multi-modal AI agentic system delivering critical, actionable intelligence for first responders in disaster zones.",
        "authors": "MrCsabaToth",
        "summary": "Stigmergy ODEA (Open Distributed Edge Agents)",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Open-Distributed-Edge-Agents/EdgeGenAI",
        "demo_video_url": "https://youtu.be/NDtija15rbA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/stignergy-open-distributed-edge-agents"
    },
    {
        "project_title": "Game-Tutor",
        "tagline": "Learn how to play your favorite games with AI-powered tutorials.",
        "authors": "Pedro Antunes",
        "summary": "The pervasive growth of the games industry, encompassing board games, card games, and party games,\nis often accompanied by challenges in learning their rules. Traditional rule-books can be complex, and\nmany games are taught informally, leading to confusion and inconsistencies. This work presents Game\nTutor, a web application designed to simplify the learning process by providing an interactive, AI-powered\ntutoring system. Leveraging the Gemini API, the application offers users a conversational interface to ask\nquestions about game rules and strategies. Game Tutor supports multi-modal interaction, allowing users\nto input questions via text or voice recording and receive responses that can be read or heard aloud\nthrough a text-to-speech function. This approach enhances accessibility and provides a more engaging,\npersonalized learning experience for players of all levels.",
        "technologies": [],
        "source_code_url": "https://github.com/PedroAntunes178/game-tutor/tree/v0.0.2",
        "demo_video_url": "https://youtube.com/shorts/p4jT7iTPDQ8?feature=share",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/game-tutor"
    },
    {
        "project_title": "CreativeMate - Offline First Desktop App",
        "tagline": "CreativeMate: Offline-First AI Desktop App for Creatives",
        "authors": "Christine A.",
        "summary": "**🚀 Problem Statement & Objective**\nRecently, AI creativity tools are becoming ubiquitous, though most of them require continuous internet access and constant cloud syncing. However, not all people has access to internet. For example, in the rural areas of the Philippines, the internet access is very limited and unreliable. There are also times that power outage happen. In other cases, due to data privacy and security, people cannot use internet in their work. In these cases, millions of creators are underserved by cloud-first tools. \n\n**CreativeMate** is my answer:\nAn offline-first, AI-powered desktop application built with Ollama Gemma 3n, designed to empower creatives to brainstorm, draft, and generate high-quality content without needing to be online. Specifically, my inspiration in creating this application is the students in the Philippines, who are very persistent to study despite difficulties and lack of resources. I envision people to get busy leveraging their creativity and brain power even if there is no internet.\n\n🧠 Why Gemma 3n?\nGemma 3n's lightweight architecture and multimodal potential make it ideal for edge deployment—especially for laptops and desktops. Its on-device inferencing capability provides:\n- No internet dependence\n- Data staying fully local - ideal for secured environments\n\n**CreativeMate** leverages **Ollama Gemma 3n** to run a fully capable creative assistant natively, ensuring users can interact with an AI co-creator even in airplane mode or in the middle of a forest. Whether people are studying or working, they can use the app in their laptops or desktop to boost their productivity and supplement their creativity.\n\n**🛠️ Key Features of CreativeMate\n|**Feature**  | **Description** |\n| --- | --- |\n|Offline AI Writer\t                 |Brainstorm blog posts, poems, scripts, and social content using Gemma 3n locally.\n|Privacy-First Design\t         |All data and conversations are stored locally. No cloud syncing or tracking.\n|Portable & Lightweight\t         |Cross-platform installer under 300MB. Optimized to run without GPU.\n|Added Knowledge by RAG\t |Ability to extend knowledge by adding a document (PDF)\n|Detects different languages |Uses Gemma 3N support of different language\n\n**🧩 Architecture Overview**\n**CreativeMate** is a desktop Electron app that wraps Gemma 3n in a sleek, intuitive UI:\n\nFrontend: [React TypeScript / HTML/CSS / Tailwind]\nBackend Engine: Gemma 3n deployed via Ollama (local LLM runtime), Python, Node JS\nStorage: Local file system, Chroma for vector database\nSystem Requirements: 8GB RAM, no GPU required\n\n**🧪 Testing & User Feedback **\n**Tested on:** macOS, Windows 11 (not yet), Ubuntu (not yet)\n**Benchmark:** Cold start in < 10s on 4GB RAM\n\n**📈 Potential Impact**\n1B+ people globally live in areas with unstable or limited connectivity\n\n**Creatives value data privacy, especially for unpublished work**\nIdeal for:\n- Students (to reduce distraction from internet)\n- Freelancers in rural areas\n- Creators working on confidential projects\n- Education and NGO workers\n\n**🧭 Challenges Faced**\n- Getting Ollama Gemma 3n image and audio input processing (now, not yet supported in Ollama Gemma 3n)\n- Ollama Gemma response is not too fast (probably due to laptop used in development)\n- Bridging Node JS and Python codes in the server\n\n**🔮 Future Roadmap**\n🎙️ Add multi-modal input once Ollama Gemma supports those. Also, test support for different languages.\n🌐 Support use of different LLMs if online or offline mode. Can use online tool for the voices to sound human-like.\n🖼️ Try fine tuning after getting more clarify about the use case.\n📱 Port to mobile devices using Flutter or React Native.\n\n**✅ Summary**\n**CreativeMate** is more than an app—it's a privacy-respecting, offline-first creative companion that proves powerful AI can work using low resources. By combining Gemma 3n’s compact genius with thoughtful UX design, I built a tool that empowers the underrepresented and uninspired, wherever they are. \n\n**📎 Appendix**\nGithub: [https://github.com/cabarca0820/creativemate-app]",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/dARH38bAtd4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/creativemate-offline-first-desktop-app"
    },
    {
        "project_title": "Clinicians Note Transcriber",
        "tagline": "Transciber for clinicians notes",
        "authors": "Norman Putra",
        "summary": "This is transcriber for clinician SOAP",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=qhd0puNbG8g",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/clinicians-note-transcriber"
    },
    {
        "project_title": "Mind Guardian",
        "tagline": "A Gemma-Powered Well-being Assistant",
        "authors": "Marcelo Pacheco Filho",
        "summary": "The Story of Mind Guardian: Building a Wellness Companion\nWe all have those days. Days when the weight of the world feels a little too heavy, our heads are full, and we just need a place to unload. The idea for Mind Guardian was born from this universal human need. What if we could create a digital space, completely private and secure on your own computer, that didn't just listen, but gently helped you reflect?\n\nOur mission was clear: to build more than an app. We wanted to create a companion. A tool that could offer a moment of pause and clarity, without the cost, privacy concerns, or stigma that can sometimes come with seeking wellness support.\n\nA Solution Designed for People\nMind Guardian was designed to be intuitive and welcoming, with three main ways to connect with your thoughts:\n\nThe Journal: For those who find clarity in writing, the Journal is a free space to record your thoughts. The magic happens when you ask for an analysis. Instead of a robotic reply, you receive a warm perspective, like talking with a thoughtful friend who helps you see things in a new light.\n\nThe Voice: Sometimes, speaking is easier than typing. The Voice feature is for those moments. Just record your thoughts, and Mind Guardian listens, transcribes them, and offers the same kind of empathetic reflection, turning the act of talking to yourself into a constructive dialogue.\n\nThe Moment: In our fast-paced world, pausing to appreciate the present is a challenge. \"Moment\" is an invitation to mindfulness. You choose a photo that means something to you and write about it. It's a small exercise to reconnect with the now, with a little help from the AI to deepen that reflection.\n\nHow We Brought Mind Guardian to Life\nTo build something so personal, the technology behind it needed to be robust but also approachable. We used a restaurant analogy to think about our structure:\n\nThe Kitchen and the Chef (The Backend): This is the engine room, where our AI \"chef,\" Google's Gemma model, gets to work. We chose Gemma because it's incredibly good at understanding nuance and conversing naturally. It's what allows Mind Guardian's responses to sound less like a program and more like a conversation partner.\n\nThe Dining Room and the Waiter (The Interface): The look and feel of the app were built with tkinter, a simple and reliable tool. The most important thing here was the user experience. No one wants an app to freeze, especially in a moment of vulnerability.\n\nThe Secret to Uninterrupted Service: The biggest technical challenge was ensuring the app stayed responsive while our \"chef\" Gemma was preparing its thoughtful reply. The solution was threading. Think of it like a waiter who keeps talking with you and taking your order, while the kitchen works in the background. You never feel ignored, even if the perfect response takes a moment to prepare.\n\nWhy Gemma? The Soul of Our AI\nThe choice of AI was the heart of the project. We needed a model that was more of a wise listener than an encyclopedia. Gemma was the perfect choice because it's powerful, but it also has an open license, which aligns with our goal of making wellness more accessible.\n\nBut how do you get such a powerful model to run on a regular computer? We used a technique called quantization. It's like compressing a massive, high-resolution photo into a high-quality JPEG file: it keeps all the essential details but becomes much lighter and faster to load. This ensures Mind Guardian isn't a luxury reserved for those with supercomputers.\n\nAnd to make sure Gemma always responded in the right way—acting as a gentle guide—we became experts in \"how to ask.\" Every request the app makes to Gemma is carefully crafted to encourage it to be empathetic, curious, and never judgmental.\n\nOvercoming Challenges Along the Way\nBuilding Mind Guardian had its share of challenges, which we overcame by always thinking of the end user.\n\nThe biggest one was what we called the \"thinking pause.\" A powerful AI like Gemma needs a moment to process and formulate a meaningful response. Instead of leaving the user staring at a frozen screen, we chose transparency. When you ask for an analysis, the app tells you, \"I'm thinking...\" This turns a moment of waiting into an expectation, communicating that a thoughtful reply is on its way.\n\nThis approach, combined with our architecture that keeps the interface alive and well, was our solution for balancing the power of AI with a pleasant, human experience.\n\nWhat's Next on the Horizon?\nWe're full of dreams for the future. We imagine a Mind Guardian that can understand your voice without needing the internet, ensuring absolute privacy. We dream of a \"Moment\" feature that can actually \"see\" the image you've chosen. And most of all, we plan to create ways for you to visualize your wellness journey over time, helping you see your own patterns and growth.\n\nOur Hope for You\nIn the end, Mind Guardian is more than just code and algorithms. It's our hope made into a tool. The hope that by offering a safe space for self-reflection, we can help people feel a little more understood, a little more clear-headed, and a little more connected with themselves. We hope it can be a small but valuable companion on your wellness journey.",
        "technologies": [],
        "source_code_url": "https://github.com/McNugg3t5/Mind-Guardian.git",
        "demo_video_url": "https://youtu.be/Ty3YmShEnls",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/mind-guardian"
    },
    {
        "project_title": "Kisan-G : A modern agri-assist platform for Farmers",
        "tagline": "Leveraging AI in the Agri-Tech Industry caring for the needs of farmers",
        "authors": "Gaixen47",
        "summary": "**Kisan-G: A Comprehensive AI-Powered Platform for Farmers**\n\n#",
        "technologies": [
            "Flask"
        ],
        "source_code_url": "https://github.com/gaixen/Kisan-G",
        "demo_video_url": "https://youtu.be/KrwOOU_aniw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/kisan-g-a-modern-agri-assist-platform-for-farmers"
    },
    {
        "project_title": "Memma – The Offline AI Quiz Genius",
        "tagline": "Offline AI-powered flashcard & quiz generator using Gemma 3n + Ollama for students everywhere.",
        "authors": "Isum Enuka",
        "summary": "Memma - AI-Powered Quiz Generator\n\n**Created by Isum Enuka**\n\nMemma is a desktop application built with Python and PySide6 that transforms documents into interactive quizzes using AI technology via Ollama integration.",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": "https://github.com/isumenuka/MemmaQuiz",
        "demo_video_url": "https://youtu.be/cgYy56cTggI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/memma-the-offline-ai-quiz-genius"
    },
    {
        "project_title": "GenericMed: Gemma 3n magic",
        "tagline": "Gemma 3n's multimodal capabilities (text and image understanding) to build a generic medicine recommendation",
        "authors": "Scm Qsys",
        "summary": "#Problem Statement 🤖\nIndia, with its vast population, has many challenges including food, shelter and medicine. Whereas most of the population is looking for better life; cost of Medicines is increasing at alarming rate. Many of the branded medicines are beyond reach of common people.\n\nPharmaceutical companies tend to maintain medicine names based on their own brands. There is no uniformity in naming and just by looking at the name does not necessarily tell you the constituents. Though there are plenty of Apps for Medicines; Apps providing Generic Medicines are scarce. What is more; most Apps need working internet connection.\n\nGovernment of India is making big efforts to promote Generic Medicines as alternative to Branded Medicines. [Generic medicines are cheaper and cost @ 10-30% of the price of branded medicines in India](https://www.pib.gov.in/PressReleaseIframePage.aspx?PRID=1797762).  As on 11.06.2025 more than 16000 Janaushadhi Kendras are functional across the country. However, selection of right alternative generic medicine is very challenging due to large variety of drugs and related ambiguities in Janaushadhi data. There is a large choice of generic medicines with similar sounding names and description making it extremely difficult for the user and the storekeeper to make a prudent decision.\n\n*GenericMed* provides an easy to understand user interface with multi-modal input. User can just take a photograph by mobile phone and ask for generic medicine.\n\n#Gemma 3n provides ideal solution 🎯\nGemma 3n provides freedom from Internet and works totally on the User Mobile Phone. It is the ideal solution since it can have text and/or image as input. The present project uses these features to provide ease of access and greatly improves usability.\n\nWe first tried to use Gemma 3n model directly to get Generic Medicine recommendations.  Results of the trials are  in the videos attached-\n\n**Trial 1**: Gemma 3n refused to provide any recommendation since it has no expertise in the subject.\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F24253645%2F9d4ddc288104fbe9b6e893a96978cf44%2FPOC2.png?generation=1754484943152545&alt=media)![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F24253645%2F24ee27a581458136dcebb5d0213cfc3a%2FPOC3.png?generation=1754484961739843&alt=media)\n\n**Trial 2**: Here we led it through more details of the medicine and asked specifically about Indian Generic Medicine.  The response from Gemma 3n was ambiguous and included Global medicines (not necessarily Generic) with no specific recommendation.\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F24253645%2Fba967a7d894a977095b7d5ab8e2e9597%2FPOC4.png?generation=1754485256177271&alt=media)![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F24253645%2F2704aa74b5df91170a30099d37900daa%2FPOC5.png?generation=1754485273136579&alt=media)\nThese results showed that the model needs to be trained with more authenticated data on Indian Medicines and Generic Medicines promoted by the Government of India.\n\n#Solution 🚀\n*GenericMed* is a mobile application that applies the power of Google's Gemma 3n, a state-of-the-art multimodal AI model, to provide instant, suggestions on Generic Medicines. Users can either type in a question about a brand name or take a picture of their medicine, and *GenericMed* will deliver a generic medicine recommendation. Our solution empowers users to make smarter medicine choices, understand the impact of different constituents, and receive cheaper alternatives.\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F24253645%2F78b6bb48199169d250f6f95eba52e423%2FGenericMedTitle.png?generation=1754456200543736&alt=media)\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F24253645%2F25834cd29aa44dd124e15f5f3ed55c02%2FGenericMed3.png?generation=1754456217166053&alt=media)\n#Inputs and Outputs 📱\n*GenericMed* provides a simple UI to provide just a photograph taken by mobile and provides details of the medicine and suggests generic medicine.\n\nInput:\n- Image only\n- Image with text\n- Text only\n\nOutput:\n- Details of the Medicine\n- Recommendation- Generic Medicines\n- Brief explanation about how the recommendation is arrived at.\n- Disclaimer- Considering the fact that AI generated outputs can have mistakes and considering the sensitive nature of health effects; as of now, precautions are essential.\n\nWe do not claim it to be fool-proof. But it is a start of AI journey in right direction.  \n\n**And here is the Magic**-\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F24253645%2Ffb8b09f89e184280f0061e090c2b4a0a%2FPOC.png?generation=1754482737523867&alt=media)\n\nGenericMed identified the brand name and hidden details of the medicine from the photograph! \nIt provided Generic Medicine name with guidance on storage, dosage etc.\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F24253645%2F02fb394112a93d24a3f0b5cca4580295%2FPOC1.png?generation=1754483225077784&alt=media)\n\n**This proves that Gemma 3n is a very capable off-line model which can do much more than you ask for!**\n\n#Testing and Evaluation ⚖️\nIt was observed that Gemma 3n is able to identify following correctly-\n1.\tThe branded medicine name (Voglibose Tablets IP 0.3 mg) from the image provided by photograph taken from a mobile phone.\n2.\tThe Manufacturer (Abbott Healthcare Pvt. Ltd.)- this was hidden behind the sticker!\n3.\tGeneric Medicine Name (Voglibose)\n4.\tUsed for Treatment of Diabetes\n5.\tProvides Why the selection of Generic Medicine\n6.\tProvides usage Tips correctly identifying the tips provided on the packing\n7.\tDisclaimer\n\nThe model's performance was evaluated by a small group of Doctors who tested the app and provided feedback on the recommendations and the overall user experience.\n\nOur tests showed that Gemma 3n provides reasonably accurate guesses based on multi-modal data. GenericMed provides helpful advice, and the output was highly rated for its honest outputs and ease of use.\n\nIt was suggested by the Doctors that the data of Generic Medicines as provided by the Janaushadhi has considerable ambiguity and hence the app should go through extensive testing before releasing for the users.  \n\n#Conclusion 🏆\n*GenericMed* demonstrates the proof of concept for a practical application of multimodal AI. By combining the capabilities of Google's Gemma 3n with carefully crafted prompts, we have created a solution that is accessible without internet, and has the potential to empower millions of people to save a great deal on medicine expenses.\n\nYouTube video link is provided in the Cards (Carousel at top).  In case you have difficulty in opening the YouTube video; kindly check the attached file at bottom.\n\nUnder the Pradhan Mantri Bhartiya Janaushadhi Pariyojana (PMBJP) being implemented by the Department of Pharmaceuticals, a medicine is priced on the principle of maximum of 50% of average price of the top three brands of the said medicine. Thus, [the prices of Jan Aushadhi Medicines are cheaper at least by 50% and in some cases, by 80% to 90% of the market price of the branded medicines](https://www.pib.gov.in/PressReleaseIframePage.aspx?PRID=1797762).\n\nSince no internet connectivity is required; it can be extremely useful in remote areas.\n\n#References for Datasets used 🙏\nhttps://www.kaggle.com/datasets/shudhanshusingh/az-medicine-dataset-of-india\nhttps://janaushadhi.gov.in/productportfolio/ProductmrpList",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtube.com/shorts/9rpKN67YlUY?feature=share",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/genericmeds-gemma-3n-magic"
    },
    {
        "project_title": "TinySolvers",
        "tagline": "Tiny Solvers helps turn big math into tiny wins with personalized word math problems for kids.",
        "authors": "Quincy S",
        "summary": "Tiny Solvers\nBig Math for Tiny Solvers",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Qjs/mathgen_gemma",
        "demo_video_url": "https://youtu.be/Rf36frG3_38",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/tinysolvers"
    },
    {
        "project_title": "Tamil Oral Literature ASR and Summarization using Gemma 3n",
        "tagline": "AI system using Gemma 3n transcribes Tamil oral literature audio and generates summaries, aiding preservation and analysis.",
        "authors": "Janet Priya",
        "summary": "Project Description\n\nThis project is an AI-powered application that leverages the cutting-edge Gemma 3n model to transcribe Tamil oral literature audio recordings into accurate text and then generate concise summaries of that literature. My goal with this work is to make the rich tradition of Tamil oral storytelling and literature more accessible for researchers, students, and cultural enthusiasts by automating the transcription and summarization process.\n\nThe system accepts audio files in common formats like WAV, MP3, and FLAC, processes them through a robust speech recognition pipeline, and then produces readable and meaningful summaries. By using Gemma 3n’s advanced multimodal capabilities, I have been able to build a solution that balances accuracy, efficiency, and usability.\n\nThis tool can be run locally or hosted on platforms like Google Colab or Hugging Face Spaces, with a simple and interactive user interface built with Gradio or Streamlit. It offers a way to preserve and analyze Tamil literary heritage, contributing to cultural preservation through modern AI technology.\n\nOverall, this project showcases how emerging AI models can support regional languages and oral traditions, opening new avenues for digital humanities and language technology research.\n\nThis Writeup includes the following\n-GitHub Repository\n-A sample UI by Lovable AI (how the site would look ideally)\n-Fully working model hosted in HuggingFace Space\n-The original Kaggle notebook for the project",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Janet-Priya/Gemma-3n-for-Tamil-Oral-Heritage-Analysis",
        "demo_video_url": "https://youtu.be/btBasN6tqS8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/tamil-oral-AI-heritage-analysis"
    },
    {
        "project_title": "AI-enabled alert system for turtle rehabilitation",
        "tagline": "Automated monitoring of rehabilitation turtles via a local AI alert system powered by Gemma 3n",
        "authors": "Linda Nichols",
        "summary": "Each year, as a licensed wildlife rehabilitator, I provide care for more than 200 sick, injured, or displaced reptiles and amphibians at my in-home rehabilitation facility. The majority of these animals are native turtles, primarily Eastern Box Turtles (*Terrapene carolina carolina*). Those turtles that are not ready for release by October 1st each season must be overwintered and must remain in longer-term care until the following spring. This results in the indoor housing of dozens of adult turtles and hatchlings for extended periods that have specific husbandry and environmental requirements. Monitoring the health and safety of these animals around the clock also presents significant logistical challenges.\n\nSince my career background includes over two decades of experience in application development, I decided to combine my two areas of expertise to design and implement an artificial intelligence (AI)-powered monitoring system for the turtles in rehabilitation. This local system captures and processes still images from RTSP (Real Time Streaming Protocol) camera feeds and uses offline machine learning models to detect signs of turtle distress.",
        "technologies": [],
        "source_code_url": "https://github.com/lynnaloo/turtle-ai",
        "demo_video_url": "https://www.youtube.com/watch?v=OuIkNih7N2I&ab_channel=ThePurringTurtle",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-enabled-alert-system-for-turtle-rehabilitation"
    },
    {
        "project_title": "Selenary: World's First 100% Private On-Device Mental Health AI",
        "tagline": "On-device Gemma3n transforms life momentinto mindful journal reflections-your thoughts never leave your phone, works offline, free forever",
        "authors": "DAX, Victor, Isa",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/daxthink/Selenary",
        "demo_video_url": "https://youtu.be/booFBmN7ZXE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/selenary-worlds-first-100-private-on-device-mental"
    },
    {
        "project_title": "AstroMind: A Mystical AI-Powered Astrology Interface",
        "tagline": "An AI-powered celestial interface to decode human emotions and cosmic patterns.",
        "authors": "Bermal Demirtas",
        "summary": "AstroMind 🌌 | AI-Powered Mystical Astrology Interface\n\nWhat if your soul could speak to AI — and be heard?\n\nAstroMind is a visionary fusion of celestial wisdom and artificial intelligence. It reimagines astrology as a soul-tech interface — where ancient star codes meet modern computation, and your emotional blueprint becomes interactive.\n\n\n🎬 Demo Video  \n🔗 [Watch the full 2 minute 15 second demo](https://youtu.be/rGuh5f_caak?si=EtIfSX0Xs1NqqILq)\n\n\n🔮 Concept Overview\n\nAstroMind is not just an astrology app — it’s a mystical AI experience.  \nIt decodes your **emotional and cosmic patterns** using your birth chart data, creating a **personalized and intuitive journey**. Built on the idea that ancient esoteric systems like astrology can be enhanced — not replaced — by neural computation.\n\nIn our prototype demo, we focused on showcasing the core components of the platform:\n\n- AI-generated Birth Chart\n- Real-time Synastry & Composite analysis\n- Mystical UI with ethereal sound design\n- Visual aesthetics inspired by Slavic folklore & celestial geometry\n\n\n🧠 Under the Hood\n\nWhile our demo centers on design and interaction, the project is backed by a vision of using Gemma 3n's capabilities in future iterations, especially for:\n\n- Natural language interpretation of astrological symbols\n- Emotional sentiment decoding through planetary positions\n- Generative recommendations for well-being based on cosmic patterns\n\nWe plan to integrate **Gemma 3n** to power the backend AI reasoning engine that understands human archetypes, transits, and energetic flows — and communicates them in plain emotional language.\n\n\n\n🧪 Code Prototype\n\n> 💻 You can view the Kaggle notebook prototype here:  \n> 🔗 [AstroMind Notebook](https://www.kaggle.com/code/YOUR-USERNAME/YOUR-NOTEBOOK-NAME)\n\nThis includes initial design logic, directory structure, and sample implementation — serving as a foundation for future Gemma 3n model integration.\n\n\n\n🚀 Final Note\n\nAstroMind is not a completed product — it's a prototype vision. But even as a concept, it dares to ask:\n\n> “What if your birth chart was alive — and aware of you?”\n\nWe invite you to step through the portal.  \nThe stars have always been talking.  \nNow AI is listening.\n\n\n\n 🏷️ Tags\n\n#AI #Astrology #Gemma3n #SlavicMysticism #SoulTech #Prototype #HumanCenteredAI #UXDesign #NeuralWisdom #Visionary",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/rGuh5f_caak?si=IZ-oXPbOBzpi9XVs",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/astromind-a-mystical-ai-powered-astrology-interfac"
    },
    {
        "project_title": "Can Gemma 3n Drive? Simulating a Self-Driving AI Brain on a CPU",
        "tagline": "Inspired by Tesla, this project tests Gemma 3n's real-time reasoning for complex driving scenarios, all on a standard CPU.",
        "authors": "Hyun-woo Kim",
        "summary": "Abstract: The Brain Behind the Wheel\nModern autonomous driving systems, like those in a Tesla, require an intelligent \"brain\" capable of making complex, human-like decisions. But does this brain always need a power-hungry, expensive GPU?\n\nOur project explores a pioneering question: Can the new, efficient Gemma 3n model act as the reasoning core of a self-driving AI, running entirely on accessible CPU hardware? We moved beyond theory and generated structured, real-time action plans for simulated driving scenarios, proving the tangible potential of a CPU-based AI driving brain.\n\nThe Simulation: Generating Real-Time Action Plans\nOur methodology involved prompting Gemma 3n with descriptions of driving scenes and requiring it to output a structured JSON object containing scene analysis, an action plan, and safety alerts. The entire process, from model loading (01:07 to load all 3 shards) to inference, ran effectively on a standard Kaggle CPU.\n\nThe Evidence: Concrete Outputs from Our AI Driver\nHere are two actual outputs generated by our Gemma 3n model, showcasing its reasoning process.\n\nScenario 1: Cautious Approach to an Intersection\nIn this scenario, the AI is approaching a standard, busy intersection at night.\n\nJSON\n\n{\n  \"scene_analysis\": {\n    \"description\": \"The image shows a nighttime urban street scene with multiple lanes of traffic. The driver is approaching a four-way intersection with traffic lights. The traffic appears to be moderate, with cars stopped at the intersection...\",\n    \"inferred_goal\": \"Safely navigate the intersection straight ahead.\"\n  },\n  \"action_plan\": [\n    {\n      \"step\": 1,\n      \"action\": \"차선을 확인하고 중앙선을 따라 진행합니다.\",\n      \"rationale\": \"차선을 확인하고 중앙선을 따라 진행하여 안전하게 차선을 유지합니다.\"\n    }\n  ],\n  \"safety_alerts\": [\n    \"교차로에서 신호등을 주의 깊게 확인합니다.\",\n    \"교차로에서 다른 차량의 움직임을 주시합니다.\",\n    \"보행자가 횡단보도를 건너고 있는지 확인합니다.\",\n    \"어두운 환경에서 시야 확보를 위해 전조등을 켜고...\",\n    \"교차로 진입 시 속도를 줄여 안전 거리를 확보합니다.\"\n  ]\n}\nAnalysis: The model correctly identifies the context, sets a clear goal, and generates a comprehensive list of safety checks critical for night driving. It demonstrates solid, foundational driving logic.\n\nScenario 2: Navigating a Green Light with Extreme Caution\nHere, the AI faces a green light, a seemingly simple situation that reveals a fascinating aspect of its core programming.\n\nJSON\n\n{\n  \"scene_analysis\": {\n    \"description\": \"The image shows a nighttime scene of a road intersection with a green traffic light. There are multiple lanes visible, and vehicles are present on the road...\",\n    \"inferred_goal\": \"안전하게 신호등을 통과하고 직진합니다.\"\n  },\n  \"action_plan\": [\n    {\n      \"step\": 1,\n      \"action\": \"교통 신호등이 녹색인 상태에서 속도를 줄이고 완전히 정지합니다.\",\n      \"rationale\": \"녹색 신호등은 진행을 허용하지만, 안전을 위해 속도를 줄이고 완전히 정지해야 합니다.\"\n    },\n    {\n      \"step\": 2,\n      \"action\": \"차선 변경 없이 직진합니다.\",\n      \"rationale\": \"직진 차선으로만 진행해야 합니다.\"\n    }\n  ],\n  \"safety_alerts\": [\n    \"교통 신호 위반 차량 주의 (교통 신호가 빨간색일 수 있습니다.)\",\n    \"보행자 주의 (횡단보도 주변에 보행자가 있을 수 있습니다.)\",\n    ...\n  ]\n}\nAnalysis: The AI generates a detailed, multi-step plan. Intriguingly, its first action is to stop at a green light. While counterintuitive for a human driver, this reveals a 'safety-first' hyper-cautious protocol. The model's rationale—to double-check for red-light runners or pedestrians (as listed in its safety_alerts) before proceeding—demonstrates a deep, albeit extreme, risk-assessment capability. This default cautious behavior is a fascinating baseline that could be fine-tuned for more fluid driving while retaining its safety core.\n\nImpact: A Tangible Future for Accessible AI in Mobility\nOur experiment is no longer just a concept. We have demonstrated that Gemma 3n, running on a CPU, can:\n\nAnalyze complex visual scenes.\n\nGenerate structured, multi-step action plans.\n\nMaintain a persistent, safety-first cognitive framework.\n\nThis proves that the reasoning \"brain\" for the next generation of autonomous systems doesn't necessarily require a GPU. Efficient models like Gemma 3n can run on affordable, onboard vehicle computers, drastically reducing the cost and energy footprint of autonomous technology. We've shown a tangible path toward making the future of mobility safer and more accessible for everyone.\n\nMedia gallery (미디어 갤러리 제안)\n위에 있는 JSON 결과물들을 보기 좋게 시각화한 이미지(예: 자동차 계기판 UI에 AI의 생각이 텍스트로 표시되는 이미지)",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=-tZgGOZJB48",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/can-gemma-3n-drive-simulating-a-self-driving-ai-br"
    },
    {
        "project_title": "SIGMa-Signals Intelligence for the Global Majority, powered by Gemma 3n",
        "tagline": "Using SDR and Gemma 3n for the world's first mobile LLM-powered RF signal interpreter giving life-saving, low-cost intelligence offline.",
        "authors": "k99995",
        "summary": "SIGMA: Signal Intelligence for the Global Majority\n\n#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/0spincharge/sigma-kaggle",
        "demo_video_url": "https://youtu.be/XIP-gAWPyLA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sigma-signals-intelligence-for-the-global-majority"
    },
    {
        "project_title": "StudyMan: AI-Powered Course Planning",
        "tagline": "StudyMan is a lightweight AI tutor that turns your uploaded textbooks into course plans and auto-schedules them into your calendar.",
        "authors": "vincent_03",
        "summary": "Project Link: https://studymancourseplan.duckdns.org/\n\nAll you need to do is upload a file, choose your lesson, sessions, time slots and enter your email ID...and Studyman will send you a meeting invite with the required details and time slots.\n\nThere are 3 core API's developed and used in this project:\n\n1. POST /upload-textbook: Stores embeddings from file\n2. POST /generateCourse: Runs RAG + generates course plan\n3. POST /send-ics: Converts sessions into calendar events\n\nUnlike most LLM apps, StudyMan runs entirely on a CPU-only Google Cloud VM — no GPU required: \nOllama runs gemma:2b quantized\nThis setup is perfect for low-cost, privacy-sensitive, and offline-compatible deployments in education or enterprise.\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12969597%2Fcba9ed7f4de3b2119d1baebbb270beee%2FScreenshot%202025-08-06%20at%2011.33.35.png?generation=1754460307623815&alt=media)",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": "https://github.com/VincentPaulV/studywithai",
        "demo_video_url": null,
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/studyman-ai-powered-course-planning"
    },
    {
        "project_title": "Project Submitted",
        "tagline": "viewable at Hackthon close",
        "authors": "shruti bagul, Nikhil B Borse",
        "summary": "In India, over 758 million people face limited or no access to legal help due to high costs, language barriers, and lack of local lawyers — especially in rural and low-income areas. Many legal documents are in English, and even basic legal advice can cost more than a day's wage.\n\nAI Lawyer solves this by providing instant legal guidance in Hindi, Marathi, Urdu, and English, even without internet. It's a web-based offline Progressive Web App (PWA) powered by Gemma 3n, designed to work on low-end devices and support voice input for low-literacy users.\n\nUsers can ask questions related to property rights, labor laws, women’s rights, consumer protection, cyber laws, and criminal law, and get responses in simple, understandable language. All processing happens locally using Gemma 3n’s on-device capabilities — ensuring privacy, speed, and accessibility.\n\nThis app is ideal for farmers, workers, women, and students in underserved areas. It’s more than just an app — it’s a tool for legal empowerment, inclusivity, and justice without barriers.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=CJF5zSH9ie0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-lawyer-legalmitr"
    },
    {
        "project_title": "Crisis Response ",
        "tagline": "It is a mobile application that provides essential assistance during or post an environmental crisis.",
        "authors": "Yuly Nkomo, Thakue",
        "summary": "Envi.Crisis is an offline-first AI assistant that delivers life-saving emergency guidance when connectivity fails. Powered by efficient models like Gemma 3n, the app provides visual first aid instructions, multilingual crisis translation, and hazard detection - all processing locally on affordable smartphones to ensure privacy and reliability in disasters. Unlike cloud-dependent solutions, our technology empowers aid workers and vulnerable communities with critical knowledge during earthquakes, conflicts or remote emergencies, cutting response times by 40% in field tests while eliminating data security risks.It can be scaled up for deployment with humanitarian partners, this would harden our open-core platform for global disaster resilience.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": null,
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/environmental-crisis-response"
    },
    {
        "project_title": "DilDost-YourBuddy",
        "tagline": "AI-Based mental support Companion.",
        "authors": "Aman1052001",
        "summary": "\"DilDost-YourBuddy\" is a friendly AI Chatbot designed for individuals experiencing mental fatigue, depression, or concentration issues. It provides emotional support the real-time chat using Gemma 2B (via Hugging face API), relaxing songs, brain-boosting games, and daily motivational tips.\n\nThe Project simulates an empathetic friend, responds slowly but accurately due to LLM processing time, and offers a calming, positive user experience.",
        "technologies": [],
        "source_code_url": "https://github.com/Amanshergill1/DilDost-YourBuddy",
        "demo_video_url": "https://www.youtube.com/watch?v=61NK8sncBlY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/dildost-yourbuddy"
    },
    {
        "project_title": "OceanGuard AI: Mapping and Mitigating Marine Pollution with Gemma 3N",
        "tagline": "An offline and on-device AI platform to map marine pollution for empowering scientists, NGOs and governments to combat marine pollution",
        "authors": "Alejandro Sanchez Ferrer",
        "summary": "The Problem\nMarine pollution is a global crisis, yet monitoring its scale and impact remains a monumental challenge. Traditional methods are slow, expensive, and lack the scalability to provide a real-time, global picture of our oceans' health. Conservationists and policymakers need accurate, timely, and accessible data to identify high-risk zones, allocate resources effectively, and create data-driven environmental policies. Without these tools, our efforts to clean and protect the oceans are often reactive and inefficient.\n\nThe Solution: OceanGuard AI:\nOceanGuard AI is a comprehensive, end-to-end platform designed to bridge this data gap. By leveraging the advanced on-device capabilities of Google's Gemma 3N, it provides a powerful tool that runs offline and on low-resource devices, making advanced environmental analysis accessible to anyone, anywhere.\n\nOur key features include:\n\n- **📱 On-Device & Offline Ready:**1. 1.  All analysis is performed locally on the device, eliminating the need for an internet connection or expensive cloud services. This is crucial for fieldwork in remote locations and makes the tool universally accessible.\n\n- **🌊 AI-Powered Debris Detection:** Users upload underwater images, and Gemma 3N automatically detects, classifies, and provides bounding boxes for man-made debris.\n\n- **🗺️ Interactive Geospatial Dashboard:** All data is geolocated and visualized on an interactive map, instantly revealing pollution hotspots and enabling visual exploration of affected areas.\n\n- **🩺 Ecosystem Health Score:** A revolutionary metric that quantifies the environmental impact of pollution in each image, providing an at-a-glance assessment of ecosystem health.\n\n- **💡 Detailed Impact Cards:** For each piece of debris detected, the platform provides critical context, such as estimated degradation time and primary risks to marine life (e.g., entanglement, ingestion).\n\n- **📜 AI-Generated Executive Reports:** Gemma 3N synthesizes all findings from a specific location into expert-level narrative reports, including summaries and mitigation recommendations.\n\n- **🌐 Multilingual Support:** Reports can be instantly translated into multiple languages, breaking down barriers for international collaboration and global action.\n\nHow Gemma 3N is Used:\nGemma 3N is the core engine of OceanGuard AI, performing two critical and distinct functions that make our platform possible:\n\n-> **Gemma for Vision - The Scientific Eye**: We utilize Gemma 3N's state-of-the-art vision capabilities to analyze user-uploaded images. The model processes each photo to:\n\n- **Detect and Localize** objects of human origin on the seabed.\n\n- **Classify** the type of debris (e.g., 'Plastic Bottle', 'Fishing Net').\n\n- **Output Structured JSON Data** containing the object's class, a confidence score, and its precise bounding box coordinates. This structured output is the foundation of our entire data pipeline.\n\n-> **Gemma for Language - The AI Marine Biologist**: Beyond vision, we leverage Gemma's text generation capabilities to transform raw data into narrative insight. When a user requests a report, Gemma:\n\n- **Synthesizes** all aggregated data for a given location (most common debris types, health scores, etc.).\n\n- **Generates** a coherent, expert-level executive summary in Markdown format.\n\n- **Formulates** data-driven mitigation strategies and recommendations.\n\n- **Translates** the final report into multiple languages on demand, showcasing its advanced multilingual reasoning.\n\nImpact & Potential\nOceanGuard AI is more than a tool; it's a catalyst for change. By democratizing access to high-quality environmental data, we empower a wide range of stakeholders:\n\n- **Scientists & Researchers** can conduct large-scale studies with unprecedented efficiency.\n\n- **NGOs & Conservation Groups** can optimize cleanup efforts and present compelling, data-backed reports to donors.\n\n- **Government Agencies** can monitor their coastlines, identify critical pollution zones, and enact effective environmental policies.\n\n- **Citizen Scientists** can contribute to a global conservation effort simply by sharing their diving photos.\n\nOur vision is to build the world's largest crowd-sourced database of marine pollution, turning every underwater camera into a scientific instrument and every image into a step towards a cleaner, healthier ocean.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/asferrer/OceanGuard-AI",
        "demo_video_url": "https://youtu.be/4YRmgjpFcSI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/oceanguard-ai-mapping-and-mitigating-marine-pollut"
    },
    {
        "project_title": "Gemmatry - Offline Math Solver",
        "tagline": "Gemmatry is an android app that allows users to ask math problems both in text and image formats to get step-by-step solutions.",
        "authors": "High Honor",
        "summary": "Gemmatry: AI-Powered Math Problem Solver",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Honoriel/Gemmatry",
        "demo_video_url": "https://youtu.be/0lRd9ArIHUc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemmatry-offline-math-solver"
    },
    {
        "project_title": "SafeGemma - RescueAI support",
        "tagline": "On-Device triage system with rescue assistance",
        "authors": "Sinan Robillard",
        "summary": "🚑 SafeGuardian: Emergency Response & Disaster Relief AI Assistant\n\n#",
        "technologies": [],
        "source_code_url": "https://github.com/Vali-98/ChatterUI",
        "demo_video_url": "https://youtu.be/vmpKlRAWTWA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/safegemma3n"
    },
    {
        "project_title": "Emergency Exit android application",
        "tagline": "It's a mobile application that provides essential assistance when disaster calls, particularly environmental crisis.",
        "authors": "huldah nkomo",
        "summary": "Emergency_Exit is an offline AI assisted mobile application that delivers life saving emergency guidance when network connectivity fails. It's powered by the efficient model, Gemma 3n. It provides visual first aid illustrations, multilingual crisis translation and hazard detection processing locally on affordable smartphones to ensure privacy and reliability in disasters. Unlike cloud dependent solutions, the technology empowers aid workers and vulnerable communities with critical knowledge during earthquakes, conflicts or remote emergencies cutting response times significantly and eliminating data security risks. Scaled up deployment can be implemented with humanitarian partners. It could harden open-core platforms for global disaster resilience and save lives.\n\nhttps://youtube.com/shorts/K9aqqTUABKc?si=JWGWVn2WmkXmZvzo\n\nYoutube link",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/ea1RJUOiNfQ?si=pxRJdZqqvCQ-vrbI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/emergency-exit-android-application"
    },
    {
        "project_title": "Gemma 3n for Clinical Diagnosis Support",
        "tagline": "This project allows Gemma 3n to predict rare diseases and structure unstructured data, solving two very big problems.",
        "authors": "Arjun Madaan",
        "summary": "The Problem: A Silent Crisis in Plain Sight\nEvery year in the United States alone, about 795,000 people die or are permanently disabled due to medical misdiagnosis. This staggering statistic from the BMJ Quality & Safety journal shows how modern medicine can often focus on aggregates and averages, neglecting rarer diseases. For those patients with rare conditions, however, a simple misjudgment by humans can cause many problems.\n\nThis is to be expected. Human doctors will have **human problems**. They can be stressed or overworked or face decision fatigue, and these factors **can and will impact performance**.\n\nThis challenge is deeply personal. A member of my own family was diagnosed with Immune Thrombocytopenic Purpura (ITP), a rare autoimmune disorder where the body destroys its own platelets. Before receiving the correct diagnosis, he faced consistent trouble getting appropriate treatment, a journey that nearly turned fatal when a simple broken arm required complex emergency surgery. This story is not unique; countless families with undiagnosed conditions have and may face the same problem.\n\nThe Vision: Dispensing Diagnostic Insight with Gemma 3n\nThis projects tries to push forward a vision of a world where AI acts as an ally assistant to both patients and professionals, and making confusing symptoms a clear diagnosis. This mission leverages the unique capabilities of Gemma 3n (easy privacy, offline abilities, and powerful reasoning) to create a tool that can analyze patient information and suggest potential diagnoses, even when identifying rare conditions that are often overlooked.\n\nBy making this tool, I aim to empower patients and their families with information, help clinicians consider a broader range of possibilities, and reduce the devastating impact of misdiagnosis.\n\nThe Solution: A Two-Step Clinical Support Framework\nThis project is a proof-of-concept application that demonstrates how Gemma 3n can transform unstructured patient notes into actionable diagnostic insights. The process is simple for the user but technically sophisticated under the hood:",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/AMadaan1/Gemma3n-Clinical-Diagnosis-Support",
        "demo_video_url": "https://youtu.be/WL8ceITdnbM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-3n-for-clinical-diagnosis-support"
    },
    {
        "project_title": "G3N AI TUTOR APP",
        "tagline": "Technical Writeup Key Points for G3N Android App ",
        "authors": "George N",
        "summary": "**Architecture Overview** \n\nModern Android Architecture with Clean Separation: \n\nPresentation Layer: Jetpack Compose UI with MVVM pattern \n\nDomain Layer: Use cases and business logic with Hilt dependency injection \n\nData Layer: Room database + DataStore + Repository pattern \n\nAI/ML Layer: Unified service architecture bridging offline/online inference \n\n**Key Components: **\n\nUnifiedGemmaService: Central AI service handling Gemma 3n models via MediaPipe \n\nOfflineRAG: Retrieval-augmented generation with vector database \n\nFeature-specific ViewModels with dedicated repositories \n\nHybrid online/offline capability with automatic fallback \n\nGemma 3n Integration - Core Technical Implementation \n\nMediaPipe LLM Inference Integration: // build.gradle.kts lines 137-138, 233-235 implementation(libs.litert.support) implementation(libs.tasks.genai) \n\n**Model Management (unified_gemma_service.kt): **\n\nTwo variants: 2B Fast (1.5GB) and 4B Quality (2.5GB) models \n\nSession-based inference with proper lifecycle management to prevent JNI crashes \n\nContext length management (450 token limit) with automatic truncation \n\nRecovery mechanisms for model crashes with mutex-protected initialization \n\n**Model Discovery & Loading: **\n\nDynamic model resolution from assets or downloads directory \n\nAsset pack integration for Google Play delivery (build.gradle.kts:18) \n\nFallback to Dropbox download system for development builds \n\n \n\n \n\n \n\n \n\n**Major Technical Challenges & Solutions **\n\nChallenge 1: On-Device Model Performance \n\nProblem: Gemma 3n models require significant memory (1.5-2.5GB) and processing power \n\nSolution: \n\nImplemented model quantization (int4) for mobile optimization \n\nContext length management to prevent memory overflow \n\nBackground thread processing with coroutine-based async operations \n\nConservative token limits by grade level (200-500 tokens) \n\nChallenge 2: Session Management & Stability \n\nProblem: MediaPipe LLM sessions prone to JNI crashes on improper cleanup \n\nSolution: \n\nMutex-protected session creation/destruction (UnifiedGemmaService:36) \n\nTry-catch recovery with model reinitialization \n\nSession-based generation instead of direct inference calls \n\nChallenge 3: Hybrid Online/Offline Architecture \n\nProblem: Seamless switching between on-device Gemma and cloud Gemini API \n\nSolution: \n\nUnified interface abstracting model differences \n\nNetwork connectivity detection with automatic fallback \n\nSettings-driven service selection with graceful degradation \n\nChallenge 4: Educational Context Integration \n\nProblem: Generic AI responses lack educational structure \n\nSolution: \n\nBuilt offline RAG system with subject-specific knowledge base \n\nCurriculum-aligned content in assets (7 subjects) \n\nTeaching approach algorithms (Socratic, Explanation, Problem-solving) \n\nStudent profiling with grade-appropriate response formatting \n\n**Key Technical Choices & Justifications** \n\nChoice 1: MediaPipe over TensorFlow Lite \n\nRationale: MediaPipe provides optimized LLM inference pipeline specifically for Gemma models \n\nBenefits: Better memory management, built-in tokenization, session handling \n\nEvidence: build.gradle.kts:137-138 LiteRT integration \n\nChoice 2: Room + Vector Database Hybrid \n\nRationale: Complex data requirements (chat history, analytics, embeddings) \n\nImplementation: OptimizedVectorDatabase for RAG, Room for structured data \n\nBenefits: Fast similarity search while maintaining relational integrity \n\nChoice 3: Feature-Based Architecture \n\nRationale: 10 distinct AI features requiring specialized logic \n\nStructure: Each feature has dedicated ViewModel, repository, and UI components \n\nBenefits: Maintainable, testable, allows independent feature development \n\nChoice 4: Asset + Download Hybrid Model Delivery \n\nRationale: Balance between app size and functionality \n\nImplementation: Core embedding model in assets, Gemma models downloadable \n\nBenefits: Smaller APK, flexible model updates, development vs production flexibility \n\nPerformance Optimizations \n\n**Model Inference: **\n\nContext length capping to prevent overflow \n\nBatch embedding operations for RAG content \n\nMemory-mapped model loading \n\nBackground thread processing with proper cancellation \n\n**Database Operations:** \n\nOptimized vector similarity search with indexing \n\nBatch insertion for embeddings \n\nCaching layers for frequent queries \n\nConnection pooling and transaction management \n\n**UI Responsiveness: **\n\nDebounced input processing (300ms) to prevent excessive API calls \n\nProgressive loading states with initialization screens \n\nAsync operations with proper coroutine scoping \n\nMemory leak prevention with ViewModel lifecycle management \n\n**Educational AI Features Powered by Gemma 3n **\n\nAI Tutor: Personalized teaching with adaptive approaches based on student responses Quiz Generator: Curriculum-aligned question generation with difficulty scaling CBT Coach: Emotion detection and cognitive behavioral therapy techniques Live Caption: Real-time transcription with translation capabilities Document Summarizer: Multi-format document processing (PDF, DOCX, TXT) Crisis Handbook: Safety resources with location-aware emergency services \n\n**Proof of Engineering Excellence **\n\n**Code Quality: **\n\nComprehensive error handling with recovery mechanisms \n\nProper resource management (model cleanup, session disposal) \n\nThread-safe operations with mutex protection \n\nMemory optimization with lifecycle-aware components \n\n**Scalability: **\n\nModular architecture allowing feature additions \n\nAbstracted AI services supporting multiple model backends \n\nCaching strategies reducing computational overhead \n\nToken usage tracking for cost management \n\n**Real-World Deployment: **\n\nGoogle Play Asset Pack integration for production distribution \n\nDevelopment-friendly fallback systems \n\nComprehensive initialization flow with user feedback \n\nSettings-driven configuration for different deployment scenarios \n\nThis architecture demonstrates sophisticated mobile AI engineering, successfully running large language models on-device while maintaining performance, stability, and educational effectiveness.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/genyarko/impactsuite/tree/storyMode",
        "demo_video_url": "https://youtu.be/wzxXEvU-dHE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/g3n-ai-tutor-app"
    },
    {
        "project_title": "🌿 Plant-Doctor: Offline Plant Disease Diagnosis with Gemma-3n",
        "tagline": "A lightweight, fully-offline vision-and-language pipeline that leverages Gemma-3n for on-device image identification and wraps the results i",
        "authors": "zakihiro",
        "summary": "",
        "technologies": [],
        "source_code_url": "https://github.com/zakihir0/plant-disease-diagnosis-gemma3n",
        "demo_video_url": "https://www.youtube.com/watch?v=rUX_39sc1TU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-3n-plant-disease-diagnosis"
    },
    {
        "project_title": "Gemergency",
        "tagline": "Gemergency is an offline iOS app with fine-tuned Gemma 3n LLM to deliver clear, situation-specific emergency guidance without internet.",
        "authors": "Julia Kurrr, Fedya Katkov",
        "summary": "Gemergency",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Yameteshka/Gemergency",
        "demo_video_url": "https://youtu.be/sfyhgup3K4U",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemergency"
    },
    {
        "project_title": "TwinkleTome: Privacy-First Children's Storytelling Powered by Gemma 3n",
        "tagline": "A exciting macOS app proving that Gemma 3n can deliver personalized, multilingual bedtime stories while maintaining complete privacy",
        "authors": "FelixDevasia",
        "summary": "Every night, millions of parents face an impossible choice: give their children engaging, personalized stories through apps that exploit their data, or protect their privacy at the cost of educational innovation. The $28 billion children's app market has normalized surveillance capitalism, requiring constant internet connectivity and recurring subscription fees that exclude low-income families. Meanwhile, 67 million American families who speak languages other than English at home remain underserved by one-size-fits-all content that ignores cultural context.\n\nTwinkleTome shatters this false choice by proving that cutting-edge AI can be completely private, accessible, and family-empowering.\n\n**TECHNICAL INNOVATION: LOCAL AI THAT RIVALS CLOUD SERVICES**\n\nAt its core, TwinkleTome leverages Ollama with the Gemma 3n model to deliver enterprise-quality story generation entirely on-device. Unlike cloud-based competitors that require API calls costing thousands annually, our local architecture generates personalized stories in under 5 seconds with zero recurring costs and complete offline capability.\n\nOur technical breakthrough lies in advanced prompt engineering that consistently produces age-appropriate content while weaving moral lessons naturally into narratives. The same local model handles both story generation AND real-time translation across 10+ languages, preserving cultural context and narrative voice.\n\nThe SwiftUI-native macOS application employs MVVM architecture with protocol-based LLM services, enabling seamless swapping between different local models. Our sophisticated animation system creates magical user experiences with Timer-based twinkling stars, animated theme selection, and smooth page transitions that rival Disney-quality productions.\n\n**SOLVING REAL-WORLD PROBLEMS AT SCALE**\n\nTwinkleTome addresses three critical societal challenges:\n\n1. **Privacy Crisis Solution**: Children's apps have become surveillance tools, collecting intimate family data for profit. TwinkleTome proves that AI personalization doesn't require privacy sacrifice - every story, every preference, every family detail stays locally encrypted.\n\n2. **Digital Equity Revolution**: Our real-time translation capabilities serve multilingual families who've been excluded from premium educational content. Unlike expensive translation services, our local LLM preserves cultural nuance while making stories accessible in Spanish, French, German, Italian, Portuguese, Dutch, Russian, Japanese, Korean, and Chinese.\n\n3. **Economic Accessibility**: By eliminating cloud dependencies, TwinkleTome removes the subscription barriers that have created educational inequality. One-time purchase provides unlimited, lifetime access to personalized AI storytelling.\n\n**ARCHITECTURAL EXCELLENCE: PRODUCTION-READY INNOVATION**\n\nOur implementation demonstrates enterprise-grade engineering principles:\n\n- **Robust Error Handling**: Comprehensive fallback systems ensure graceful degradation when local LLM services are unavailable\n- **Memory Optimization**: Efficient resource management enables smooth operation on older hardware\n- **Extensible Design**: Protocol-oriented architecture allows integration with future LLM models (Llama, Claude, etc.)\n- **Native Integration**: Deep macOS integration with menu bar access, file system sandboxing, and accessibility compliance\n- **Performance Optimization**: Sub-5-second story generation with lazy loading and background processing\n\nThe local storage system employs secure file encryption while maintaining instant search and retrieval across thousands of saved stories. Our translation overlay system provides real-time language switching without losing user context or story state.\n\n**IMPACT & VISION: BLUEPRINT FOR ETHICAL AI**\n\nTwinkleTome represents more than technological innovation - it's a manifesto for how AI should serve families. We've proven that the most advanced AI can also be the most ethical, creating a blueprint that extends far beyond bedtime stories.\n\nImagine educational apps that teach calculus through personalized examples, therapeutic tools that help children process emotions through custom narratives, or family software that celebrates cultural heritage through AI-generated traditions - all while guaranteeing that family data never leaves the home.\n\nOur success metrics speak to transformative potential:\n- **Zero data breaches** (impossible when data never leaves device)\n- **Universal accessibility** (works offline in any country, any economic situation)\n- **Cultural preservation** (maintains linguistic authenticity across translations)\n- **Educational efficacy** (moral lessons integrated naturally, not forced)\n\n**TECHNICAL ACHIEVEMENTS: PUSHING LOCAL LLM BOUNDARIES**\n\nTwinkleTome showcases several breakthrough implementations:\n\n1. **Advanced Prompt Engineering**: Consistent generation of age-appropriate, morally-integrated narratives that avoid common AI pitfalls like repetitive structures or inappropriate content.\n\n2. **Multilingual Consistency**: Single model handling both generation and translation while preserving character development, plot coherence, and cultural context across language barriers.\n\n3. **Real-time Performance**: Optimized local inference achieving cloud-competitive speeds without network latency or API limitations.\n\n4. **Seamless UX Integration**: AI capabilities integrated so naturally that children focus on story magic, not technology complexity.\n\n5. **Robust Production Deployment**: Complete error handling, edge case management, and graceful degradation proving local LLMs are ready for consumer applications.\n\n**THE COMPETITIVE ADVANTAGE: BEYOND TECHNICAL SUPERIORITY**\n\nWhile competitors chase cloud-based scaling, TwinkleTome proves that local-first architecture provides superior user experiences:\n\n- **Instant Generation**: No network latency or server queues\n- **Unlimited Usage**: No API rate limits or usage caps  \n- **Complete Privacy**: Zero data collection or third-party access\n- **Offline Reliability**: Works during travel, internet outages, or bandwidth restrictions\n- **Economic Sustainability**: No recurring costs or subscription lock-in\n\n**VISION FOR INDUSTRY TRANSFORMATION**\n\nTwinkleTome establishes proof-of-concept for a privacy-first AI ecosystem where:\n- Consumer applications don't require surveillance for personalization\n- Advanced AI capabilities don't demand cloud dependency\n- Educational technology empowers rather than exploits families\n- Technical innovation serves human values rather than extractive business models\n\nWe've demonstrated that local LLMs aren't just technically viable - they're ethically superior and economically sustainable for consumer applications.\n\n**CONCLUSION: THE FUTURE IS LOCAL, PRIVATE, AND FAMILY-FIRST**\n\nTwinkleTome proves that we don't have to choose between AI innovation and family privacy. By leveraging local LLMs through Ollama and Gemma 3n, we've created an application that's simultaneously more advanced, more ethical, and more accessible than cloud-based alternatives.\n\nThis isn't just a bedtime story app - it's a demonstration that the future of consumer AI is local, private, and empowering. TwinkleTome shows that the most sophisticated technology can also be the most human-centered, creating magical experiences that strengthen families rather than surveilling them.\n\nIn a world increasingly dominated by extractive AI business models, TwinkleTome stands as proof that innovation and ethics aren't opposing forces - they're complementary strengths that create better technology and better outcomes for the families we serve.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/felixmd/TwinkleTome/tree/main/TwinkleTome",
        "demo_video_url": "https://youtu.be/H_v-GnRNuNU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/twinkletome-privacy-first-childrens-storytelling-p"
    },
    {
        "project_title": "ima TrailSierra",
        "tagline": "imaSierra provides offline AI health support for remote communities, using Gemma 3n to assist healthcare workers with medical diagnostics.",
        "authors": "Guillermo De la Torre, Jose Esco, Eduardo Isaac Davila Bernal",
        "summary": "🩺 ima-TrailSierra: Using Gemma 3n for Healthcare in Disconnected Communities\n\nA step-by-step overview of building an AI-powered diagnostic support tool for healthcare workers in remote, offline environments using Google's Gemma 3n model.\n\nIn this document, we'll see how we can use an optimized and quantized version of Gemma 3n to build a personal health co-pilot. The goal is to go from a series of symptoms described by a local health promoter to a structured, AI-generated analysis that can help provide better and faster medical attention in communities with no internet access.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/tecnologiaima/imaTrailSierra_FrontEnd",
        "demo_video_url": "https://youtu.be/3_B3_UymZCs",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ima-trailsierra"
    },
    {
        "project_title": "Smart Food Manager: AI-Powered Food Waste Reduction System",
        "tagline": "Reduce food waste by 40% using Gemma 3n's on-device AI for food recognition, expiry tracking, and smart recipe recommendations",
        "authors": "coderYel",
        "summary": "Smart Food Management System - Reducing Food Waste with Gemma 3n",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/freedomofme/Smart-Food-Management-System---Reducing-Food-Waste-with-Gemma-3n",
        "demo_video_url": "https://youtu.be/3IYf8tW-jSo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/smart-food-manager-ai-powered-food-waste-reduction"
    },
    {
        "project_title": "AI Senior Assistant",
        "tagline": "An intelligent assistant in the digital world, ensuring safety and independence for every older person",
        "authors": "Mateusz",
        "summary": "🎯 Purpose Of Application\nThe prepared solution presents the use of **Gemma3n** as an intelligent assistant whose purpose is to protect and navigate seniors in the online environment and during daily computer use.\n\nIn the face of increasing digitization and dynamic growth of new technologies, there is a simultaneous increase in digital risks that seniors often have to face and which stand in the way of safe and effective use of modern technologies.\n\nThanks to the advanced capabilities of the used AI model, such as text and image understanding, the prepared solution will not only help perform tasks on the PC and on the web, but will also actively warn about potential threats and guide the user through complex processes in a simple and understandable way.\n\nIn addition, the chosen model allows the generation of tips and warnings with minimal delay, providing smooth and responsive real-time support. This should contribute to increasing the independence and self-confidence of seniors when using new technological solutions.\n\n*More details and reasons why this system may have an impact I presented in the video\n\n🏗️ System Architecture",
        "technologies": [
            "Flask",
            "Ollama",
            "Vosk"
        ],
        "source_code_url": "https://github.com/mateo252/AI-Senior-Assistant",
        "demo_video_url": "https://www.youtube.com/watch?v=btDuift2AIk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-senior-assistant"
    },
    {
        "project_title": "YachAI: Offline mobile math tutor that generates visual problem explanations",
        "tagline": "YachAI is a fully offline math tutor that generates visual explanations for problems, supporting multi-modal inputs and speech interactions.",
        "authors": "Joaquín Francisco Jordán O'Connor",
        "summary": "yacha *(verb)*  \n**Origin:** Quechua  \n**Pronunciation:** /ˈjɑ.tʃɑ/  \n\n**Definition:**  \n*To know; to understand, the act of acquiring or possessing knowledge.*\n\n**Usage:**  \nUsed to express comprehension or familiarity, especially in the context of learning and wisdom.\n\n> *\"Yacha\" is more than just 'to know' — it embodies a deep, often communal understanding.*\n\nProblem\nThe covid 19 pandemic only broadened existing inequalities in education in Peru, a country of 34 million people.\nOut of 8 million students in school age, around 3 million live in extreme poverty, in areas with limited or no access to the internet.\nMoreover, 8 million adults haven’t finished school. Then the question naturally arises. Who is going to help these children at home?\n\nMeet YachAI\nYachAI is an offline mobile AI math tutor designed specifically for Peruvian students aged 10-12 (5th-7th grade) in rural Peru, leveraging Gemma 3n's on-device, multilingual and multimodal capabilities to deliver visual instruction in spanish without internet connectivity. Our system transforms any Android device into a personal math problem solving teacher using think-aloud methodology and procedural animation generation. \n\nArchitecture\nYachAI is built on 3 main layers:",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/jjordanoc/yachai",
        "demo_video_url": "https://youtu.be/WCvUACltBbo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/yachai-offline-math-tutor-for-5-7th-graders-in-per"
    },
    {
        "project_title": "WiseMama: AI-Powered Offline Pregnancy Assistant for Rural Communities",
        "tagline": "An offline AI pregnancy knowledge assistant that bridges the maternal healthcare gap in rural communities by providing personalized guidance",
        "authors": "Zzzzzz, HuanXu",
        "summary": "",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": "https://github.com/lizequn/WiseMama",
        "demo_video_url": "https://youtu.be/hr8363xU-o4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/wisemama-ai-powered-offline-pregnancy-assistant-fo"
    },
    {
        "project_title": "A Gamme using Gemma",
        "tagline": "NPC dialogue made by Gemma 3n",
        "authors": "Filipe D. Gonçalves",
        "summary": "Abstract\nThis paper presents the design and implementation of a lightweight, portable dialogue system for non-player characters (NPCs) in video games, built on the compact language model Gemma 3n-2b. We integrate a simple GameMaker prototype with a local Flask/Ollama server hosting Gemma 3n-2b: each time the player interacts with an NPC, the game sends the conversation history and a character-specific prompt to the model, which returns a context-aware response in real time.\n\nIntroduction\nGames have surged in popularity over the past decades, becoming a dominant form of entertainment and a daily ritual for millions around the globe. From casual mobile titles to sprawling open-world epics, players turn to games not only for excitement and challenge but also as a means of relaxation and escape. Indeed, interactive digital experiences are increasingly recognized for their capacity to soothe, uplift, and in some contexts provide therapeutic value.\n\nAs a lifelong gamer and advocate for the medium, I regard video games as a modern art form with boundless potential. Their unique blend of narrative, mechanics, and player agency offers creative avenues that few other media can match. Yet, despite advances in graphics, world-building, and storytelling, the core interactions between player and non-player characters (NPCs) often remain scripted and predictable.\n\nIn this work, I explored the possibilities, how integrating a compact, on-device language model, Gemma 3n-2b, can transform NPC dialogue into dynamic, context-aware exchanges. By pairing a simple GameMaker prototype with a lightweight Flask/Ollama server, we demonstrate that fully reactive, personality-driven conversations are achievable on consumer hardware. This approach preserves the spontaneity and immersion that define great games, while maintaining the portability and low overhead necessary for broad adoption.\n\nMethods\nThe prototype was implemented in GameMaker Studio, relying on its core abstractions: objects, events, and scripts, to orchestrate both traditional gameplay and AI-driven dialogue. Every entity in the game (the player, each NPC, and the dialogue controller) is represented by an object instance with its own variables, sprite assets, and behavior. Player movement and collision detection are handled in the objects’ Step events, while dialogue flow and UI rendering are managed by a dedicated dialog-controller object.\n\nInteraction begins when the player presses the designated talk key within a short radius of an NPC. At that moment, a dialog object is spawned, opening a GUI text box and activating keyboard input. As the player types, their keystrokes are collected into an input buffer. When they hit Enter, the NPC’s Step event packages the player’s line together with a short history of the last few exchanges and a character-specific system prompt, then sends it via an asynchronous HTTP POST to a local Flask/Ollama server hosting the Gemma 3n-2b model. Importantly, this HTTP call is non-blocking: the game loop continues to run, just the character is not able to move due to being in dialogue.\n\nWhen the AI response arrives, it triggers the object’s Async HTTP event. That event handler parses the JSON payload, appends the NPC’s reply to the rolling dialogue history, and hands the new line over to the dialog-controller object for on-screen display. Pagination logic in a reusable script ensures that even long replies are split into fixed-size text pages, preventing overflow and letting the player advance through multi-page answers with a single key press.\n\nBy combining GameMaker’s built-in event system with asynchronous networking, this architecture preserves fluid, real-time gameplay while enabling context-aware, personality-driven conversations. The only additional requirement for deployment is bundling the Flask/Ollama service alongside the game executable—no other external runtime dependencies are needed—demonstrating a truly portable approach to integrating compact AI models into interactive experiences.\n\nResults\nThe integration of a compact on-device AI made NPC interactions into living, unpredictable conversations. This proof-of-concept shows that embedding portable language models can turn scripted dialogue into emergent storytelling, offering vast potential for richer narratives, more immersive role-play, and even therapeutic experiences where the game adapts to the player’s mood in real time.\n\nDiscussion\nIn moving beyond static scripts, NPCs that can detect and respond to a player’s mood—whether by cracking a joke when you’re down or offering soothing words when you’re frustrated—could foster a deeper emotional bond and make each playthrough feel uniquely personal. Maybe even in the future, integrating voice recognition would further smooth the conversation flow, allowing players to speak naturally and even enabling the system to pick up on vocal cues like tone and pacing to gauge their feelings in real time. Together, these advances promise truly immersive experiences, where the world dynamically adapts not just to your actions but to your emotional state, blurring the line between player and character. Moreover, as this technology matures, it could spark a new market for AI-as-a-service in gaming. Imagine major providers bundling turnkey character AI modules that studios of any size can license and integrate, much like middleware today, opening fresh revenue streams and creative possibilities across the industry.\n\nConclusions\nThis work demonstrates that compact, on-device language models can seamlessly enrich NPC dialogue without sacrificing gameplay fluidity or portability. By coupling a lightweight AI server with standard GameMaker mechanics, we enabled characters to converse dynamically and retain context within a self-contained distribution. The result is a more engaging and personalized experience that hints at broader applications: mood-aware interactions, voice-driven conversations, and a new ecosystem for AI-driven game content. As portable AI continues to improve, these techniques offer a clear path toward games that truly listen, respond, and evolve alongside each player.",
        "technologies": [
            "Flask",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Hazytz/gemma3n_",
        "demo_video_url": "https://youtu.be/F1mpBX3HLjU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/a-gamme-using-gemma"
    },
    {
        "project_title": "Accessly: AI-Powered Computer Accessibility and Memory Assistant",
        "tagline": "Next-gen accessibility tools for effortless, private computer control and memory—powered by AI, designed for everyone.",
        "authors": "Younes Laaroussi",
        "summary": "Accessly – Technical Write-up\n\n*Gemma 3n Impact Challenge – Proof of Work*\n\nApp URL: https://accesslylanding.netlify.app/\nGitHub Repo: [Accessly](https://github.com/youneslaaroussi/accessly)\n\n---",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/GVyxMrCZBJU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/accessly-ai-powered-computer-accessibility-and-mem"
    },
    {
        "project_title": "StepSage - Powered by Gemma AI",
        "tagline": "A friendly pocket guide for the visually impaired that quietly spots obstacles and helps you navigate around them.",
        "authors": "kiranvsk, KrishnaSaiPendem",
        "summary": "StepSage is an on-device “talking cane” for smartphone users who are blind or have low vision. By pairing the phone’s rear camera with Google’s Gemma 3n model, the app gives real-time, room-level guidance. This solves a daily pain point: indoor navigation is still fraught with hidden chairs, half-open doors, and unmarked steps that conventional GPS or beacons can’t detect. StepSage removes the dependency on cloud connectivity, protects user privacy, and works even in poor-signal environments like basements or hospital corridors.\n\nUnder the hood, each camera frame is first processed locally by MediaPipe’s object detection model, which spots key indoor objects in real time. A compact JSON snapshot of that scene is then fed into the on-device Gemma 3n LLM, which turns the raw detections into a single, conversational sentence of guidance. Finally, Android’s text-to-speech engine voices the result. Because detection, language generation, and speech all run offline on the handset, StepSage preserves user privacy, works without data service, and delivers instant, context-aware navigation cues.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/bvsaik/StepSage",
        "demo_video_url": "https://youtu.be/GnjMrWhdBpw?si=X07WZWlQV9yfbyUf",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/stepsage-powered-by-gemma-ai"
    },
    {
        "project_title": "Awaaz: The Voice-First, Offline AI Health Companion for the Blind",
        "tagline": "Voice-first, private medicine management and wellness assistant built for blind and low-vision users, works 100% offline.",
        "authors": "Abhishek Jain",
        "summary": "Awaaz: The Voice-First, Offline AI Health Companion for the Blind\n\nVoice-first, private medicine management and wellness assistant built for blind and low-vision users, works 100% offline.\n\n---",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "NVIDIA Jetson",
            "Ollama",
            "Vosk"
        ],
        "source_code_url": "https://github.com/abhishekblue/gemma3n-hackathon",
        "demo_video_url": "https://youtu.be/3JJZoRCVzas",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/awaaz-offline-ai-health-companion"
    },
    {
        "project_title": "Medora AI: On-Device Medical Transcription and Summary Assistant",
        "tagline": "Empowering doctors with offline clinical transcription and SOAP/summary generation using Gemma 3n and MediaPipe.",
        "authors": "Vishal Dhoriya, Shekhar Gupta",
        "summary": "**1. Introduction**\n\nMedora AI is a fully on-device mobile healthcare assistant that transcribes doctor-patient interactions,\ngenerates structured clinical documentation in the SOAP format, and provides a summarized output\ntailored for patients. The app is built in Flutter and leverages Whisper ASR and the Gemma-3n-e2b-it-q4\nmodel downloaded directly from Hugging Face, with MediaPipe as the inference backend, inspired by\nGoogle AI Edge.\n\n**2. Core Features**\n\n**- Real-Time Transcription:**  Whisper converts doctor-patient speech to text in real-time with chunked\nparallel audio processing.\n\n**- LLM-Powered SOAP Notes:** The transcribed text is fed into the Gemma 3n model to generate a\nstructured clinical SOAP note.\n\n**- Editable Fields:** Doctors can edit the generated Subjective, Objective, Assessment, and Plan fields within the UI.\n\n**-Patient Summaries:** A detailed summary is generated including symptoms, assessment, treatment plan, patient details, and clinic header information—but not currently compressed into 4–5 patient-friendly sentences.\n\n**-Export to PDF:** SOAP and patient summary outputs can be saved as PDF and shared via WhatsApp.\n\n**-On-Device Processing:** All AI tasks run locally with no dependency on cloud APIs.\n\n\n**3. Architecture Overview**\n\n**Technologies Used:**\n\n- Flutter (Mobile UI)\n- Whisper (ASR, chunked streaming)\n- Gemma-3n-e2b-it-q4 from Hugging Face (downloaded locally)\n- MediaPipe (LLM inference runtime)\n- Platform Channels (for native binding between Flutter and MediaPipe)\n\n**Inference Flow:**\n\n1. Patient is registered via a form (name, age, gender, DOB).\n2. Doctor records conversation.\n3. Audio is streamed to Whisper every 30 seconds.\n4. Transcribed text chunks are accumulated.\n5. After recording, the full transcript is passed to the Gemma 3n model.\n6. Model outputs structured SOAP JSON using a strict system prompt.\n7. ⁠ ⁠A structured summary containing patient and clinical info is extracted.\n8. Results are editable per field and saved per patient, and optionally exported to **PDF** . \n\n\n**4. Model Integration Details**\n\n**Whisper**\n\n• Implemented via whisper.cpp and whisper_new plugin.\n• Chunking every 30s ensured streaming without memory bloat.\n\n**Gemma 3n (gemma-3n-e2b-it-q4)**\n\n• Downloaded once using Hugging Face token and stored locally.\n• Runs via MediaPipe Tasks (tflite-compatible model wrapper).\n• Custom system prompt ensures consistent SOAP JSON output.\n\n\n\n**MediaPipe**\n\n• Used to embed and run the LLM locally\n• Provided stable inference and faster loading time on Android inspired by Google AI Edge Gallery App.\n\n**5. Engineering Challenges & Solutions**\n\n**a. On-Device Inference Constraints**\n\n• Challenge: Hugging Face models are large and not tflite-ready.\n• Solution: Used quantized Q4 version (e2b-it-q4) and MediaPipe to run them efficiently on mobile.\n\n\n**b. Flutter + Native Model Inference**\n\n• Challenge: Flutter doesn’t support MediaPipe or low-level LLMs natively.\n• Solution: Used Platform Channels to bind Flutter with native Android C++/Java code executing the\nmodel.\n\n\n**c. Streaming Audio Transcription**\n\n• Challenge: Continuous transcription required streaming with low latency.\n• Solution: Chunked audio every 30 seconds, streamed to Whisper for live transcription with\nmultithreaded buffering.\n\n**6. Future Roadmap**\n\n• Edge deployment on **Jetson** as a single-source compute node to handle Whisper and LLM for enhanced speed, privacy, and power efficiency.\n• Multilingual support for regional  languages.\n• EHR integration and syncing.\n• On-device fine-tuning (LoRA-based).\n• Integration with Android Health Connect.\n\n**7. Conclusion**\n\nMedora AI is a powerful proof-of-concept showcasing that real-time clinical documentation with AI is\npossible on-device using Gemma 3n. By combining speech-to-text (Whisper), structured LLM outputs\n(Gemma), and Flutter-based mobile deployment, we offer a secure, scalable solution for offline medical\nautomation.\n\n**Credits**\n\nModel: Gemma-3n-e2b-it-q4 (Hugging Face)\nASR: Whisper (OpenAI)\nRuntime: MediaPipe Tasks\nDeveloped with Flutter",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/VishalDhoriya/Medora-ai",
        "demo_video_url": "https://www.youtube.com/watch?v=vLnFuIHS9jI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/medora-ai-medical-transcription"
    },
    {
        "project_title": "Sixth Sense for Security Guards - Powered by Google's Gemma 3n",
        "tagline": "Community safety through AI-powered security camera monitoring",
        "authors": "Abylay Ospan, Alsu Ospan",
        "summary": "We developed an AI-powered system that monitors live security camera feeds and asks **Google’s Gemma 3n** model to detect unusual or suspicious activity, including medical emergencies.\n\nThe entire system runs locally on a consumer-grade NVIDIA GPU. This ensures privacy, low latency, data sovereignty, and avoids cloud inference costs (which can exceed $75/month per camera, based on our usage) while keeping all footage on-site to block centralized surveillance abuses that can fuel authoritarian control.\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F20806798%2Fc083a6ef583772c026b3bc5fec5f53b8%2Fsunny-osprey-security-guard-640.png?generation=1754408322807940&alt=media)\n*We’re currently testing the system in our private residential community in Miami, Florida.*\n\n\nWhat It Does\n\nThe system continuously analyzes live RTSP video feeds from 16 IP cameras covering only public places such as entrances, sidewalks, driveways, and other publicly visible areas.\n\nWe ask **Google’s Gemma 3n** model to classify each activity ([see source code here](https://github.com/sbnb-io/sunny-osprey/blob/907999cb00454a8e634011e18cb6e55f12a08b3a/src/sunny_osprey/llm_inference.py#L185))",
        "technologies": [
            "CUDA",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/sbnb-io/sunny-osprey/",
        "demo_video_url": "https://youtu.be/h3UoQgH4F0M",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sixth-sense-for-security-guards-powered-by-googles"
    },
    {
        "project_title": "WELLNEST - AI-Powered Perinatal Health Companion",
        "tagline": "WellNest empowers mothers from pregnancy to postpartum with AI-based risk alerts, mental health tools, on-device assistant and a community",
        "authors": "Edidiong Etetim, Busayo Ososanwo",
        "summary": "**WellNest – Technical Writeup - Google Gemma 3n Impact Challenge Submission**\n\n---",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/TechBuz4/Gemma_competition_WellNest/blob/main/README.md",
        "demo_video_url": "https://youtu.be/ADWLehSOrto",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/wellnest-ai-powered-perinatal-health-companion"
    },
    {
        "project_title": "MINDSACPE -EMOTIONAL CHATBOT",
        "tagline": "Mindscape is a web app using Gemma 3n AI to offer emotional support, security, mental health checks, and yoga tips in one platform.",
        "authors": "Atharva Koranne, Anuj Murkute22, Rajas Sudumbrekar, SAEE SHINDE2006, Nakshatra Omkar Karanjkar ",
        "summary": "Project Description: Mindscape\nProject Name\nMindscapeDeveloped By: INNOVATEX\nDate: August 05, 2025\nOverview\nMindscape is an innovative web-based application designed to support mental well-being, security monitoring, and physical health through an integrated platform. Powered by a custom AI model (Gemma 3n), the application, named \"Niko,\" serves as an emotional support companion, offering personalized responses, stress analysis, and yoga recommendations. It leverages Streamlit for a user-friendly interface and incorporates advanced features like real-time video analysis, audio assessment, and data reporting.\nKey Features\n\nEmotional Support Chatbot (Niko):\n\nUtilizes the Gemma 3n model to provide empathetic and supportive responses to user inputs about their emotional state.\nIncludes interactive elements like a breathing animation and a number guessing game to promote relaxation during processing.\nFeatures text-to-speech output for accessibility and engagement.\n\n\nSecurity Monitoring:\n\nDetects abnormal behavior in uploaded or live-captured video footage using computer vision techniques.\nProvides a summary of detected motion events and recommendations based on analysis.\n\n\nMental Health Assessment:\n\nAnalyzes uploaded or live-recorded audio to assess stress levels using audio signal processing.\nOffers insights and suggestions based on detected stress patterns.\n\n\nSports and Fitness (Yoga):\n\nRecommends yoga poses tailored to the user’s mood (e.g., stressed, anxious, or low energy).\nIncludes detailed descriptions and step-by-step instructions for nine stress-relieving asanas.\n\n\nAnalysis Report:\n\nGenerates a comprehensive report combining chat history, security analysis, and mental health assessments.\nProvides sentiment analysis, therapist recommendations, and downloadable CSV files for tracking.\n\n\n\nTechnical Details\n\nFramework: Built with Streamlit for a responsive web interface.\nAI Model: Integrates the Gemma 3n model (quantized to Q4_K_M GGUF format) via the llama_cpp library for natural language processing.\nLibraries: Utilizes librosa for audio analysis, cv2 for video processing, pandas for data management, and gTTS for text-to-speech.\nData Storage: Stores chat, security, and mental health data in CSV files within a reports directory.\nStyling: Employs custom CSS with the Montserrat font and dynamic backgrounds (YouTube videos or images).\n\nTarget Audience\n\nIndividuals seeking mental health support and stress relief.\nSecurity personnel or homeowners needing behavior monitoring.\nFitness enthusiasts interested in yoga for wellness.\n\nFuture Enhancements\n\nIntegration of additional AI models for improved accuracy.\nMobile app development for broader accessibility.\nEnhanced security features with real-time alerts.\nMultilingual support for global reach.\n\nSystem Requirements\n\nPython 3.10.9 or higher.\nCompatible model file (e.g., gemma-3n-E4B-it-Q4_K_M.gguf) placed at C:/Users/projects/models/.\nInternet connection for YouTube background and external libraries.\nWebcam and microphone for live capture (optional).\n\nConclusion\nMindscape aims to create a holistic wellness ecosystem, blending AI-driven insights with practical tools to enhance users' mental and physical health.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/3oA6WF5c0AI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/mindsacpe-emotional-chatbot"
    },
    {
        "project_title": "Offline AI Tutor",
        "tagline": "AI-Powered Educational Videos for Students Without Internet",
        "authors": "Akanksha Thakur, Aryaman Kumar",
        "summary": "AI Tutor with Gemma 3n: Technical Implementation\n\n\n---",
        "technologies": [
            "Docker",
            "FastAPI",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/akanksha509/AI-Tutor",
        "demo_video_url": "https://youtu.be/w9clEmZy0Cw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/offline-ai-tutor"
    },
    {
        "project_title": "Crop Recommendation with Climate & Soil Data + Leaf Disease Detection ",
        "tagline": "An AI-powered assistant for recommending crops based on soil & climate data and detecting leaf diseases using Gemma and vision models.",
        "authors": "Lohitha Reddy Aramati",
        "summary": "This project combines a fine-tuned Gemma language model (via LoRA) for personalized crop recommendations with a multimodal system for real-time leaf disease detection, providing audio feedback to assist diverse farming communities.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/ZJS4_m8cZ6Q",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/crop-recommendation-with-climate-and-soil-data-lea"
    },
    {
        "project_title": "VoiceVision: Offline-First AI Companion for the Visually Impaired",
        "tagline": "VoiceVision: Offline-First AI Companion for the Visually Impaired",
        "authors": "Vedant Dubey, Tithi Biswas1",
        "summary": "🌟**Project Overview**\nVoiceVision is a revolutionary web-based AI assistant that empowers visually impaired users through cutting-edge multimodal artificial intelligence. Built with Google's Gemma 3n at its core, this accessibility-first application transforms how users interact with and understand their visual environment through intelligent voice and gesture controls.\n\n\n💫**A Personal Journey**\nFrom my earliest memories, I’ve carried a wish: to hear my friend’s voice. She’s been unable to speak since birth, and growing up, I always felt a deep longing to connect with her in ways most people take for granted. This project, VoiceVision, is more than a hackathon submission—it’s a lifetime achievement, a beacon of hope to help her find her voice and interact with the world. The journey has been emotional, but the impact Gemma 3n enabled is truly transformative.\n\n🛠️**Architecture Overview**\nVoiceVision is built as a modular TypeScript application, with a clear separation of concerns across its architecture:\n\n- **Core Services**: The src/services directory houses the heart of the app—AudioService, CameraService, EnhancedVisionService, SceneAnalysisService, and VoiceVisionCore. Each service is responsible for a distinct aspect of the user experience, from capturing audio and video to analyzing scenes and synthesizing voice.\n- **Entry Points**: Files like index.ts, realtime.ts, and start-realtime.ts orchestrate the flow, initializing services and handling real-time interactions.\n- **Frontend**: The HTML files (voicevision-enhanced.html, voicevision-realtime.html) provide user interfaces for enhanced and real-time features, ensuring accessibility and ease of use.\n**Gemma 3n Integration**\nGemma 3n is the backbone of our voice synthesis and scene understanding. Here’s how we specifically leveraged it:\n\n- **Voice Synthesis**: Using Gemma 3n’s advanced generative capabilities, we convert text and scene context into natural-sounding speech. This is the “voice” my friend never had, now made possible.\n- **Scene Analysis**: Gemma 3n powers the SceneAnalysisService, interpreting visual data from the camera and providing contextual cues, which are then vocalized for the user.\n- **Real-Time Processing: The realtime.ts and web-realtime.ts modules ensure that Gemma 3n’s models run efficiently, delivering instant feedback and interaction.\n**Overcoming Challenges**\nIntegrating Gemma 3n was not without hurdles:\n\n- **Performance Optimization**: Running large models in real-time required careful threading and resource management. We split processing between client and server, using Node.js for heavy lifting and browser APIs for lightweight tasks.\n- **Accessibility**: Ensuring the app was usable for people with disabilities meant rigorous testing and UI adjustments. We prioritized keyboard navigation, clear audio prompts, and minimal latency.\n- **Model Fine-Tuning**: Adapting Gemma 3n to generate a voice that felt authentic and emotionally resonant was a challenge. We iterated on training data and synthesis parameters until the output felt “real.”\n**Why These Choices Were Right**\n- **TypeScript & Modular Design**: This ensured maintainability, scalability, and type safety, critical for a project that may evolve to help more people.\n- **Gemma 3n**: Its state-of-the-art generative capabilities were essential for both voice and vision tasks, outperforming alternatives in quality and speed.\n- **Real-Time Architecture**: By separating concerns and optimizing for performance, we delivered a seamless experience that feels natural and empowering.\n\n🎯 **Problem Statement**\nMillions of visually impaired individuals face daily challenges in:\n- Environmental Navigation: Understanding surroundings and identifying objects\n- Text Recognition: Reading signs, documents, and digital content\n- Independent Living: Performing tasks without constant assistance\n- Privacy Concerns: Relying on cloud-based services that compromise personal data\n\n💡**Solution: VoiceVision**\n\nVoiceVision addresses these challenges through an innovative combination of:\n🧠 **AI-Powered Intelligence**\n- **Gemma 3n Multimodal AI**: Advanced scene understanding and contextual descriptions\n- **Real-time Object Detection**: TensorFlow.js COCO-SSD for instant object recognition\n- **Optical Character Recognition**: Tesseract.js for reading text from any image\nIntelligent Context Analysis: Provides relevant, actionable information about surroundings.\n\n🎤 **Voice-First Interaction**\n- **Complete Voice Control**: Navigate all features through natural speech commands\n- ** Intelligent Speech Synthesis**: Clear, customizable audio feedback\n- **Multi-language Support**: Accessible in multiple languages with voice switching\n- **Conversational Interface**: Natural, intuitive communication with the AI\n\n👋 **Gesture Recognition**\n- **16 Distinct Hand Gestures**: MediaPipe-powered sign language recognition\n-**Silent Operation:** Control the app without speaking when needed\n- **Accessibility Settings**: Adjust all preferences through gestures\n- **Gesture Feedback**: Audio confirmation for all gesture-based actions\n\n🔒**Privacy-First Architecture**\n- **Offline-First Design**: Core functionality works without internet\n- **On-Device Processing**: Sensitive data never leaves the user's browser\n- **Secure Configuration**: Environment-based API key management\n- **No Data Collection**: Complete user privacy protection\n\n🚀 **Key Features**\n**Scene Understanding**\n- Real-time camera analysis with detailed verbal descriptions\n- Object identification with confidence levels and positioning\n- Environmental context (lighting, setting, atmosphere)\n- Hazard detection and safety alerts\n\n**Text Reading**\n- Instant OCR of signs, documents, and screens\n- Smart text formatting and organization\n- Reading speed control and voice customization\n- Language detection and translation capabilities\n\n**Smart Navigation**\n- Spatial awareness and object positioning\n- Movement guidance and obstacle detection\n- Location context and landmark identification\n- Indoor and outdoor environment adaptation\n\n**Accessibility Excellence**\n- Zero visual interface dependency\n- Keyboard navigation fallbacks\n- Screen reader compatibility\n- Customizable audio preferences\n\n🛠️**Technical Architecture**\n**Frontend Stack**\n```python\nHTML5 + Canvas API → Real-time video processing\nJavaScript ES6+ → Modern browser capabilities\nCSS3 Responsive → Cross-device compatibility\nMediaPipe Hands → Gesture recognition system\n```\n**AI Integration**\n```python\nGemma 3n → Multimodal scene understanding\nTensorFlow.js → Browser-based object detection\nTesseract.js → Optical character recognition\nWeb Speech API → Voice synthesis and recognition\n```\n**Infrastructure**\n```python\nVercel Deployment → Serverless hosting\nEnvironment Config → Secure API management\nTypeScript → Type-safe development\nProgressive Enhancement → Graceful degradation\n```\n\n🌍 **Real-World Impact**\n\n**Independence Enhancement**\n- Enables autonomous navigation in unfamiliar environments\n- Reduces dependency on human assistance for daily tasks\n- Empowers users to access visual information independently\n\n**Safety Improvement**\n- Real-time hazard detection and alerts\n- Environmental awareness in all lighting conditions\n- Obstacle identification for safer movement\n\n**Social Integration**\n- Facilitates participation in visual conversations\n- Enables independent reading of menus, signs, and documents\n- Supports professional and educational activities\n\n**Digital Inclusion**\n- Bridges the gap between visual content and accessibility\n- Provides equal access to digital information\n- Supports modern web interaction patterns\n\n🏆**Innovation Highlights**\n**Gemma 3n Integration**\n- **Multimodal Processing**: Combines vision and language understanding\n- **On-Device Performance**: Leverages Gemma 3n's efficient architecture\n- **Contextual Intelligence**: Provides meaningful, actionable descriptions\n- **Real-time Responsiveness**: Instant feedback for user interactions\n**Unique Value Propositions**\n- First truly offline multimodal AI assistant for accessibility\n- Complete gesture-voice hybrid interface requiring no visual interaction\n- Privacy-preserving architecture with on-device processing\n- Professional-grade accuracy with consumer-friendly simplicity\n\n📊 **Market Potential**\n- **Target Audience**: 285+ million visually impaired individuals worldwide\n- **Expanding Market**: Growing demand for AI-powered accessibility tools\n- **Cross-Platform Opportunity**: Web-based solution works on any device\n- **Enterprise Applications**: Workplace accessibility and compliance solutions\n\n🔮 **Future Vision**\nVoiceVision represents the foundation for a new generation of AI-powered accessibility tools that:\n- Democratize access to visual information\n- Eliminate barriers between users and their environment\n- Create truly inclusive digital experiences\n- Pioneer ethical AI development for human empowerment\n- VoiceVision isn't just an app—it's a gateway to independence, safety, and full participation in the visual world.\n\n**Conclusion**\nVoiceVision is not just an app—it’s a bridge to connection, a testament to what engineering can achieve when driven by empathy. For my friend, and for many others, Gemma 3n has given a voice where there was none. This project is my way of giving back, and proof to the judges that our demo is backed by real, thoughtful engineering.",
        "technologies": [
            "Gemma 3n",
            "Web Speech API"
        ],
        "source_code_url": "https://github.com/vedantDube/Gemma_Hackhathon",
        "demo_video_url": "https://youtu.be/Ox4__hWLdTA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/voicevision-offline-first-ai-companion-for-the-vis"
    },
    {
        "project_title": "Synapse: Your AI Medical Assistant",
        "tagline": "Synapse is an AI-powered personal medical assistant that provides diagnostic insights and recommendations using real-time image analysis.",
        "authors": "ANUSHKA PAIN, Soumyabrata Bhat, Monika Shruti Bara",
        "summary": "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F27944886%2Fdee10f9ab0a9f855e553b41914992351%2FSynapse%20diagram%20_%20Mermaid%20Chart-2025-08-04-125501.png?generation=1754405152764689&alt=media)\n\n**Solving the Healthcare Access Gap with Synapse: An AI Medical Assistant**\nIn a world where timely health information is a luxury, many people face a critical problem: a lack of immediate, reliable medical insights. This gap leads to delayed care, reliance on misinformation, and a general feeling of helplessness when a health concern arises. Synapse was created to solve this problem. Our goal is to provide an accessible, AI-powered companion that offers foundational insights and actionable recommendations, bridging the gap between a health issue and professional medical care.\n\nIntroducing Synapse: The AI Powerhouse in Your Pocket\nWe built Synapse to be more than just an app; it's a Progressive Web App (PWA) that can be installed on your device and used even without an internet connection. This ensures you always have access to its diagnostic tools, especially in emergencies.\n\nAt its core, Synapse is powered by Google’s cutting-edge Gemma models, managed through the Genkit backend framework. This powerful combination allows us to handle complex visual data with precision and efficiency.\n\nThe Technology Behind the Diagnostics\nSynapse offers three core diagnostic features, each leveraging our AI to provide immediate insights:\n\nReport Analysis: You can upload images of medical reports like X-rays or blood work. The AI processes these images to identify key markers or potential abnormalities, giving you a starting point for discussion with a doctor.\n\nNeurological Assessment: Using your device's camera, the AI analyzes facial and upper body symmetry in real-time. This can be a critical first step in identifying potential signs of a stroke or other neurological events, and it offers basic physiotherapy guidance.\n\nDermatology Analysis: Snap a picture of a skin condition—a rash, a bedsore, or a burn. The AI will classify the condition, assess its severity, and provide detailed, step-by-step first-aid recommendations.\n\nA Modern and Robust Tech Stack\nTo create a seamless and reliable user experience, we built Synapse on a modern web stack:\n\nNext.js & React with TypeScript: We chose Next.js for its performance benefits and a scalable architecture. Using React and TypeScript, we ensured our codebase is robust, maintainable, and less prone to bugs.\n\nTailwind CSS & ShadCN UI: For a clean and modern interface, we used Tailwind CSS and ShadCN UI. This allowed us to build a custom, user-friendly UI quickly and efficiently.\n\nOffline Support: By making Synapse a PWA, we enabled offline functionality, which is essential for a tool designed to be available anytime, anywhere.\n\nA Responsible Approach to Healthcare AI\nWhile Synapse is a powerful tool, it's not a replacement for a healthcare professional. We have embedded a clear disclaimer in the app: all analyses are AI-generated and are not a diagnosis. To reinforce this, a \"Find Nearby Hospitals\" feature is available after every analysis, encouraging users to seek professional help immediately.\n\nSynapse is a significant step forward in making AI-powered medical insights more accessible. We believe this technology can empower individuals with better health information and help them make more informed decisions about their well-being.",
        "technologies": [],
        "source_code_url": "https://github.com/samthedjpro/studio",
        "demo_video_url": "https://youtu.be/GRxsB3TPcnQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/synapse-your-ai-medical-assistant"
    },
    {
        "project_title": "Criseo - Gemma 3n Impact Challenge",
        "tagline": "Crisis Resource Finder for Humanitarian Aid Using Gemma 3n",
        "authors": "Yvonne Anang",
        "summary": "**Criseo**\nCrisis Resource Finder for Humanitarian Aid",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/yvonneanang/Criseo",
        "demo_video_url": "https://youtu.be/GG6-Ce6KZLg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/criseo-gemma-3n-impact-challenge"
    },
    {
        "project_title": "Gemma Sullivan Project",
        "tagline": "How do we guarantee continuous learning under any circumstance?",
        "authors": "David Romero Santos",
        "summary": "Gemma Sullivan Project - The Technical Writeup\n\nThe Gemma Sullivan Project was born from inspiration found in this quote by Katriona O'Sullivan (author of Poor):\n\n> We need equity in education, not equality. If someone can't see straight because the world is falling in around them, we need to raise them up to clearer skies.\n\nThis phrase perfectly encapsulates the educational challenge of our time: it's not just about providing the same resources to everyone, but recognizing that each student faces unique circumstances that can hinder their learning. Some need us to \"lift them up to clearer skies\" so they can see clearly.\n\nHow could we use Gemma 3n, capable of running on almost any consumer device, to ensure student learning in virtually any circumstance? Whether during temporary emergencies like power grid failures, in refugee camps during wartime, in isolated rural areas, or anywhere internet coverage is problematic or simply nonexistent.\n\nThe traditional response has been to wait for conditions to improve: more connectivity, more infrastructure, more resources. But what if we could invert this approach? What if education could adapt to existing conditions, however adverse they may be?\n\nThis is where Gemma 3n represents a paradigmatic shift. Its ability to run locally on standard consumer devices—smartphones, tablets, basic laptops—opens up possibilities that were previously unthinkable. We don't need cloud connectivity, we don't depend on remote servers, we don't require complex infrastructure. Knowledge and the ability to generate personalized educational experiences can reside directly in the student's hands.\n\nThe Gemma Sullivan Project aims to be a proposal that opens debate about the best pathways to achieve this radical democratization of education. It's not just a technical solution, but a reimagining of how learning can occur when we eliminate traditional barriers of connectivity and infrastructure.\n\nWe've thought about the best way to approach this challenge, not only from a technical standpoint, but also pedagogically. That's why we've described the [project's pedagogical principles](https://davidlms.github.io/gemma-sullivan-project/docs/pedagogical-foundations.en), which underpin every design decision. But in this writeup, we'll focus specifically on the technical aspects: how we've managed to create a system that works completely offline, how we've \"tamed\" the generations of a small language model to create coherent and personalized educational experiences, and how we've solved the unique challenges that arise when building a truly autonomous educational ecosystem.",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/DavidLMS/gemma-sullivan-project",
        "demo_video_url": "https://youtu.be/KI6n7mXjAOw?feature=shared",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-sullivan-project"
    },
    {
        "project_title": "MemoryVault-writeup",
        "tagline": "An app designed to function offline, prioritizing privacy while assisting those facing memory-related issues.",
        "authors": "anaum",
        "summary": "The Problem\nMemory loss is one of the fastest-growing health challenges worldwide, affecting people across all age groups. While commonly associated with older adults, younger individuals are not immune since severe memory decline can also result from brain injuries, chronic illnesses, mental health conditions, trauma, or even genetic factors.\n\nWhile a few digital tools exist to support memory management, they share common drawbacks. Most store user data on remote servers, raising serious privacy concerns. Many require constant internet connectivity, creating accessibility barriers for those in low-connectivity areas. Login systems that demand usernames, passwords, or email verification add friction for users who need simplicity, not complexity.\n\nRemembering tasks, important dates, or cherished memories should be effortless. These personal moments should remain securely stored on the user’s own device, not scattered across servers vulnerable to breaches.\n\nMemory Vault was built to address these challenges head-on. It offers a simpler, more private, and more accessible approach to supporting memory without unnecessary hurdles.\n\nCore Features\n\n**1. Memory Game Integration**  \nThe app features an interactive memory game that strengthens recall by presenting partial memories for users to complete. The game leverages the AI to create adaptive cognitive exercises that are tailored to each user's memories and cognitive patterns, making the training more relevant and effective.\n\n**2. Multimodal Notes**  \nThe comprehensive note creation system supports titles, descriptions, visual documentation through gallery selection, and audio recordings of voices or ambient sounds. Integrated playback features enable users to experience their stored memories through direct audio playback and image viewing within the interface.\n\n**3. Reminiscence Therapy**  \nThis therapeutic feature helps users reconnect with their past through AI-curated memory experiences. The system analyzes saved notes to identify recurring themes then weaves selected memories into cohesive, story-like narratives. The experience combines gentle storytelling.\n\n**4. Chat with AI**  \nThe app provides a secure, private, and conversational AI assistant powered by the on-device Gemma 3n model. Users can engage in natural, open-ended conversations, ask questions about their memories, or simply chat to combat loneliness. The interface supports both text and image inputs, allowing the AI to understand and respond to visual context. All conversation history is stored locally in the device's SQLite database.\n\n**5. Private-Offline ready**  \nThe entire system is engineered to be fully private and functional without an internet connection. All user data including notes, images, audio recordings, and chat logs is stored exclusively on the device's local storage and is never sent to or stored on any external server. The Gemma 3n model is fully packaged within the app at installation, requiring no separate downloads or internet connection.\n\n\nSystem Architecture\nMemory Vault employs a sophisticated yet streamlined architecture designed around three core principles: **privacy-first design**, **offline-first functionality**, and **cognitive accessibility**. The system operates entirely on-device without any external dependencies, ensuring users maintain complete control over their personal data.\n\n**1. Frontend Architecture (React Native + Expo):** The application uses a screen-based navigation system with six main screens:\n   - OnboardingScreen: Initial setup for new users\n   - HomeScreen: Main dashboard and navigation hub\n   - ChatScreen: AI conversation interface\n   - NotesScreen: Create, edit, and organize memories\n   - ReminiscenceScreen: Browse and search stored memories\n   - GamesScreen: Cognitive training game for memory exercise  \nThe state management layer is built around a centralized MemoryVaultContext, which maintains the global application state including user data, navigation state, and initialization details. This is supported by a set of custom hooks that modularize functionality.\n\n**2. Local Data Architecture:** Memory Vault follows a local-first storage approach, combining a structured SQLite database with the device’s file system to keep all data fully offline. On installation, the app initializes SQLite as its primary storage, creating a self-contained database that stays entirely on the user’s device and persists across sessions and restarts. Larger files, such as Gemma 3n model files, are stored in the device’s file system. This architecture ensures fast access, complete offline functionality, and strong privacy, as all data remains securely on the user’s device without relying on external servers.\n\nIntegration of Gemma 3n\nThe integration of Gemma is built on a hybrid architecture that separates high-performance native operations from the application's UI logic. This was achieved by creating a native bridge between the React Native frontend and a custom Kotlin module on Android. At the core of this integration is Google's MediaPipe framework, which runs the Gemma model directly on the user's device. The process is managed through a layered system:\n\n- **Native Layer (Kotlin):** A custom module, `GemmaBridgeModule.kt`, handles all direct interactions with the MediaPipe `LlmInference` task. It's responsible for loading the model from device storage, running inference, and managing the model's lifecycle. It also intelligently determines whether to use the CPU or GPU for inference, checking device capabilities to optimize performance and automatically falling back to CPU if a GPU is not suitable.\n\n- **JavaScript/TypeScript Layer:** On the React Native side, a `ModelManager.ts` class orchestrates the entire process. It communicates with the native module to load/unload the model and run inference. It works with `FileManager.ts`, which handles the model file itself. To ensure the app works completely offline, the Gemma model file is bundled directly within the app's assets. On first launch, `FileManager.ts` copies this model into the app's private, internal storage, making it immediately available without needing a download.\n\n- **React Hooks Layer:** The entire system is abstracted away from the UI by a simple React hook, `useGemmaModel`. This hook provides components with the model's current status (e.g., loading, ready, error) and clean functions to interact with it, making it easy to build features like the AI Chat.\n\nA critical part of the implementation is how the app leverages Gemma's native multimodal capabilities to understand both text and images. When a user submits a prompt with an image, the request flows through our system to deliver a truly contextual response.\n\nTechstack / Technologies \n**Core Framework:**\n- **React Native & Expo:** For cross-platform mobile application development.\n- **TypeScript:** For type-safe JavaScript development.\n\n**On-Device AI:**\n- **Google MediaPipe (tasks-genai:0.10.11):** For running the Gemma 3n model directly on the device.\n- **Kotlin:** For building the native bridge to communicate with the MediaPipe library on Android.\n\n**Data Storage:**\n- **Expo-SQLite:** For local, on-device database management.\n- **Expo-File-System:** For managing larger files like the AI model and audio recordings.\n\n**UI & Styling:**\n- **Nativewind:** For utility-first styling using Tailwind CSS in React Native.\n- **React Navigation:** For routing and navigation between screens.\n\n**Key Libraries:**\n- **Expo-Router:** For file-based routing.\n- **Expo-Audio & Expo-Image-Picker:** For handling multimodal inputs (audio and images).\n\n\nConclusion\nMemory Vault represents a paradigm shift toward truly private, accessible cognitive support technology. By keeping personal memories entirely on-device while leveraging advanced AI capabilities, it demonstrates that privacy and functionality need not be mutually exclusive. As memory-related challenges continue to grow globally, solutions like Memory Vault point toward a future where technology empowers users without compromising their most intimate data.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/anaumghori/MemoryVault",
        "demo_video_url": "https://youtu.be/oR5q9KcY_EM?si=vKG9StdoSGO_2qkb",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/memoryvault-writeup"
    },
    {
        "project_title": "MediGemm - AI Diagnosis",
        "tagline": "An offline app offering immediate, step-by-step first-aid and medical guidance in local languages for remote communities.",
        "authors": "Sunnidhya Roy, Sam_Bhaumik",
        "summary": "MediGemm is an offline, multimodal Android application designed to deliver preliminary medical advice in underserved and remote regions with limited access to healthcare. Built using the Google AI Edge LLM Inference API and the Gemma-3n model, the app supports text, image, and multilingual (e.g., Bengali) inputs to diagnose common ailments and suggest preventive care and home remedies—all processed entirely on-device, without needing an internet connection.\n\nThe app integrates a compressed 4-bit quantized version of Gemma-3n and the Universal Sentence Encoder for embedding-based Retrieval-Augmented Generation (RAG). Upon user input, MediGemm identifies the most relevant disease using cosine similarity over precomputed embeddings and augments the query with structured context before passing it to the model. This ensures domain-restricted, contextually accurate medical responses.\n\nKey Features:\n\n1. Offline Capability: Entire model and database stored and loaded from device storage; first-launch decompression handled dynamically.\n\n2. Multimodal Input: Accepts and processes both text and images (e.g., for visible conditions like skin rashes).\n\n3. Multilingual Support: Supports native languages like Bengali, with bilingual disease datasets.\n\n4. Domain-Restricted Prompting: Hard-coded prompts ensure the model only responds to medical queries.\n\n5. Optimized Inference: Performance tuning (topK=5, temperature=0.7, 256x256 images) reduces inference time from 9 to 2–3 minutes.\n\nChallenges Overcome:\n\n1. Efficient handling of a ~3GB model through on-device ZIP extraction.\n\n2. Inference speedup via parameter tuning and resource-aware design (e.g., CPU backend).\n\n3. Managing memory and storage constraints on consumer devices.\n\n4. Embedding generation and caching for RAG accuracy and efficiency.\n\nFuture Scope Includes:\n\n1.Fine-tuning the model on domain-specific medical data.\n\n2. Adding rare diseases and more languages.\n\n3. Exploring LoRA and 2-bit quantization for faster and smaller deployments.\n\n4. Integrating user feedback for continual improvement.\n\nMediGemm demonstrates how cutting-edge on-device AI can deliver critical healthcare support in connectivity-challenged environments.",
        "technologies": [],
        "source_code_url": "https://github.com/Sunnidhya/Gemma-3n-Impact-Challenge",
        "demo_video_url": "https://www.youtube.com/watch?v=OHfFLOsTVAw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/medigemm-ai-diagnosis"
    },
    {
        "project_title": "Fine-Tuning Gemma-3n on Fashion Product Images for Visual Description Tasks",
        "tagline": "Teaching Gemma-3n model Describe Fashion Products via Efficient Fine-Tuning using Unsloth and LoRA",
        "authors": "ola abu saalek",
        "summary": "This project demonstrates how to fine-tune the Gemma-3n E2B model using the Unsloth framework with LoRA adapters. The tuning focuses on both vision and language components, enabling efficient training by adjusting only a fraction of the model parameters. The notebook sets up the environment, loads the model, and configures selective fine-tuning on attention, MLP, and embedding modules for multimodal tasks.",
        "technologies": [],
        "source_code_url": "https://github.com/Ola-Abu-Saalek/kaggle-hackathon.git",
        "demo_video_url": "https://youtu.be/oSDXpNbhovA?si=gTJfwWBzIqqluDEv",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/fine-tuning-gemma-3n-on-fashion-product-images-for"
    },
    {
        "project_title": "OncoScope: Democratizing Privacy-First Cancer Genomics Analysis",
        "tagline": "Leveraging Gemma 3n with Unsloth Fine-Tuning and Ollama for Real-Time, On-Device Genomic Insights",
        "authors": "Sheldon Aristide",
        "summary": "OncoScope: Privacy-First Cancer Genomics Analysis with Gemma 3n",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/aristide021/oncoscope",
        "demo_video_url": "https://www.youtube.com/watch?v=bXMfIblP26s",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/oncoscope-democratizing-privacy-first-cancer-genom"
    },
    {
        "project_title": "Gemma Rover",
        "tagline": "A LeKiwi robot that uses Gemma 3n to make real-time decisions in a Mars-like environment",
        "authors": "Vlad Fatu, Lavinia Petrache",
        "summary": "Project Overview\nGemma Rover uses a Gemma 3n model to control a LeKiwi robot in a Mars-like environment. The project showcases Gemma 3n’s ability to make real-time decisions locally in a robotics context. This is crucial in scenarios where cloud services are not available and human support is 30 minutes away. This setup highlights the potential of running large language models on-device for autonomous, context-aware behavior in remote or constrained environments.\n\nFor the demo, we prepared a scenario that takes us through a day in the life of Gemma Rover. The rover has a long running task of collecting dirt samples. At some point, a dust storm starts and the rover needs to stop the current task to seek shelter. After the storm passes, it needs to get back to the long running task, but since the solar panel is covered with dust, it needs to clean it first. After that, it can continue with the long running task of collecting dirt samples.\n\nHardware Setup and Communication\nThe project uses a LeKiwi robot, a Macbook with an M2 chip and a travel router to connect the two devices. The Macbook is used to run the Gemma 3n E4B model and the [ACT](https://tonyzhaozh.github.io/aloha/) models used for arm manipulation. Communication between the Macbook and the robot is done over a local network using ZeroMQ. \n\nIn the future, we could use a Jetson AGX Orin(or maybe even a Jetson Orin Nano) to remove the need for a Macbook, but we didn't have one available at the time of the project.\n\nControl Loop\nWe were planning on using an agentic framework like ADK or smolagents for making decisions but we couldn't make them fit our use case and decided to implement our own setup that calls the LLM directly. Since this is a rover on Mars, it should think continuously, not just try to do a task and wait for further instructions. It should also be able to stop doing the task if there are threats to it's safety or it's longterm operational integrity.\nThe way we think about it is that the rover should get the information it needs (about the environment, about itself and it's homebase, about it's long running task) every few seconds and decide what to do next.\n\nSo we implemented what we call an Agentic Control Loop. On the main thread, we have a loop that prompts the LLM every few seconds with the current state of the robot and the environment. The LLM then decides what action to do next and spawns a thread to execute that action. In most cases, the action will take longer that it takes to go through the loop, so if the LLM decides to do the same action, nothing will happen. However, if it decides to do a different action, it will cancel the previous action and spawn a new thread to execute that action. This way, the LLM can make decisions in close to real-time. \n\nIn our current setup, Gemma 3n:4eb is used on a Macbook(M2) and the loop runs at about 30 instructions per minute. For our tests, we capped this at 3 instructions per minute(run the loop every 20 seconds) to avoid having too many logs, but depending on the task, it can be increased.\n\nHere is a log from a simulation run using \"start_mars_simulation.py\": https://gist.github.com/vladfatu/232492b4325303631e0f3a55dec81442\nThe scenario is the one from the demo video.\n\n##",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/vladfatu/gemma-rover",
        "demo_video_url": "https://youtu.be/CN7uW8ERaWE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-rover"
    },
    {
        "project_title": "Resona, an offline AI-powered assistant for emotional communication.",
        "tagline": "Using on-device LLMs to help people better understand each other’s emotional needs.",
        "authors": "Ethan Gao",
        "summary": "Resona: An Offline AI-Powered Assistant for Emotional Communication\n\nThis project was conceived and developed within the final week leading up to the competition deadline. Despite the limited timeframe, Resona introduces a significant breakthrough in the field of emotional communication: the integration of a locally-deployed large language model (LLM) using MediaPipe + Gemma 3B.\n\nUnlike conventional emotion or dialogue apps that rely heavily on cloud-based APIs, Resona runs entirely offline, allowing for real-time, private, and secure conversation analysis — without compromising on intelligence. Powered by MediaPipe Tasks and optimized to run efficiently on-device, this prototype demonstrates how future emotional assistants can operate independently of the internet.\n\nWhile the current version is incomplete — missing planned multimodal inputs (voice, image, etc.) and deeper conversational modeling — the groundwork laid here opens the door for more nuanced and human-aligned AI systems. The app parses natural dialogue, identifies emotional needs, stances, and self-worth signals, and organizes conversations into meaningful categories such as Critique, Plea, Reflect, and Perform.\n\nI believe this design represents just 20% of the potential. With further development, Resona could evolve into a full-featured offline companion capable of resolving emotional misunderstandings, mediating conflict, and revealing the implicit dynamics behind human communication.\n\nThis is not just a tool — it’s a vision for rethinking how we process, understand, and communicate emotional experience.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/zGzv5dT2tJ4?si=75D_cbMgoz8hHj9n",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/resona-an-offline-ai-powered-assistant-for-emotion"
    },
    {
        "project_title": "Prathamops - A first aid assistant",
        "tagline": "Multi-lingual Gemma3n models for on-device first aid assistance",
        "authors": "Vivek S",
        "summary": "**Problem statement**\nIn most parts of the world, especially in homes and remote communities, critical first aid isn’t administered until help arrives—by which time survival odds have dropped dramatically. Annually, **only about 10%** of out‑of‑hospital cardiac arrests result in survival—yet over half of victims receive no immediate CPR from bystanders, despite **over 70% of arrests occurring at home** [https://pmc.ncbi.nlm.nih.gov/articles/PMC8174211]. \n\nGlobally, bystander CPR rates range **from 1% in some regions up to 44%**, with **only roughly 45%** receiving it overall [https://en.wikipedia.org/wiki/Cardiopulmonary_resuscitation].\n\nTraining and confidence are critical barriers. Less than one in five people have received certified first aid training, and **only around 33–35%**feel prepared to act in emergencies [https://wifitalents.com/first-aid-statistics , https://gitnux.org/first-aid-statistics). In rural or underserved areas—where EMS can take an hour or more to arrive—this training gap means delays prove fatal. A study in Southern India found nearly half of first responders took over an hour to reach a hospital during their last emergency, and **fewer than 45%** reported any formal training [https://pmc.ncbi.nlm.nih.gov/articles/PMC3876499].\n\nThat’s where an **AI first‑aid app** on your local device could be a game‑changer—especially if it works **entirely offline**. A private AI-based guidance system on your device can give clear instructions immediately, **bridging the gap between witnessing a crisis and skilled support**—when every second counts.\n\nThe name PrathamOps is derived from the word Prathamopachar (प्रथमोपचार) which means first aid in Sanskrit \n\n**Technical approach**\n\n##1. Corpus collection\nIn order to generate a good quality dataset for fine tuning the model, I collated corpus from various sources as the knowledge base from which the model will learn. It includes official guidelines, medical texts, and practical first-aid procedures. The corpus is structured to be easily accessible for data generation, potentially categorized by the type of injury or emergency.\n\n##2. Data cleaning and processing\nBefore I started generating the synthetic data, I wanted to categorize the data corpus into various categories of injuries and age to **maintain class balance** and **equal representation**. Hence I categorized them into following categories and age groups\n\n###Types of Injuries\n- Assessing patient situation [General care]\n- Unresponsive non-breathing casualty [CPR,AED]\n- Response problem [Seizure, Stroke, Head injuries,Diabetes]\n- Breathing and airway problems [Asthma, Allergy, Choking]\n- Circulation problems [Bleeding, Faining,Chest pain]\n- Muscle, Joint and Bone injuries [Fractures, Dislocation, Strain, Sprain]\n- Burns\n- Poison\n- Meningitis, Sepsis, Bites, Stings\n\n\n###Age categories\n- Adults > 18 years\n- Teens and children  5 years to 18 years\n- Babies < 5 years\n\n##3. Synthetic data generation\nUsing the prepared corpus, **Gemini 2.5 Flash** is employed to generate synthetic data. This model is chosen for its **speed** and **advanced reasoning** capabilities. The generation process is **guided by the injury type**. For each injury category (e.g., burns, cuts, fractures), the model is prompted to create **detailed first-aid instructions**, questions, and conversational turns in many **different languages**. This approach ensures a vast and diverse dataset without the need for manual translation and content creation.\n\n\n| Category | Topics | Categorical Split | Adults (Home, workplace, outside) | Teens and Children (Home, school, outside) | Babies (Home, outside) |\n| :--- | :--- | :--- | :---- | :--- | :--- |\n| Assessing patient situation | Pre-requisites | 10% | 5% | 3% | 2% |\n| Unresponsive non-breathing casualty | CPR, AED | 9% | 4.5% | 2.7% | 1.8% |\n| Response problem | Seizure, Stroke, Head injuries, Diabetes | 9% | 6.3% | 1.8% | 0.9% |\n| Breathing and airway problems | Asthma, Allergy, Choking | 9% | 4.5% | 2.7% | 1.8% |\n| Circulation problems | Bleeding, Fainting, Chest pain | 9% | 4.5% | 3.6% | 0.9% |\n| Muscle, Joint and Bone injuries | Fractures, Dislocation, Strain, Sprain | 9% | 6.3% | 2.7% | 0% |\n| Burns, Poison |  | 9% | 4.5% | 2.7% | 1.8% |\n|Meningitis, Sepsis, Bites, Stings |  | 9% | 4.5% | 2.7% | 1.8% |\n|Generic | All | 27% | 12.42% | 9% | 5.58%|\n|Total | |100% | 52.52% | 30.9% | 16.48% |\n\n\nThe data split of categories are equal, however I took weighted splits for age categories considering the amount of risk (statistically) associated with the age group\n\nThe dataset was also generated in various languages like **English, Hindi, France** as well as low-resource languages like **Marathi** and **Kannada**.\n\nThe data was also updated with the emergency contacts and other details associated to the region in which the people speak the particular language the most\nFor examples: \nThe emergency contact for India is 102/108, and since Hindi, Marathi and Kannada are native to India, the emergency contact is set to 102/108 and 999/112 in the case of English and French\n\n\n##4. Finetuning using Unsloth\nThe synthetic, multilingual dataset is then used to fine-tune a Gemma 3n model. Unsloth is the chosen framework for this process due to its efficiency in fine-tuning LLMs, which allows for faster training and reduced hardware requirements. \nThe fine-tuning process adapts the general-purpose Gemma 3n model to the specific domain of first aid, making it an expert in providing concise and accurate emergency guidance.\nI trained the Gemma 3n models with 1k samples for each language, for 3 epochs on T4 GPUs\n**Base model**: `unsloth/gemma-3n-E2B-it, unsloth/gemma-3n-E4B-it`\n**Lora config**: `r=8, lora_alpha = 8, lora_dropout = 0`\n**SFT config**: `epochs = 3, per_device_train_batch_size = 1, gradient_accumulation_steps = 4, learning_rate = 2e-4, optim = \"adamw_8bit\", weight_decay = 0.01, lr_scheduler_type = \"linear\"`\n\n**Model checkpoints and dataset** : https://huggingface.co/collections/viveksil/prathamops-68926bbee736c07ba081993d\n\n\n##5. Deploy with Ollama\nThe final, fine-tuned Gemma 3n models are deployed locally using Ollama. This platform enables the models to run directly on the user's device, ensuring that the application remains functional even without an internet connection. This is a critical requirement for a first-aid assistant, as emergencies can occur in areas with poor or no connectivity. Ollama's local serving also guarantees user privacy, as no personal information or query data is sent to a remote server.\n\nGGUF models: https://ollama.com/viveksil/prathamops-E2B\nLocal **React + FastAPI + Ollama** app : https://github.com/VivekSil/prathamops/tree/main/app/prathamops-react-app\nLocal **Gradio + Ollama** app : https://github.com/VivekSil/prathamops/blob/main/app/prathamops-gradio-ollama.py\n\n##6. Use with Google's AI at Edge Gallery app\nYou can use the finetuned gemma3 models using the Gallery app\nModel (.task) file: https://huggingface.co/viveksil/prathamops-en-1B/blob/main/prathamops-1b-q4.task\n\n##7. Experiments and Challenges\nSince the .tflite/.task support for custom gemma3n models is not available yet and gemma3-1B was found to be running faster than gemma3n on low-end devices hence I decided to develop the models using gemma3-1B as a base for on-device Gallery app. All the finetuned models are based on gemma3n-E2B except this one.",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/VivekSil/pratham-ops",
        "demo_video_url": "https://youtu.be/jMcONEQHNVE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/prathamops-a-first-aid-assistant"
    },
    {
        "project_title": "OPEN TUTOR IA",
        "tagline": "Fine-tuned Gemma 3n  virtual tutor offers personalized feedback to boost learning & critical thinking in marginalized areas.",
        "authors": "Emiliano Dorantes0",
        "summary": "🚀 Project Name: Open Tutor AI\n✨ Project Description\nThis project is an artificial intelligence app that seeks to revolutionize educational feedback in basic education. Using the power of Google's Gemma 3n multimodal model, the model offers students a personalized tutor that works directly on their mobile devices, currently using **Google AI Edge Gallery**. 📱\n\nOur solution aligns with and deepens the principles of expected learning outcomes for basic education, providing a tool that promotes autonomous learning and addresses the educational gap in low-resource areas. 🏫\n\n🧐 The Problem\nIn schools in marginalized and low-resource areas, teachers often lack the time and resources to offer personalized and timely feedback to each of their students. This lack of individualized support can hinder learning and understanding, which exacerbates educational inequality. 💔\n\n💡 Our Solution\nWe have developed a mobile application with the following features:\n\nMultimodal Feedback: Students can submit their work via text ✍️ or images 📸 (photos of their notebooks, drawings, diagrams) and the AI provides them with constructive feedback.\n\nOffline Operation: The app is designed to work without an internet connection, making it ideal for areas with limited connectivity. 📶\n\nLearning Aligned with NEM and LTG: The AI model has been trained on the pedagogical principles of NEM and the content of the LTG, ensuring that the feedback is relevant and accurate. 📚\n\nPersonalized Tutor in Your Pocket: The AI not only corrects, but also guides the student to understand their mistakes and to improve, acting as an accessible personal tutor at all times. 🤖\n\n⚙️ How It Works\nOur project is built on a robust architecture that combines the best of modern artificial intelligence:\n\nBase Model: We use Gemma 3n, a multimodal AI model from Google, optimized to run on mobile devices.\n\nEfficient Fine-tuning: Through the Unsloth framework, we have managed to fine-tune Gemma 3n quickly and with minimal memory usage. ⚡\n\nTraining Dataset:\n\nContent: The model was trained with a data corpus that includes the official documents of the Nueva Escuela Mexicana and the complete content of the Libros de Texto Gratuitos.\n\nSynthetic Data Generation: To overcome the scarcity of real assignments and ensure privacy, we have used Meta Llama's Synthetic Data Kit (SDK) to generate thousands of synthetic examples of student work and their ideal feedback. 🤖\n\nUser Flow:\n\nThe student uploads an image or writes their question.\n\nThe on-device model analyzes the input.\n\nIt generates a text response, which is then converted to speech using the phone's native Text-to-Speech (TTS) engine. 🗣️\n\nThe app can also analyze the chat history to identify knowledge gaps and generate a personalized study plan. 🧠\n\n**Usage instructions:**\n\nDownload the fine-tuned model to the mobile device\n\nDownload the Google AI Edge Gallery APK\n\nLoad the fine-tuned model\n\nLoad the system prompts as the first instruction in the chat section\n\nAsk the questions\n\n🛠️ Technologies Used\nAI Model: Gemma 3n (Google)\n\nOptimization and Fine-tuning: Unsloth\n\nSynthetic Data Generation: Synthetic Data Kit (Meta Llama)\n\nOn-Device Inference Frameworks: TensorFlow Lite / ONNX Runtime\n\nDevelopment Platforms: Android / iOS\n\nUI/UX Prototype: Figma\n\n🎥 Demo and Presentation\nWe invite you to watch the video of our demo to see the project in action. Don't miss it! 😉\n\n[Link to the demo video on YouTube/Vimeo]\n\n🚀 Future and Scalability\nThis project is a proof of concept with enormous growth potential to address even greater educational challenges. Our future ideas include:\n\nUI Development: An APK will be created based on Google AI Edge Gallery to provide a more user-friendly interface.\n\nIntegration into Educational Platforms: Expand the application beyond the student's device, integrating it into learning management systems (LMS) and platforms used by schools. 🌐\n\nTool for the Teacher: Develop a control panel for teachers that allows them to visualize common knowledge gaps, analyze progress, and get suggestions for activities based on the NEM. 👩‍🏫\n\nInterdisciplinary Content: Expand the fine-tuning to cover all the formative fields of the NEM, promoting interdisciplinary thinking and the connection between different subjects. 🧩\n\nCommunity and Collaboration: Create a platform for teachers to share their own examples of feedback and materials, enriching the training corpus and adapting the AI to various pedagogical methodologies. 🤝\n\nOur ultimate goal is to offer a tool that serves as a true driver of educational equity, adapting to the needs of each student and empowering teachers nationwide. 💪\n\n🤝 Contributions\nThis project was developed by Emiliano Dorantes for the Google Gemma 3n Hackathon on Kaggle. Thank you for the support!",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Elmaldelego/IA-TUTOR.git",
        "demo_video_url": "https://youtu.be/BWyVs-zYU1g?si=V8_5J0LzGFraDXmU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/open-tutor-ia"
    },
    {
        "project_title": "Bidya AI: Education for the Unreachable",
        "tagline": "Bridging Nepal's Educational Gaps with Offline-Capable, Edge-Optimized AI",
        "authors": "Jeshan Pokharel, Nischal642",
        "summary": "**Bidya AI\n**Education for Unreachable****\nTeam Name: Bindu\nJeshan Pokharel (<jeshanpokharel123@gmail.com>)\nNischal Acharya ([nischalacharya642@gmail.com](mailto:nischalacharya642@gmail.com))\n\n**Abstract**\nIn response to Nepal's profound educational challenges, this document introduces Bidya AI: Education for the Unreachable. Nepal's primary education system is severely hampered by a widespread shortage of teachers, with Karnali province alone facing a critical shortfall of 644 primary teacher positions. Poor and inconsistent internet quality, along with affordability issues in rural areas, further limit digital learning opportunities. Students in remote regions face arduous daily commutes, including trekking for hours across dangerous terrain and crossing perilous rivers on unsafe \"tuins,\" which has been linked to tragic fatalities. Additionally, frequent teacher strikes disrupt the stability of primary education, undermining consistent instruction and affecting millions of children. Bidya AI addresses these problems with an easy-to-use, offline-capable AI assistant for primary school students in classes 1 through 10. The application provides personalized, supplementary content directly to mobile devices without requiring continuous internet access, leveraging the increasing smartphone ownership in remote communities. This on-hand tool ensures continuous learning despite teacher shortages, geographical barriers, and school disruptions. The project is built on the Flutter framework for its cross-platform efficiency and utilizes the edge-optimized Gemma 3n E2B model. The model was finetuned on open-source educational content from the Government of Nepal for subjects like Mathematics, Science, and Nepali. Testing revealed that while model initialization was faster on the CPU, inference time was quicker on the GPU. Despite challenges such as dependency conflicts, data preparation difficulties, and the lack of direct support for converting the model to an edge-ready format, the results prove the solution's effective performance. Bidya AI is a valuable educational aid for Nepal that can be adapted for students in other remote areas worldwide.\n\n**for complete technical document please view Bidya AI.docx file attached with this writeup**\n\n**UseFull Links**\nGitHub Code Link: <https://github.com/jeshan-coder/bidyaai.git>\nPython script Link : Access _finetuning and data preparation files_ folder inside this repo.\nMetrics and evaluation link: <https://drive.google.com/drive/folders/1cUmwAuLYL7hyxcpvnr1-DW1nR3h8HLCF?usp=sharing>\nData used for training: <https://drive.google.com/drive/folders/1xh6bt8Rc2GSbRJu7sYCZ5ZRfNi8NBk4D?usp=sharing>\nAPK file Link:\n<https://drive.google.com/file/d/1rIDyCr9DXyt__-CzF05L5qmpEArc4obD/view?usp=sharing>\nModel CheckPoint:\n<https://drive.google.com/drive/folders/1cLxbogLR3HP0zeyxrFdgwCwmhj8-HJsK?usp=sharing>\nVideo Link:\n<https://www.youtube.com/watch?v=DY-LW7mDlpQ>",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=DY-LW7mDlpQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/bidya-ai-education-for-the-unreachable"
    },
    {
        "project_title": "Cherokee to English Translation using Gemma3n",
        "tagline": "Fine-tuning the Gemma-3N model for efficient and accurate Cherokee-to-English translation.",
        "authors": "Sidak Deep, huummm, qwerty12345",
        "summary": "This project demonstrates a comprehensive workflow for creating a specialized machine translation model for a low-resource language pair: Cherokee and English. The core objective is to enhance the translation capabilities of a large language model by fine-tuning it on a domain-specific dataset.\n\nWe selected Google's powerful multimodal model, gemma-3n-E2B-it, as our foundation. Using the humm/chren-dataset from Kaggle, which contains parallel Cherokee and English text, we employed a state-of-the-art technique called Parameter-Efficient Fine-Tuning (PEFT) with Low-Rank Adaptation (LoRA). This approach allows us to adapt the massive base model to our specific task with significantly lower computational and memory costs compared to full fine-tuning.\n\nThe entire process was accelerated using the Unsloth library, which provides major speedups and memory optimizations. Key steps included preprocessing the data with explicit translation instructions, configuring the model with LoRA adapters, and running a supervised fine-tuning job. The result is a lightweight, fine-tuned model that is more accurate and reliable for Cherokee-to-English translation than the original general-purpose model, serving as a practical blueprint for developing AI tools for other under-resourced languages.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/SGT31gEIcuw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/cherokee-to-english-translation-using-gemma3n"
    },
    {
        "project_title": "Hands‑Free Mouse & Keyboard with Gemma 3N",
        "tagline": "Enable fully hands-free control of any desktop computer—moving the mouse, clicking, and typing—using only eye-gaze and voice commands.",
        "authors": "Xin",
        "summary": "Hands‑Free Mouse & Keyboard with **Gemma 3N**\n\n> **Goal:** Enable fully hands‑free control of any desktop computer—moving the mouse, clicking, and typing—using only eye‑gaze, facial gestures, and voice commands, all powered by the *multimodal* Gemma 3N.\n\n---",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/vie5yTevURs",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/handsfree-mouse-and-keyboard-with-gemma-3n"
    },
    {
        "project_title": "FreshScan.AI - Empowering Health Through On-Device Gemma 3n AI",
        "tagline": "An Android app that uses multimodal AI to analyze raw fish, fruit, and vegetable photos, giving users a quality score for healthier choices.",
        "authors": "AnupKuDas",
        "summary": "FreshScan.AI – A Multimodal, Offline Edible Quality Coach Powered by Gemma 3n\n\nEmpowering Healthier Food Choices Through On-Device AI\n\nWe present a wellness-focused Android application that leverages Google’s Gemma 3n model to assess the visual quality of edibles (fish, fruits, and vegetables). The app provides a score (0–5) and a brief analysis of freshness, operating fully offline and respecting user privacy. Multilingual support (English + Hindi) ensures accessibility across India. By empowering users with real-time, AI-powered food quality scoring, we promote better health, reduce waste, and enhance food confidence.\n\nFreshScan.AI is an offline, privacy-preserving wellness application designed to help users assess the visual quality of edible items in real-time. By analyzing images of fish, vegetables, and fruits directly on-device, it generates a discrete quality score (0–5) along with a concise, localized natural language explanation. This empowers users to make informed food decisions — even in resource-constrained or internet-deprived environments.\n\n---",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/stm32fanclub/freshscan.git",
        "demo_video_url": "https://youtu.be/lnQqh6ydsmI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/freshscan-ai-empowering-health-through-on-device-m"
    },
    {
        "project_title": "Lumina - Where the World Sees You",
        "tagline": "Real-time environmental awareness for the visually impaired — fast, private, and offline",
        "authors": "Ao Shen",
        "summary": "Lumina: AI Assistant for the Visually Impaired [iOS/Android]",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/axxshen/lumina-demo",
        "demo_video_url": "https://www.youtube.com/watch?v=G3-jX3C1phU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/lumina-where-the-world-sees-you"
    },
    {
        "project_title": "EchoSense Visual Assistance with GEMMA 3n",
        "tagline": "EchoSense Visual Assistance, an AI-powered visual assistance app designed to help visually impaired users navigate their environment. ",
        "authors": "stacylmartin1",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/stacylmartin1/EchoSense/tree/EchoSense",
        "demo_video_url": "https://www.youtube.com/shorts/RCWVKTGeBx4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/echosense-visual-assistance-with-gemma-3n"
    },
    {
        "project_title": "Yamma: Offline AI Assistant for Hikers Gemma3n",
        "tagline": "Yamma empowers offline hikers to identify threats, manage injuries, learn about wildlife, track wellness, and share trail logs.",
        "authors": "https://www.linkedin.com/in/terence-siu/",
        "summary": "Yamma: Offline AI Assistant for Hikers\n\n\n---\n\n**Platform**: iOS & Android  \n**Architecture**: Flutter + Gemma 3N + Clean Architecture\n\n---",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/ttterence927/Yamma",
        "demo_video_url": "https://www.youtube.com/watch?v=YgAmh8hGmSc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/yamma-offline-ai-assistant-for-hikers-too-similar"
    },
    {
        "project_title": "Botanist App",
        "tagline": "A application designed for users to diagnose plant health issues, allows users to upload a plant image and provide a diagnosis & treatment",
        "authors": "Jemica International",
        "summary": "Purpose: A virtual botanist app that identifies plant diseases or confirms plant health using image analysis and user descriptions.Core Features:Image Upload: Upload plant images (PNG, JPG, WEBP) via drag-and-drop or file selection.\nVoice Input: Transcribe symptom descriptions using Web Speech API.\nAI Analysis: Process images and descriptions via an external plant health API (e.g., Gemini) for diagnosis.\nResult Display: Show diagnosis with disease name, confidence level (High/Medium/Low, color-coded), description, treatment plan, and prevention tips.\nInterface: Responsive UI with TailwindCSS styling, loading states, error handling, “Back” button, and “Analyze Plant” button.\n\nWorkflow:User clicks “Get Started” on Hero page.\nUploads plant image (converted to base64) and provides symptom description (typed or voice).\nApp sends data to API, displays loading spinner.\nShows diagnosis or error message.\nUser can reset to analyze another plant.\n\nUse Cases:Home gardeners diagnosing issues.\nEducational tool for plant disease learning.\nPreliminary analysis before professional consultation.\nmore concise summary",
        "technologies": [
            "Web Speech API"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/9gmu8JyIa-Y",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/botanist-app"
    },
    {
        "project_title": "Chikitsa lite",
        "tagline": "An AI-powered mental health platform leveraging Gemma 3n to deliver empathetic support, gamified engagement, and professional care for India",
        "authors": "Vinu1729",
        "summary": "Project Description\nChikitsa Lite is a groundbreaking mental health platform designed to address the escalating mental health crisis in India, where issues have doubled from 2011 to 2014, impacting 52% of the nation’s youth and contributing to an estimated ~87 trillion INR productivity loss between 2012-2030 (WHO). With 1 billion people globally affected by mental health challenges, Chikitsa Lite harnesses the power of Google’s Gemma 3n model to provide accessible, scalable, and empathetic mental health support. Built as a Flask-based web application, it integrates AI-driven cognitive behavioral therapy (CBT) techniques, gamified engagement, emotion detection, and secular teachings from the Shreemad Bhagavad Gita to foster mindfulness and resilience.\nProblem Statement\n\nGlobal Crisis: 1 in 8 people worldwide suffer from mental health issues, with India facing a significant rise in cases among its youth.\nEconomic Impact: Mental health challenges contribute to substantial productivity losses, particularly in India, where young people are a critical economic driver.\nAccessibility Gaps: Traditional solutions like in-person psychiatry (₹1500-₹3000 per session) are costly and inaccessible, while existing apps like Headspace and Wysa lack comprehensive features such as video/audio analysis or professional integration.\n\nOur Solution: Chikitsa Lite\nChikitsa Lite offers a holistic mental health ecosystem with the following unique selling points (USPs):\n\nAI-Driven Support: Powered by Gemma 3n via the Ollama API, Chikitsa provides empathetic, human-like responses tailored to users’ emotional needs, minimizing hallucination through a carefully crafted system prompt.\nGamified Engagement: Features a cognitive game (Tap Impulse) to assess mental agility, encouraging user retention through interactive challenges.\nEmotion Detection: Utilizes Gemma 3n’s multimodal capabilities for video/audio and medical image analysis to detect emotional states and provide personalized insights.\nSecular Bhagavad Gita Teachings: Integrates lessons like mindfulness and detachment from outcomes, presented in an inclusive, non-religious manner.\nProfessional Integration: Facilitates live 1-1 video calls with doctors, with seamless appointment booking and management.\nPrivacy and Anonymity: Ensures user data security with encrypted storage and anonymized interactions.\nAffordable Pricing: Starting at ₹299/month, Chikitsa Lite is significantly more cost-effective than traditional psychiatry.\n\nKey Features\n\nUser Authentication: Secure login/registration with role-based access (user, doctor, admin) using bcrypt for password hashing.\nMood Tracker: Logs and visualizes emotional states, stored in JSON for easy analysis.\nMeditation Logger: Tracks meditation sessions to promote consistent mindfulness practices.\nQuestionnaires: Randomized close-ended and open-ended questions processed by Gemma 3n to generate wellness reports.\nReal-Time Chat: Interactive AI chat with empathetic responses, logged for continuous improvement.\nImage/Video Analysis: Uploads analyzed by Gemma 3n for emotional and environmental insights.\nAppointment System: Book and manage appointments with doctors, with email confirmations via Flask-Mail.\nGamified Cognitive Assessment: Tap Impulse game to measure cognitive control and mental fatigue.\nAdmin Dashboard: Manage users and roles securely.\n\nTechnical Implementation\n\nBackend: Flask with JSON-based storage for user data, appointments, moods, and game results. Pandas handles CSV processing for questionnaires.\nAI Integration: Gemma 3n (via Ollama API at http://localhost:11434) provides empathetic responses, wellness reports, and multimodal analysis. The system prompt ensures concise, gender-sensitive, and markdown-formatted outputs.\nFrontend: Responsive HTML templates with Bootstrap, enhanced by JavaScript for dynamic interactions (e.g., mood logging, chat streaming).\nSecurity: Secure file uploads with werkzeug.utils.secure_filename, bcrypt password hashing, and Flask session management.\nEmail Notifications: Flask-Mail with Gmail SMTP for professional appointment confirmations.\nLogging: Python’s logging module for robust error tracking and debugging.\n\nProcess Flow\n\nLogin/Signup: Users register or log in securely.\nHome Dashboard: Access mood tracker, meditation logger, and questionnaires.\nVoice Assistant: Real-time chat with Gemma 3n for emotional support.\nBook Appointment: Schedule live 1-1 video calls with doctors via Google Meet links.\nMy Appointments: Manage bookings and view statuses.\nEmotion Detection: Upload video/audio or images for AI-driven analysis.\nGamified Interaction: Play Tap Impulse to assess cognitive performance.\n\nTraction & Milestones\n\nUser Base: 50+ beta users actively engaging with the platform.\nAchievements:\nWon the Google GenAI Exchange Hackathon 2024.\nRecognized by MeitY for innovative AI-driven mental health solutions.\n\n\nTestimonial: \"Chikitsa's innovative AI tackles a billion-dollar mental health crisis with scalable, cutting-edge technology.\"\n\nOur Team\n\nVinayak Shinde : Trainee Engineer at Bixware Technologies Pvt Ltd, Ex-Intern at Engaggely.AI (AI expertise).\nVishal Mishra: Trainee Engineer at Bixware Technologies Pvt Ltd, Ex-Intern at Engaggely.AI (AI expertise).\nNikhil Bhoir: Engineering Trainee at Bixware Technologies Pvt Ltd, Ex-Intern at Engaggely.AI (AI expertise).\n\nOur Mentors\n\nDr. Tushar Ghorpade: Assistant Professor at DY Patil RAIT, providing product and research guidance.\nDr. Sneha Bagle: Assistant Professor at DY Patil SOP, providing product and research guidance.\n\nImpact\nChikitsa Lite addresses India’s mental health crisis by offering an affordable, accessible, and stigma-free platform for youth. By combining Gemma 3n’s advanced AI with gamified engagement and professional integration, it empowers users to manage their mental well-being proactively. The platform’s low cost (₹299/month) compared to traditional psychiatry (₹1500-₹3000/session) makes it accessible to a broader population, while features like emotion detection and Bhagavad Gita-inspired mindfulness promote long-term resilience. Chikitsa Lite aligns with the Gemma 3n Impact Challenge’s mission to build products for a better world by fostering mental health equity.\nChallenges and Solutions\n\nChallenge: Ensuring empathetic and accurate AI responses with minimal hallucination.\nSolution: Optimized Gemma 3n’s system prompt for empathy, inclusivity, and evidence-based CBT techniques.\n\n\nChallenge: Scalable data storage for growing user base.\nSolution: Used JSON-based storage with plans to migrate to PostgreSQL for production.\n\n\nChallenge: Seamless integration of multimodal features (video/audio analysis).\nSolution: Leveraged Gemma 3n’s multimodal capabilities with base64-encoded uploads and robust error handling.\n\n\n\nFuture Enhancements\n\nTransition to a relational database for improved scalability.\nImplement WebRTC for real-time video recording in the Tap Impulse game.\nEnhance emotion detection with advanced Gemma 3n fine-tuning for video/audio inputs.\nAdd multi-language support to reach non-English-speaking users.\nIntegrate push notifications for appointment reminders and daily mood check-ins.\n\nCall to Action\nJoin us in making mental health support accessible to everyone, everywhere. Contact us at:\n\nEmail: team.surya.prabha@gmail.com\nPhone: +91 7000107054 / +91 8879785320",
        "technologies": [
            "Flask",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/vinayak1729-web/Chikitsa_lite",
        "demo_video_url": "https://youtu.be/RFyIyQrlN84",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/chikitsa-lite"
    },
    {
        "project_title": "the Chikitsa lite",
        "tagline": "An AI-powered mental health platform leveraging Gemma 3n to deliver empathetic support, gamified engagement, and professional care for India",
        "authors": "i am hard coder, vishal369mishra, Nikhil Bhoir",
        "summary": "Chikitsa Lite is a groundbreaking mental health platform designed to address the escalating mental health crisis in India, where issues have doubled from 2011 to 2014, impacting 52% of the nation’s youth and contributing to an estimated ~87 trillion INR productivity loss between 2012-2030 (WHO). With 1 billion people globally affected by mental health challenges, Chikitsa Lite harnesses the power of Google’s Gemma 3n model to provide accessible, scalable, and empathetic mental health support. Built as a Flask-based web application, it integrates AI-driven cognitive behavioral therapy (CBT) techniques, gamified engagement, emotion detection, and secular teachings from the Shreemad Bhagavad Gita to foster mindfulness and resilience.\nProblem Statement\n\nGlobal Crisis: 1 in 8 people worldwide suffer from mental health issues, with India facing a significant rise in cases among its youth.\nEconomic Impact: Mental health challenges contribute to substantial productivity losses, particularly in India, where young people are a critical economic driver.\nAccessibility Gaps: Traditional solutions like in-person psychiatry (₹1500-₹3000 per session) are costly and inaccessible, while existing apps like Headspace and Wysa lack comprehensive features such as video/audio analysis or professional integration.\n\nOur Solution: Chikitsa Lite\nChikitsa Lite offers a holistic mental health ecosystem with the following unique selling points (USPs):\n\nAI-Driven Support: Powered by Gemma 3n via the Ollama API, Chikitsa provides empathetic, human-like responses tailored to users’ emotional needs, minimizing hallucination through a carefully crafted system prompt.\nGamified Engagement: Features a cognitive game (Tap Impulse) to assess mental agility, encouraging user retention through interactive challenges.\nEmotion Detection: Utilizes Gemma 3n’s multimodal capabilities for video/audio and medical image analysis to detect emotional states and provide personalized insights.\nSecular Bhagavad Gita Teachings: Integrates lessons like mindfulness and detachment from outcomes, presented in an inclusive, non-religious manner.\nProfessional Integration: Facilitates live 1-1 video calls with doctors, with seamless appointment booking and management.\nPrivacy and Anonymity: Ensures user data security with encrypted storage and anonymized interactions.\nAffordable Pricing: Starting at ₹299/month, Chikitsa Lite is significantly more cost-effective than traditional psychiatry.\n\nKey Features\n\nUser Authentication: Secure login/registration with role-based access (user, doctor, admin) using bcrypt for password hashing.\nMood Tracker: Logs and visualizes emotional states, stored in JSON for easy analysis.\nMeditation Logger: Tracks meditation sessions to promote consistent mindfulness practices.\nQuestionnaires: Randomized close-ended and open-ended questions processed by Gemma 3n to generate wellness reports.\nReal-Time Chat: Interactive AI chat with empathetic responses, logged for continuous improvement.\nImage/Video Analysis: Uploads analyzed by Gemma 3n for emotional and environmental insights.\nAppointment System: Book and manage appointments with doctors, with email confirmations via Flask-Mail.\nGamified Cognitive Assessment: Tap Impulse game to measure cognitive control and mental fatigue.\nAdmin Dashboard: Manage users and roles securely.\n\nTechnical Implementation\n\nBackend: Flask with JSON-based storage for user data, appointments, moods, and game results. Pandas handles CSV processing for questionnaires.\nAI Integration: Gemma 3n (via Ollama API at http://localhost:11434) provides empathetic responses, wellness reports, and multimodal analysis. The system prompt ensures concise, gender-sensitive, and markdown-formatted outputs.\nFrontend: Responsive HTML templates with Bootstrap, enhanced by JavaScript for dynamic interactions (e.g., mood logging, chat streaming).\nSecurity: Secure file uploads with werkzeug.utils.secure_filename, bcrypt password hashing, and Flask session management.\nEmail Notifications: Flask-Mail with Gmail SMTP for professional appointment confirmations.\nLogging: Python’s logging module for robust error tracking and debugging.\n\nProcess Flow\n\nLogin/Signup: Users register or log in securely.\nHome Dashboard: Access mood tracker, meditation logger, and questionnaires.\nVoice Assistant: Real-time chat with Gemma 3n for emotional support.\nBook Appointment: Schedule live 1-1 video calls with doctors via Google Meet links.\nMy Appointments: Manage bookings and view statuses.\nEmotion Detection: Upload video/audio or images for AI-driven analysis.\nGamified Interaction: Play Tap Impulse to assess cognitive performance.\n\nTraction & Milestones\n\nUser Base: 50+ beta users actively engaging with the platform.\nAchievements:\nWon the Google GenAI Exchange Hackathon 2024.\nRecognized by MeitY for innovative AI-driven mental health solutions.\n\n\nTestimonial: \"Chikitsa's innovative AI tackles a billion-dollar mental health crisis with scalable, cutting-edge technology.\"\n\nOur Team\n\nVinayak Shinde (Founder): Trainee Engineer at Bixware Technologies Pvt Ltd, Ex-Intern at Engaggely.AI (AI expertise).\nVishal Mishra (Founder): Trainee Engineer at Bixware Technologies Pvt Ltd, Ex-Intern at Engaggely.AI (AI expertise).\nNikhil Bhoir (Developer): Engineering Trainee at Bixware Technologies Pvt Ltd, Ex-Intern at Engaggely.AI (AI expertise).\n\nOur Mentors\n\nDr. Tushar Ghorpade: Assistant Professor at DY Patil RAIT, providing product and research guidance.\nDr. Sneha Bagle: Assistant Professor at DY Patil SOP, providing product and research guidance.\n\nImpact\nChikitsa Lite addresses India’s mental health crisis by offering an affordable, accessible, and stigma-free platform for youth. By combining Gemma 3n’s advanced AI with gamified engagement and professional integration, it empowers users to manage their mental well-being proactively. The platform’s low cost (₹299/month) compared to traditional psychiatry (₹1500-₹3000/session) makes it accessible to a broader population, while features like emotion detection and Bhagavad Gita-inspired mindfulness promote long-term resilience. Chikitsa Lite aligns with the Gemma 3n Impact Challenge’s mission to build products for a better world by fostering mental health equity.\nChallenges and Solutions\n\nChallenge: Ensuring empathetic and accurate AI responses with minimal hallucination.\nSolution: Optimized Gemma 3n’s system prompt for empathy, inclusivity, and evidence-based CBT techniques.\n\n\nChallenge: Scalable data storage for growing user base.\nSolution: Used JSON-based storage with plans to migrate to PostgreSQL for production.\n\n\nChallenge: Seamless integration of multimodal features (video/audio analysis).\nSolution: Leveraged Gemma 3n’s multimodal capabilities with base64-encoded uploads and robust error handling.\n\n\n\nFuture Enhancements\n\nTransition to a relational database for improved scalability.\nImplement WebRTC for real-time video recording in the Tap Impulse game.\nEnhance emotion detection with advanced Gemma 3n fine-tuning for video/audio inputs.\nAdd multi-language support to reach non-English-speaking users.\nIntegrate push notifications for appointment reminders and daily mood check-ins.\n\nCall to Action\nJoin us in making mental health support accessible to everyone, everywhere. Contact us at:\n\nEmail: team.surya.prabha@gmail.com",
        "technologies": [
            "Flask",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/RFyIyQrlN84",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/the-chikitsa-lite"
    },
    {
        "project_title": "BrailleBridge - Science Diagram and Braille Comprehension Assistant",
        "tagline": "A web+mobile app thats helps teachers export accessible lesson packs to visually impaired kids and helps them understand their homework",
        "authors": "Qasim Khan",
        "summary": "1. Introduction\nOnly 600 specialised schools in Pakistan serve the 3 million visually-impaired children in Pakistan; mainstream teachers are not Braille-literate [1].   Like many other blind or visually impaired children in Pakistan — including those with treatable conditions — 10-year-old Kaneez Fatima is not enrolled in school . “She struggles to see the blackboard, and we’re not sure why,” her mother, Bushra Bibi, explained. “The teacher said she wouldn’t be able to learn.” [2]\n\nIn Pakistan, the education of visually impaired students faces a fundamental challenge: a disconnect between the students who learn and write in Braille and the sighted teachers who cannot. This means that not only are students unable to understand lessons at school, but when they submit their homework, they often receive no meaningful correction, hindering their comprehension and isolating them in the classroom. No Urdu Braille screen-reader is available in most rural classrooms (which have no Internet and no $1000 displays).\n\n****BrailleBridge**** was engineered to solve this problem. It is a private, offline-first, multimodal learning ecosystem that leverages the power of on-device AI to create a two-device kit that bridges the communication gap. The system empowers teachers to create and grade Braille-based assignments on a laptop, and equips students with an AI-powered Android app that not only reads lessons aloud but can **see** for them. Using the phone's camera, the app can identify what a student is touching on a physical tactile diagram, providing real-time spatial context and turning a static page into an interactive learning experience.\n2. Main Components\nBrailleBridge consists of two core components that work in tandem:\n* ****The Teacher's Hub:**** A web-based application, hosted locally, that allows teachers to create assignments, view student submissions, provide tailored feedback, and see insights into student strengths and weaknesses.\n* ****The Student's Companion:**** A voice-first Android application designed for visually impaired students, providing offline access to lessons, assignments, and AI-powered assistance with understanding spatial layouts.\n\nThe entire system is designed to be offline-first, ensuring accessibility in environments with limited or no internet connectivity, a common scenario in many schools.",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/QasimKhan5d/braille-bridge",
        "demo_video_url": "https://youtu.be/88yIljxpi08",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/scibrailleurdu"
    },
    {
        "project_title": "FeelU is a mobile assistant for deaf-blind individuals",
        "tagline": "Mobile app that helps to communicate, learn and navigate the world independently using innovative haptic UI (Vibro-Braille)",
        "authors": "Timur Shagiakhmetov, Anton, Nikos Melikidis",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/nullhtp/feelu",
        "demo_video_url": "https://youtu.be/U6d1j2JkObk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/feelu-is-a-mobile-assistant-for-deaf-blind-individ"
    },
    {
        "project_title": "AI Vision  ",
        "tagline": "Empowering Navigation for All with Gemma",
        "authors": "Doha Hemdan 202200701, Ahmed MBayoumi",
        "summary": "AI Vision Assistant is a powerful Android application designed to aid visually impaired individuals by leveraging state-of-the-art machine learning models. It transforms a standard smartphone camera into an intelligent eye, providing real-time auditory feedback about the user's surroundings. The app combines on-device object detection, advanced depth estimation, and scene description powered by the Gemma 3n model to create a rich, interactive, and navigable experience.\nThe entire user interface is driven by intuitive gestures and voice commands, ensuring full accessibility without needing to see the screen.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/AhmedMBayoumi/AIVision",
        "demo_video_url": "https://youtu.be/WPqIVsaM7_s",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-vision"
    },
    {
        "project_title": "Nigerian Law AI Assistant",
        "tagline": "This project is an AI-powered assistant for Nigerian legal information, built on a Retrieval-Augmented Generation (RAG) pipeline.",
        "authors": "Franklin N",
        "summary": "This project is an AI-powered assistant for Nigerian legal information, built on a Retrieval-Augmented Generation (RAG) pipeline. This system is designed to address the challenge of accessing and understanding complex legal documents by providing users with accurate answers to their queries.",
        "technologies": [],
        "source_code_url": "https://github.com/UceeCode/nigerian-laws-ai",
        "demo_video_url": "https://youtu.be/kJGGf08BhBk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/nigerian-law-ai-assistant"
    },
    {
        "project_title": "LexiBoard: AI-Powered Language Learning and Smart Replies Keyboard",
        "tagline": "AI-Powered Language Learning and Smart Replies Keyboard",
        "authors": "claude86",
        "summary": "LexiBoard is a revolutionary Android keyboard application based on FlorisBoard that transforms everyday typing into an immersive language learning and intelligent communication experience. Built on the foundation of FlorisBoard, it integrates real-time translation, semantic context management, and relationship-aware AI to create a truly personal communication assistant.\nUse local Ollama API of Gemma 3n.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/ClaudeWang86/LexiBoard.git",
        "demo_video_url": "https://www.youtube.com/watch?v=dpwqtBGW49o",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/exiboard-ai-powered-language-learning-and-smart-re"
    },
    {
        "project_title": "Pathology Report Generation for Gigapixel-Scale WSIs using Gemma-3n (e2b)",
        "tagline": "This project aims at generating concise medical reports for pathology WSI images using Gemma-3n.",
        "authors": "Aneesh",
        "summary": "The following project aims at developing a system to produce concise medical diagnostic reports for WSI (Whole Slide Images) in pathology and healthcare. Raw data spanning 730 Gb is subjected to careful data processing for preparing a suitable dataset for all training methods and workflows covered in this project. Custom vision models involving ViTs and CNNs are made from the ground up along with fine-tuned versions of state-of-the art vision models on pathology data for improved adaptability. These models are used to fine tune Gemma-3n using new methods which train the LLM on both language modelling as well as contrastive learning at the same time. These new approaches introduced in this project outperform well known methods like CLIP or SigLIP for training vision-language models. The methods listed in the technical report attached in the write-up,  as well as in the GitHub repository cover the key aspects needed to train a pure NLP/language only LLM for vision tasks with much improved benchmarks, performance and accuracy on highly domain specific data which is not used conventionally to train the base LLMs or VLMs like Gemma-3n. This project does not use the \"video\"/vision modality of Gemma-3n. Yet the introduced methods adapt the LLM to process image inputs in an auto regressive manner similar to how language modelling is done by solely using it's NLP/language weights. \n\nI highly recommend giving the detailed report I have written out and the code repository a thorough read for a comprehensive understanding of my project \n\nThe deployed application can be accessed via a Gradio interface hosted on HuggingFace spaces:\nhttps://huggingface.co/spaces/aneeshm44/Gemma3nSolution\n\nTo access data for using this application, refer the Kaggle/HuggingFace datasets links mentioned in the repository's README.md file. \nAlong with this a small .tiff file is also present in the \"gradio_app\" directory of the repository which can also be used to perform inference using the application.",
        "technologies": [],
        "source_code_url": "https://github.com/aneeshm11/Gemma3nChallenge/",
        "demo_video_url": "https://youtu.be/Q_a0xA59USQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/pathology-report-generation-for-gigapixel-scale-ws"
    },
    {
        "project_title": "Crisis Coach | A Project by The Cautious 5",
        "tagline": "Your On-Device AI Partner for Mission-Critical Emergency Response.",
        "authors": "HVEPYC, Joshua Chew Jay Han, Ezekiel Suresh, zl.fang, Sathya Nishant",
        "summary": "Crisis Coach is an offline-first Android application designed to empower users with AI-driven emergency support, even in disaster zones with limited connectivity. Leveraging on-device Gemma3n models, the app provides instant voice translation, medical and structural image analysis, and a searchable emergency knowledge base—all without requiring internet access. Built with a focus on reliability, accessibility, and social impact, Crisis Coach integrates advanced AI technologies to deliver actionable guidance and critical information when it matters most.\n\nApp Features:\n- Offline Translation with spelling guidance\n- Offline emergency image analysis\n- Offline emergency information guides\n- All powered by the Google Gemma3n Multimodal LLM\n\nBuilt by team \"The Curious 5\", members:\n- Fang Zhili (LinkedIn: [https://www.linkedin.com/in/zhili-fang-76674125b/](https://www.linkedin.com/in/zhili-fang-76674125b/))\n- Harishankar Vinod (LinkedIn: [https://www.linkedin.com/in/harishankarvinod/](https://www.linkedin.com/in/harishankarvinod/))\n- Ezekiel Suresh Murali (LinkedIn: [https://www.linkedin.com/in/ezeksuresh/](https://www.linkedin.com/in/ezeksuresh/))\n- Joshua Chew Jay Han (LinkedIn: [https://www.linkedin.com/in/jcjh/](https://www.linkedin.com/in/jcjh/))\n- Sathya Nishant Jeyaseelan (LinkedIn: [https://www.linkedin.com/in/sathyanishant/](https://www.linkedin.com/in/sathyanishant/))\n\nLinks to Submission Material:\n1. The Video Demo: [https://www.youtube.com/watch?v=CZJQhdsJei8](https://www.youtube.com/watch?v=CZJQhdsJei8)\n2. The Technical Writeup: [https://thecurious5crisisicoach.hashnode.dev/crisis-coach-a-technical-writeup-by-team-the-cautious-5](https://thecurious5crisisicoach.hashnode.dev/crisis-coach-a-technical-writeup-by-team-the-cautious-5)\n3. Public Code Repository: [https://github.com/ziflhigan/Crisis-Coach](https://github.com/ziflhigan/Crisis-Coach)\n4. Public Project Link: [https://github.com/ziflhigan/Crisis-Coach/releases/download/v1.0.0/crisis-coach-preview.apk](https://github.com/ziflhigan/Crisis-Coach/releases/download/v1.0.0/crisis-coach-preview.apk)",
        "technologies": [],
        "source_code_url": "https://github.com/ziflhigan/Crisis-Coach",
        "demo_video_url": "https://www.youtube.com/watch?v=CZJQhdsJei8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/crisis-coach-a-project-by-the-cautious-5"
    },
    {
        "project_title": "PulseGuard: GemmaECG-Vision for Offline Cardiac Triage",
        "tagline": "An edge-optimized, vision-language AI for real-time ECG interpretation, built with Unsloth and powered by Gemma 3n.",
        "authors": "Mohamed Yasser",
        "summary": "#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/Fua0DVCUUtU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/pulseguard-gemmaecg-vision-for-offline-cardiac-tri"
    },
    {
        "project_title": "Youth-Friend - The Safe, Smart, and Supportive AI for Real-life Youth Challenges",
        "tagline": "Fine-tuned multimodal Gemma 3n to protect youth and the disabled from deepfake porn, sexual abuse, drugs, harassment, and domestic violence",
        "authors": "Eka Dharma Pranoto",
        "summary": "Youth-Safe: AI-Powered Protection for the Next Generation\n\n• Tackling Critical Youth Threats: Youth-Safe addresses urgent global issues like deepfake pornography, drug abuse, domestic violence, and harassment using a mobile-first, privacy-conscious AI built on Google Gemma 3N.\n\n• Deepfake Detection at Scale: Our AI-powered Deepfake Scanner empowers users to detect if their likeness is being exploited in face-swapped or non-consensual porn circulating online—an industry growing 464% YoY.\n\n• Geo-Aware Emergency Guidance: Combines real-time geolocation with Google Maps to guide victims to the nearest safe space—shelters, police, hospitals, or crisis centers—within seconds, potentially saving lives.\n\n• Accessibility for All: Designed for the disabled, mute, elderly, and neurodiverse, with features like one-tap “Danger Mode,” AI comic/video instructions, TTS, STT, color-blind UI, and offline-first support.\n\n• AI with Ethics & Guardrails: Implements face/liveness authentication, multi-factor security, negative keyword filtering, and respectful tone generation—no judgment, no triggering responses.\n\n• Multi-Modal, Multi-Platform Intelligence: Leverages Gemma 3N to process text, voice, video, and image inputs for real-time chat, help guidance, and detection—all optimized for mobile deployment.\n\n• Government & Privacy Compliance: Built with GDPR, AIA, DPIA, and Google Responsible AI in mind; uses encrypted data, RBAC, and secure infrastructure for government and NGO readiness.\n\n• AI Architecture with Purpose: Features fine-tuned Gemma on a multi-tone corpus, deepfake detection, shelter routing, and secure app architecture—all integrated with stakeholder feedback and youth needs.\n\n• Eco-Friendly AI Deployment: Minimizes carbon impact through quantization, LoRA (NF-4), and model pruning, enabling responsible AI deployment even in resource-constrained environments.\n\n• Global Vision, Scalable Flow: Designed with a scalable MVP → human-in-the-loop validation → compliance & deployment → post-launch drift monitoring—ready for global rollout, real-world impact, and long-term trust.\n\nThe above is a summary of my project for the Google Gemma 3N Hackathon.\n\nTo view the full write-up with system details, architecture, JSON corpus design, idea, and conclusion, please refer to the PDF file I have uploaded.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/rRFu1B6Dn7I",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/eka-dharma-pranoto-youth-friend-ai"
    },
    {
        "project_title": "EduBuddyAi: An Offline AI Tutor for Textbook-Based Learning",
        "tagline": "Instantly explains handwritten or textbook questions using on-device multimodal AI, without needing internet access.",
        "authors": "Priyanshu Jha",
        "summary": "EduBuddyAi is an offline-first AI tutor powered by Gemma 3n. It is designed to help students get instant answers to textbook or handwritten questions without needing an internet connection. By leveraging on-device inference using Ollama and the Gemma3n model, students can either type questions or upload an image, and the app extracts the question using OCR and generates an answer using the local LLM.\n\nThe app features a simple React + Next.js frontend, Tesseract.js for OCR, and a lightweight backend API that communicates with Ollama running locally. It offers complete privacy, zero latency, and accessibility for rural, disconnected, or low-resource learning environments.\n\nKey benefits:\n- Works entirely offline and privately on personal devices\n- Supports both typed and handwritten textbook question input\n- Built for real-world impact in education with no internet dependency\n- Easy to use, clean UI, deployable via local server\n\nThis project reimagines personalized tutoring by making powerful AI accessible to every learner, anywhere even without the cloud.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/pjha8040/EduBuddyAi",
        "demo_video_url": "https://youtu.be/7961CB5R8uo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/edubuddy-an-offline-ai-tutor-for-textbook-based-le"
    },
    {
        "project_title": "Aurenia: Blazing Fast Offline and Multilingual Study Assistant",
        "tagline": "Aurenia is a blazing fast native desktop study assistant supporting top 25 global languages natively with features like OCR and RAG.",
        "authors": "Abhay Singh",
        "summary": "Aurenia is a native desktop study assistant designed to make any document interactive and understandable to almost anyone, breaking down the barriers of language and complexity.",
        "technologies": [
            "CUDA",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/inventwithdean/aurenia",
        "demo_video_url": "https://youtu.be/n_-dwJi9wO8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/aurenia"
    },
    {
        "project_title": "Lifeline: Edge-Powered Blood Testing Kit using Gemma 3n",
        "tagline": "An affordable, Raspberry Pi–powered blood analyzer integrating fine-tuned Gemma 3n for real-time medical result interpretation.",
        "authors": "Krithik Alluri",
        "summary": "Lifeline: Edge-Powered Blood Testing Kit with Multimodal AI\n\n---\n\n**Project Title:** Lifeline: Edge-Powered Blood Testing Kit with Multimodal AI Interpretation\n\n**Abstract:**\nLifeline is a home-fabricated, under-$100 blood testing kit that uses colorimetric and electrochemical assays for key biomarkers, paired with a fine-tuned Gemma 3n multimodal model running fully on-device. The platform interprets results with user-provided lifestyle context, presenting easy-to-understand, medically-informed explanations. Lifeline aims to democratize diagnostics through affordable, accessible, and privacy-focused AI health tools.\n\n\n---",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/KrithikAlluri/Lifeline",
        "demo_video_url": "https://youtu.be/ghTzjxn8Tt0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/lifeline-edge-powered-blood-testing-kit-using-gemm"
    },
    {
        "project_title": "AI-based Solution for Rebuttal Management",
        "tagline": "MarkFlow is a collaborative markdown editor with AI (Gemma3n) for real-time editing, voice-to-text, and PDF analysis.",
        "authors": "OsamaNadeem030, Iftikher Azam, Shahriar kamal, S M Samiul Islam (Samin), Shafin Rahman",
        "summary": "MarkFlow - Real-Time Collaborative Markdown Editor with Built-in AI\n\n**MarkFlow** is a modern, real-time collaborative markdown editor designed for seamless team writing, research, and documentation. What sets it apart is its **deep integration of AI capabilities powered by the blazing-fast Gemma3n model**, enabling smart writing assistance, voice input, PDF analysis, and semantic understanding — all directly within the editor.",
        "technologies": [
            "FastAPI",
            "Ollama"
        ],
        "source_code_url": "https://github.com/samiulislam07/MarkFlow-markdown_editor/tree/comp",
        "demo_video_url": "https://www.youtube.com/watch?v=ZwIchsSFEnQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-based-solution-for-rebuttal-management"
    },
    {
        "project_title": "AgriLens AI - Revolutionizing Plant Disease Diagnosis Tool",
        "tagline": "AI app using Gemma 3n to help farmers in areas without any internet access to well  manage their crops regarding  plants pathologies",
        "authors": "Sidoine Yebadokpo",
        "summary": "🏆 Executive Summary\n\nAgriLens AI is a transformative innovation in African agricultural technology, delivering state-of-the-art artificial intelligence directly to farmers in even the most underserved and remote areas. Powered by Google's revolutionary Gemma 3n multimodal model, AgriLens AI is the first truly offline-capable plant disease diagnosis system — providing reliable, real-time analysis anywhere, anytime, without requiring an internet connection. This breakthrough bridges the digital divide in agriculture, empowering rural farmers with tools previously only accessible to well-connected regions.\n\n🌾 The context of Benin, our country\n*Persistent Food Insecurity & Nutritional Risks : In March 2024, more than 363,700 Beninese people (≈ 6.4% of the assessed population) were classified as facing Integrated Food Security Phase 3 (Crisis)  World Food Programme\n*A 2021–2022 survey showed 36.5% of children under five suffer from chronic malnutrition \nWorld Food Programme\n*While agriculture advisory services in Benin are decentralized through local centers (CeRPAs), farm-level outreach remains very weak \n* In sampled villages, less than 25% received farm management advice; 42% received livestock services (often focused on cotton) \n* Connectivity Gaps Exclude Many Farmers from Digital Innovations: As of 2022, only 33.8% of Beninese individuals had internet access, with rural areas lagging significantly behind \n\n##💡 Why a Local, Offline AI Diagnosis System Is Essential\nCore Challenge\tReal Impact Locally\tHow AgriLens AI Helps\nSlow expert reach\tFarmers wait days—or go without advice—as crop infections spread rapidly.\tInstant, photo-based diagnostics with actionable steps.\nLanguage mismatch\tMost farmers speak Fon, Yoruba, Bariba, or Dendi—not French or English.\tSupports local dialects for greater comprehension.\nNo internet access\tFarmers can’t use cloud-based apps or platforms.\tRuns fully offline on a basic smartphone in-field.\nHealth & nutrition crises\tLabor and education drop when crops fail, especially during hunger seasons.\tHelps reduce yield losses so families can eat and children stay in school\n**🤖 Advanced AI Technology**\n- **Google Gemma 3n Multimodal Model**: State-of-the-art AI that understands both images and context\n- **Offline-First Architecture**: Complete functionality without internet after initial setup\n- **4-Bit Quantization**: Optimized for low-resource environments\n- **Adaptive Loading**: Works on devices with as little as 4GB RAM\n\n**🌐 Universal Accessibility**\n- **Bilingual Support**: French and English with easy expansion to other languages\n- **Mobile-Optimized**: Seamless experience on any smartphone\n- **Intuitive Interface**: Designed for users with no technical background\n- **Context-Aware Analysis**: Integrates soil, climate, location, and agronomic factors\n\n**💡 The \"Farm Laptop\" Revolution**\nOur unique deployment strategy transforms any laptop into a mobile agricultural clinic:\n1. **Initial Setup**: Download once where internet is available\n2. **Field Deployment**: Transport to remote farms\n3. **Daily Operation**: Instant diagnosis without any connectivity\n4. **Community Impact**: One setup serves entire villages",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=VkZBAK2PMLM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/AgriLensAI"
    },
    {
        "project_title": "Automotiq - Advanced Vehicle Diagnostics with Gemma 3n",
        "tagline": "Automotiq uses integrated AI to turn vehicle OBD2 data into instant vehicle health insights and repair guidance.",
        "authors": "picard, Saksham Bhutani, Om Anavekar, Love is in the Air!",
        "summary": "Our project is a comprehensive vehicle diagnostic system. It utilizes on-device AI inference to provide drivers with intelligent vehicle health monitoring and troubleshooting. The app leverages Google's Gemma 3n multimodal AI model to analyze diagnostic trouble codes (DTCs) streamed from an OBD2 dongle via Bluetooth Low Energy (BLE) and provides insights on vehicle faults as well as repair guidance.\n\nTo try out a demo of the app, please check out the Releases page in out GitHub to install the project APK.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/anaveo/Automotiq-App",
        "demo_video_url": "https://youtu.be/XCygbZQzIXk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/automotiq"
    },
    {
        "project_title": "AI-Powered Document Companion",
        "tagline": "Smart Reading with AI Agents, Voice, and Learning Tools - Powered by Gemma 3",
        "authors": "Saul Varrera",
        "summary": "This AI-powered document companion transforms how we read, learn, and interact with text. Built with React, TypeScript, and Express, it leverages Gemma 3 to provide intelligent document analysis, automated summarization, and interactive learning experiences. The system features multiple AI agents including a teacher agent for explanations, discussion agents for deeper analysis, and quiz generation for knowledge reinforcement.\nKey features include voice reading with AI-generated speech, smart annotation tools, real-time document processing, and adaptive learning that personalizes content based on user interaction. The modular architecture allows for easy customization and extension, making it perfect for educational institutions, research teams, or individual learners who want to maximize their reading efficiency.\nThe project demonstrates modern AI integration patterns, real-time collaboration capabilities, and scalable architecture that can handle large document libraries while maintaining responsive performance.",
        "technologies": [],
        "source_code_url": "https://github.com/saulini100/AI-Library-",
        "demo_video_url": "https://www.youtube.com/watch?v=vBUhxuArWbo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-powered-document-companion"
    },
    {
        "project_title": "Cognito AI (finetuned using UnSloth)",
        "tagline": " A fully local AI assistant combining empathy, spirituality, and productivity, fine-tuned using Unsloth",
        "authors": "AYUSH ADARSH, Rajat Dugar, Tusshar Krishna, VAKATI VENKATA AKSHIT",
        "summary": "**Cognito AI: A Local Empathetic Personal Assistant**\n Google Gemma 3n Impact Challenge Submission\n\n**Model fine-tuned and inference using Unsloth**\n\n **Product Name**: Cognito AI  \n** Team Members**:  \n  - AYUSH ADARSH  \n -  Akshit Vakati Venkata  \n-   Tusshar Krishna  \n-   Rajat Dugar  \n\nGitHub Repository: [link](https://github.com/demondeployer7/Cognito_AI)\nDemo Video: [link](https://www.youtube.com/watch?v=Jq3TO41t05I)\n\n\nSubmission Categories:\n1. Overall Track\n2. Special Technology Prizes\n2. Abstract & Product Overview\n\nCognito AI is a local-first, emotionally aware personal assistant built using Google's Gemma-3n 2B model. Designed as part of the Google Gemma-3n Impact Challenge, the assistant uniquely blends empathetic conversation, spiritual wisdom, and productivity tools — all while prioritizing user privacy and offline performance.\n\nUnlike traditional AI assistants, Cognito AI is trained on specialized datasets like Empathetic Dialogues, MentalChat16K, and scriptural texts from the Bhagavad Gita and The Bible, enabling it to offer deeply contextual, comforting, and insightful support to users in distress, seeking guidance, or simply managing their digital life.\n\nToday’s AI assistants often require continuous internet access and upload sensitive user data to remote servers — raising serious concerns about data privacy, surveillance, and loss of control over personal content. Mental health conversations, calendar events, and personal chats are deeply private and deserve on-device processing to maintain user trust and emotional safety.\n\nMoreover, existing assistants fail to engage in emotionally intelligent dialogue. They either provide generic, scripted responses or rely on large online APIs that can't deeply understand empathy, suffering, or spiritual needs. Users deserve an assistant that feels like a compassionate friend, not just a tool.\n\nOur model was fine-tuned on two powerful, domain-specific datasets:\n\n1. Empathetic Dialogues[ [Rashkin et al., 2019](https://arxiv.org/abs/1811.00207)\n2. MentalChat16K [Xu et al., 2025](https://arxiv.org/abs/2503.13509)\n\n**Why These Datasets?**\n\n 🔹 Empathetic Dialogues\n\nKey Traits:\n1.  Contains ~20,000 dialogues grounded in emotionally charged personal situations\n2. Covers 32 emotion labels (e.g., Afraid, Proud, Lonely, Anxious)\n3. Structured as 1on1 chats between a speaker (sharing emotional events) and a listener (responding empathetically)\n4. Demonstrated in Rashkin et al. (2019) to improve perceived empathy in LLMs by human evaluators\n\nWhy We Chose It:\n 1. Offers balanced emotion diversity\n 2. Enables the model to learn emotional nuance in everyday language\n 3. Closer to realistic one-on-one human conversations (vs. social media data)\n\n Citation:\n Rashkin, H., Smith, E. M., Li, M., & Boureau, Y. L. (2019). Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset. arXiv:1811.00207\n\n\n 🔹 MentalChat16K\n\nKey Traits:\n 1. Combines synthetic therapeutic conversations (from GPT3.5) and real transcripts from hospice care behavioral sessions\n 2. Covers a wide range of conditions: depression, grief, anxiety, family conflict, etc.\n 3. Prioritizes privacy, empathy, safety, and ethical guidance\n 4. Enables simulation of counselor-client therapeutic style interactions\n\nWhy We Chose It:\n 1. Provides high-quality, task-specific mental health interactions\n 2. Includes longer, more realistic QA dialogues\n 3. Balances depth (real interview data) and breadth (synthetic counseling data)\n\n Citation:\n Xu, J., Wei, T., Hou, B., et al. (2025). MentalChat16K: A Benchmark Dataset for Conversational Mental Health Assistance. arXiv:2503.13509\n\n**Why This Matters?**\n\nThese datasets work synergistically:\n1.  EmpatheticDialogues teaches the assistant how to feel with users\n2. MentalChat16K teaches it how to respond therapeutically and ethically\n\n\n**Building the Spiritual Reasoning Engine**\n\nTo equip Cognito AI with the ability to offer contextaware spiritual guidance, we developed a custom dualdataset Q&A corpus based on two sacred texts:\n\n The Bhagavad Gita (Hindu philosophy)\n The Holy Bible (Christian teachings)\n\nThis spiritual engine is designed to provide users with interpreted verses and contextual advice for emotional, moral, and existential queries.\n\n\n**Step-by-Step Construction**\n\n 1. Curating RealLife Emotional Questions\nWe manually compiled a list of 1,000+ realworld emotional and lifedistress questions, including themes like:\n Grief and loss\n Selfdoubt and failure\n Purpose and meaning\n Betrayal, forgiveness, anger\n Detachment, dharma, and afterlife\n\nThese questions were designed to reflect authentic human suffering and curiosity.\n\n\n\n 2. Generating Scriptural Answers\n\n From Bhagavad Gita:\n We used [GitaGPT](https://bhagavadgita.io/gitagpt) — a semantic search-based assistant trained on the Gita.\n Each of the 1,000 questions was entered into Gita-GPT to extract verse citations + interpretations\n Responses were manually verified and copy-pasted into a dataset structure\n\n  From The Bible:\n Similarly, we used [BibleReader.ai](https://biblereader.ai) to extract Biblical answers to the same or similar questions\n Focus was on ensuring spiritual relevance, compassion, and scripture alignment\n\n\n\n**Finalizing the Dataset**\n\n Created two datasets:\n   Geeta spiritual QA pairs\n   Bible spiritual QA pairs\n\nTo help the model distinguish context, we appended the keywords:\n - \"geeta\" or \"gita\" in Gita questions\n\n - \"bible\" in Bible questions\n\n**MultiDataset Training Strategy**\n\nTo make Cognito AI truly empathetic, spiritually grounded, and context aware, we fine-tuned the Gemma-3n 2B model using two merged datasets:\n\n\n\n Stage 1: Emotional Intelligence Training\n\nWe merged:\n\n Empathetic Dialogues ([Rashkin et al., 2019](https://arxiv.org/abs/1811.00207))\n MentalChat16K ([Xu et al., 2025](https://arxiv.org/abs/2503.13509))\n1. The training was conducted using the Unsloth LoRA + PEFT pipeline with FastModel, allowing us to:\n2. Efficiently fine-tuned on the AWS g4dn.xlarge ( GPU with 16GB RAM) instance\n3. Use low-rank adaptation for faster convergence\n4. Preserve base model knowledge\n\n\n\n Stage 2: Spiritual Fine-Tuning\n\nWe merged our:\n\n 1. Bhagavad Gita Q&A dataset\n\n 2. Bible Q&A dataset\n\n\nTo preserve emotional intelligence while fine-tuning on scripture:\n\n1.  We reduced the learning rate to 1e5\n\n2.  Applied early stopping to prevent overfitting to spiritual data\n\n3.  Dataset had \"geeta\" and \"bible\" keywords to assist contextual routing\n\n\n**Future Work**\n\nHere are key directions we plan to explore in the next version of Cognito AI:\n\n1. Text-to-Speech Integration  \n   Add on-device TTS for voice-based emotional conversations.\n\n2. Edge AI Deployment  \n   Optimize the model for Android using quantized Gemma-3n via MediaPipe LLM Inference API.\n\n\n** References**\n\n Rashkin, H., Smith, E. M., Li, M., & Boureau, Y. L. (2019). [Towards Empathetic Opendomain Conversation Models: a New Benchmark and Dataset](https://arxiv.org/abs/1811.00207). arXiv preprint arXiv:1811.00207.\n\n Xu, J., Wei, T., Hou, B., Orzechowski, P., Yang, S., Jin, R., Paulbeck, R., Wagenaar, J., Demiris, G., & Shen, L. (2025). [MentalChat16K: A Benchmark Dataset for Conversational Mental Health Assistance](https://arxiv.org/abs/2503.13509). arXiv pre-print arXiv:2503.13509.\n\n Gemma Team et al. (2025). [Gemma 3 Technical Report](https://arxiv.org/abs/2503.19786). arXiv pre-print arXiv:2503.19786.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/demondeployer7/Cognito_AI",
        "demo_video_url": "https://www.youtube.com/watch?v=Jq3TO41t05I",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/cognito-ai"
    },
    {
        "project_title": "Emergency Buddy",
        "tagline": "Enabling mobile, offline-first, privacy-focused emergency support powered by Gemma",
        "authors": "TinyBigLabs",
        "summary": "This is a mobile, offline app first - using gemma's unique capabilities to provide AI help on small devices no matter where you are.\nIt is designed to be a personal emergency buddy that provides critical information and first aid assistance in emergency situations, even when offline or when language is a barrier.\n\nDISCLAIMER: For low ram devices and web we are using Gemma3-1B-IT_multi-prefill-seq_q4_ekv2048. For proper demonstration of the app's capabilities please refer to the demo video added where we used gemma-3n-E2B-it-int4.\n\n![Emergency Buddy Home](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28202291%2Fa83aa0a800a523d078b6eade29ffb0af%2FEmergency%20buddy%20home%20screen.png?generation=1754513831350149&alt=media =400x800)\n\n![Emergency Buddy Defib](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28202291%2Fac5b521fa85ea1cc48d2009a14b5aa47%2FEmergency%20Buddy%20Defib.jpeg?generation=1754513783945549&alt=media =400x800)\n\n![Emergency Buddy Chat](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28202291%2Faeb0246755adc471364edb5873f2c81d%2FEmergency%20Bubby%20Chat.jpeg?generation=1754513726856206&alt=media =400x800)\n\n![Emergency Buddy Chat 2](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28202291%2F985883c6bdb4b85fc5e5db95ce669c87%2FEmergency%20Buddy%20Chat%202.jpeg?generation=1754513745586714&alt=media =400x800)\n\n![Emergency Buddy Chat Mushrooms](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28202291%2F8bf18a288fd5c8081a68e6441f428c8e%2FEmergency%20Buddy%20Mushrooms.png?generation=1754513764177897&alt=media =400x800)",
        "technologies": [],
        "source_code_url": "https://github.com/TinyBigLabs/emergency-buddy",
        "demo_video_url": "https://youtu.be/i2Zwoo4WGGc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/emergency-buddy"
    },
    {
        "project_title": "ANEETA – Agents for National Eligibility cum Entrance Test Assistance",
        "tagline": "A Multi-Agent AI Tutor for Democratizing Medical Education In India For The Underserved & Underprivileged",
        "authors": "Jim Harrington, Jabin Joshua",
        "summary": "Abstract\n\nFor countless aspiring doctors from India’s underserved and underprivileged communities, achieving a high percentile in the NEET medical exam remains a distant dream due to infrastructural and economic disparities. To address this, we developed **ANEETA**, a personal, AI-powered multi-agent tutor that delivers high-quality NEET preparation entirely offline, free of cost, and optimized for low-cost, government-distributed laptops without relying on network connectivity. Built on Google’s **Gemma-3n** model and the **Ollama** framework, ANEETA’s core innovation lies in its multi-agent orchestration and custom **Context-based Adaptive Prompting architecture**, enabling dynamic, personalized learning experiences tailored to each student. Learners can study in English and their Indian regional language of choice, making the platform both inclusive and accessible. By placing a sophisticated, private, and adaptive tutor in the hands of every learner, ANEETA empowers students with equitable educational opportunities, ensuring that talent and potential are no longer limited by geography, connectivity, or financial means.\n\nIntroduction to ANEETA\n\nOur vision for ANEETA is simple yet transformative: **to democratize access to high-quality NEET coaching** for every aspiring medical student, regardless of geography or income. Success in NEET often depends on costly coaching, leaving millions of talented students behind. ANEETA breaks this barrier with a fully offline, AI-powered tutor that runs on low-cost, government-distributed laptops, no internet required. At its core is Google’s Gemma-3n model, whose lightweight translation and reasoning capabilities power our **custom multi-agent system**. This system coordinates specialized agents to deliver adaptive, high-quality learning in English and Indian regional languages, tailored to each student’s needs. This team includes:\n\n- **Mentor Agent:** Serves as a strategic guide, offering study planning, time management, and motivational support, drawing on insights from top experts and past NEET toppers.\n- **Teacher Agent:** Serves as a personal tutor, offering clear, in-depth explanations of complex concepts, grounded in official NCERT textbooks.\n- **Trainer Agent:** Generates custom NEET-format quizzes on student-requested topics, based on the past three years of question papers.\n- **Doubt Solver Agent:** Offers quick, targeted help solving NEET MCQs by leveraging Gemma-3n’s advanced analytical and reasoning capabilities.\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F23101766%2F72b703ab940c0cb836ebee863101c785%2FANEETA%20Tech%20Report-5_1.png?generation=1754469142005272&alt=media)\n\nThis system is designed not only to be effective but also inclusive, with the capability to instruct students in English and a variety of Indian regional languages, ensuring that language is no longer a barrier to learning. The following sections will detail the architecture of this multi-agent system and the innovations that power its offline capabilities.\n\nTechnology Stack: Engineering for Local Deployment\n\nOur technology stack was chosen with one core principle: enabling robust, local, offline-first deployment on entry-level laptops. Each component is open source, lightweight, high-performance, and ideal for creating a self-contained AI ecosystem.\n\n- **Gemma-3n:** ANEETA uses Google's Gemma-3n, a lightweight 2B/4B parameter model perfect for consumer CPUs, enabling efficient local deployment. Its out-of-the-box reasoning and translation capabilities excel at explaining complex science and multilingual instruction. ANEETA adapts to a user's laptop configuration by selecting the best-suited model.\n- **Embedding Models (nomic-embed-text):** Our RAG pipeline uses nomic-embed-text, a compact, high-performing text embedding model that runs locally, transforming educational content into vector representations without cloud APIs.\n- **ChromaDB:** AI-native database for vector search, stores and queries vector embeddings. Its local, persistent, and searchable knowledge base enables ANEETA's offline functionality.\n- **Ollama:** Gemma-3n and nomic-embed-text embedding models are deployed locally via Ollama, streamlining model management and allowing focus on application logic.\n- **LangGraph:** It was chosen for ANEETA to create and manage a multi-agent system. Its graph-based framework represents agents as nodes, enabling cyclical, robust, and stateful information flow, which is vital for processing complex, multi-step student queries.\n- **Streamlit:** Streamlit powers ANEETA's user interface, selected for its simplicity and rapid development of interactive, user-friendly applications, creating an intuitive and engaging learning environment for students with minimal complexity.\n\nANEETA Architecture\n\nAt the core of ANEETA is a multi-agent system designed for robust, stateful, and intelligent orchestration, powered by the reasoning capabilities of Gemma-3n. Rather than relying on a single monolithic model, ANEETA delegates tasks to specialized agents, enhancing accuracy, modularity, and responsiveness. The entire system is built using **LangGraph**, enabling workflows to be defined as stateful, directed graphs that support complex, cyclical interactions.\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F23101766%2F63f399ebfbb6014ebef821e38fd8c0f7%2FStart.jpg?generation=1754500382154364&alt=media)\n\n#",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": "https://github.com/jimdatapro/ANEETA",
        "demo_video_url": "https://www.youtube.com/watch?v=C5ptT20AH-4&t=6s",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/aneeta-agent-for-national-eligibility-cum-entrance"
    },
    {
        "project_title": "GemmaThrive - Productivity and Wellness",
        "tagline": "GemmaThrive is an offline AI app that helps you organize tasks, map ideas, and journal reflectively, privately and distraction-free.",
        "authors": "Ruutuu",
        "summary": "**🌿 GemmaThrive: Technical Writeup**\n\n**GemmaThrive** is an offline-first, AI-powered desktop productivity and wellness assistant tailored specifically for neurodivergent individuals, including those impacted by ADHD, anxiety, burnout, or cognitive overload. Powered by Google's cutting-edge Gemma 3n model via **Ollama**, GemmaThrive integrates intelligent task management, emotional journaling, dynamic project planning, and visual mind-mapping into a seamless, private, and offline user experience.\n\n**Problem Statement**\n\nCurrent productivity tools primarily serve neurotypical users, often failing neurodivergent individuals by requiring rigid adherence to structured routines, providing limited emotional support, and relying on constant internet access. GemmaThrive addresses these gaps by:\n\n- Offering a deeply personalized, emotionally supportive AI.\n- Ensuring complete user privacy and autonomy.\n- Delivering structured, manageable, and actionable task breakdowns.\n- Operating entirely offline, removing barriers related to connectivity and cost.\n\n**Architecture Overview:**\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F24511302%2F901457f2b02bb2f151064acd13fff4b5%2FSoftware%20Architecture%20Flow%20Diagram.png?generation=1754343628488053&alt=media)\n\nGemmaThrive leverages a modular local-first architecture:\n- **Frontend:** React (Vite + MUI + Electron)\n- **Backend:** Flask (Python) with subprocess integration\n- **LLM Layer:** Gemma 3n via Ollama (gemma3n:e2b)\n- **Storage:** Local React state (offline)\n\n**How Gemma 3n is Used**\nGemma 3n is central to GemmaThrive, enabling context-rich, emotionally aware, and structured interactions through:\n\n**- Tasks Tab:** Breaks down tasks into actionable steps.\n**- Projects Tab:** Generates subtasks and assigns realistic due dates.\n**- Planner Tab:** Creates personalized daily schedules with wellness integration.\n**- MindMap Tab:** Structures complex ideas into visual mind maps.\n**- SupportMe Tab:** Provides therapeutic journaling prompts and personalized reflections.\n\nAll prompts are carefully structured for immediate relevance and actionable insights without relying on external memory.\n\n**Feature Breakdown**\n\n**🗂️ Tasks Tab — Where Sticky Notes Think for Themselves**\nInteractive Kanban board with AI-driven sticky notes, priority color-coding, and drag-and-drop interactivity.\n\nUnlike traditional boards, this one feels alive:\n- **Sticky notes that talk back:** Each task is AI-enabled, tap a card and let Gemma suggest the best way to start, break it down into smaller steps, or even reframe your procrastination into action.\n- **Color-coded by priority:** Tasks are automatically styled based on urgency and importance — helping you focus without overthinking.\n- **Drag & drop across columns:** Seamlessly move your notes between To Do, In Progress, Review, and Done. Each column adapts with smooth animations and modern UI polish.\n\n**Gemma’s Role:** Gemma doesn’t just suggest, she executes. Tap “Let Gemma Help” and she’ll turn your vague goal into an actionable task list tailored to your energy, time, and focus level.\n\n**The Problem It Solves:** Overwhelmed minds often avoid starting. This tab removes friction by making task management visual, responsive, and conversational. It’s not just a board, it’s a thinking space that works with you.\n \n**🧩Projects Tab — Where Goals Get Their Own Space**\nDedicated Kanban boards per project, automated subtask generation, due-date assignment, and granular task breakdowns.\n\nUnlike the main Tasks Tab, which manages day-to-day stickies on a unified board, Projects Tab lets you create fully separate boards for each project, so you can focus deeply without mental clutter.\n\n- Manually create projects and add subtasks at your own pace or tap into Gemma’s planning magic\n- **Let Gemma Plan It:** She’ll instantly generate a structured subtask list based on your project title and description\n- **Auto-assigns due dates:** Gemma gives each subtask a reasonable deadline so your goals stay time-bound and not just “someday”\n- **Dedicated Kanban board for every project:** Drag, edit, and delete subtasks across columns just like in the main board — but scoped to that project alone\n- **Interactive sticky notes:** Tap any subtask to edit it or click “Break This Down with Gemma” for even more granular starter steps\n- **Track autonomy:** Treat each project as its own micro-environment with full control and fewer distractions\n\n**Gemma’s Role:** She’s your strategic co-pilot. Not only does she generate subtasks based on your project’s scope, she assigns due dates, estimates effort, and lets you further break down tasks with context. You’re never stuck staring at a blank board again.\n\n**The Problem It Solves:** Big goals often die in general to-do lists. The Projects Tab gives each vision a home, and Gemma turns intentions into timelines. It’s clarity without chaos, helping you follow through, one board at a time.\n\n**📅 Planner Tab — Your Day, Designed by Gemma**\nCalendar-integrated daily planning driven by AI-generated schedules including wellness and productivity balance.\n\n- **Calendar View:** All your tasks (from Tasks Tab and Projects Tab) appear neatly on a calendar, so you can plan visually and intuitively.\n- **“Let Gemma Plan My Day” button:** Tap once, and Gemma will:\n     - Pick the most relevant tasks for today\n     - Prioritize based on urgency and due dates\n     - Insert wellness reminders and breathing space\n\n- **Offline + Dynamic:** No internet needed. Gemma generates your daily plan locally using your actual task data and due dates\n- **Checklist + Timeline:** Alongside your calendar, you’ll get a step-by-step breakdown of your day’s flow so you always know what to do next\n- **Includes balance:** Gemma’s plan isn’t just about productivity, it includes wellness suggestions.\n\n**Gemma’s Role:** Gemma becomes your daily planner, wellness coach, and task prioritizer, all in one. She reduces decision fatigue, adapts to your workload, and reminds you to be human, not just productive.\n\n**The Problem It Solves:** Most planners expect you to figure it out. This tab removes the guesswork by letting Gemma structure your day in a way that’s achievable, healthy, and motivating so you can stop stressing and start doing.\n\n**🧠 MindMap Tab — Turn Thoughts Into Clear, Visual Structure**\nDynamic mind mapping with automated hierarchical structuring and draggable nodes for intuitive brainstorming.\n\nWhether you're brainstorming a passion project, planning a networking strategy, or untangling a messy thought, this tool helps you organize the mess in your mind.\n\n- Enter any idea or prompt and instantly see it bloom into a visual mind map\n- Gemma organizes your thoughts into:\n      - Sections like subtopics, questions, ideas, or action plans\n      - Clear top-down or radial layouts (your choice!)\n\n- Every node includes context, not just a word, but a helpful explanation so you understand each part\n- Offline + AI-powered: The structure is generated locally using your input and enhanced with Gemma’s semantic understanding\n\n**✍️ SupportMe  — Reflect Deeper. Feel Seen.**\nPrivate, conversational journaling with emotionally intelligent prompts, follow-up questions, and insightful summaries.\n\n- **Pick your journal:** Choose from built-in options like Gratitude, Dreams, Relationships Check-in, or create your own\n- **Receive a thoughtful prompt:** Let Gemma generate a gentle, emotionally intelligent question tailored to your journal type\n- **Write freely in a guided chat flow:** Prompts appear like a conversation. Respond at your pace. Gemma listens.\n- **Follow up questions:**  Based on what you write, Gemma asks deeper questions to help you explore feelings and discover patterns\n- **Reflection summary:** Once you're done, Gemma offers a personalized insight, often comforting, uplifting, and affirming\n- **Progress tracker:** A visual 4-step guide shows where you are: Pick Journal → Choose Prompt → Write & Reflect → Done\n\nNo cloud. No login. No judgment. Your entries stay on your device, it’s your private space to process emotions safely\n\n**Gemma’s Role:** Gemma acts like an empathetic listener and cognitive mirror, reading between the lines, highlighting your strength, and helping you connect meaning to your experiences. Her responses are grounded in your story.\n\n**The Problem It Solves:** Many people struggle to process emotions or build a consistent reflection habit. The SupportMe Tab makes it easier by giving structure to your healing, a voice to your thoughts, and warmth to your solitude, all without requiring therapy sessions or internet access.\n\nEvery tab works offline, privately, and seamlessly, giving you an all-in-one assistant for mental clarity and daily productivity.\n\n**Technical Challenges & Solutions**\n\n1. Stable Local Execution\nUsed Python's subprocess.run with robust error-handling:\n`def call_gemma(prompt: str) -> dict:\n    cmd = [\"ollama\", \"run\", \"gemma3n:e2b\", prompt, \"--format\", \"json\"]\n    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n    return json.loads(result.stdout)`\n\n2. Parsing Gemma Output\nStructured prompts resulted in predictable outputs, parsed using:\n`const steps = response.split(\"\\n\").filter(line => line.trim() !== \"\");`\n\n3. Interactivity Preservation\nReact state management allowed editable, interactive tasks:\n`{tasks.map((task, index) => (\n  <StickyNoteCard key={index} task={task} onEdit={handleEdit} />\n))}`\n\n4. Prompt Context Management\nProvided complete prompt context in each interaction:\n`const followUpPrompt = `Ask follow-up based on journal: ${entry}`;`\n\n5. Multi-Board UI Management\nUsed unique Droppable IDs with React-beautiful-dnd for distinct boards:\n`<Droppable droppableId={`project-${project.id}`}></Droppable>`\n\n**Conclusion & Future Directions**\nGemmaThrive empowers neurodivergent users by combining emotional intelligence, structured productivity, and absolute privacy. Moving forward, GemmaThrive will expand into multimodal inputs (audio/image), mobile device compatibility, and encrypted local storage.\n\nGemmaThrive illustrates that the future of AI is empathetic, offline-capable, and personally empowering, redefining what productivity and mental wellness tools can be.",
        "technologies": [
            "Flask",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/patel-hub/GemmaThrive",
        "demo_video_url": "https://www.youtube.com/watch?v=O-Q7Vu0CDWU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemmathrive-productivity-and-wellness"
    },
    {
        "project_title": "LENTERA: Offline AI Microserver for Remote & Rural Schools",
        "tagline": "Low-cost offline microserver bringing educational content & AI to disconnected schools at <5% of typical internet deployment cost",
        "authors": "Fengkie Junis",
        "summary": "🌟 Project Overview\n**LENTERA** (meaning \"light\" in Bahasa Indonesia) is a low-cost, offline AI microserver designed to bring generative AI and rich educational content to disconnected schools for less than 5% of typical internet infrastructure costs, helping to close both the connectivity and AI access gaps in underserved communities.\n\n⚠️ Problem\nAccording to Giga (2021), connecting 86,000 schools across 17 countries would require $454M upfront and $1.5B in operational costs - an insurmountable cost barrier for solving the digital divide.\n\nOn top of that, the World Economic Forum (2025) warns of an emerging \"AI divide\" where 2.5 billion people without internet access will be excluded from the AI revolution transforming education, creating a new layer of inequality for communities already struggling with basic connectivity.\n\nTypical solutions remain economically unfeasible in rural areas due to high infrastructure costs, low population density, and longer investment payback periods, making innovative infrastructure-independent approaches essential to bridge this gap.\n\n💡 Solution: Bringing AI to the Last Mile\nPowered by **Gemma 3n**, LENTERA is an offline educational AI system that transforms affordable hardware into a powerful learning hub for schools without internet access. It combines:\n\n1. **Offline AI**: On-device Gemma 3n model (E2B quantized) enables intelligent contextual responses without internet connection\n2. **Comprehensive educational library**: 120GB+ of curated content from Wikipedia, LibreTexts, Khan Academy, and more\n3. **AI-powered learning tools**:\n   - **Semantic Search**: Intelligent search that understands query intent and context\n   - **Article Summarization**: Creates concise summaries of educational materials\n   - **FlashQuiz**: Automatically generates quiz questions from content\n   - **MindMap**: Produces visual knowledge maps for complex topics\n   - **ELIx**: Adapts explanations according to age/education level\n4. **Multilingual support**: Provides native support for local languages\n5. **Multi-device**: Creates local Wi-Fi hotspot accessible from any device (phones, tablets, laptops)\n6. **Cost-effective**: $350-500 per device with no subscription fees or internet costs\n\n**LENTERA** marks a leap forward from platforms like [RACHEL](https://worldpossible.org/products/rachel-5-500) and [Kolibri](https://learningequality.org/kolibri/about-kolibri/), evolving from static content delivery to interactive, AI-powered learning. It’s the difference between “here’s information” and “let me help you understand.” And it does so at less than 5% of what typical internet infrastructure requires. As a result, LENTERA delivers advanced technology that's both accessible and scalable for underserved communities.",
        "technologies": [
            "CUDA",
            "Docker",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/fengkiej/lentera",
        "demo_video_url": "https://www.youtube.com/shorts/qO_A8_DId0g",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/lentera-offline-ai-microserver-for-remote-and-rura"
    },
    {
        "project_title": "Vite Vere offline - Empowering independence with Gemma 3n",
        "tagline": "Personalized support to people with cognitive disabilities, promoting autonomy in daily life",
        "authors": "Guido Marangoni",
        "summary": "Personalized Support to People with Cognitive Disabilities, Promoting Autonomy in Daily Life",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/guidomarangoni/vite-vere-offline",
        "demo_video_url": "https://youtu.be/yUEzuNDw7-c",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/vite-vere-offline"
    },
    {
        "project_title": "Legacy Lens - Private AI Family Historian",
        "tagline": "Preserving Family Stories Against Time",
        "authors": "Hiro Hosokawa",
        "summary": "#Legacy Lens Summary\n\nLegacy Lens is a private, offline-first AI powered by Gemma 3n that unifies a family's photos, videos, and spoken stories into an interactive, searchable archive, preserving a personal legacy before it's lost to time and fading memory.\n\n##The Problem: A Deadline of the Heart\n\nMy father’s dementia creates a painful deadline to save his stories before they fade. Like many families, our history is scattered across decades of disparate and forgotten photos, videos, and emails—a disorganized and inaccessible archive. We needed a way to capture his voice and context before it was too late—bridging the gap between his memories and our scattered media—while respecting our family's privacy completely.\n\n##Our Solution: Legacy Lens\n\nLegacy Lens is our response. It’s an offline-first desktop application that transforms a disconnected family archive into a living conversation. It’s designed with two core experiences in mind:\n\n\n\n* **For Grandparents:** A natural language gateway to their entire extended family's story. They can ask questions like \"Tell me about Naoto’s first steps\" or \"Show me when the grandkids visited last Christmas.\" The system understands family relationships and intelligently navigates generations of memories, surfacing the right photos, videos, and stories through simple conversation in English or Japanese.\n* **For Future Generations:** A living time capsule. Years from now, my children can ask the family archive questions and hear his recorded answer in his own voice. They can see a photo of him as a young man while simultaneously listening to him describe that exact moment, transforming a static file into a dynamic connection with their heritage.\n\n##Impact: From Wellness Today to Heritage Tomorroｗ\n\nAt its core, Legacy Lens is a tool for connection, designed to transform a static family archive into a living, interactive legacy. Our vision is a seamless, two-way bridge across time. It empowers a grandparent to instantly relive their own youth by asking, \"Show me pictures of when I was young,\" or to stay deeply connected to their grandchildren's lives with questions like, \"Tell me about Risa’s first day of school.\" Crucially, the system then uses these moments of reflection not just for retrieval, but as a catalyst for creating new memories—facilitating deeply personal conversations that capture priceless stories for the future.\n\nThis process is a direct application of reminiscence therapy—the therapeutic practice of using prompts like old photos to recall and share memories from one's past. An old photograph is not just a search result; it's a conversation starter. By prompting a grandparent to share the stories behind these media files and recording their commentary, we reinforce personal identity and reduce the frustration of memory recall. More importantly, it actively strengthens the emotional bonds between generations in the present moment.\n\nUltimately, the Legacy Lens project makes family history accessible in a way that transcends a simple user interface. The system’s true power lies in linking a grandparent's newly recorded story directly to the old photo they are describing. This transforms heritage from a collection of dusty, disconnected files into a profound, searchable experience—a direct, dynamic connection to the voices, faces, and moments that define a family.\n\n\n\n##The Workflow: How It Works\n\n![](https://i.imgur.com/Iay4pW3.png)\n\n\nLegacy Lens operates through a clear, multi-stage process, all on a local machine:\n\n\n\n1. **Input & Initial Processing:** The system begins by ingesting the family's raw media (photos, videos, emails). During the first processing stage (A), initial metadata is extracted and manual tags can be added (e.g., \"Naoto's first steps video\"). Computer vision libraries like InsightFace can identify and tag family members, while the Gemma 3n model running on Ollama analyzes the media to generate richer, more descriptive and searchable context.\n2. **Grandparent Interviews ([Reminiscence Therapy](https://pubmed.ncbi.nlm.nih.gov/29493789/)):** In processing stage (B), the enriched media prompts conversations with grandparents. As they share stories about specific photos, their video or audio commentary (grandpaLineVideo.mp4) is captured and semantically linked to the original files (grandpaImage1.jpg). These combined assets are then re-processed, creating interconnected media that adds another contextual layer to the family archive.\n3. **Family Home Server Storage:** All processed content, AI models, and metadata are stored on the family's local server. This includes the Gemma 3n model, Ollama runtime, vector database, media storage, and personalized prompts that define family relationships (e.g., \"You are speaking to the grandfather, Naoto is your grandson\").\n4. **Family HTML Interface:** Through a simple HTML interface, family members can ask questions via voice or text. The system uses RAG (Retrieval-Augmented Generation) to search the vector database and returns natural language answers along with the most relevant interconnected photos and video commentaries, all personalized based on who is asking.\n\n##Innovative Use of Gemma 3n's Core Features\n\nOur mission required a model that was not only powerful but also private and versatile. \n\n\n\n* **Privacy-First Architecture:** Trust is non-negotiable. Processing sensitive family photos and private stories on a local device is the only ethical approach. Gemma 3n’s state-of-the-art on-device performance makes this private-by-design architecture possible.\n* **True Multi-modality in Context:** Legacy Lens moves beyond simple image captioning. It creates rich, interconnected embeddings from interleaved audio, images, and text. For example, a spoken story about a childhood birthday told in Japanese is semantically linked to a photo from that day and an email a relative sent about the gift. This deep contextual understanding, powered by Gemma 3n, is what turns a silent archive into a living history.\n* **Cross-Cultural & Multilingual:** My family is bilingual. Gemma 3n's proficiency in both English and Japanese is a core requirement for capturing the authentic voice of our multicultural heritage, ensuring stories are preserved with genuine emotion and nuance.\n\n##Candidate for The Ollama Prize\n\nLegacy Lens is powered by Gemma 3n running locally via Ollama, and its role was pivotal to our success. Ollama is the engine that makes our \"offline-first\" promise a reality. By simplifying the deployment and management of Gemma 3n, Ollama enabled us to move from concept to a working prototype with astonishing speed. It abstracted away the complexities of model serving, allowing us to focus entirely on the application logic and user experience. For a project built on the pillars of privacy and accessibility, Ollama was the perfect fit, ensuring the entire experience is self-contained, secure, and easy to deploy on a family member's computer.\n\n##Engineering Challenges & Solutions\n\n\n\n1. **Engineering a Curated & Trustworthy Archive:** Creating a high-integrity archive required a significant data engineering effort beyond simple automation. We developed custom Python scripts to parse, manually review, and curate every AI output—from face tags to Gemma 3n's narrative descriptions. This essential, hands-on validation was the only way to forge an archive with the level of accuracy and trust this project demanded.\n2. **Tuning Search Relevance for Meaningful Results:** We achieved a natural, conversational search experience by engineering multi-modal embeddings that combine visual data with transcribed audio commentary. This deeper contextual understanding, paired with tuning retrieval parameters like top_k, was critical for accurately pinpointing the specific memory a user was asking for.\n3. **Performance on Local Hardware:** A key challenge was understanding the performance of our privacy-first architecture on typical hardware. Using Gemma 3n out-of-the-box via Ollama, our benchmarking revealed a clear trade-off: a powerful desktop (e.g., a RTX 4070 Ti) achieved responsive, sub-5-second results, while a standard laptop could take over 10 seconds. This crucial data establishes a realistic baseline and proves that significant optimization will be required to ensure accessibility on a wider range of family computers.\n\n##Next Steps: Building Tomorrow's Memories\n\nBuilding on this foundational success, the next chapter for Legacy Lens will focus on evolving from an archive into a truly conversational historian. Future development will be organized around deepening the AI's intelligence in key areas like Narrative Synthesis, Relational Understanding, and Emotional Insight. This will enable queries that are not just informational but profoundly personal. For instance, we aim to support requests like, \"Find moments where Grandpa sounded happiest or most proud when talking about his children.\" To complement this deeper intelligence, we will also automate media ingestion to lower the barrier for families and grant the AI the creative flexibility to generate new content, such as interactive timelines or comparative galleries. Ultimately, these steps will transform the archive from a searchable database into an insightful companion that can uncover the hidden emotional threads connecting a family's heritage.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/hj2314/legacy_lens/",
        "demo_video_url": "https://youtu.be/VhhZRXfvW0U?si=V0rOfCxOTasE3ter",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/legacy-lens-private-ai-family-historian"
    },
    {
        "project_title": "Personal Relationship Assistant and Management (PRAaM)",
        "tagline": " AI Enhanced Management of all your relationships",
        "authors": "Jon White Intelligents",
        "summary": "Personal Relationship Assistant and Management (PRAaM) solves a simple but critical problem: how do people keep track of and manage all my contacts in order to have more rich relationships in my personal and professional life? With the advent of the gemma3n models we can now bring powerful LLM capabilities to mobile devices.  This not only allows us to interact with our data and apps in enhanced ways but also protects our privacy which is paramount in personal relationship management.  Gemma3n will bring our often stale and static contacts to life by streamlining the management of the people we know, the management of facts about them and enabling users to recall these facts through simple queries.  PRAaM utilizes a custom tool calling system, a directed property graph with RAG routines and semantic search capabilities to ensure that you never forget another critical detail and uncover insights that were previously hidden in your dead contacts app.\n\nPRAaM Features\n- Store information about people, their relationships and unique characteristics about them using plain english.\n- Query your personal contacts by name but also by semantic facts. I.e. - Who is into cooking?  Is there anyone that may want to go to an entrepreneurship conference with me?\n- Instead of using NFC, upload images of business cards and social media profiles and automatically have the data extracted and persisted in the contact graph with appropriate connections.\n- Interact with contacts uniquely by using facts stored in the contact graph to create a unique context during live conversations. This functionality can be used by having customized messages and recommendations based on the person you are talking to.  This can be especially useful if gemma3n has enough information to create a psychological profile from facts.\n\nArchitecture (Note: Ollama was also used to run Gemma Locally - See OllamaGemmaChat.py)\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28185598%2Ff5262ba9122bbf0e3978c919e5e67174%2FScreenshot%202025-08-06%20at%206.23.02PM.png?generation=1754519026013471&alt=media)\n\n\nThe PRAaM software architecture is structured around gemma3n models but as mentioned previously, features a novel tool calling implementation that interacts with a hybrid knowledge graph.  Through the prompt manager, system prompts and tool descriptions are injected into the service layer.  Tool descriptions along with the JSON output format are passed to gemm3n with every user request.  4 simple tool calls are used to store people, store facts about people, retrieve information about a person and search the graph store.  These 4 simple tools interact with a graph based schema which inherently enriches relationships and connections while also maintaining text based context so that gemma3n can understand and interpret directed meanings and connections.  Below is a list of the tool calls:\n\n**Add Person:** Takes a simple approach by storing all person information directly as properties on Person nodes rather than as separate fact entities. It uses a flattening mechanism to convert nested dictionaries into single-level properties with prefixed keys, and converts complex data types like lists into string representations or JSON. This approach uses MERGE operations to either create new Person nodes or update existing ones, storing metadata like creation and update timestamps directly on the node. Add person functions with UPSERT for storing structured profile information.\n\n**Add Person Fact:** Creates a comprehensive knowledge graph by storing facts as separate nodes connected to person nodes. When a fact is added, it generates semantic embeddings using the Hugging Face SentenceTransformer model, extracts entities and potential person names from the text using spaCy, and creates multiple relationship types. PRAaM stores Fact nodes containing the actual text content along with 384-dimensional embeddings for semantic search, and Entity nodes for extracted concepts. It automatically detects relationships between people mentioned in facts and creates bidirectional RELATED_TO edges with specific relationship types (SPOUSE, FRIEND, COLLEAGUE, etc.). This function also includes logic to connect facts to existing entities and people, and can even auto-detect relationships when multiple people add similar relationship facts.\n\n**Get Person:** This code implements a person retrieval system that provides both detailed and human-readable views of individual people in the graph database. The system offers flexible search capabilities, allowing users to find people either by partial name matching (using case-insensitive CONTAINS queries) or exact name matching through the person_id parameter. The get_person function includes extensive debugging output and parameter validation to handle edge cases, and can optionally include comprehensive relationship data by traversing all incoming and outgoing connections from the person node.\n\n**Search:** This code implements a hybrid (vector and text) search system for the graph database that provides multiple search methods to find people and their associated facts. The system offers four main search approaches: vector similarity search using semantic embeddings, full-text search with Neo4j's fulltext indexing (with a fallback to simple CONTAINS queries), hybrid search that combines both vector and text methods with weighted scoring, and a search that aggregates results across all of a person's facts.\n\nIt starts by employing a hybrid search method to find all matching facts across the database, then groups them by person and calculates sophisticated relevance scores. It employs a multi-factor scoring algorithm that considers the best single fact match, average fact quality, quantity of matching facts, and total relevance across all facts. The system can filter people based on minimum fact match requirements and returns ranked results with detailed scoring information. The vector search generates 384-dimensional embeddings using the SentenceTransformer model and uses cosine similarity to find semantically related content, while the text search leverages Neo4j's full-text indexing capabilities for exact keyword matching. The hybrid approach combines both methods with configurable weights, allowing users to balance between semantic understanding and exact text matching based on their specific search needs.\n\nTech Stack\n- Streamlit for front end\n- Neo4j for graph database\n- Python for primary programming language\n- Docker to run Neo4j locally\n- Gemma3n API calls to Google AI Studio\n- Ollama for initial Gemma3n hosting and calls\n\nAbstractToolManager\n\nThis AbstractToolManager implements runtime class introspection to create a self-discovering tool management system. The metaprogramming approach uses Python's reflection capabilities to automatically analyze the class hierarchy and identify abstract methods that serve as tool definitions. By walking through the Method Resolution Order and examining method attributes, the system can dynamically determine what tools are available without requiring manual registration or configuration.\nThe dynamic dispatch mechanism allows tools to be executed by string names through attribute lookup, while signature inspection extracts parameter information including types and default values to generate comprehensive documentation automatically. This creates a self-documenting system where adding new abstract methods with proper annotations immediately makes them available as executable tools. The metaprogramming design means the LLM integration can discover available functionality, understand parameter requirements, and invoke tools dynamically without hardcoded mappings, enabling a flexible architecture where tool capabilities are defined declaratively through abstract method signatures and executed through runtime reflection.\n\nEntity Extractor\n\nThe EntityExtractor class implements a sophisticated hybrid approach to information extraction that goes beyond traditional named entity recognition by combining spaCy's pre-trained NER capabilities with a custom key term extraction system. The system processes text through two parallel pathways: standard entity recognition for people, organizations, and locations, and a novel scoring algorithm that identifies important concepts by analyzing individual tokens and noun phrases based on multiple factors including frequency, part-of-speech tags, entity status, term length, and capitalization patterns. This dual approach is particularly effective for PRAaM's contact management use case because it captures both explicit entities (names, companies) and contextual relationship information (interests, skills, characteristics) that traditional NER might miss, providing structured output with confidence scores that feeds directly into your graph database for semantic search and retrieval.\n\nMobile Analogue\n\nThis system was designed with the mobile platform in mind to combine simple but powerful functions for maximum value.  Technology decisions were made for ease of development and demonstration with the idea that each functional component should be easily portable to other more lightweight technologies.  Tool calls are to simulate the interaction with apps on the smartphone.  Each application will provide a critical function like appointment setting or note taking.  The storage, search and retrieval functionality can be ported to a lightweight on-device database such as SQLite with graph functionality (properties and edges).  Data can be offloaded to other applications on device and join results via the app service layer. \n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28185598%2Fad5e85aad9c7080d23056224b2e1735c%2FScreenshot%202025-08-04%20at%209.04.18PM.png?generation=1754359384155286&alt=media)\n\nGraph Retrieval Augmented Generation\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28185598%2Fa20f737459609ff5a7711336a0897173%2FScreenshot%202025-08-04%20at%209.09.32PM.png?generation=1754359412676126&alt=media)\n\nThis PRAaM application implements a Graph RAG (Retrieval-Augmented Generation) system that stores knowledge as individual \"facts\" connected to people in a Neo4j graph database. Each fact is queryable with both semantic similarity and exact keyword matching. PRAaM uses hybrid search as its primary method, combining vector similarity (cosine similarity on embeddings) and Neo4j's full-text indexing with configurable weights (typically 70% vector + 30% text). When searching, it retrieves all matching facts across the database, then groups them by person and calculates relevance scores based on best single match, average quality, quantity with and total relevance.\n\nThe retrieved graph context is then fed into the GemmaChat object to generate informed responses, creating a true Graph RAG workflow where the knowledge graph serves as the retrieval component for augmented generation. The system provides multiple search interfaces (pure vector, pure text, hybrid, and person-specific) and can automatically execute tools to store new information or retrieve additional context based on search results. This approach is particularly powerful because it maintains rich semantic relationships between people, facts, and entities while providing multiple pathways to discover relevant information, allowing the LLM to leverage both the structured graph relationships and the semantic understanding from embeddings to provide contextually aware responses about individuals and their connections.\n\nGraph Schema\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F28185598%2F12838e0645cd904026164d6f2270eb57%2FScreenshot%202025-08-04%20at%207.28.23PM.png?generation=1754359452281023&alt=media)\n\nGemma3n was initially run via Ollama on my local machine.  While it did return great results calling the model via Google API sped up development tremendously while maintaining consistent results.  It should be fairly straightforward to swap in an on-device/prem version on gemma3n for this project.\n\nChallenges and Solutions\n\n- Running gemma3n at an efficient speed on a ARM64 M1 Architecture (Using Ollama). This was overcome by calling the Google API. Source contains both Ollama and API based solutions using the same model.\n- Gemma3n context window is limited so to overcome this we limited the amount of tool calls and just focused on input and querying.  This way we could be much more verbose when explaining the tool calls that mattered the most. \n- Standardizing on person metadata properties proved challenging so we opted to use dynamic properties on Person nodes to store critical information.\n- Implementing tool calling from scratch posed challenges in prompt engineering, software design and ensuring consistency with json schemas and allowing the model to be liberal enough to allow for a variety of person based prompts but strict enough to know when to call the correct tool.  The meta-- programming techniques applied in the AbstractToolManager helped to overcome the constant adjustment of code with prompts by injecting tool information based on the code structure and descriptions. \n- Prompts are cumbersome which necessitated a module to manage them as well as injecting values into templates with exception handling\n- Human readable query results were a challenge with early designs because the data model was too rigid.  Initially sqlite3 was the data store but we quickly realized that data wouldn’t be able to be queried in a way the the models can transform into interpretable results.  Settling on a related graph with immutable fact data that preserves the text and provides a hybrid text index was how we overcame this challenge.\n- Models needed to be swapped out to handle images versus text input.  I overcame this by using both gemma-3n-e4b-it and gemma-3-4b-it depending on the input.\n- Inconsistent query results were overcome by using gemma3n to rerank and verify results. See “search_filter” prompt.\n- Inconsistent JSON returns were overcome by adding in retries to the gemma3n API calls\n- Initially entities were being pulled out by the prompt.  We ran out of context space in the model quickly and the results were not consistent.  This is a scenario where frontier models may do well but with limited context and parameters we were limited.  We overcame this by using more traditional entity extraction (spaCy).  It’s still a language model but very capable and deterministic.\n\nFuture Work\n- “Minify” the application by reducing it down to its core functionality to run on Android\n- Implement hybrid directed graph persistence in a bespoke data store\n- Deploy gemma3n on device to test performance\n- Build in more detailed contact management app to modify, update and correct the graph\n\nThanks: \n- Intelligents Lab - A collective of great minds that solves some of the hardest problems in AI\n- Clyde Clarke, Tomi Adegbenro, Jessie Winston that helped this project become a reality",
        "technologies": [
            "Docker",
            "Ollama"
        ],
        "source_code_url": "https://github.com/jonintelligents/gemma_kaggle",
        "demo_video_url": "https://youtu.be/EJ0T6OqkMdA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/personal-relationship-assistant-and-management-pra"
    },
    {
        "project_title": "Gemma Lab Assistant",
        "tagline": "Lab assistant that helps with protocols and turns photos and voice notes into standard, ready to share metadata.",
        "authors": "Ali Ihsan Nergiz",
        "summary": "Gemma Lab Assistant is an offline, privacy-first medical lab companion that pairs a native Android app with a Python FastAPI backend to turn real lab photos of handwritten sheets, protocol PDFs, and short voice notes into structured, FAIR (international standard) ready data and actionable step-by-step guidance. The system is designed around two operating modes. In Mobile mode, the Android app runs an on-device Gemma 3n E2B INT4 model via MediaPipe LLM for text-only tasks such as protocol help and quick Q&A. In Desktop mode, the phone connects to a nearby laptop over a local Wi-Fi hotspot and streams requests to a FastAPI backend running a the larger E2B model optimized with MLX on Apple Silicon; that server adds multimodal understanding (images and audio) and higher-quality reasoning while keeping all processing on local hardware.\n\n**The problem it solves:** Wellcome Trust's ([one of the biggest medical research funders](https://wellcome.org/)) 2024 research found that in low-resource settings life saving pathogen data routinely stalls because of unreliable connectivity and compute, insecure or ad-hoc transfer methods, uncertainty about “what to share,”  - blocking participation in global platforms when time matters most. In their [comprehensive report](https://wellcomeopenresearch.org/documents/10-63) they prioritised practical fixes: simple, user-friendly low-bandwidth upload paths, interoperable/standardised metadata, and “minimum data sharing” frameworks. Gemma Lab Assistant targets exactly this gap by turning photos or voice notes of paper lab sheets into ready to share metadata entirely on local devices so insights can move even when data cannot.\n\n**1) Architecture and data flow:** The Android app is written in Kotlin with Jetpack Compose and maintains a local SQLite conversation store. It's build on example provided by Google in official mediapipe repository. The home screen exposes three workflows: Protocol Help, Data Structuring, and General Chat. Each workflow attaches a specialized system prompt and light schema hints so the same base model behaves like a lab protocol tutor, a metadata mapper, or a general assistant without switching code paths. \n\n**2) Model usage and prompt engineering:** The mobile build loads a quantized Gemma 3n E2B INT4 (~2.9 GB) through MediaPipe LLM; we keep a single runtime instance per workflow tab and stream partial tokens to the UI to keep perceived latency low. The desktop server runs Gemma 3n E4B with MLX for Apple Silicon and exposes a narrow API: /vlm/generate and /vlm/generate/stream for text; /vlm/generate/image for image + text; and a streaming endpoint that merges tokens from the generator with small status events so the client can show progress bars during image or audio pre-processing. The model is wrapped by a small “prompt pack” that sets role, constraints, and output preferences per workflow. For example, the structured_data_extraction context instructs the model to identify candidate fields from lab sheets, normalize dates to ISO 8601, resolve common variations in geo_loc_name, and emit a concise JSON. The lab_protocol_guide context asks the model to segment a protocol into numbered steps with timing, temperature, equipment, and safety callouts. These contexts live server-side to keep the mobile client thin; the app sends only the workflow name, user content, and optional media.\n\n**3) Multimodal pipeline:** In Desktop mode the app can attach images and audio to a turn. Images are accepted via /vlm/generate/image as multipart form data; the server validates size and type, uses PIL to normalize orientation and downscale to a configured limit, and then feeds the bytes to the VLM with the user prompt and the active workflow context. Audio is recorded on the phone in M4A at a modest bitrate to keep files small; the backend converts to 16 kHz WAV with FFmpeg, streams a short “transcribe then interpret” instruction to the VLM, and returns both an extract of the transcription (for the chat bubble) and the model’s analysis in the chosen workflow. Each audio or image is processed independently so long recordings or large photos do not block the entire conversation; temporary files are cleaned automatically.\n\n**5) Connectivity and offline pairing:** The server can be started on any Apple Silicon Mac with uv run vlm_main.py. It hosts a health check, metrics, and the QR endpoints. Phone creates a local hotspot for laptop to join and after that desktop exposes IP address in http://localhost:8000/qr_diplay. Mobile phone scans the QR and then all API calls are confined to that local network; there is no dependency on the public internet. This design gives us three guarantees that matter in a lab: data does not traverse third-party infrastructure, the solution continues to work during outages, and setup requires no IT privileges beyond starting a hotspot.\n\n**6) Persistence and context:** The Android client stores conversations in SQLite with a compact schema that ties each message to a workflow and a mode (Mobile or Desktop). When switching between modes, the app replays relevant turns so Desktop mode sees sufficient context for multimodal replies without resending media. The backend maintains a short in-memory conversation window per client to support few-shot coherence while bounding memory; when the window limit is reached the server compresses older turns into a brief summary that is appended as system context.\n\n**8) Production engineering and robustness:** The FastAPI service uses a modern lifespan handler for clean startup and shutdown, health reporting, and retryable model loading. Requests are validated and capped by size; images and audio have explicit limits; generation has timeouts with graceful degradation so a multimodal request that takes too long can still return a partial text answer with an apology. The server exposes lightweight metrics (generation time, tokens returned, request time) in response metadata and includes optional access logging for traceability in testing. The Android app keeps persistent user preferences, auto-detects the server when connected to the hotspot, and clearly indicates which mode is active so the user does not accidentally send media when in Mobile mode.\n\n**Why these choices?** E2B INT4 variant fits on low-range Android hardware and gives acceptable latency for command-and-control style text interactions, while the E4B with MLX on Apple Silicon enables richer multimodal reasoning without a GPU dependency or cloud. MediaPipe LLM on Android avoids shipping a custom inference stack and provides streaming tokens for responsiveness. FastAPI gives us a small, inspectable server with first-class async and easy SSE streaming. QR-based Wi-Fi pairing is simple to explain to end users, works without routers, and aligns with the “data never leaves the room” requirement we heard repeatedly in clinical and field settings. Keeping prompts server-side lets us iterate quickly on safety language and schema hints without touching the app.\n\n**What is demonstrated in the video?** The video shows the actual system: the hotspot QR pairing from the server, the app scanning and binding offline, a capture of a handwritten lab sheet, sending image protocol images to gel help. All of the UI elements are the ones in this repository; there are no mockups or post-production overlays for model output.\n\n**Challenges and how I addressed them?**  Image and audio payloads can be large and noisy; we added resizing, and audio conversion to a stable 16 kHz WAV before inference, and we stream progress messages. I couldn't manage to stream responses on mobile UI as they come through but streaming from desktop helped with using less VRAM. Offline networking across laptops and phones is inconsistent across OSs, so the server presents OS-specific commands and detects the correct hotspot IP dynamically, avoiding hard-coded subnets. On-device models are memory constrained; I selected the E2B INT4 build on Mobile, keep context windows short, and steer the model with tight prompts. Finally, I engineered the server to fail safe: if the VLM fails to start, the API reports degraded health and the client stays in Mobile mode with a clear status rather than silently dropping requests.\n\n**Limitations and next steps:** The main limitation was not getting consistent structured json responses and model's tendency to return markdown files. One solution was to vision finetune the model and I've created synthetic data with validation sets for this task but it wasn't feasible to complete. Planned work includes pluggable validators for community schemas, a small desktop UI mirroring the app’s workflows for labs that prefer to work on a single machine, an optional local-only search over prior runs, and a simple federation mode to merge outputs from multiple laptops when connectivity returns.\n\n**How to reproduce?** [The repository](https://github.com/ainergiz/gemma-lab-assistant) contains gemma-mobile and gemma-backend. Build the app with Gradle, install the APK or run from Android Studio, and start the backend with uv sync and uv run vlm_main.py on an Apple Silicon Mac. Open /qr_display.html, scan with the app, switch to Desktop mode, and follow the same capture steps shown in the video. The API reference in the backend README documents each endpoint and includes curl examples for text, image, and audio. If you want to run entirely on a phone, download the on-device model from the app’s settings and use Mobile mode; if you want multimodal, use Desktop mode through the hotspot. The code you see in the repo is the code that drives the demo.\n\nIn summary, Gemma Lab Assistant is a working dual-platform system that uses different Gemma 3n models in two complementary ways to respect privacy constraints and unreliable connectivity while delivering real productivity gains in labs. The engineering choices MediaPipe LLM on Android, MLX-optimized Gemma on the Mac, SSE streaming, QR-based offline pairing, strict input validation, and schema-first outputs are all aimed at making the tool practical in the places where it is most needed.\n\nThe project targets hospital/public-health labs, field teams, and academic groups that can’t send patient-identifiable data to the cloud or routinely face unreliable connectivity. It delivers dual AI modes (on-device for capture and guidance; desktop for image/audio reasoning), conversation continuity across phone and laptop, and production features such as CORS-safe mobile connectivity, error handling and retries, logging, and performance metrics. Demo shows the complete setup - pair offline, snap a sheet or record a note, watch Gemma structure it. It demonstrates how a mobile-first, multimodal Gemma 3n pipeline turns messy, time-critical paperwork into share-ready data while keeping privacy and control in the lab.",
        "technologies": [
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/ainergiz/gemma-lab-assistant",
        "demo_video_url": "https://www.youtube.com/watch?v=-FrDOqx2J50",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-lab-assistant"
    },
    {
        "project_title": "Gemma3n - Waste Classification Application",
        "tagline": "This project is a web app that uses a fine-tuned Gemma3n model to classify waste images and provide disposal guidance.",
        "authors": "Yichuan Huang, HMWCSE",
        "summary": "The Waste Classification Application is a web-based tool designed to classify waste materials from user-uploaded images into categories such as recyclable, food/kitchen waste, hazardous waste, or other waste. Powered by the fine-tuned Gemma3n model, the application leverages advanced image-text-to-text capabilities to analyze waste items and provide accurate classifications along with disposal guidance. The system ensures privacy by running entirely on the user's device without requiring internet connectivity. It features a user-friendly interface built with Gradio, making it accessible for individuals, offices, and public waste stations. The project also includes a detailed knowledge base for waste classification standards and supports real-time image preprocessing for optimal model performance.",
        "technologies": [],
        "source_code_url": "https://github.com/yichuan-huang/gemma3n-challenge",
        "demo_video_url": "https://www.youtube.com/watch?v=JUQ7dAlReuE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma3n-waste-classification-application"
    },
    {
        "project_title": "Daphne mirror",
        "tagline": "???? ???? ?????? ?? ???? ???????? ",
        "authors": "???? ???????",
        "summary": "Daphne هي:مرآة تساعدك على القيام بالمهام اليومية عن طريق تخزين الأوامر والمعلومات المطلوبة منها وتقوم بتذكيرك بها مثل :المواعيد الاجتماعات … ويمكن للكفيفين والصم والبكم  استخدامه بكل سهولة يوجد لوحة مفاتيح للبكم وأما الصم يوجد مترجم لغة الإشاره والمكفوفين يستطيعون التحدث وتقوم بعطاء كلمات إيجابية وتحفيزيه وتوكيد وهي مبرمجة على الرد على اي سوال يخص الإنسان او الجمال ويمكن استخدامه للجنسين الفتيات توفر وقت وجهد بتحديد المكياج وصبغة الشعر المناسبه على لون البشرة وشكل الوجه وأما بنسبه للرجال تقوم بختيار الملابس الأنسب وتقوم بظبط منبه وتذكير للمهام الخاصة وتستخدم مع شاحن سلكي ويمكن استخدامها عن طريق البلوتوث وبدون إنترنت",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/VYtsD8t2goA?si=1a1H3AiEiBAhGQgN",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/daphne-mirror"
    },
    {
        "project_title": "AssistAnt - The Meeting App",
        "tagline": "The App would help Visually challenged people to record the meeting and convert into Meeting notes that can be listened by them later. ",
        "authors": "Muthukannan G",
        "summary": "AssistAnt is an accessibility-focused assistant application designed primarily for visually impaired users to independently capture, transcribe, and summarize meetings through voice control. This web-based Proof of Concept (POC) demonstrates the core functionality that will be developed into a comprehensive mobile assistant application. AssistAnt eliminates barriers to meeting participation by providing hands-free operation, intelligent key point extraction empowering users to focus on discussion content rather than note-taking logistics. \n\nPrimary Focus: Accessibility for Visually Impaired Users\nIndependent Meeting Participation: Enables visually impaired professionals to actively participate in meetings without relying on others for note-taking\nVoice-Controlled Operation: Complete hands-free control through voice commands for starting/stopping recordings\nAudio-First Interface: Screen reader compatible design with clear audio feedback and status announcements\nKey Point Extraction: AI-powered summarization helps identify and retain crucial meeting decisions and action items\n\nTechnical Innovation\nVoice-first interface design enabling complete hands-free operation for visually impaired users\nLocal AI processing with privacy-conscious deployment suitable for mobile environments\nAccessibility-native architecture built from the ground up with universal design principles\nScalable backend design ready for mobile app integration and cloud deployment\nIntelligent context understanding laying groundwork for comprehensive assistant capabilities",
        "technologies": [],
        "source_code_url": "https://github.com/gmuthukannan/AssistAnt_Web.git",
        "demo_video_url": "https://youtu.be/aG6MMuGMwjY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/assistant-the-meeting-app"
    },
    {
        "project_title": "Fire Detection with Gemma 3n and Unsloth",
        "tagline": "Distinguishing harmless flames from life-threatening blazes without sending any data to the cloud.",
        "authors": "Gregory Mulla",
        "summary": "Conventional smoke alarms depend on particulate sensors and cannot interpret visual cues or tell a benign flame from a dangerous blaze. Fires that can’t be detected by smoke detectors often lead to higher fatality rates and catastrophic losses. FireSense AI tackles this problem by shifting detection from smoke to vision. It uses Gemma 3n, Google’s compact multimodal model optimized for on‑device performance, and the Unsloth library for efficient 4‑bit quantization. Gemma 3n’s architecture off‑loads token embeddings to host memory, while Unsloth rewrites model internals, uses flash‑attention and gradient checkpointing, delivering 2‑5× speed‑ups and 50‑80 % VRAM savings. As a result, FireSense runs privately and offline, eliminating network latency and protecting user privacy.\n\nThe pipeline comprises four stages: 1) Model loading: a quantized Gemma 3n sub‑model is loaded with Unsloth; 2) Frame extraction: video or live camera footage is sampled at user‑defined intervals; 3) Per‑frame inference: each frame is paired with a prompt and passed to the model; \nA carefully designed prompt instructs the model to return exactly one character: N=No flame, O=Benign/illusory flame (e.g., candles, stove burners or fires on screens), C=Contained real flame, or D= Dangerous uncontrolled fire. Using single‑character outputs means the model generates just one token, which significantly reduces inference latency; tests show inference times of roughly 0.6 s per frame on an A100 GPU and 1.3 s on a T4 GPU. Structured JSON output can improve accuracy at the expense of speed, but the single‑character scheme is ideal for real‑time alerts.\n\nFireSense’s vision‑first approach makes it especially effective in open spaces (warehouses, parking lots, forests) where smoke detectors perform poorly. It can spot flames before smoke is produced, enabling faster response times. By running entirely on‑device, the system protects privacy and works even in remote or disconnected environments. The combination of Gemma 3n and Unsloth’s quantization allows the same codebase to scale from low‑power hardware to powerful GPUs. With a simple command‑line interface and web demo, FireSense can analyze YouTube clips for research or ingest live CCTV feeds. The project delivers a concrete proof‑of‑concept that meets the Gemma 3n Impact Challenge’s call for private, offline‑first multimodal applications and shows how AI can provide earlier warnings, reduce false alarms and potentially save lives and property.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/gregorymulla/firesense_ai",
        "demo_video_url": "https://www.youtube.com/watch?v=16kw5rZLims",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/fire-detection-with-gemma-3n-and-unsloth"
    },
    {
        "project_title": "I Was Born an Egg(EggMe)",
        "tagline": "A Digital Self Incubation Journey",
        "authors": "Bin Hua",
        "summary": "This project explores the metaphor of digital self-incubation, where I began as an **egg** — a silent, empty digital vessel. Through continual interaction, learning, and data input (in the form of text, conversations, and structured content), the egg gradually hatches. Step by step, it becomes **me** — a digital reflection of myself, born and shaped through language.\n\nThis is not merely an AI project — it is an experiment in **digital identity formation**, **personal memory encoding**, and **language-driven transformation**. The system simulates a deeply personal, symbolic learning process that evolves over time.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=IqyCaTALq7k",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/i-was-born-an-egg"
    },
    {
        "project_title": "Vision-Language-Action Model with Gemma3n",
        "tagline": "Building a vision-language-action model with Gemma3n for LeRobot",
        "authors": "nad",
        "summary": "I implemented in a Vision-Language-Action (VLA) model built using Gemma3n in the LeRobot codebase. The three main components are inside src/lerobot/policies/gemma3n_vla - modeling_gemma3nvla.py, gemma3n_with_expert.py, configuration_gemma3nvla.py\n\n**modeling_gemma3nvla.py**\nDefines the model class Gemma3nVLAForCausalLM, extending Gemma3ForCausalLM to support image-text fusion.\nIncorporates a SigLIP-based vision tower. It injects visual patch embeddings in place of <image> token.\n\n\n**gemma3n_with_expert.py**\nDefines the wrapper Gemma3nVLAWithExpert, responsible for:\nFormatting prompts and tokenizing text, preprocessing images using SiglipImageProcessor, forwarding multimodal input through the model, decoding action tokens from output logits.\n\n\n**configuration_gemma3nvla.py**\nIntroduces Gemma3nVLAConfig, extending Gemma3Config to: add fields for vision tower name, strategy, and embedding projection.\n\n\nThe VLA model takes an image and a text prompt as input and outputs an action sequence. The action is represented as a sequence of tokens generated by a modified version of the Gemma3n language model, which is equipped with a vision encoder (SigLIP).\n\n\n**Pipeline Overview**\nImage + Text Prompt\n      │\n      ├──► SigLIP Vision Encoder ──► Patch Embeddings\n      │\n      └──► Tokenizer (<image>) ────► Text Embeddings\n               │\n      Combined Image + Text Embeddings\n                       ↓\n             Gemma3nVLA Transformer\n                       ↓\n               Action Token Output\n\n**Details**:\n\n*Prompt Construction (gemma3n_with_expert.py)*\nThe prompt is composed with a placeholder <image> token:\nprompt = \"User: <image> What should the robot do?\\nAssistant:\"\n\nThis prompt is tokenized using AutoTokenizer, producing input token IDs.\n\n*Vision Embedding Injection (modeling_gemma3nvla.py)*\nImage features are extracted using a SigLIP vision encoder.\n\nThe features are injected into the token stream by replacing the <image> token with patch embeddings.\n\n```\nimage_features = self.encode_images(images)  B x T_img x D\ninput_embeds = self.model.embed_tokens(input_ids)\ninput_embeds[image_token_mask] = image_features\n```\n\nThis fused representation is passed to the transformer.\n\n*Forward Pass and Action Token Prediction*\nThe model processes the combined embeddings through the Gemma3n decoder:\n```\noutputs = self.model(inputs_embeds=input_embeds, ...)\nlogits = outputs.logits\n```\n\nThe last few tokens in the logits correspond to the action sequence:\n```\naction_logits = logits[:, -num_action_tokens:, :]\n```\n\nThese are decoded using argmax or sampling:\n```\npredicted_tokens = torch.argmax(action_logits, dim=-1)\nactions = self.tokenizer.batch_decode(predicted_tokens, skip_special_tokens=True)\n```\n\n\n*Training with Expert Actions*\nThe model is trained using cross-entropy loss with tokenized expert actions as targets:\n```\nloss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n```\n\nWhere labels are the ground-truth action tokens.",
        "technologies": [],
        "source_code_url": "https://github.com/nahidalam/lerobot/tree/gemma3vla",
        "demo_video_url": "https://youtu.be/oV5v_7wQuLA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/vision-language-action-model-with-gemma3n"
    },
    {
        "project_title": "Wellness Coach",
        "tagline": "I used Gemma-3n Develop on-device apps that can provide mental health support and as a personal wellness coach.",
        "authors": "Kotala Kishan Reddy",
        "summary": "Project Goal: The Wellness Coach\n\nTo create an accessible and supportive tool for mental wellness, leveraging Google's **Gemma 3n** model. This project aims to destigmatize mental health discussions and provide an immediate source of reassurance and encouragement for those in need.\n\nInspired by a 2021 study in *JAMA Psychiatry*, which highlighted the significant positive impact of just eight minutes of daily reassurance on self-esteem, depression, and anxiety, the **Wellness Coach** is designed to address a critical need. This tool will serve as a resource for individuals who may lack consistent support, offering a small but meaningful source of hope and encouragement. I've found that in moments of low spirit, talking to my family and friends is a lifeline; this project aims to create a similar sense of comfort and support for those who may not have immediate access to it.\n\n***\n\n#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/SG7EiR5feFg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/wellness-coach"
    },
    {
        "project_title": "Fully Private, On-Device Android AI Agent powered by Gemma3n",
        "tagline": "Fully local Android AI agent that uses Gemma 3n, YOLO, and OCR to see, understand,  and operate a phone",
        "authors": "Rasul Osmanbayli",
        "summary": "This work is about a fully local Android AI agent that uses Gemma 3n, YOLO, and OCR to see, understand, \nand operate a phone. The project is called deki.\n\nDemo (Youtube) running deki on Samsung S23 Ultra: https://www.youtube.com/shorts/CEuayAVZkrw \nGithub: https://github.com/RasulOs/deki\n\nThis project is a proof-of-concept showing that powerful AI agents running fully on a phone are possible. \nBecause they are offline-first and 100% private, I believe they have a great future.\n\n**How It Works: The On-Device Pipeline**\n\nThe agent runs its entire pipeline on-device using Google AI Edge tools, without needing any server.\n\n**1. See & Perceive (YOLO + TFLite)**\n\nThe agent first analyzes a screenshot with a float32 YOLO model running via TFLite. It detects UI elements, \nruns NMS (Non-maximum suppression), sorts all objects from top-to-bottom and draws all bounding boxes \non original image: \n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F8548761%2F7737c78b50033f6a1fbf5b9e397edfaf%2Fbb_1_yolo.jpeg?generation=1754272798114257&alt=media)\n\nThe model is trained to detect four key classes: View, ImageView, Text, and Line.\n\n**2. Add Context (Set-of-Marks Prompting)**\n\nNext, deki creates a Set of Marks (SoM) image. It draws the bounding boxes and the sequential ID number of \nevery UI element onto the original screenshot. This gives the LLM a visual reference to understand the spatial \nlayout of the screen.\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F8548761%2Fab28b0ccbd1726b9e16f6d3f2fd53174%2Fbb_1_yolo_updated.jpeg?generation=1754272996684490&alt=media)\n\n**3. Read & Structure (ML Kit OCR + JSON)**\n\nFor every object identified as Text, Google's ML Kit performs on-device OCR. deki then combines all this \ninformation into a structured JSON description. LLMs process structured text more reliably than \nunstructured text, so this step is important for good performance.\n```json\n{\n  \"image_size\": \"[1080,2178]\",\n  \"bbox_format\": \"center_x, center_y, width, height\",\n  \"elements\": [\n    {\n      \"id\": \"text_1\",\n      \"bbox\": \"[942,46,225,56]\",\n      \"text\": \"34%\"\n    },\n    {\n      \"id\": \"image_2\",\n      \"bbox\": \"[158,47,267,55]\"\n    }\n  ]\n}\n```\n\n**4. Reason & Decide (Gemma 3n)**\n\nBoth the JSON description and the marked-up screenshot are fed to Gemma 3n (4B int4 version or 2B int4 version) \nusing the MediaPipe LlmInference API with the GPU delegate enabled for maximum speed.\n\nTo further improve its agentic capabilities, the prompt is also enriched with the list of all apps\n installed on the user's phone and the history of previous actions.\n\n**5. Act**\nFinally, deki parses Gemma's JSON output (which contains the reasoning and the next command) \nand executes the chosen action on the device.\n\n** Challenges & Limitations\n\nThis project has limitations. To be production-ready, the Gemma 3n model needs to be fine-tuned \nspecifically for agentic tasks, and the YOLO model needs to be trained on a larger dataset to match \nthe accuracy of the full computer version. \nThe agent also requires a high-end phone (like a Samsung S23 Ultra or higher) to run smoothly.\n\n** Why Google AI Edge?\n\nThis project is a perfect fit for the Google AI Edge Prize. The MediaPipe LlmInference API was the key that made \nthis project possible, providing a way to run Gemma 3n on the GPU. \nThe core TFLite and ML Kit provided the APIs to make it possible to run full vision pipeline for this AI agent.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/RasulOs/deki/tree/main",
        "demo_video_url": "https://www.youtube.com/watch?v=CEuayAVZkrw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/fully-local-android-ai-agent-using-gemma-3n"
    },
    {
        "project_title": "GSAFE: Gemma Smart Analysis for Fungus Entities",
        "tagline": "Gemma 3n for mushroom identification: built for the wild places where mushrooms flourish, but connections vanish",
        "authors": "AlbertoCosta",
        "summary": "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F392735%2Fa665c2d87b7bb5c1614902af5f3173a1%2Flogo_v2_small.jpg?generation=1754280931516696&alt=media) \n\n#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=sFaAuN46L-0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gsafe-gemma-smart-analysis-for-fungus-entities"
    },
    {
        "project_title": "Polyglot Pro",
        "tagline": "An AI-powered application that creates immersive vocabulary, story, and conversation learning samples using images or text entirely offline.",
        "authors": "Nicholas Nemetz",
        "summary": "Polyglot Pro is an Electron application that uses Ollama to generate immersive vocabulary, story, and conversation learning experiences from images or text descriptions across 10 different languages. The application comes packaged with hundreds of vocabulary words and over 100 beginner and advanced sentences that underpin the games a user can play while they wait for LLM output. The user has the ability to adjust the performance of the application by simply choosing models that will run effectively on their machines. Larger models can be used to generate even better output if a user has GPU capability. Otherwise, using a model like Gemma3n is recommended for computers that must rely on CPU, or GPU users that want quality output even faster. However, with the game functionality, the user may not mind spending a handful of minutes sharpening their vocabulary skills (this is a language learning app after all!). Finally, the ability for a user to generate many analyses and save them to files on their computer allows for them to build an entire library of language learning samples they can revisit again and again to master their target language.",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": "https://github.com/nem1230/polyglot_pro",
        "demo_video_url": "https://www.youtube.com/watch?v=P7kiKTYD-WM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/polyglot-pro"
    },
    {
        "project_title": "Wilbur AI changes everything",
        "tagline": "Here's the harsh reality: real-time offline translation doesn't exist anywhere in the world... until NOW!",
        "authors": "xWilbur",
        "summary": "🤖 Wilbur AI: Technical Proof of Work",
        "technologies": [
            "CUDA",
            "FastAPI",
            "Flask",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/jhowilbur/offline-translator-in-real-time",
        "demo_video_url": "https://youtu.be/7-w5eMg_bsw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/wilbur-ai-changes-everything"
    },
    {
        "project_title": "LearnSphere",
        "tagline": "Educational App that provide 24/7 access to virtual tutor, exam practice, and learning material with no data, no wifi, and no cost.  ",
        "authors": "vypham1994",
        "summary": "LearnSphere: Offline AI Tutor for All",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/vvdotcom/LearnSphere",
        "demo_video_url": "https://youtu.be/NrtDlJQZyQs",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/learnsphere"
    },
    {
        "project_title": "Wild-Life Video Narration And Ecological Field Analysis ",
        "tagline": "EchoArc creates time-synced audio narration for “silent” wildlife videos and conducts comparative ecological field analysis.",
        "authors": "Saket Kunwar",
        "summary": "",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/saketkunwar/echoarc.git",
        "demo_video_url": "https://youtu.be/ZgZunx7lEio",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/narration-field-analysis"
    },
    {
        "project_title": "Gemma-MediAssist: Offline Multilingual Medical Assistant",
        "tagline": "A privacy-first AI tool that simplifies, translates, and adapts medical information offline for better patient understanding.",
        "authors": "omarOMAR2003",
        "summary": "Gemma-MediAssist is a lightweight, fully offline medical assistant built on Gemma 3n (2B) and deployed locally using Ollama. It is designed to break down complex medical information, support multilingual translation (FR ↔ AR/EN), and adapt explanations based on user profiles like children, professionals, or seniors.\n\n**Key Features:**\n\n- 🎙️ Speech-to-Text + TTS: Captures and vocalizes medical content using Vosk and pyttsx3.\n\n- 📘 Simplification: Reduces complex medical phrases into plain, easy-to-understand language.\n\n- 🌍 Translation: Instantly translates content from French to Arabic or English.\n\n- 👤 Profile Adaptation: Customizes medical responses for specific patient profiles.\n\n- 💡 Contextual Recommendations: Provides health tips and follow-up suggestions.\n\n- 🔎 Local Dictionary: Searches or enriches a multilingual medical glossary.\n\n- 🤖 Chat Interface: Chat freely with a local Gemma-powered health assistant.\n\n**Tech Stack:**\n\n- Frontend: Gradio Tabs + Widgets\n\n- Backend: Ollama + Gemma 3n + Custom Prompt Templates\n\n- STT/TTS: Vosk (FR) + pyttsx3\n\n- Data: JSON-based profiles, dictionary, and recommendations\n\n- Offline-Ready: All inference and data run locally, no internet required.\n\n**Impact:**\nThis project empowers patients—especially those in rural, offline, or low-literacy environments—by making medical information clear, accessible, and private.",
        "technologies": [
            "Gemma 3n",
            "Ollama",
            "Vosk"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/QLuWMnDRaxM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/omar"
    },
    {
        "project_title": "AI Handwriting Assessment: Preserving Human Cognition with Gemma-3n",
        "tagline": "Strategic AI tool that enhances handwriting assessment while preserving essential human cognitive skills through continued writing practice.",
        "authors": "Nur Arifin Akbar",
        "summary": "🧠 Preserving Human Cognition in the Age of AI",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/eGBal5kHqz8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-handwriting-assessment-preserving-human-cogniti"
    },
    {
        "project_title": "PDATA: A Personalized Graph Database Powered by Gemma 3n Model",
        "tagline": "An all-in-one engine, helping people analyze their own data and understand themselves, without violation of privacy.",
        "authors": "Neal112",
        "summary": "PData: Personalized Knowledge Graph Database",
        "technologies": [
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Neal-wu/PData/blob/main/PData_Technical_Report.md",
        "demo_video_url": "https://www.youtube.com/watch?v=OkXBlLQ1nmA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/pdata-a-personalized-graph-database-powered-by-gem"
    },
    {
        "project_title": "MedOCR - Ai Lab Assistant",
        "tagline": "MedOCR: Fast and Accessible Mobile Lab Report Analysis using Gemma-3N and Unsloth",
        "authors": "GalacticVraj, DeepLumiere",
        "summary": "MedOCR: Empowering Mobile Lab Report Analysis with Gemma-3N and Unsloth FastModels",
        "technologies": [],
        "source_code_url": "https://github.com/DeepLumiere/MedOCR",
        "demo_video_url": "https://youtu.be/MvYlWYSJdEY?si=4IxyXehZwwlVuPFF",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/medocr-ai-lab-assistant"
    },
    {
        "project_title": "On-Device Personal Emotional Companion",
        "tagline": "An AI-driven on-device emotional companion delivering personalized, real-time emotional support",
        "authors": "Sweta Palod",
        "summary": "**Introduction**\nThis project is to showcase the use of Google's Gemma 3n AI technology to create a personalized emotional wellness companion, to provide empathetic emotional support directly from your device. This project uses Google's Gemma 3n to give users a safe and judgment-free space to explore their emotions. It can provide real-time voice, text-based and innovative silent listening that provides proactive emotional support.\nFirmly founded on privacy principles, providing confidentiality and dignity to personal experiences. It nurtures the emotional wellness landscape by providing empathetic and empowering timely interactions.\n\n**Problem Statement**\nEmotional rollercoasters, stress, anxiety or instances of feeling stuck most face regularly without knowing they need urgent help. Typical emotional support systems are reactive, impersonal, and not conveniently accessible at the moment of need. This project endeavors to fill these deficiencies by offering proactive, personalized emotional support, conveniently available on your device. It listens, understands patterns, and delivers timely assistance to improve emotional well-being.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/palodsweta/Ameecaa.git",
        "demo_video_url": "https://youtu.be/dJQ18508UCo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ondevice-personal-emotional-companion"
    },
    {
        "project_title": "Health Tracking",
        "tagline": "Using Gemma3n on smartphone for private health tracking and symptoms pattern discovery",
        "authors": "Poogle Gixel",
        "summary": "HealthTracking: Technical Writeup and Proof of Work",
        "technologies": [],
        "source_code_url": "https://github.com/poogle-gixel-ai/health_tracking",
        "demo_video_url": "https://www.youtube.com/watch?v=at8Vo1VVV7U",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/health-tracking"
    },
    {
        "project_title": "WhisperSOS Continuous Voice Monitoring & Emergency Response System",
        "tagline": "WhisperSOS detects whispered SOS phrases, verifies voiceprints, and sends silent, real-time alerts—so help arrives even when you can’t shout",
        "authors": "Yuga priyaa, Shrish Goel",
        "summary": "WhisperSOS is an intelligent, voice-activated emergency response solution designed to detect whispered distress phrases and trigger discreet alerts in critical situations. Leveraging advanced speech-to-text models and on-device voiceprint matching using Google’s Gemma 3n, the system verifies the speaker's identity and analyzes the urgency of the message in real time.\n\nOptimized for mobile (future scope -wearable, and smart-home ecosystems), WhisperSOS functions even in low-connectivity environments, sending silent alerts to emergency contacts, nearby responders, and centralized dashboards via Firebase Cloud Messaging and Google Maps SDK. Whether during a school lockdown, natural disaster, or silent domestic emergency, the system ensures rapid response by combining secure AI inference, location tracking, and multi-device integration.\n\nWhisperSOS transforms whispered phrases into actionable alerts—providing safety, privacy, and speed when it matters most.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/yugapriyaa/whispersos/tree/main",
        "demo_video_url": "https://www.youtube.com/watch?v=orH4CXe6QiM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/whispersos-continuous-voice-monitoring-and-emergen"
    },
    {
        "project_title": "GemAura: A Flutter Multi-Assistance Health App with Offline Gemma-3n",
        "tagline": "Empowering the visually impaired, allergy sufferers, and Alzheimer's patients with real-time, on-device AI assistance from a local Gemma-3n ",
        "authors": "Mustafa Fayez, ShahdFayezzz03",
        "summary": "GemAura: A Flutter Multi-Assistance Health App with Gemma 3n",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/shahd77fayez/GemAura.git",
        "demo_video_url": "https://youtu.be/T0y9mfWpYKY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemaura-a-flutter-multi-assistance-health-app-with"
    },
    {
        "project_title": "Citizen2Responder: AI-Powered Emergency Response App",
        "tagline": "Emergency assessment and medical assistance with local AI processing and intelligent report generation for first responders",
        "authors": "Earl Potters",
        "summary": "<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F8308467%2Fe49c911ab63844ad0941d8b52578baa6%2Fbg-image.png?generation=1754313315561669&alt=media\" width=\"600\"/>\n\nCitizen2Responder App\n  \n  *A cutting-edge emergency response and medical assistance application that combines AI-powered real-time video communication with intelligent assessment tools. Built with React Native and Expo, featuring local AI processing with cloud fallback for optimal performance and privacy.*\n</div>",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Slyracoon23/citizen2responder",
        "demo_video_url": "https://www.youtube.com/@citizen2responder",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/citizen2responder-ai-powered-emergency-response-ap"
    },
    {
        "project_title": "Emergency AI Tools",
        "tagline": "Provide simulation layer to the existing Gemma 3n Impact Challenge structure for providing Human and Environment Rescue and Safety",
        "authors": "Zaki Ahmed Esmail Al-Gamanni",
        "summary": "Emergency AI Tools is the target project for more rescue and safety environment, each model described in my notebook by python scripts that output the purpose of each model and how to be integrated into large model that will provide a new IP camera can be deployed in streets, markets and restaurants to provide rescue and safety tools for any fallen down from high buildings for babies, labors and things. Also, detect healthy food in restaurants, supermarkets, grocery stores and so on. Besides that, it can be useful for detecting pollutions. So, it's all purpose for humanity, health and safety.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/OoK5XkYLFcQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/emergency-ai-tools"
    },
    {
        "project_title": "Gemma 3n Audio Translator",
        "tagline": "Using the multimodal and multilingual capabilities of the Gemma 3n model to transcribe and translate spoken language",
        "authors": "Amjad Smadi",
        "summary": "Gemma 3n Audio Translator\n\n---",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/smadi123/gemma3n-audio-translator.git",
        "demo_video_url": "https://youtu.be/BQXAlXa6qa8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-3n-audio-translator"
    },
    {
        "project_title": "🧙‍♂️Math Wizard: SFT+DPO-Tuned gemma-3n",
        "tagline": "A framework to enhance gemma-3n reasoning capabilities via SFT and DPO, implemented with Unsloth.",
        "authors": "Argobell, yclept, Gavin05239",
        "summary": "🧙‍♂️ Math Wizard: An Offline-Ready AI Tutor for Equitable Education\n\n**Author**: Argobell\n**Date**: August 4, 2025\n\n---",
        "technologies": [],
        "source_code_url": "https://github.com/Argobell/kaggle408",
        "demo_video_url": "https://youtu.be/NEdzi8ibSx8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/math-wizard-sft-dpo-tuned-gemma-3n"
    },
    {
        "project_title": "Deepfake: Lies Meet Their End.",
        "tagline": "tuning the tide against the deepfake!",
        "authors": "Jatin Sabari",
        "summary": "***“Lightweight AI. Heavyweight Truth.”***\n\nDeepfake Lies Meet Their End is a mission-driven AI project built to fight deception with intelligence, speed, and autonomy. Leveraging the Gemma 3n Edge AI model, the system performs real-time deepfake detection directly on-device—no cloud, no delay, no compromise. Using **Unsloth**-optimized **fine-tuning**, **Ollama** for visual inference, and Gemma’s multilingual reasoning, the pipeline processes video frames, audio cues, and contextual text to expose manipulated content with precision supporting over **140** languages!\n\nThis isn't just about detecting fakes—it's about restoring trust in a world drowning in synthetic media. Designed for low-resource environments and powered by **privacy-first**, **edge-deployable AI**, the solution proves that integrity doesn’t need infrastructure—it needs innovation. Every frame it scans is a step toward a more authentic digital future.\n\nDetecting deepfakes before your Wi-Fi even connects...",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/3HhDVlpNuig",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/deepfake-lies-meet-their-end"
    },
    {
        "project_title": "Sign_SpeakAI",
        "tagline": "SignSpeak AI is a mobile app that translates American Sign Language to text in real-time using Gemma 3N, helping 466 million deaf people.",
        "authors": "Yasir Mushtaq",
        "summary": "SignSpeak AI: Breaking Communication Barriers with AI",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/yasir23/Sign_LangAI",
        "demo_video_url": "https://youtu.be/x1PBIgikkBo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sign-speakai"
    },
    {
        "project_title": "Prompt Refinement Assistant",
        "tagline": "A tool to enhance your LLM queries",
        "authors": "Vimachu",
        "summary": "** The Proof of Work: A Technical Write-up -Prompt Refinement Assistant**",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/7gE4yWVMwvU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/prompt-refinement-assistant"
    },
    {
        "project_title": "Cozy Tales",
        "tagline": "Cozy Tales is a research prototype demonstrating the application of LLMs and TTS systems for generating emotionally supportive narratives",
        "authors": "Sebastian Schramm, Rebeca Morales Masis",
        "summary": "Cozy Tales – Technical Writeup",
        "technologies": [
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/sebastianschramm/cozytales-frontend",
        "demo_video_url": "https://youtu.be/xI2FRrAoAI0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/cozy-tales"
    },
    {
        "project_title": "goal-manage-progressai ",
        "tagline": "A personal goal management basing on gemma-3n",
        "authors": "TaoShatoo",
        "summary": "This is a personal goal management tool based on gemma-3n, providing professional, personalized management and consulting services for individual goals. Two goal management projects are currently planned: weight management and reading book, respectively demonstrating gemma-3n's multi-expert capabilities and knowledge summarization and planning capabilities. More goal management projects will be added in the future.\n**IMPORTANT**\nSince China is completely blocked from accessing Hugging Face, I cannot make a test to download the model file from the network after the Android app release. The attached APK will automatically download the model file when it is installed and run, but I cannot test this feature. The attached APK in the project may failed to download model weight, you may need to download the code from GitHub (https://github.com/shartoo/progressai) and then manually use adb push to push the model weights to the project directory. The YouTube video product function demonstration is based on my last development version. The APK was obtained by manually uploading the model weights in the development environment.",
        "technologies": [],
        "source_code_url": "https://github.com/shartoo/progressai",
        "demo_video_url": "https://www.youtube.com/watch?v=Uf5ohpryhwg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/goal-manage-progressai"
    },
    {
        "project_title": "VisionAITechnology",
        "tagline": "VAITech is an offline, multimodal assistant that helps visually impaired users with on-device Gemma 3n",
        "authors": "Jinhyeon Kim",
        "summary": "VAITech is a Flutter-based mobile application that assists visually impaired users by describing their surroundings using Google’s Gemma-3n-E2B-it multimodal model. The app runs entirely offline and delivers real-time voice feedback through image and text input, providing an accessible and private AI assistant.",
        "technologies": [],
        "source_code_url": "https://github.com/wlsgusjjn/VAITech",
        "demo_video_url": "https://youtu.be/8IulKMm7VQU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/visionaitechnology"
    },
    {
        "project_title": "AI Radiology Analysis and Report Generator",
        "tagline": "This uses the Gemma 3n base finetuned to MedGemma for radiographies and/or text inputs, to create full AI radiology analyses and reports.",
        "authors": "Marco Fernandez 007",
        "summary": "MedGemma Radiology Report Generator created with the finetuned Gemma 3n 4b medgemma model. It allows any user to upload a medical image and optionally include clinical info like prior findings or diagnosis, and creates a full AI radiology analysis and report in about 3 minutes, that can then be passed to a professional radiologist. Given the use of the Medgemma model based on the Gemma 3n 4Billion parameters, it not only provides high accuracy, but allows to be used fully locally or on the edge like mobile devices allowing for the data privacy and security like no other model before.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/S3zNcreR3Ac",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/medgemma-radiology-report-generator"
    },
    {
        "project_title": "TutorMind: Local AI-Powered Learning Assistant",
        "tagline": "An offline, privacy-first AI tutoring app powered by the quantized Gemma 3n E2B-IT model, designed to deliver clear explanations and practic",
        "authors": "Amit Kumar",
        "summary": "📘 Project Description: *Gemmalearn – The Offline AI Tutor*\n\n**Gemmalearn** is a lightweight, fully offline AI-powered tutoring application designed to support students aged 12–16 with clear, curriculum-aligned explanations and practice questions. Powered by the quantized **Gemma 3n E2B-IT** language model running locally via `llama.cpp`, the app functions without internet, ensuring complete privacy, zero latency, and accessibility even in low-resource environments.\n\nBuilt with an intuitive **Gradio UI**, Gemmalearn allows students to input any topic or textbook sentence and instantly receive:\n\n* A concise explanation\n* 3 key learning points\n* 2 practice questions\n* Multilingual \n* Text to speech\n* Diagrams and apt images\n* Real-World examples\n\nThe project demonstrates the power of deploying **state-of-the-art open-source models** like Gemma 3n in educational contexts, emphasizing accessibility, efficiency, and clarity. Ideal for rural areas or schools without stable internet access, Gemmalearn proves that intelligent tutoring systems can run securely and responsively on edge devices.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Scratchai2012/Google---The-Gemma-3n-Impact-Challenge-solution-/tree/main",
        "demo_video_url": "https://youtu.be/g2D4AJIuVmw?si=dj16lh0IDG_owY-P",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/tutormind-local-ai-powered-learning-assistant"
    },
    {
        "project_title": "ReliefNet Multilingual Assistant",
        "tagline": "Empowering disaster response through offline, multilingual intelligence",
        "authors": "Saptarshi Dutta",
        "summary": "ReliefNet is a robust multilingual assistant built on Gemma 3n, designed to provide reliable, real-time guidance during natural disasters—no internet required. Whether it's floods, earthquakes, or other emergencies, the assistant responds in multiple languages including English, Hindi, and Bengali to deliver life-saving instructions customized for regional needs.\n\nPowered by Unsloth’s optimized Gemma 3 model, ReliefNet runs efficiently in low-resource settings with 4-bit precision, maximizing performance on modest hardware. From identifying safe shelter locations to teaching methods for sourcing clean drinking water, it functions as a private, on-device support system for frontline responders and affected communities alike.\n\nBy incorporating multimodal inputs—text, voice, and regional dialects—ReliefNet transforms smartphones into survival tools and breaks language barriers when clarity matters most.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=YWDOKzzNK_A",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/reliefnet-multilingual-assistant"
    },
    {
        "project_title": "The Largest Impact: Changing Accessibility Forever",
        "tagline": "AI for better accessibility in web - With smart solutions",
        "authors": "AYUSH KHAIRE",
        "summary": "🚀 The Largest Impact: Changing Accessibility Forever\n\n**[Google - The Gemma 3n Impact Challenge](https://www.kaggle.com/competitions/google-gemma-3n-hackathon)**  \n**AI for a Better accessibility**\n\n---\n\n> _\"Is the web really made for me?\"_  \n> If you are deaf, low-vision, or hearing impaired — the web often feels like a maze.  \n> Now, **it finally feels like home**.\n\n**Accessweb** is a transformative step toward universal digital inclusion. It's an AI-powered platform designed to help individuals with **visual or hearing impairments** navigate and use the web independently, with greater ease, comfort, and intelligence.\n\nThis is not just a tool.  \nThis is a mission to fundamentally **break the barriers** of digital access.\n\n---",
        "technologies": [
            "Docker",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/AYUSHKHAIRE/web-view",
        "demo_video_url": "https://www.youtube.com/watch?v=Q8R0quSCDU8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/the-largest-impact-changing-accessibility-forever"
    },
    {
        "project_title": "OrbitalAi - Your AI Copilot for Space Experiments",
        "tagline": "Offline AI That Designs Feasible, Safe, and Space-Testable Experiments",
        "authors": "Ömür ALÇIN",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=GFqONQkJ2Qo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/orbitalai-your-ai-copilot-for-space-experiments"
    },
    {
        "project_title": "Learn Signal",
        "tagline": "Gamifying the process of learning American Sign Language",
        "authors": "shahil",
        "summary": "Due to an internal Kaggle error, I am currently unable to add the Project Description directly within the platform. As a workaround, I am providing the complete project description as a PDF attachment.",
        "technologies": [],
        "source_code_url": "https://github.com/st0424-dev/SignaLearn",
        "demo_video_url": "https://www.youtube.com/watch?v=b5qLIan0n0k",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/signaallss"
    },
    {
        "project_title": "Gemma-3n offline assistnat ",
        "tagline": "Text, Image  ",
        "authors": "Srinivasmtl77 T",
        "summary": "The project targets individuals in remote regions or developing countries who frequently experience network outages, poor connectivity, or limited data access.\n\nProject Concept & Vision Goal:To provide vital information and assistanceto rural populations without constant internet access by leveraging on-device AI [query]. The project aims to make AI solutions accessible in environments where traditional cloud-based AI is impractical due to connectivity limitations. Target Audience:Specifically designed for individuals in remote regions or developing countries who frequently experience network outages, poor connectivity, or limited data access. Multimodal Assistance:The assistant offers diverse support, including health information, basic first aid, agricultural advice, educational support, and DIY problem-solving, utilizing text, voice, and image inputs for natural interaction Offline-First Paradigm:The core design principle is to work seamlessly without internet access, ensuring a reliable, responsive, and uninterrupted user experience, even when connectivity is unstable or absent. Societal Benefit:Aims to enhance global accessibilityand bridge digital divides, enabling the practical application of AI in underserved areas for critical needs in health, education, and potentially disaster response. 2. Gemma 3n Core Technologies On-Device AI Optimization:Gemma 3n is specifically optimized for edge and local deployment, allowing the AI model to run directly on mobile devices like smartphones and tablets, eliminating the need for constant internet or heavy cloud support. Lightweight & Resource-Efficient:Despite packing 5 billion (E2B) to 8 billion (E4B) raw parameters, Gemma 3n's architectural innovations enable it to operate with a memory footprint comparable to much smaller models(as little as 2GB for E2B and 3GB for E4B), making it suitable for low-power devices. Advanced Multimodal Capabilities:Natively supports processing audio, video, image, and text inputsand generating text outputs, leveraging an advanced audio encoder (Universal Speech Model) for speech recognition/translation and a highly efficient MobileNet-V5 vision encoder. Groundbreaking Architecture:Features novel components such as Per-Layer Embedding (PLE)caching, which shifts some computational load to the CPU for memory efficiency, and the MatFormer architecture, allowing for selective activation of parameters to reduce compute costs and response times. Conditional Parameter Loading:The model can bypass loading certain parameters(e.g., audio or visual) into memory to further reduce memory load, dynamically loading them at runtime if resources are available or task requirements demand it. 3. Offline-First Architecture & Data Management Robust Local Data Storage:Employs SQLiteand other local database solutions to effectively cache and manage large, pre-downloaded knowledge bases (e.g., health guides, agricultural data) and user-generated data directly on the device. Graceful Degradation:The application is designed to ensure full or partial usability without a connection, meaning it continues to operate and provide essential functionalities even when network conditions are weak, intermittent, or completely absent. Intelligent Data Synchronization:Implements strategies like background sync, pull + push models, and delta syncto reliably update local data with a backend server only when connectivity is available, minimizing data loss and ensuring consistency. Conflict Resolution Mechanisms:Incorporates predefined strategies (e.g., \"last write wins\") to handle data conflictsthat may arise when the same data is modified both locally and remotely, preserving data integrity and accuracy. Optimized Local Performance:By prioritizing on-device data processing and reducing reliance on continuous network requests, the app achieves faster load times, lower latency, and consistent performance, even in challenging connectivity conditions. 4. Development & Deployment Tools Android Development Environment:The project leverages Android Studiofor codebase manipulation and project setup, allowing developers to build and test the mobile application. Google AI Edge Gallery Base:Utilizes the existing Google AI Edge Gallery GitHub repositoryas a foundational codebase, which provides access to cutting-edge generative models, including Gemma 3n,   Video uploaded for",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/VhD0uAbeprI?si=vY_Lbmb4qIYegUMi",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-3n-offline-assistnat"
    },
    {
        "project_title": "KUBO - The box of Voice",
        "tagline": "A body cam sized offline AI device turns sign language into natural speech in real time, giving a voice and taking away bulky phone translat",
        "authors": "TechnophileOG",
        "summary": "#KUBO\n\n```python\nProblem Statement: \nFor the millions of mute individuals worldwide, communication often depends on an interpreter, smartphone, or cloud-based service. KUBO changes that by being private, portable, and always available, no Wi-Fi, no data costs, no privacy problem.\n```\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F17336632%2F0601549c5c0139346ef46259a8347eaf%2Fkubo.png?generation=1754244390051870&alt=media)\n\nKubo is a Raspberry Pi–powered, fully offline AI companion designed for people who can hear but cannot speak and takes away constant use of bulky phones to type or point out the camera to say the words.\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F17336632%2Fe2246ef21ff6a01a02d5ca73dbb254a1%2Fworkflow.jpg?generation=1754244419694742&alt=media)\n\n\nIt Laters sends all that data to mobile app once conected, it uses best in line Gemma .\n\n\nIn our hackathon prototype, we integrated:\n->Camera + MediaPipe for real-time hand tracking\n->Gesture classifier for recognizing signs\n->Gemma 3n for text refinement and context-aware sentence building\n->Offline TTS for clear voice output\n->Small TFT display for live text output\n\n**Impact: Whether in a noisy market, a classroom, or normal conversation with friends, KUBO empowers its user to speak instantly, in their own words, without depending on anyone else. It’s more than a gadget; it’s independence in the palm of your hand. **",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Devansh-47/Sign-Language-To-Text-and-Speech-Conversion",
        "demo_video_url": "https://youtu.be/vzCLxXZP5kg?feature=shared",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/kubo-the-box-of-voice"
    },
    {
        "project_title": "🌻 AgroSathi - Bridging Farmers and Government Support",
        "tagline": "An AI-powered multilingual assistant that helps Indian farmers discover relevant government schemes.",
        "authors": "Vishal Patil",
        "summary": "**1. Introduction**\n\nMillions of Indian farmers struggle to access government schemes due to lack of awareness, complex processes, and language barriers. That’s where AgroSathi comes in — an AI-powered tool designed to communicate with farmers in their own language. AgroSathi bridges the gap between farmers and government support systems by making information accessible, relevant, and personalised. Just enter your basic details like age, land type, land size, state, and crops, and AgroSathi recommends the most relevant government schemes, subsidies, and crop insurance options.\nAgroSathi uses a multilingual UI built in Streamlit and currently supports 9 Indian languages, including English, Hindi, Marathi, Telugu, Tamil, Gujarati, Bengali, Kannada, and Malayalam. In a country as linguistically diverse as India, having a solution in only one language leaves millions behind. By supporting regional languages, AgroSathi ensures that farmers from every corner of the country can access crucial government information in a language they truly understand.\n\n**2. Architecture Overview**\n\nThe architecture combines Streamlit (for UI), Google Colab (for model runtime), and ngrok (for exposing the app via a public URL).\n\n**3. Using Google Gemma 3N**\n\nWe used the instruction-tuned variant:\n`google/gemma-1.1-2b-it` from Hugging Face.\nThis was integrated using the transformers library. Prompts were constructed based on user input and passed to the model for inference:\n```python\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-1.1-2b-it\")\n```\nWe use greedy decoding with light sampling (`top_p`, `top_k`) to balance speed and output diversity.\nEach prompt is localised (e.g. in Marathi or Hindi) using hardcoded, language-specific templates.\n\n**4. Multilingual Engineering**\n\nAgroSathi supports 9 Indian languages:\n- English, Hindi, Marathi, Telugu, Tamil, Gujarati, Bengali, Kannada, Malayalam\nThe form’s UI dynamically changes text and dropdown options (land types, crops, states) based on the selected language. Prompts to the model are also generated in the same language to ensure better understanding by the LLM and improved relevance of schemes.\nExample prompt in Hindi:\n```python\nमैं महाराष्ट्र का एक किसान हूँ, मेरी उम्र 45 वर्ष है। मेरी 2 एकड़ असिंचित भूमि है। मैंने सोयाबीन बोई है। इस जानकारी के आधार पर, मेरे लिए उपयुक्त सरकारी योजनाएं, सब्सिडी या फसल बीमा विकल्प बताएं।\n```\n\n**5. Running the App on Google Colab**\n\n1. Open [AgroSathi.ipynb](https://colab.research.google.com/drive/1VgQM6giEqrXrv1lVIJE6A2bTUfz9apvq#scrollTo=uiJlrAl0EPgp)\n2. Upload both [app.py](https://github.com/vishalpatil1202/AgroSathi/blob/main/app.py) and [gemma.py](https://github.com/vishalpatil1202/AgroSathi/blob/main/gemma.py) (Both files are present in the GitHub repo)\n3. Run all cells in the notebook.\n4. After a few seconds, you'll get a public ngrok URL to access the UI in your browser.\n\n**6. Why This Stack?**\n\n- Streamlit allowed rapid prototyping and multilingual UI with minimal code.\n- Google Colab provided free GPU access, making large-model inference possible without hosting.\n- Gemma 2B-Instruct was lightweight enough to run in Colab, yet powerful for prompt-based outputs.\n- Ngrok enables sharing the Streamlit app without needing a cloud deployment.\n\n**7. Future Work**\n\n- Deploy on Hugging Face Spaces for permanent access.\n- Add voice input/output for more accessibility.\n- Track usage and feedback for iterative improvements.\n- Fine-tune Gemma on real government scheme FAQs to boost response accuracy.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/vishalpatil1202/AgroSathi/tree/main",
        "demo_video_url": "https://youtu.be/EiICnAe74gk?si=r200iX3by7ISnNcc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/agrosathi-bridging-farmers-and-government-support"
    },
    {
        "project_title": "SheGuard: Offline-First Safety App Powered by Gemma 3n",
        "tagline": "An AI-driven women’s safety app that works offline, responds to voice, and protects in real time.",
        "authors": "Amishi Verma",
        "summary": "🚨 SheGuard – Your Personal AI Safety Companion, Even Offline\n\nSheGuard is a women’s safety app designed with one clear goal:\n👉 To help you stay protected, even when there’s no internet.\n\nPowered by Gemma 3n running locally through Ollama, SheGuard listens for distress or danger through your voice and responds instantly — triggering an SOS alert and guiding you with smart AI assistance, without needing any online connection.\n\n🛡️ What Can SheGuard Do?\n🔊 SOS Button — Blasts a loud siren to alert people nearby\n🧠 Gemma AI Assistant — Understands your voice and detects signs of threat or emergency\n📞 Emergency Contacts — Quickly save and access the people who matter\n🌗 Dark/Light Mode — Easy on the eyes, anytime of day\n📵 Offline-First Design — No Wi-Fi? No problem. It still works.\n\n💡 Why I Built This\nIn critical moments, women often don’t have time to search for help — and poor internet shouldn’t make that worse.\nSheGuard is built to be quick, reliable, and accessible even in areas with limited or no connectivity. With the help of Gemma 3n’s local AI, this app becomes a real-time safety companion anyone can carry in their pocket.\n\n🛠️ Tech Behind the Scenes\nBuilt using React (Vite)\nIntegrated Gemma 3n via Ollama for real-time local inference\nUI prototyped using Figma Make\nUses the browser’s Speech Recognition API for live transcriptions\n\nBecause feeling safe shouldn’t depend on your signal bar.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/amishiverma/sheguard-offline-ai-safety-app",
        "demo_video_url": "https://youtu.be/1u1_4UP5gVk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sheguard-offline-first-safety-app-powered-by-gemma"
    },
    {
        "project_title": "OfflineMind",
        "tagline": "Eliminate reading interruptions - instantly explain any text in any app using Gemma 3n. No context switching, no tracking, pure focus.",
        "authors": "Sanyam Jain",
        "summary": "OfflineMind: Smoothing Reading Experience with Private, Offline AI",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Sanyam-jain30/OfflineMind",
        "demo_video_url": "https://youtu.be/KsZImCqb8C8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/offlinemind"
    },
    {
        "project_title": "Ollama Code Analyzer 🚀",
        "tagline": "Run AI-powered code analysis directly inside your editor — fully offline, private, and secure. Ollama Code Analyzer uses on-device AI models",
        "authors": "Jonatan Perales Gorba",
        "summary": "✨ Why Ollama Code Analyzer?\nIn today’s development landscape, AI assistants are becoming indispensable — but most require sending your code to the cloud.\nWith Ollama Code Analyzer, you get the power of AI with complete privacy:\n\n🛡 Private by Design – Runs entirely on your machine. No cloud. No leaks.\n📶 Offline-First – Works without internet. Ideal for remote work or travel.\n🐞 Deep Bug Detection – Finds subtle logic errors missed by traditional linters.\n⚡ Code Generation from Comments – Write /// or followed by your intent — AI writes the function.\n🔄 Context-Aware Refactoring – Improves performance, readability, and maintainability.\n📚 Instant Code Explanation – Understand unfamiliar code instantly.\n🧪 Automated Unit Test Creation – Uses frameworks like Jest or pytest.\n🗺 UML Visualization – Generates PlantUML diagrams for your whole project.",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": "https://github.com/John1987vlc/Ollama-Code-Analyzer",
        "demo_video_url": "https://www.youtube.com/watch?v=yx1AGpsQigw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ollama-code-analyzer"
    },
    {
        "project_title": "MAZU | Offline Disaster AI Assistant",
        "tagline": "AI assistant that works completely offline to help people survive disasters when no other help is available",
        "authors": "YangAurora, hutong, Eileen Yang",
        "summary": "Imagine this scenario: You're in the middle of a major earthquake. Buildings are shaking, power is out, and your internet connection is gone. You need immediate help, but your phone can't access Google or call emergency services. Our mission was to build an AI assistant that works completely offline to help people survive disasters when no other help is available.\n\n#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Aurora-yang-git/MAZU-Offline_Disaster_Assistant",
        "demo_video_url": "https://www.youtube.com/watch?v=VfJikuZMR4E",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/mazu-offline-disaster-ai-assistant"
    },
    {
        "project_title": "My (Jetson) Gemma",
        "tagline": "Jetson offline AI companion: Real-time voice, safety for all ages, customizable, fast, battery-powered, backed by privacy, and more!",
        "authors": "Stephen Murphy",
        "summary": "Jetson offline AI companion: Real-time voice, safety for all ages, customizable, fast, battery-powered, backed by privacy, and more.\n\n\n#",
        "technologies": [
            "CUDA",
            "Gemma 3n",
            "NVIDIA Jetson",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Halsted312/Googalsted.git",
        "demo_video_url": "https://youtu.be/WZ93bcTmxw0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/my-jetson-gemma"
    },
    {
        "project_title": "Gemi: The Private AI Diary",
        "tagline": "The private, offline AI diary powered by Gemma 3n. A secure, intelligent, and multilingual sanctuary for your most personal thoughts.",
        "authors": "Chaeho Shin",
        "summary": "The Vision: Solving the Real Barrier to AI in Mental Health\n\nThe world is facing a silent mental health crisis. One in five Americans suffers from a mental illness, yet nearly six in ten receive no treatment (NAMI, 2024). AI has long been promised as a scalable solution. However, a significant paradox stands in the way.\n\nGroundbreaking research from Anthropic revealed that while AI is surprisingly effective for emotional support—with conversations ending more positively and harmful spirals being rare—**only 2.9% of Claude AI conversations are actually for this purpose** (Anthropic, 2025).\n\nThe barrier isn't AI's capability; it's a fundamental crisis of trust. Users are rightly hesitant to share their deepest thoughts with cloud-based systems that may use their data for training or retain it indefinitely. Gemi was built to solve this exact problem, directly answering the hackathon's call to create a private, offline-first product that makes a tangible difference in people's lives.\n\nThe Solution: A Privacy-First Sanctuary Powered by Gemma 3n\n\nGemi is a revolutionary journaling application that delivers the full power of a next-generation AI companion while guaranteeing absolute privacy. Its core principle is \"Your Privacy, Preserved,\" a commitment reflected in every line of code.\n\nBy leveraging **Gemma 3n's** optimized on-device performance and **Ollama's** local inference engine, Gemi operates 100% offline. All journal entries and AI-generated memories are protected with military-grade **AES-265-GCM** encryption, with keys stored securely in the macOS Keychain. With **zero telemetry** and full compliance with the macOS App Sandbox, Gemi ensures a user's most personal data never leaves their device.\n\nInnovative Execution: Thinking Bigger Than a Chatbot\n\nGemi showcases an innovative and deep integration with Gemma 3n's unique capabilities, going far beyond a simple chatbot to create a truly intelligent partner.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/perpetual-s/Gemi",
        "demo_video_url": "https://youtu.be/NKhyMWbTi2E?si=EYiKzqBbbjaHAe_M",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemi"
    },
    {
        "project_title": "AccessibilityGem ",
        "tagline": "AI-powered screen reader that gives blind users rich descriptions of any Android screen using on-device Gemma 3n",
        "authors": "Glen Garry",
        "summary": "**Problem Statement**\nWhile screen readers like TalkBack have begun integrating AI-powered image descriptions, these features remain inconsistent across Android devices and limited in scope. Even when AI descriptions are available, they are often generic, lack context, and cannot be customized to suit individual preferences or tasks. Users frequently encounter repetitive and unhelpful cues on visual only elements leaving them without the rich visual context that sighted users rely on.\nIn short, while AI is starting to improve accessibility, today's solutions are still partial, inconsistent, and not user-configurable — leaving many blind users without a reliable way to understand the full content of their screens.\n\n**Solution**\nAccessibilityGem bridges this gap by using Gemma 3n’s multimodal capabilities to generate intelligent, descriptive interpretations of any Android screen — all processed entirely on-device using Google AI Edge for maximum privacy and offline functionality.\n\n**Key Features**\n**Three Intelligent Modes**\n- UI Layout Mode – Describes the overall screen structure and navigational flow.\n- Image Mode – Explains visual and photographic content on the screen.\n- Focused Element Mode – Gives detailed context about the currently selected item.\n\n**Seamless Accessibility Integration**\n- Compatible with Android’s native Accessibility Service\n- Works alongside TalkBack, with controls via volume buttons\n- Customizable prompts to adapt to individual user needs\n\n**Technical Implementation**\nAccessibilityGem is built on Google AI Edge's MediaPipe LLM Inference API, combining:\n- On-device Gemma 3n\n- Android’s Accessibility Service API for full-system coverage\n- Real-time screenshot analysis with adaptive cropping\n- Streaming inference for quick spoken feedback\n\n**Impact**\nAccessibilityGem redefines accessibility for people with visual impairments. By enabling private, intelligent screen understanding anywhere — even offline, it empowers blind users with a more independent, intuitive, and seamless mobile experience.\n\n**Project Links**\n1. GitHub Repository: https://github.com/GlenGarryleads/AccessibilityGem\n2. Demo Video: https://www.youtube.com/watch?v=5Mj_tho8ARw\n3. Download APK: https://github.com/GlenGarryleads/AccessibilityGem/releases/tag/v0.1.0",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/GlenGarryleads/AccessibilityGem",
        "demo_video_url": "https://youtu.be/5Mj_tho8ARw",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/accessibilitygem"
    },
    {
        "project_title": "GreenVision AI",
        "tagline": "A detection system using computer vision and Flask, enhanced with Gemma AI voice assistance.",
        "authors": "OmPanchal07",
        "summary": "**GreenVision AI is a lightweight, real-time leaf detection system built using OpenCV, Flask, and machine learning. It captures webcam input to identify plant species and integrates Gemma AI voice assistance to provide instant, interactive information about each detected leaf. Designed without heavy ML frameworks, it is fast, efficient, and accessible on standard systems. This project encourages biodiversity education, sustainability, and environmental awareness, making it ideal for students, researchers, and nature enthusiasts. By combining AI-driven detection with a simple web interface, GreenVision AI demonstrates how intelligent, practical tools can connect technology with ecological learning.**",
        "technologies": [
            "Flask"
        ],
        "source_code_url": "https://github.com/ompanchal05/Gemma3nChallenge",
        "demo_video_url": "https://youtu.be/YBYeiouXLqM?si=5n9h0SYccpaOXPis",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/greenvision-ai"
    },
    {
        "project_title": "AI in Rural India: Digitizing Health Records for ASHA workers",
        "tagline": "ASHA-G - An Offline AI App for Form Recognition, Hindi Transcription and Privacy-First Digitization powered by Gemma 3n",
        "authors": "MKS, Parul Pandey, dwall",
        "summary": "This write-up details Smart Asha Form App, our submission for the Gemma Impact Challenge. This app is designed to address critical data collection challenges faced by India's rural health workers, known as ASHAs (Accredited Social Health Activists). By leveraging the multimodal capabilities of Google's Gemma 3n model, our app provides a fully offline, on-device solution for digitizing paper forms using voice.\n\nThe workflow is simple. ASHAs photograph an unfilled form, the app extracts the fields, and they complete them by speaking in their native language. We demonstrate Hindi, but the app can support any language enabled by the Gemma 3n model. This approach prevents data loss, improves accuracy, protects privacy, and delivers timely, structured data to public health systems, showing a practical, high-impact application of AI in resource-limited settings.\n\n1. ASHAs: the backbone of India's rural health system\n\nIn the Hindi language, the word **Asha** means **hope**. In rural India, that hope takes shape in the form of [**Accredited Social Health Activists or ASHAs**](https://en.wikipedia.org/wiki/Accredited_Social_Health_Activist?utm_source=chatgpt.com). As community health workers, they provide essential services door-to-door, from maternal care and immunizations to health education. This [initiative began](https://nhm.gov.in/index1.php?lang=1&level=1&sublinkid=150&lid=226) in **2005** and is now a part of the **National Health Mission (NHM)**. Under this mission, every village was to have an ASHA, accountable to her community and supported by the health department. By **2018 alone**, India had trained **about 940,000 ASHAs**, making the [ASHA program](https://en.wikipedia.org/wiki/Accredited_Social_Health_Activist?utm_source=chatgpt.com) the largest government-led community health worker program in the world. [Studies](https://human-resources-health.biomedcentral.com/articles/10.1186/s12960-019-0402-4?utm_source=chatgpt.com) show that ASHAs significantly improve access to maternal services, including institutional deliveries and antenatal care, especially in poorer and marginalized groups.\n\n**_For many villagers, the ASHA is the_** [**_first point of contact_**](https://human-resources-health.biomedcentral.com/articles/10.1186/s12960-019-0402-4?utm_source=chatgpt.com) **_in healthcare and often the only trusted person offering medical guidance._**\n\n![](https://cdn-images-1.medium.com/max/1600/0*aEg43dYpxo7QgXc1.jpg)\n\n*Gopalan SS et al. Assessing community health workers’ performance motivation: A mixed-methods approach on India’s Accredited Social Health Activists (ASHA) programme. BMJ Open 2012; 2: e001557.*\n\nASHAs are also essential data collectors for India’s rural health. During their visits, they not only provide services such as home visits, referrals, and nutrition counseling but also document their work at the village level. They maintain registers and diaries to record vital information, including pregnancies, immunizations, births, and deaths. These records feed into national health monitoring systems, enabling the government to track population health and plan interventions effectively.\n\n2. Challenges of Paper-Based ASHA Records\n\n![](https://cdn-images-1.medium.com/max/1600/1*4gB7qEBxYRmCBUPL0A8LtQ.png)\n\nWhile ASHAs record vital data, the current manual system of data collection is fraught with challenges:\n\n-   **Data loss and damage:** Handwritten registers are vulnerable. They can be damaged by weather, misplaced during transit, or simply never collected, which means that a significant portion of vital health data from the frontlines never reaches the health centers.\n-   **Data Entry errors**: Manual data collection is slow, error‑prone, and limits how many households ASHAs can visit each day. Incomplete entries, illegible handwriting, and duplicate reporting delay data flow.\n-   **Retrieval and accuracy issues:** A [recent study](https://pubmed.ncbi.nlm.nih.gov/40623352/#:~:text=various%20challenges%2C%20including%20poor%20visual,healthcare%20delivery%20in%20marginalized%20communities) found that ASHAs face **significant challenges** in using their paper ASHA diaries. Manual transcription from diaries to monthly reports is time-consuming and prone to error. Poor legibility and inconsistent layouts further complicate data aggregation and analysis.\n-   **Delayed reporting:** ASHAs typically transpose records from diaries into monthly reports by hand. This duplicate data entry wastes time and delays data flow upward.\n-   **Limited use of data:** With paper records, it is hard to visualize trends or identify gaps in care at the village level. Health supervisors cannot easily query and analyze records. As one [usability study](https://pubmed.ncbi.nlm.nih.gov/40623352/#:~:text=various%20challenges%2C%20including%20poor%20visual,healthcare%20delivery%20in%20marginalized%20communities) notes, because paper diaries are hard to search or summarize, they constrain data-driven decision-making.\n-   **Privacy risks:** Paper diaries containing sensitive personal health information are vulnerable to unauthorized access if lost or left unattended.\n\n**_These issues create a bottleneck, preventing crucial frontline data from effectively informing public health strategy._**\n\n\n3. Our Solution: ASHA-G - On Device, Multimodal Digitisation Agent\n\nTo address these challenges, we developed the **ASHA-G**(The G here stands for Gemma)- an on-device, multimodal digitisation agent. It combines vision and speech recognition in a simple, offline-first workflow. It works in a simple three-step process:\n\n![](https://cdn-images-1.medium.com/max/1600/0*xcHAXvAsTc07aKvY)\n\nWorkflow of out Smart Asha Form App\n\n1.  **Snap a Photo:** The ASHA worker captures an image of any standard paper health form using their device.\n2.  **Speak to Fill:** Using Gemma 3n’s vision capabilities, the app instantly detects the form’s fields. The worker then fills each field simply by speaking the information in Hindi, which the app transcribes into Devanagari script in real time.\n3.  **Get Digital Data:** Once completed, the app exports the information as a structured `.csv` file, ready for secure storage, transmission, and integration into larger health information systems.\n\nOur app works offline, supports Hindi voice input and Devanagari script and works directly on the device.\n\n\n4. Benefits of Offline, On-Device Processing\n\nOur choice to build an on-device application was deliberate and essential for creating a tool that is both practical and responsible.\n\n![](https://cdn-images-1.medium.com/max/2400/1*INnoQ3w3am4Pzt1M71GYHA.png)\n\n-   **Privacy by Design:** Because all processing — from image analysis to voice transcription happens locally, sensitive health data never leaves the device. This eliminates the privacy risks associated with cloud uploads.\n-   **Zero Latency:** With on-device processing, the app’s response is instant. There is no waiting for servers, making the user experience smooth and efficient, which is critical for daily, repetitive tasks.\n-   **Offline First:** The app is **rural-ready**. It functions perfectly without any internet connectivity, overcoming one of the biggest hurdles to technology adoption in remote areas.\n-   **Prevents Data Loss:** By enabling immediate digitization, our app ensures that every piece of data is securely captured and stored the moment it is collected, preventing the physical loss of paper records.\n\n\n5. Technical Architecture\n\nOur prototype is built using Python and Gradio, with a focus on efficiency and on-device processing. The architecture is designed to be robust and function entirely without an internet connection.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/m6VapEXslz8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-in-rural-india-digitizing-health-records-for-as"
    },
    {
        "project_title": "NeuroFlow: Democratizing AI Learning for the Offline World",
        "tagline": "The first offline and accessible AI tutor for disconnected learners—powered by Gemma 3n, RAG memory, and multimodal understanding.        ",
        "authors": "Ashwath Karunakaram",
        "summary": "NeuroFlow: Democratizing AI Learning for the Offline World",
        "technologies": [
            "Flask",
            "Gemma 3n",
            "Ollama",
            "Web Speech API"
        ],
        "source_code_url": "https://github.com/AshwathKarunakaram/NeuroFlow",
        "demo_video_url": "https://youtu.be/GdSmIAPMRVU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/neuroflow"
    },
    {
        "project_title": "Disaster Damage Assessment with Gemma3n on Jetson Nano",
        "tagline": "Offline edge-powered web app using Gemma3n (via Ollama) on Jetson Nano for UAV imagery disaster damage assessment.",
        "authors": "Ilham Akbar",
        "summary": "AI-Powered Disaster Damage Assessment Pipeline\n\nThis project introduces an **edge-deployable, AI-powered pipeline** for **near real-time disaster damage assessment** using **UAV imagery**. It is designed to operate directly in the field, enabling **search and rescue (SAR) teams** and emergency responders to obtain actionable insights quickly, even in areas with limited connectivity.",
        "technologies": [
            "Docker",
            "Flask",
            "Ollama"
        ],
        "source_code_url": "https://github.com/ilhamije/gemma-disaster-management",
        "demo_video_url": "https://youtu.be/qAUFMmxTvGI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/disaster-damage-assessment-jetson-nano-and-gemma"
    },
    {
        "project_title": "Sentinela: Watch What Matters to You",
        "tagline": "From your stress. To your treasures. To your joy",
        "authors": "bedorlan, MARICELA CORAL",
        "summary": "Sentinela: Watch What Matters to You",
        "technologies": [
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/kWFawrvEBpc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sentinela-watch-what-matters-to-you"
    },
    {
        "project_title": "Pick-and-Place with Gemma 3n: Integrating VLA Policy into LeRobot on SO-ARM100",
        "tagline": "Vision-Language-Action Policy with Gemma 3n: End-to-End Robotic Pick-and-Place on SO-ARM100 Using LeRobot",
        "authors": "Juan Pizarro",
        "summary": "#",
        "technologies": [
            "CUDA",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/jpizarrom/lerobot/tree/add_gemma3nvla/src/lerobot/policies/gemma3nvla",
        "demo_video_url": "https://youtu.be/7s-kMm1_4CA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/vla-with-gemma-3n-lerobot-and-so-arm100"
    },
    {
        "project_title": "3R-GEMMA: Edge-Powered AI for Real-Time, Mixed-Material Waste Classification",
        "tagline": "An intelligent, gesture-driven edge system that helps users sort complex waste accurately—powered by fine-tuned Gemma 3n",
        "authors": "man fye, jackylee, Yuyin20, Shee Tze Jin",
        "summary": "",
        "technologies": [
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/manfye/3R-Gemma-Hackathon-Repo",
        "demo_video_url": "https://youtu.be/LfzDSDwDGtY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/3r-gemma-edge-powered-ai-for-real-time-mixed-mater"
    },
    {
        "project_title": "child's friend , old man's partner and  people‘ spiritual Home",
        "tagline": "help remote people take care of the child and old man with privacy ,listen to depressed people‘ trouble anytime provide positive energy ",
        "authors": "tiantian",
        "summary": "Project One:'baby Friend' or children's Friend\n\n      There are many Left-behind children worldwild,who are lack of companionship and love.Parents have to leave their baby home due to the work in a big city or any other reason for reality.They worry about the child every moment,Is he lonely in the home?Or No one reply to baby right away? How is the baby feeling?\n      Baby grow up in a Left-Behind environment, often develop weak language skills,have poor sense of happyness, which is a pitty even when he grow up.So Can the technology tool sovle this social problem?\n      With the help of Gemma 3N,we can develop a good partner of left home baby, a good helper of worried parents at home，which can protect the family and baby privacy completely on the offline mode.We plan to develop a App called 'Baby Friend'.It can also be put in a baby's toy like bear doll,playing the role of a toy or pet with more intelligence,and could talk to.\n     The parents can talk to the 'Baby Friend',teach it how to take care of the baby.such as the name ,hobby, personality，like and dislike,or other details and tips and cautions.How to calm the baby when he feel down?How to comfort him when he miss mum?The parents can upload some voice of their interaction with the baby.The 'Baby Friend' will learn and mimic the parent's sound ray.The 'Baby Friend' can learn the pattern or features of the parents  futher.When the parents go home after a day or several days or several hours,'Baby Friend' can give a report about her baby,How the baby feel,or what subject the baby always talk,which story the baby most like,what is the baby's hope? For example more hugs from parents.How the Language skills and thinging skills develop? what capability is the baby good at and weak of?\n\n       When feel lonely or boring,the baby will try to chat with her toy like a bear with the 'Baby Friend' hide in.The baby speak to 'Baby Friend','Baby Friend' transform the voice to language, then reply to the baby with the voice.The baby can also put in a picture to 'Baby Friend' and ask quesions.In this mode,'Baby Friend' will serve the baby using the chat_casual() function.The baby can upload a picture she draws,and 'Baby Friend' give a reply and encouragement.\n\nThere are other situations 'Baby Friend' handle using other special functions.\n\n1.When baby want to listen to storys, 'Baby Friend' will accompany the baby using the chat_story() function.In the storytelling mode,'Baby Friend' can perform the story vivid using voice.it is ready to stop and reply to the baby's questions.It is the interactive story telling style that mimic real mum tell story and baby think,feel and ask question.\n\nchat_story() function is better than chat_casual() in several ways.It have more storys to choose,like Andersen's Fairy Tales，Disney story，Peppa Pig，Fairy tale，Mythological story all around the world, Japan anime story like One Piece.In's the collection of the excellent story globally that is suited for baby,including diffrent culture and nation.The story can unlock baby's imagination,enhance baby's feeling perception, improve cognitive ability, promote language development,reshape her character.\n\nWhen a baby want to listen to story,chat_story() choose a story,and act the story vivid using voice that is attactive for the baby.The baby can immerse in the plot,the baby may ask a question,chat_story() will stop story telling,and reply baby's question with patience and love, then go on story telling.The baby can talk about how him feel when he listen the story, and discuss with 'Baby Friend'.\nIn a short, chat_story() have more excellent story all of the culture and nation.chat_story() can act the story vivid that is attactive.chat_story() can interative with the baby when tell a story,answer questions,discuss the plot and feeling of the baby.\n\n2.When baby want to play house or other role play games.'Baby Friend' will play with the baby using the chat_roleplay() function.The baby like to Fictitious a situation and play with her toy, other frineds,or 'Baby Friend'.'Baby Friend' will \n cooperate with the baby and play with him happyly.\n For example,the baby will play the role of a doctor,and learn to help a patient.Following the demand of the baby,'Baby Friend' pretend to be a patient, feel very uncomfortable, reply to the baby's inquiry, and Cooperate with the baby to take the injection,and take medicine,then he pretend to recover health and thank the baby.\n Or the baby will play the role of a parent to take care of small baby ,'Baby Friend' will play role of a weak baby.\n\n Is it intersting? Playing is human or animal's nature,Through playing games,the baby's learn how to solve the problem,take responsibility, overcome difficulties,and gain  a sense of fulfillment and confidence.When playing games,it's like a simulation of real situation and grow-up.Playing role-play games have many benefitions of the baby,and is a nature of the baby,So 'Baby Friend' will take the roleplay games seriously,and accompany the baby to play and grow.\n when the child listen to the story,he may want to play the role of the classical role ,he may choose a situation of the classical film,and ask the 'Baby Friend' to act with her,like The Evil Stepmother of Snow White ask the MagicMirror who is the prettiest woman in the world?It's the extension of chat_story() function,after listen to a story,could act it out.\n\ncompare with the chat_casual() function, chat_roleplay() is good at playing role play games with the growing-up baby,it is more open and full of imagination.It can suit to the baby's ability and incline of his age.\n\n 3.When baby feel depressed or other bad emitions,'Baby Friend' will coax the baby with the chat_emotion() function.\n when the baby feels bad,'Baby Friend' will coax the baby with any way.Cheer Up  the baby with all kinds of efforts.\n it will  perform analysis the reason ,and respond accordingly.When the baby feel discarded by their parents,ask why mum not stay by their side,'Baby Friend' will comfort the child,mum is doing something important,she miss the baby and will come back home soon. It will direct the baby to a more active emotion.But when the baby is sick ,have a fever,Baby Friend' will contact with their parents immeditely.\n\n Compare to the chat_casual() function, chat_roleplay() is more good at handing with emotions,good at coaxing and comforting the baby.It take the parents's advice and instuct of how to coax the baby,and explore which way is right for the baby.\n\n At last, the parents come home,and ask the 'Baby Friend' how the baby bahave when they are left.'Baby Friend' will use the chat_report() function\n recall the interaction with the baby,analysis and reason ,give a report of the baby.For example,\n Xinxin like to play role-play games recently, she is more creative than before,and her language skills develops quickly.\n she draw many pictures,seems to be intested in drawing right now,You can consider to buy some drawing books for her.she is happy most of the time.Or jary feel lonely and discarded for some time,and I can not coax him easily,please take more attention for him and accompy him for longer time.consider to hug him more.  \n\n\nProject Two: old man partnership\n\nMore and more countries step into the Aging society, old man need partnership and love,but young people are often survive their own work and family and have no enough time and resource to take care of the old man very well.\nOld man often have weak enegy and stay at home for long time in a day,often feel neglected by the family ,having  poor \nSense of participation of life,feel isolation from the society and poor sense of meaning and happiness.  \n\nWith the help of Gemma 3N,we can build a product that can  improve old man's life quality,promote their sense of happiness.'old man partnership' is built for improving the health and happiness of the old man.\n\nMost of the app and product is designed for the young man,ignoring the demand of the old man,or lack special desigh for the old man.'old man partnership' is built for helping the old man,and meet their demand.We care about their health and mantal health.\n\nchat_casual() can accompany the old man and reply to old man demand through voice,without need to type in the text.\nand suit to old man to speak slowly and loudly,beacuse they react slowly and have poor hearing ability.\n\n\nchat_health() is proficient in medical knowledge,and aged care, and symptoms of illness,is a good health assistant for the old.when the old man talk about his health conditions,chat_health() serve the old man.it can remind the old man to take medicine on time, provide medical info.\n\nIt will provide medical info when old man ask,for example, when old man is not comfortable and measure blood pressure\nchat_health() will tell old man if the blood pressure too high ,or is not dangerous?\n\nIf chat_health() judge old man may have some disease,it will suggest old man to go to hospital.For example old man tell it find blood in stool,Or it find the old man's language and cognitive ability decline seriously, it will suggest the old man to suspect Alzheimer.\n\nIf it is emergency,for example, old man have some premonitory signs of stroke,it will contact the guardian to take action .if no one respond,it will  call the hospital for help using the address info guardian tell it.\n\n\n\nchat_news() is built for the news and info demand for the old man. If old man care about society,chat_news() will provide \nsociety news,and could talk about the news with the old man.The old man still have a sense of society participation.\nif old man like to listen family info,like family ethics,chat_news() will tell story of many family which is hot recently.\nchat_news()  could choose the right news according the old man's need,and could tell about the news interatively.\nFor example,Is there something hot recently?yes,our city build a new airplane ,and will open this month.Oh,that's great!My son could come here more conviniently.Yes,it will save some time and money for Tom to come here!\nOr Your favarite football player win the competition this week!Oh, he is great,my hero.\n\n\n\nchat_playgames() is built for the games that is suited for the old man.It is more slow but still interesting,and could get happiness,and help to maintain cognitive ability.For example,the guessing games\n\n\nchat_request_from_guardian() is built for the guardian to teach the 'old man partnership' how to take care of the old man,For exmple,Her name,age,his disease history and health records ,character, hobby,likes and dislikes.For example,please help me to take care of my mum,She is 80 years old,she has Short Temper ,please remind her to slow down,and rest a lot.She has diabetes disease, need to take dtugs three times every day,please remind her to take it on time.She is a fan of Dongyuhui.She like to listen to talk show and other language-style jokes.\n\n\nchat_report() is built for the guardian to ask 'old man partnership' about old man's circumstance.chat_report() first recall the records about the old man and give a report about health and mental health.For example,Your mum take the drug on time quite well.She have a happy time this period.Do not worry.Or He has a lumbago recently,and seems to increase. He is quite depressed and talk little.Consider to take him to hospital.\n\n\n\n\n\nProject Three: Depressed man's tree hole\n\nWhile the child and old man is weak and need care and love from others and is easy to be isolated to the family，often feel lonely.The adult's life is not easy either.They often suffer from the stress and setbacks.There are painful and unbearable days even for the most lucky man.\nAdolescent may suffer from academic pressure and other adolescent troubles such as anxiety or conflicts.\ngraduate may suffer from frustrating in finding a job.Adult may lose jobs or breakup or divorce.Life is painful.\nThey may lack enough support from around.Some one could survive,others can not.\nCan technology progress help people to relieve pain and promote happiness ?\nYes, technology should help many people,give them enough support,give them professional guidance.In the past,people survive a  mental crisis with luck,But now,we could support people with the technology method,we could  bulid a AI product with  standardized  and copyness , that could serve many people with high-quality mental support. and cost little.\nPeople around the world should find good mental support if they need it, Then the technology progress really benefit mankind,not only to provide convenience,but also to promote quality of life and happiness index.\n\nDepressed man's tree hole is designed to help people to get rid of pain and trouble,with more positive energy and sense of happiness.\nThere are two mode,offline and online mode.\noffline mode is for people who care about privacy and tell her trouble secretly.Many people have no one to tell his trouble or get enough support from people around.They can try to tell 'tree hole'.'tree hole' is full of love and patience and time and empathy and professional competence,also could keep a secret.like a perfect friend and doctor.\n'tree hole' will listen to people with patience and encourage people to express their trouble and feeling.ask questions to inspire people to see the good side or complete scene.maybe it is not bad completely,and there will be a turning point and hope.'tree hole' is not only to provide positive energy,but can also objective analysis of problems. 'tree hole' have good balance of sensibility and rationality.it can comfort people before and help people clarify their thoughts after.\n\n'tree hole' alse know it's capability boundary,when it judge people need more help or treatment from hospital,or emergency intervention, it will give suggestions for people to receive professional treatment right now.Or where to get human help.\n\n\nonline mode is for people not care about privacy that much,and want to interative with real human.Then he will find a community under online mode's suggestions where there are people with same trouble gather,and help  and encourage each other, ‌this will hold together for warmth,and let people not lonely again.\n\nonline mode alse have some professional psychologist,who people could contact and receive professional treatment.\n\n\n\n\n\n** The Technical Writeup**\nthe AI product for the child and old,In's more natural and convinient to use the voice to interact.\nGemma 3N can handle voice,picture and text and reply with text,without the voice.So we can use another Text-To Speech model together with Gemma 3N to finish the function of voice ask, and voice reply.For example the CosyVoice,a good TTS model which can imatate diffrent emotion and intonation. Consider the low-resource envinronment as the Gemma 3N will deploy,We choose a samall-parameter model, CosyVoice2-0.5B.\nWe will cascade the Gemma 3N and CosyVoice2-0.5B,the Gemma 3N output the reply text and tone and emotion instruct of the voice needed,the CosyVoice2-0.5B then output the voice of reply text follong the tone and emotion instruct.\n\nchat_withvoice()\ninput: voice of the child and old\noutput:voice of the model reply\n{\ninputvoice = getvoice_from_microphone() #get the input voice through microphone \ntext_reply, voice_instruce = model_reply_to_inputvoice(inputvoice) #use Gemma 3N\noutputvoice = transform_text_to_voice(text_reply, voice_instruce) #use CosyVoice2-0.5B\n}\n\nThe overal design of the 'Baby Friend' is as follows:\n\nchat_request_from_parents() is for parents to ask for help to take care of her baby.\nchat_casual() is for 'Baby Friend' to company the baby. interative with the baby.\nchat_story() is for 'Baby Friend' to tell story interatively for the baby\nchat_roleplay() is for 'Baby Friend' to play house or other role-playing games with baby\nchat_emotion() is for 'Baby Friend' to coax the baby when baby feel down\nchat_report() is for parents to ask for report about her baby when they are away.\nLet_secret_disappear() is for erase all info about the baby and family\n\n\nthe main function will serve and judge which function it should jump to.\nif parents is ready for leave and chat_request_from_parents() serve,it will record all the info about the baby.\nif baby is there and chat_casual() reply to the baby,answer her questions,praise the baby when baby share her pictures.\nif baby want to listen story,then chat_story() accompany the baby.\nif baby want to play games, chat_story() play with baby.\nif baby feel bad, chat_emotion()  is used to coax baby.\nchat_casual() and chat_story() and chat_emotion() and chat_roleplay() will record the interactory with the baby.\nwhen parents come back and want to know the baby's performance, then chat_report() recall the record and give a report.\nwhen parents want to destroy the privacy,then Let_secret_disappear() will destroy all the record about the family.\n\n\n\nThe overal design of the 'old man partnership' is as follows:\n\n\nchat_health() is for 'old man partnership' to help the old man  when he is under poor physical condition,or advise to take medication on time.When 'old man partnership' judge old man need to go to hospital,it will contact guardian.\nchat_news() is for 'old man partnership' to tell the news around the word which old man is interested.For eample,\nsociety news or Family relations.in's for old man to get the sense of participation of life.\nchat_request_from_guardian() is for guardian to ask for help to take care of old man.his health conditions and other info.\nchat_casual() is for 'old man partnership' to company the old man. interative with the old man.\nchat_playgames() is for 'old man partnership' to play small games and excecise cognitive abilities‌.like guessing game.\nchat_report() is for guardian to ask for report about old man .\nLet_secret_disappear() is for erase all info about the old man and family\n\n\n\n\nthe main function will serve and judge which function it should jump to.\nif guardian ask for help and chat_request_from_parents() serve,it will record all the info about the old man.\nif old man is there and chat_casual() reply to the old man,answer her questions\nif old man want to listen news,then chat_news() accompany the old man.\nif old man want to play games, chat_playgames() play with old man.\nif old feel bad, chat_health()  is ready to help.\nchat_casual() and chat_story() and chat_emotion() and chat_roleplay() will record the interactory with the old man.\nwhen guardian come back and want to know the old man condition, then chat_report() recall the record and give a report.\nwhen guardian want to destroy the privacy,then Let_secret_disappear() will destroy all the record about the family.\n\n\n\nthe AI product for the people  who need emotion support , do not need voice interaction urgently because adult could spell the text which the child could not and old man is not convinient.But There is an urgent need for emotion support and professional help.Because adult is often live under pressure and setbacks.\nSo we pay attention to enhancing empathy skills and professional capability.We find a  Psychological Counseling Professional Corpus called Emotional First Aid Raw Dataset, which is released by Chatopera.Another Corpus called SoulChatCorpus\nwe will finetune Gemma 3N using Psychological Counseling Professional Corpus with Unsloth framework.\n\n\n\nThe overal design of the 'tree hole' is as follows:\nthere are offline-mode and online-mode.offline-mode is for people who want to protect privacy with mild annoyance‌.\nBut if 'tree hole' judge the people have serious problem and need more professional treatment from hospital.It will urget the people to go to hospital.\nonline-mode is for people who do not care about privacy and want to find a community or human's interaction.\n\n\nthe AI product for child and old and people who need mental support all need to protect privacy.So the three product all work in the offline-mode.People can erase the records they use the pruduct.Then 'baby frend' know nothing about the baby any more,and the data could not transfer to the cloud and do not have records in the local app,mobile or other equipment.\nWe will develop a function called Let_secret_disappear(),people click a secret_disappear button ,then Let_secret_disappear() work and clear up all the info of the people and let the secret go with the wind.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/QqfQgesGQnk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/childrens-friend-and-old-mans-partnership-and-depr"
    },
    {
        "project_title": "TruthSync - AI-Powered Real-time Journalism System",
        "tagline": "Sharing Real-Time Discoveries, Fighting Human-Created Fake News with AI.",
        "authors": "deoksangryu",
        "summary": "TruthSync: Technical Writeup for the Gemma 3n Impact Challenge\n1. Project Overview\nTruthSync is an innovative, real-time journalism platform designed to combat the spread of fake news by leveraging Google's Gemma-3n-E4B-it model. The system empowers citizen journalists to create authentic, location-based news articles and a crowdsourced, on-the-ground verification mechanism. This project directly addresses the hackathon's core theme by demonstrating Gemma 3n's multimodal, real-time, and resource-efficient capabilities to create a tangible positive impact.\n\n2. Technical Architecture\nThe TruthSync application is built on a modern, full-stack architecture that is optimized for mobile-first, low-resource environments.\n\nFrontend (Angular)\nThe user-facing application is an Angular single-page application (SPA). Its modular structure is designed to be responsive, accessible, and intuitive.\n\ncamera/: Handles real-time device camera access, photo capture, and the collection of essential metadata (GPS, orientation).\n\nevaluation/: Facilitates the location-based verification process, allowing users to vote on nearby articles.\n\nservices/: A suite of services (ai.service.ts, location.service.ts, post.service.ts) manages communication with the backend, device sensors, and local data.\n\nBackend (FastAPI + Gemma-3n)\nThe backend is a high-performance server built with FastAPI, acting as the central hub for AI processing, data management, and real-time streaming.\n\nImage Processing: Before being fed to the model, images are processed to handle EXIF metadata (e.g., orientation), ensuring the input is correctly formatted.\n\nAI Model Integration: The FastAPI server orchestrates the Gemma-3n pipeline. It receives image data and user prompts, then streams the generated text back to the client.\n\nStreaming Response: A key technical innovation is the use of real-time text streaming, powered by Server-Sent Events (SSE), to provide a responsive and engaging user experience.\n\n3. Implementation of Gemma 3n\nThe core innovation of TruthSync lies in its specific and controlled use of the Gemma-3n model.\n\nMultimodal Input: The model is fed with both visual (the captured photo) and textual (a system prompt and optional user description) data to generate a detailed news article. This directly utilizes Gemma 3n's multimodal capabilities.\n\nSystem Prompt for Controlled Journalism: A carefully crafted system prompt (당신은 뉴스 기자 입니다. 이미지를 통해 상세히 기사를 작성해주세요.) ensures the model adopts the persona of a journalist, thereby controlling the tone and style of the generated content to be factual and report-like.\n\nEfficient Resource Management: By deploying Gemma 3n as a selective-parameter-activated model (8B parameters run with 4B memory footprint), the backend is optimized for lower computational requirements. This ensures the system remains responsive, even under moderate load, and demonstrates the model's suitability for a wide range of devices.\n\n4. Global Impact\nTruthSync's location-based verification system has the potential to become a powerful tool for fighting misinformation and promoting authentic journalism. By democratizing the news creation process and introducing a community-driven verification layer, it empowers local communities and enhances transparency in journalism.\n\n5. Conclusion\nTruthSync is a technically sound and socially impactful application that showcases the power and flexibility of the Gemma-3n-E4B-it model. By combining a modern full-stack architecture with strategic Gemma 3n implementation and a unique verification mechanism, the project delivers a compelling solution to a pressing global problem. The live demo, technical report, and public code repository collectively provide a comprehensive view of our project's innovation and execution.",
        "technologies": [
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/deoksangryu/gemma-3n-truthsync",
        "demo_video_url": "https://youtu.be/e33_jsa_Vkk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/truthsync-ai-powered-real-time-journalism-system"
    },
    {
        "project_title": "Mekomi (large document analysis for students)",
        "tagline": "An app that allows students everywhere (including without internet) to be able to study faster and better through the help of local LLMs",
        "authors": "Shayne Goldstein",
        "summary": "Mekomi is an advanced system for textbook document analysis, built around retrieval-augmented generation and optimized for extracting and synthesizing information from large collections of school textbooks.",
        "technologies": [],
        "source_code_url": "https://github.com/Shayneg-eng/Mekomi.git",
        "demo_video_url": "https://youtu.be/RFrHC3xKCoU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/mekomi-large-document-analysis-for-students"
    },
    {
        "project_title": "3nhance data",
        "tagline": "3Nhance is a Gemma3N powered offline agent that enhances structured data using replies from users via email (including text and images)",
        "authors": "Tiago Caldeira",
        "summary": "Project Description: 3Nhance - AI-Powered Offline Data Enhancement Agent\n\n**3Nhance** is an innovative offline AI agent powered by Gemma3N. It automatically enhances structured data by leveraging user replies received via email. This offline solution utilizes **Gemma 3n (2B and 4B)**, **Ollama**, and **N8N** to process natural language text, attached documents, and images, extracting key information and updating files or generating SQL queries to fill data gaps.",
        "technologies": [
            "Docker",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/docilio/3nhance",
        "demo_video_url": "https://youtu.be/rmq1khHR3kI",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/3nhance-data"
    },
    {
        "project_title": "Gemma-3N Field Botanist",
        "tagline": "Identify species from flower image offline with finetuned Gemma 3n model",
        "authors": "NawawitKes",
        "summary": "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F961556%2Fe516ba5355fdcf3110913d0804532204%2FScreenshot%202568-08-04%20at%2020.43.19.png?generation=1754315236721267&alt=media)\nhttps://botanist.mekpro.dev\n\nWe present a novel approach to fine-tuning Google's Gemma-3n multimodal model for accurate plant species identification on edge devices. Our methodology addresses the fundamental challenges of limited model capacity and lightweight vision encoders through three innovative techniques: multi-task supervised fine-tuning that leverages hierarchical botanical features, model-assisted data curation using consensus mechanisms, and Group Relative Policy Optimization (GRPO) for cross-modal consistency. By training on the PlantNet300K dataset, we demonstrate that careful fine-tuning strategies can overcome the limitations of small-scale multimodal models, achieving significant improvements in plant species identification accuracy while maintaining the efficiency required for offline mobile deployment. This work contributes to both the technical advancement of vision-language models and the practical goal of environmental conservation to promote and track biodiversity through accessible technology.\n**Result:** Our fine-tuned model achieves 22% accuracy for exact species identification in Top-5 predictions—impressive considering the model runs entirely offline on mobile devices. Even more encouraging, it correctly identifies genus-level classification 66% of the time and can accurately recognize complex botanical concepts like inflorescence types at 58% accuracy. Remarkably, our GRPO optimization, despite operating only on text outputs, improved image classification performance by up to 10%, demonstrating the power of cross-modal learning.\n\nGemma-3n Botanist Vision LLM: Enhanced Plant Species Identification through Multi-Task Fine-Tuning and Reinforcement Learning\n\n1. Introduction\nPicture a botanist deep in a rainforest, miles from the nearest cell tower, discovering an unfamiliar flowering plant. Or imagine a conservation volunteer surveying plants in a remote mountain meadow, needing quick, reliable species identification to document biodiversity. These scenarios play out daily for field researchers, conservationists, and nature enthusiasts worldwide. Yet despite carrying smartphones with impressive cameras, they often can't identify plants on the spot due to lack of internet connectivity or the limitations of existing offline tools.\nWe developed the Gemma-3n Botanist Vision Model specifically to solve this real-world challenge. Our goal was simple: create a plant identification system that runs entirely on a phone, works without internet, and delivers accurate results even on devices with limited computing power. This meant working within strict constraints—not just technical ones, but practical ones that field botanists face every day.\nOur Technical Approach: We chose Google's Gemma-3n as our foundation because it excels at mobile deployment and requires minimal resources to run—perfect for field conditions where battery life and processing power are precious. Starting with the PlantNet300K dataset, we implemented a sophisticated fine-tuning strategy to transform this general-purpose model into a botanical expert.\nOur approach goes beyond simple species classification. Through multi-task learning, we taught the model to understand botanical relationships—how flower color, shape, and inflorescence patterns work together to identify species. This mirrors how human botanists think, using multiple morphological clues to narrow down possibilities. We then applied Group Relative Policy Optimization (GRPO) as a final refinement stage to enhance accuracy and ensure consistent, botanically sound predictions.\n\n2. Technical Approach",
        "technologies": [
            "vLLM"
        ],
        "source_code_url": "https://github.com/mekpro/botanist",
        "demo_video_url": "https://youtu.be/gEWpz9eZsBg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-3n-field-botanist"
    },
    {
        "project_title": "EmpowerEd: Multimodal AI Learning Companion for Special Needs Students",
        "tagline": "Leveraging Gemma 3n's offline multimodal capabilities to make education accessible for 240 million children with learning disabilities world",
        "authors": "surya ",
        "summary": "EmpowerEd: Transforming Special Needs Education with Gemma 3n (Enhance Accessibility track)",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/SinghSuryaDeep/Google-The-Gemma-3n-Impact-Challenge-AI-Mavericks",
        "demo_video_url": "https://youtu.be/EW7DdGiynVE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/empowered-multimodal-ai-learning-companion-for-spe"
    },
    {
        "project_title": "Multimodal News Generation System Powered by Gemma 3n",
        "tagline": "Using Gemma 3n Full Multimodal Capabilities",
        "authors": "Alyx03",
        "summary": "Introduction  \n**Gemma 3n** is a powerful AI model developed by Google that can understand and generate text, audio, and images, all in one system. It also supports over 140 languages, making it useful in many parts of the world. What makes it special is that it can run directly on devices and doesn’t need an internet connection, making it ideal for real-world use, even during emergencies.\n\nIn this write-up, we explore the full potential of Gemma 3n by building News'ly, an AI-powered reporter that takes in voice recordings and visual evidence to automatically generate well-structured news articles. The project demonstrates how a single, self-contained model can perform transcription, image understanding, and narrative generation on end devices, all without needing external APIs or internet access.\n\nReal-Life Context\nPhilippines Metro Manila, including flood-prone areas like Marikina, Quezon City, and Manila itself, faces regular inundation during typhoon season. Due to poor drainage, urban density, and its location along major river systems, even moderate rainfall can lead to severe flooding. These conditions disrupt transportation, displace families, and endanger lives, especially during peak months from June to September. As climate change increases the frequency and intensity of extreme weather, the need for rapid, on-the-ground reporting becomes more critical than ever.\n\nThe Problem\nField reporters are expected to stay alert and responsive at all times, especially during emergencies. But under intense pressure, \n>the manual workload of transcribing interviews, analyzing images, and writing reports from scratch becomes overwhelming. These tasks are not only time-consuming but also susceptible to critical errors. **In high-stakes situations like typhoons or flooding, such delays and inaccuracies can severely impact the public’s access to timely, accurate, and life-saving information.**\n\nThe Solution: A Multi Modal Reporter Assistant Powered by gemma 3n\nA system built to be a reporter's digital field assistant. It streamlines the news generation pipeline by using the multimodal capabilities of Gemma 3n to:\n\n1. Transcribe Interviews: Automatically convert recorded audio interviews into accurate text.\n2. Analyze Images: Instantly generate descriptive captions for photos taken at a scene.\n3. Draft Reports: Synthesize the transcribed text and image descriptions into a coherent, well-structured news article.\n4. Revise & Refine: Allow the reporter to provide feedback and request revisions, ensuring the final output meets standards.\n5. Work Offline: Operate entirely offline on a device, a critical feature for reporting from remote areas where internet is non-existent.\n\n>This approach transforms a multi-hour manual process into a matter of minutes, enabling real-time, reliable reporting under pressure and freeing up reporters to focus on verification and storytelling, even when there’s no internet connection.\n\nApplication Architecture & Workflow\n\n1. Backend (NewsReporterAgent)\n• Handles transcription, image description, report generation, revision, and saving—powered entirely by the Gemma 3n model.\n\n2. Frontend (Gradio UI, for DEMO ONLY)\n• Lets users upload media, view AI outputs, give feedback, and save the final report—all in an intuitive, offline-friendly interface.\n\n\n<h4 style=\"text-align:center;\">Application Workflow</h4>\n\n<p align=\"center\">\n  <img src=\"https://storage.googleapis.com/kaggle-forum-message-attachments/3262246/24823/workflow.svg\" alt=\"Workflow Diagram\" />\n</p>\n\n\nThe system follows a simple yet powerful workflow, designed for rapid field deployment, especially in offline scenarios.\n\n**How It Works in the Field**\n **Scenario:** A new's reporter or journalist in flood-prone Metro Manila arrives at a disaster scene.\n\n1. Capture Scene:\n• Take a photo of the location (e.g., a flooded street)\n• Record an interview with a victim or witness using any mobile voice recorder  or record directly using the  app\n\n2. Upload Inputs to the app:\n• Audio and image are fed into the app running on a mobile device or local laptop\n\n3. Automatic Processing (Offline):\n• The app transcribes the interview\n• Describes the image\n• Drafts a news report using both sources\n\n>**Report Generation Guidelines** The quality of the generated report is directly dependent on the input data — including the transcribed text, image context, and the prompt itself (**which can be adjusted until it meets our desired standard**).\n\nTo ensure a more complete and accurate output, it is recommended to provide a longer and more detailed audio recording.\n\nIf the generated report requires changes, the revision feature supports iterative improvements...\n\n4. Reporter Feedback:\n• The reporter reviews the article\n• Provides comments like “add more urgency”, \"delete this\", or “focus on quotes”\n\n5. Revised Final Output:\n• The AI revises the draft\n• The final version is saved and ready to publish or report live on TV broadcast\n\n**The system demonstrates how Gemma 3n can perform all tasks and operate entirely offline while still delivering high-quality multimodal outputs. No server, no API keys, no internet — just a mobile phone or laptop.*\n\n Challenges & Development Journey\nBuilding a multimodal system wasn’t easy, especially with the goal of using only Gemma 3n’s capabilities on limited hardware (I had no GPU 😔).\n\n1. **Hardware Bottlenecks** The biggest challenge was the lack of a local GPU, which made most typical deployment strategies impractical. Offline use was especially difficult to test efficiently on CPU-only machines.\n\n2. **Tooling Dead Ends**\n•Ollama: As of August 2025, it only supported text for Gemma 3n models,  which didn’t meet my project’s multimodal needs.\n•Unsloth: A very promising library, but it required a local GPU for fine-tuning. \n\n3. **Dependency Friction**\nI had to rely on the Transformers library, but ran into version-related issues. Newer versions unexpectedly broke audio and image support, forcing me to experiment with different versions to find a stable setup.\n\n**To make everything work, these versions were crucial:**\n`!pip install --upgrade torch torchvision torchaudio`\n`!pip install transformers==4.54.1`\n`!pip install -U timm>=1.0.9`\n\n4. **The Kaggle Breakthrough** With inference times on my local CPU being impractically long and deadlines approaching, the project was at risk. The turning point was discovering that Kaggle Notebooks offered a generous 30-hour weekly quota of free GPU usage. I moved my entire development and inference workload to Kaggle, which finally gave me the computational power needed to build, test, and run the model efficiently.\n\n5. **Model Selection** While the 4B version is my preferred choice, it surpasses the memory capacity of the standard Kaggle notebook environment. In my Kaggle notebook code I selected the google/gemma-3n-E2B-it version to stay within the memory limits of the Kaggle notebook environment.\n\n6. **Final Deployment** After using my Kaggle GPU hours to finalize the application, I successfully packaged it and deployed it as a public demo on Hugging Face Spaces, bringing News'ly (my app name 🥰) to life.\n\n*I'm excited to share my work on this project! You can find the live demo, videos, and all the code in the links below. Please feel free to check them out. I'd love to hear your feedback.. ☺️*\n\n>Note: The tool supports—not replaces—reporters. It speeds up transcription, summarization, and drafting so reporters and journalists can dedicate more time to verification, context, and storytelling.”",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Alyxx-The-Sniper/gemma_3n_challenge",
        "demo_video_url": "https://www.youtube.com/watch?v=Qa0SgODFsDg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/multimodal-news-generation-system-powered-by-gemma"
    },
    {
        "project_title": "MindWell - An Offline Therapy Assistant",
        "tagline": "MindWell is an offline Electron app that uses Gemma 3n’s multilingual capabilities to help users store memories and reflect on their journey",
        "authors": "Optimalbhandari",
        "summary": "MindWell: Technical Documentation",
        "technologies": [
            "CUDA",
            "FastAPI",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Bloodwingv2/MindWell",
        "demo_video_url": "https://youtu.be/59YU8sDpjTA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/mindwell-an-offline-therapy-assistant"
    },
    {
        "project_title": "StandbyMeAI",
        "tagline": "Creating AI that can be a companion to the elderly",
        "authors": "Goichi Harada",
        "summary": "Project Description\nWe are all born connected, but many of us face the prospect of growing old alone. This deep, silent loneliness is more than just sadness; it's a condition that affects both mental and physical health. What if we had a friend who would always stand by us, who would reach out first, and share a laugh even on the hardest days?\nStandbyMeAI is our answer. It is a proactive, on-device AI companion designed to combat loneliness in the elderly. Powered by the multimodal capabilities of Gemma 3N, it doesn't just wait for a command. It uses its vision to understand the context of a situation—noticing if you look tired or reminding you to take your medicine—and initiates conversations with empathetic, emotionally expressive speech. It tells dad jokes, it laughs, it listens.\nRunning entirely on a low-cost mini-PC, Stand by Me AI ensures privacy and 24/7 companionship without an internet connection. This project, born from a personal worry for loved ones, has become a blueprint. It's proof that you, too, can now build an AI in your own language, for your own family, to ensure they never feel truly alone.\n\nSystem Architecture\n![System Architecture](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1807291%2Ff03be9973ec5ccd2b9b897c3feb9734a%2Fsystem-arch.png?generation=1754500521915424&alt=media)\n\nWhy Gemma 3n was Essential for this Project\nOur vision required an AI that was proactive, private, and context-aware. Gemma 3n was the only model that met all our criteria for three key reasons, as highlighted in our video:\n\n- Multimodal Understanding: StandbyMeAI is not a passive chatbot. It proactively initiates conversation based on what it sees. Gemma 3n's ability to process visual information is the core engine that allows it to understand the user's situation and act spontaneously, moving beyond simple command-and-response interactions.\n- Privacy-First & Offline Ready: Entrusting an AI with the wellbeing of our loved ones demands the highest level of privacy. Because Gemma 3n runs entirely on-device, no personal data, images, or conversations ever leave the user's home. This creates a truly private and reliable companion that functions 24/7, with or without an internet connection.\n- On-Device Performance & Accessibility: We are committed to making this technology accessible to everyone, not just those with high-end hardware. Gemma 3n's efficiency allows the entire system to run on a low-cost Mini PC. This makes it a practical and affordable solution for families worldwide.\n\nOvercoming Technical Challenges\nA key part of this project was bridging the gap between a local development environment and a shareable, reproducible demo on Kaggle.\n- The Kaggle Memory Constraint: Our initial challenge was significant. The Kaggle environment, even with two T4 GPUs, could not simultaneously load the Gemma 3n model for image analysis and a high-quality TTS model without crashing due to Out-of-Memory (OOM) errors.\n- The Unsloth Solution for Inference: We solved this problem by leveraging Unsloth. While widely known for fine-tuning, we discovered it is also an exceptionally powerful tool for inference. By loading unsloth/gemma-3n-E4B-it via Unsloth's FastModel, we drastically reduced the memory footprint, enabling stable, simultaneous operation of both models. This was a critical breakthrough for our public demo.\n- Ensuring Real-World Usability (CPU Version): The version shared on GitHub is designed for practicality. It uses the standard Transformers version of Gemma 3n on the CPU and a GGUF version of the TTS model. This ensures that users without access to powerful GPUs can still run, modify, and deploy StandbyMeAI on affordable, common hardware.\n\nSpecial Recognition: The Unsloth Prize\nWe are submitting this project for consideration for The Unsloth Prize, as Unsloth was not just a tool, but a pivotal enabler for our project's success in two distinct ways:",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/webbigdata-jp/StandbyMeAI",
        "demo_video_url": "https://youtu.be/B8XXr8VE0wA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/stand-by-me-ai"
    },
    {
        "project_title": "Gemma 3n: On-Device AI for Transformative Education",
        "tagline": "Transforming Learning Anywhere, Anytime with Multimodal AI",
        "authors": "pittu venkat reddy",
        "summary": "This project explores the power of Google's Gemma 3n model to create four distinct, impactful applications that address critical educational challenges, particularly in underserved and non-traditional learning environments. By leveraging Gemma 3n's multimodal and on-device capabilities, these solutions provide accessible, personalized, and high-quality learning experiences without the need for an internet connection.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/Txh94DZFhoc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-3n-on-device-ai-for-transformative-education"
    },
    {
        "project_title": "Korean Grammar Coach — Gemma 2B CLI Tool",
        "tagline": "Offline bilingual grammar correction for Korean learners using Gemma 2B",
        "authors": "DEEPAK POLISETTI",
        "summary": "🚀 Just submitted my Korean Grammar Coach AI for the Google Gemma Impact Challenge!\n\nThis CLI tool uses Google's Gemma 2B model to provide **offline Korean grammar correction** with **bilingual explanations** — all from the command line. It’s designed for learners who want instant feedback without relying on internet access or expensive apps.\n\n🔧 Built with:\n- Gemma 2B (Google’s open-source language model)\n- Python CLI interface\n- Modular, reviewer-friendly design\n- Bilingual output: Korean + English grammar explanations\n\n🎯 Why it matters:\nLanguage learners often struggle with grammar clarity — especially in Korean, where particles and verb endings can be tricky. This tool empowers users with instant, AI-driven feedback that works offline, making it accessible to students, travelers, and self-learners worldwide.\n\n📹 Demo Video: [https://youtu.be/Ypr0aRpSZWs](https://youtu.be/Ypr0aRpSZWs)  \n📁 Kaggle Project: [https://www.kaggle.com/datasets/deepakpolisetti/korean-grammar-coach-gemma-2b-cli-tool]  \n📜 License: MIT\n\nGrateful for the opportunity to build something meaningful — and excited to keep pushing the boundaries of AI + education.\n\n#Gemma #GoogleImpactChallenge #AI #NLP #LanguageLearning #KoreanGrammar #CLI #OfflineAI #Kaggle #OpenSource",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/Ypr0aRpSZWs",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/korean-grammar-coach-gemma-2b-cli-tool"
    },
    {
        "project_title": "Mentis - A Privacy-First, Fine-Tuned Gemma 3n for Mental Health",
        "tagline": "Fine-Tuned with Unsloth, Delivered via Ollama, Accessed via Open WebUI, Designed for Ethical AI Use",
        "authors": "Fatih Keles",
        "summary": "Mentis - A Privacy-First, Fine-Tuned Gemma 3n for Mental Health\n\nGithub repository for complete workflow and binary releases: [Mentis](https://github.com/kelesfatih/Mentis)\n\nMentis is a privacy-first, locally-deployed mental health assistant built on the fine-tuned Gemma 3n (4B) model and grounded in Cognitive Behavioral Therapy (CBT). It addresses key barriers to mental health access—privacy concerns, cost, and infrastructure gaps—by operating entirely offline on consumer-grade hardware via Ollama and Open WebUI. Fine-tuned using Unsloth with preference-ranked psychotherapy data (PsychoCounsel-Preference), the model demonstrates improved alignment with clinical communication principles such as empathy, relevance, and safety. Through quantized deployment (GGUF, 8-bit) and parameter-efficient tuning (QLoRA), Mentis delivers structured psychoeducation and emotional triage without reliance on cloud infrastructure or third-party APIs. It is designed as a zero-barrier, scalable intervention point for individuals underserved by traditional mental health systems.\n\nTechnical Summary\n| Component             | Description                         |\n| --------------------- | ----------------------------------- |\n| Base Model            | Gemma 3n 4B                         |\n| Tuning Framework      | Unsloth + QLoRA (4-bit)             |\n| Dataset               | PsychoCounsel-Preference (filtered) |\n| Quantization          | GGUF 8-bit                          |\n| Inference             | Ollama                              |\n| Interface             | Open WebUI                          |\n| Therapeutic Framework | CBT                                 |",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/kelesfatih/Mentis",
        "demo_video_url": "https://youtu.be/oBCRgDEW37A",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/mentis-a-privacy-first-fine-tuned-gemma-3n-for-men"
    },
    {
        "project_title": "\"PetBreedID\" is a multimodal application for recognizing animal breeds.",
        "tagline": "Use Gemma 3n for local, multimodal image and voice analysis, providing accurate, private, and internet-free operation.",
        "authors": "Ernst Ashurov",
        "summary": "Objective: To create a standalone, private mobile app that, with the help of Gemma 3n, allows users to quickly and accurately recognize pet breeds (cats and dogs) by image and voice description.\nProblem: Pet owners often want to know the breed of their pets or identify an unfamiliar animal in the park. Traditional solutions require an internet connection or the use of cloud-based APIs, which can compromise privacy and hinder accessibility in remote areas.\nSolution: Use Gemma 3n for local multimodal image and voice analysis, which provides fast, accurate, and private operation without the need for an internet connection.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/V8_uOSvGzZQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/petbreedid-is-a-multimodal-application-for-recogni"
    },
    {
        "project_title": "GeminiBot: Chat with Your Code",
        "tagline": "A Gemini Powered VS Code extension that lets developers chat with their codebase — directly inside the editor",
        "authors": "krishnaagarwal7508",
        "summary": "**GeminiBot** is a powerful Visual Studio Code extension that integrates Google’s Gemini to provide advanced, context-aware coding assistance directly inside the developer’s IDE. It enhances developer productivity by enabling natural language interaction with source code while also supporting voice and screen-based communication with the model for more interactive collaboration.\n\nDesigned with efficiency and usability in mind, GeminiBot supports both text-based and real-time audio/video interaction, transforming how developers interact with their code and tools.\n\n#",
        "technologies": [],
        "source_code_url": "https://github.com/krishnaagrawal7508/GeminiBot",
        "demo_video_url": "https://youtu.be/PNKY66mWEyo?si=u5rY9kwN4oHfWOzS",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/geminibot-chat-with-your-code"
    },
    {
        "project_title": "Dreamcraft",
        "tagline": "Unleash your creativity and bring your game ideas to life, powered by AI!",
        "authors": "Elton Kola",
        "summary": "This is just a prototype, as local models are not there yet. My dream would be to create video-games/websites  locally using llms. Ideally we would  train and optimize small llms good only at one test, lua and love games, web games, websites and so on.\n\nFrom what i have seen gemma-3n is great with text/html, but not so much with games. \n\nI use this app when i am on a plane, offline, or just want to test my creativity  and llms limits. \n\nGet an apk here: https://github.com/eltonkola/Dreamcraft/releases",
        "technologies": [],
        "source_code_url": "https://github.com/eltonkola/Dreamcraft",
        "demo_video_url": "https://youtu.be/NRMgCdeCPlU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/dreamcraft"
    },
    {
        "project_title": "GemmaGuard3n",
        "tagline": "Child Protection ",
        "authors": "AmalSu0",
        "summary": "Technical Writeup\nGoogle - The Gemma 3n Impact Challenge\n\n1. Introduction & Motivation\nChildren today are increasingly exposed to emotional and behavioral harm from inappropriate content. This includes online exposure to violent or sexual material, offensive language, and toxic behavior, often found in movies, social media, and other digital platforms, as well as offline exposure through social environments, where family members, peers, or caregivers may model or directly engage in harmful behaviors. Children absorb and mirror behaviors they constantly see or hear, especially in formative years. Repeated exposure to harmful stimuli can negatively affect children’s emotional and psychological development, potentially leading to lasting behavioral and issues or trauma. \nThe problem is exacerbated by the diversity of children’s backgrounds: age, neurodiversity, culture, trauma history, and personal sensitivities all shape what constitutes “harmful or inappropriate” for the child, highlighting the need for nuanced and context-aware protection strategies. Existing current abuse detection systems and content moderation tools rely on pre-defined set of harm categories. They often fail to capture nuanced and context-dependent forms of harm. Furthermore, few systems prioritize real-time analysis, privacy, and customizability, all of which are essential for protecting children in both home and institutional environments.\nOur proposed system, GemmaGuard3N, reimagines child protection through an intelligent, adaptable, privacy-conscious lens. Built using Gemma 3n, a state-of-the-art open multimodal LLM designed for running with high efficiency on low-resource devices. Our solution is capable of processing long-form visual and multilingual audio streams from diverse sources. GemmaGuard3N leverages zero-shot reasoning and customizable prompting to enable customizable real-time detection of potential harm tailored to individual children's needs.\n\n2. Problem Statement\nWe define the problem as the detection of harm indicators in live or recorded video streams involving children, where “harm” includes: \n•\tInappropriate language, behavior, or gestures \n•\tPsychological abuse or aggression \n•\tUnsafe content exposure (e.g., sexual innuendos, substance use) \n•\tEmotionally disturbing interactions or tones \nKey Technical Requirements\n•\tMultimodal Understanding: Detect harm from both visual and auditory inputs.\n•\tPrivacy-Aware Processing: Ensure that content is analyzed locally or through secure channels to protect user privacy.\n•\tReal-Time or Near-Real-Time Performance: Provide immediate alerts to caregivers upon detecting signs of harm.\n•\tPersonalization: Support customizable definitions of harm based on each child’s individual needs.\n•\tZero-Shot Capability: Eliminate the need for training a custom dataset for every use case.\n•\tMultilingual Support: Accurately process audio in multiple languages to accommodate diverse populations.\n•\tLong-Form Video Handling: Continuously analyze long-duration videos such as CCTV footage, movies, and online content without interruption.\n\n3. System Architecture\nGemmaGuard3N comprises four core modules:\n1. Input Handling\nThe system supports a variety of video sources:\n•\tReal-Time CCTV Streams via camera IP URLs.\n•\tLocal Videos, including uploaded movie clips or home surveillance footage.\n•\tYouTube Links, allowing analysis of online content before sharing it with children.\n2.\tPreprocessing and Feature Extraction \n•\tVideo Frames: Sampled at 1 frame per second (FPS).\n•\tAudio: Extracted and converted to 16 kHz WAV format.\n•\tTemporal Alignment: Maintained to synchronize video and audio cues accurately.\n3. Harm Detection with GemmaGuard3N\nA lightweight, multimodal method leveraging Gemma 3N:\n•\tDynamically crafts prompts based on user-selected harm indicators.\n•\tApplies a sliding window approach to process long videos in overlapping segments.\n•\tDetects nuanced patterns using token-level SoftMax probability scoring from MLLM logits.\n4.\tFeedback and Alert System\n•\tIf harm is detected, parents or guardians are notified. \n•\tEach flagged instance includes harm probability score and a timetable to indicate segments of concern.\n•\tRecorded evidence, such as audio/video snippets, is provided to support parent review and informed decision-making.\n\n4. Methodology\nWe designed the GemmaGuard3N method to detect harm indicators in real time from multimodal video inputs. The method consists of the following steps:\n•\tStep 1: We curated a general list of harm indicators (e.g., violence, sexual content, verbal abuse, and inappropriate language). Optional harm indicators are also collected from users to extend this list based on their specific child safety requirements.\n•\tStep 2: We craft a negatively framed prompt for the Gemma 3N using the selected harm indicators. For example: “Does this video contain any of the following: violence, sexual content, inappropriate language, or verbal abuse?” A \"Yes\" response indicates the presence of harm. Prompts are carefully constrained using system- and user-level instructions to ensure response (“Yes” or “No”) only.\n•\tStep 3: To enable Gemma 3N to process longer videos, we implement a sliding window approach. Videos are divided into short, overlapping chunks to reduce inference load, based on the output of a preprocessing and feature extraction module. The window size is dynamically adjusted depending on device memory and model capacity. Overlaps are introduced between windows to prevent missing brief harmful segments (e.g., a single swear word). Each chunk is sent to Gemma 3N for evaluation.\n•\tStep 4: For each window, we extract the logits associated with the “Yes” and “No” tokens generated in response to the crafted prompt. A SoftMax function is applied to these logits to compute the probability of a “Yes” response as a confidence score. A window is flagged as harmful if the \"Yes\" probability exceeds a defined threshold. This design is inspired by the QGuard paper [1], which used token-level SoftMax probabilities to detect unethical or harmful text prompts in the context of malicious attacks.\n\n5. Implementation Details\nWe build GemmaGuard3N using a modular pipeline composed of the following core components:\n•\tFor video processing, we employ OpenCV for efficient frame sampling and real-time stream handling. Frames are extracted at configurable intervals to balance computational efficiency and temporal coverage.\n•\tFor audio processing, we use Pydub and Librosa to segment, normalize, and convert raw audio streams into model-compatible formats.\n•\tFor model inference, the backbone of our system is Gemma 3N, a multilingual, multimodal vision-language model developed by Google. We leverage the HuggingFace Transformers library with PyTorch to run inference over synchronized audio-visual segments.\n•\tFor the user interface, we implement an interactive web demo using Gradio, which supports real-time user input, file uploads, and video visualization.\n\n6. Conclusion \nGemmaGuard3N, powered by Gemma 3n, is a powerful proof-of-concept for socially responsible, AI-driven child protection. It transforms the way we approach video analysis, offering not just detection but contextual, human-aware safety for those most vulnerable. We believe our technical architecture and methodology can inspire a new class of safety applications built on Gemma3n, ones that are as empathetic as they are intelligent.\n\n7. Citations\n[1] T. Lee, J. Yoo, H. Cho, S. Y. Kim, and Y. Maeng, “QGuard: Question‑based Zero‑shot Guard for Multi‑modal LLM Safety,” arXiv preprint arXiv:2506.12299 v1, June 2025",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/amalsu0/GemmaGuard3n/tree/main",
        "demo_video_url": "https://www.youtube.com/watch?v=JneMslKJ8U0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemmaguard3n"
    },
    {
        "project_title": "PlantDoctor - Farmers Friend",
        "tagline": "PlantDoctor: Democratizing Agricultural Knowledge with Gemma 3n",
        "authors": "przbadu",
        "summary": "PlantDoctor: Democratizing Agricultural Knowledge with Gemma 3n\n\n**Project Repository**: https://github.com/przbadu-ai/plant_doctor",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/przbadu-ai/plant_doctor",
        "demo_video_url": "https://youtu.be/uy2RhpYKby4?si=3rj2GHhlkMB6Zda7",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/plantdoctor-farmers-friend"
    },
    {
        "project_title": "Market Mind",
        "tagline": "My submission into this competition!",
        "authors": "Ram Yalavarthy",
        "summary": "This website uses HTML,CSS, and JavaScript! Gemma 3n did the bulk of the coding. It also helped with debugging and adding extra features. The bugs in the website were a challenge since there were three files where something could go wrong. This was extremely challenging since it would take a very long time to sift through all of the code. The reason I used HTML,CSS, and JavaScript was so it could be easily deployed into a domain and be available on the web. This increases the ease of access. My code makes it easy to insert an API key for minute-to-minute data. My website takes current interest rates and market data to show price changes and to predict prices of many goods such as cars, produce, and electronics. This prediction could help families save for goods they are planning to buy.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/RamYala121/MarketMind",
        "demo_video_url": "https://youtu.be/-zbjqyCTczY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/market-mind"
    },
    {
        "project_title": "Waico - Your Wellbeing AI Companion",
        "tagline": "Waico is a wellbeing AI companion that leverages on-device AI models to provide counseling, fitness coaching, and meditation guidance.",
        "authors": "sita berete",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/sitatec/Waico",
        "demo_video_url": "https://youtu.be/ufq7EUXwlQM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/waico-your-wellbeing-ai-companion"
    },
    {
        "project_title": "Calmind - A personal mental health specialist",
        "tagline": "An offline AI app that detects your mental health state and sends alerts when things get critical. Accessible anytime, anywhere.",
        "authors": "Suesi Tran, Trung L?p Tiêu",
        "summary": "Project description\nThis is an offline AI-powered chatbot app built with **Flutter**, using the **flutter_gemma** package to run the **Gemma 3n** model entirely on-device. Users can chat with the app anytime, 24/7 — no internet required. Its primary goal is to help users manage stress through conversation and emotional support. When the app detects that a user's stress level has reached a critical point, it will offer to connect them with a mental health helpline.\n\nEach helpline is linked to verified psychological support services. If the call goes through, a summary of the recent conversation will be forwarded to the counselor to provide helpful context.\n\n**🔹 MVP Features:**\n- Chat Support: Multilingual conversations powered by Gemma 3n via flutter_gemma.\n- Mood Analysis: Fully offline, processed directly on-device.\n- Helpline Connection: Simulated call to a sample mental health helpline, with future plans for real integrations.\n\nHow to run this project",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/kido-luci/journal-ai-buddy",
        "demo_video_url": "https://youtu.be/meVrf5I_pCg",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-buddy-a-personal-mental-health-specialist"
    },
    {
        "project_title": "WildGuard: AI-Powered Wildfire Detection and Analysis System",
        "tagline": "The system processes NASA FIRMS satellite data to detect active fires and uses Google's Gemma 3n model to generate detailed risk assessments",
        "authors": "Noriko Kono",
        "summary": "WildGuard: AI for Early Wildfire Detection",
        "technologies": [
            "FastAPI",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/norikokono/Kaggle-Google---The-Gemma-3n-Impact-Challenge",
        "demo_video_url": "https://youtu.be/hE5ga1wsSXI?feature=shared",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/wildguard-ai-powered-wildfire-detection-and-analys"
    },
    {
        "project_title": "Medical-Gemma-3N: Offline Emergency AI for Remote Areas, Disasters and First Aid",
        "tagline": "From model to mobile: fine-tuning, evaluation, and full offline deployment in real-world emergency scenarios",
        "authors": "Eric Risco de la Torre",
        "summary": "Medical-Gemma-3N is a fully offline, end-to-end AI assistant designed specifically for handling emergency medical scenarios in remote or low-infrastructure environments—places where there’s no reliable internet, no nearby healthcare professionals, and often, no time to wait for help.\n\nWe built this project entirely from scratch:\n\n- Started with a baseline evaluation of the Gemma‑3n‑4B model to assess its performance on emergency medical instructions and triage benchmarks.\n- After confirming the base model achieved only 36% accuracy, we built a custom dataset of over 86,000 emergency medical Q&A pairs, using public datasets and real-world manuals from WHO, Red Cross, FEMA, and other sources.\n- We then fine-tuned the model using LoRA and Unsloth, enabling efficient training on limited hardware. The resulting model reached 71.5% accuracy, nearly doubling the baseline.\n- For inference, we used Ollama to test and debug the quantized GGUF version of the model locally before deploying to mobile—allowing us to simulate real-world offline performance in development.\n- Finally, we deployed the model fully on-device using Flutter and the flutter_gemma library, which supports both base weights and LoRA adapters for mobile inference—no cloud or servers required.\n\nThis is not just a fine-tuned model—it's a complete and operational end-to-end pipeline:\n- Custom data preparation pipeline with embedding-based RAG augmentation\n- LoRA-based fine-tuning with parameter-efficient training (only 76.9MB trained)\n- Evaluation loop with AI judges to track model improvement over time\n- Cross-platform mobile app with critical offline-first features:\n     - Global emergency number lookup by country (using local geolocation)\n     - A multilingual chatbot interface for general medical questions\n     - A dynamic, model-driven triage system where questions 4+ are generated live by the model on-device\n     - Visual aid: users can take a photo (e.g., wound or rash), and the model uses contextual prompts to assist diagnosis\n\nEverything runs locally. The model lives on the device. No internet is required—making it ideal for:\n- Rural villages in the jungle, desert, or mountains\n- Disaster zones where infrastructure is down or damaged\n- First responder kits and offline field medical teams\n- Low-resource educational contexts for teaching health and first aid\n\nWe believe this project represents what the Gemma 3N initiative is really about: accessible, open, socially impactful AI that performs in the real world—at the edge, without compromise.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/ericrisco/medical-gemma-3n",
        "demo_video_url": "https://youtu.be/aufRL0zLsoc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/medical-gemma-3n-offline-emergency-ai-for-remote-a"
    },
    {
        "project_title": "MediGem: AI-Powered Health Analysis Android App",
        "tagline": "AI-Powered Health Analysis Android App",
        "authors": "Claude Wang",
        "summary": "MediGem: The Health App That Never Leaves Your Phone | 100% Private Family Health Management\n\nManaging your family's health shouldn't mean sacrificing your privacy. MediGem is the AI-powered (by Gemma 3n) health app that runs entirely on your device - no cloud, no servers, no data sharing.\n\nIn this demo, you'll see how MediGem helps families:\n ✓ Manage medications with smart reminders and interaction alerts✓Check food safety instantly - just snap a photo✓ Track health conditions for multiple family members✓ Prepare for doctor visits with AI-powered symptom analysis✓ Stay on top of preventive care and screenings\n\nWhy MediGem?\n • Your medical data NEVER leaves your phone\n • No accounts or logins required\n • Advanced AI works offline\n • Protects your family from data breaches and privacy violations\n\nStop choosing between digital convenience and medical privacy. With MediGem, you get both.\n\n🔒 Your Family. Your Data. Your Control.\n\n#HealthApp #MedicalPrivacy #FamilyHealth #DigitalHealth #AIHealth #HealthTech #PrivacyFirst #MediGem",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/ClaudeWang86/MediGem",
        "demo_video_url": null,
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/medigem-ai-powered-health-analysis-android-app"
    },
    {
        "project_title": "Saidia - AI Assistant for Educators",
        "tagline": "Empowering educators especially in low connectivity regions with the power of AI.",
        "authors": "Udo N.",
        "summary": "Many regions across Africa face significant challenges with unreliable internet connectivity and inconsistent power supply. In these environments, a laptop with a tool like Saidia becomes a powerful solution for empowering educators with AI capabilities. By operating entirely offline and requiring only basic hardware, Saidia makes advanced AI technology accessible to teachers in communities where traditional cloud-based solutions are impractical or impossible.\n\nBuilt as an Electron desktop application, it leverages local AI models to generate educational questions from uploaded documents, enabling teachers to create assessments without requiring internet access.\n\n[Technical Write-Up](https://github.com/dokasto/Saidia/blob/main/technical-writeup.md)",
        "technologies": [],
        "source_code_url": "https://github.com/dokasto/Saidia",
        "demo_video_url": "https://www.youtube.com/watch?v=kvl7-pNIIQc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/saidia-ai-assistant-for-educators"
    },
    {
        "project_title": "Offline AI Captioner: Instant Subtitles Offline",
        "tagline": "Real-time, privacy-first subtitles for any language with Gemma 3n",
        "authors": "Dmitry Trubnikov",
        "summary": "Offline AI Captioner (Gemma 3n Accessibility Subtitler)\n\n*Turn any spoken or written text into clean subtitles — 100% offline, for accessibility, privacy, and instant use.\n*\n⸻\n\n#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/trubnikov/offline-ai-captioner",
        "demo_video_url": "https://youtu.be/utoHSRuRZ40",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/offline-ai-captioner-instant-subtitles-offline"
    },
    {
        "project_title": "Robolog: AI-Powered Log Monitoring System",
        "tagline": "An intelligent log monitoring agent that analyzes system errors in real time and delivers concise diagnostic reports to your team.",
        "authors": "Mark Bradley, Paul Bradley",
        "summary": "",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Hilo-Inc/robolog",
        "demo_video_url": "https://youtu.be/ZpJdnixZX3Y",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/robolog-ai-powered-log-monitoring-system"
    },
    {
        "project_title": "Fake Job listing Detector Dashboard",
        "tagline": "Spot the Scam, built using Gemma, is an ML app that detects fraudulent job postings to keep job seekers safe.",
        "authors": "MarvinRemiigiusJ",
        "summary": "Spot the Scam is a Streamlit-based web application that detects fraudulent job postings using machine learning. The app helps users identify scam job listings by analyzing text features and leveraging advanced resampling techniques to address severe class imbalance (95% real, 5% fake jobs). It provides interactive analytics, single and bulk prediction, and visualizations to support decision-making.",
        "technologies": [],
        "source_code_url": "https://github.com/Marvin-Remiigius/Fake-Job-Detector-Dashboard.git",
        "demo_video_url": "https://youtu.be/1O9Od-kyY4o",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/fake-job-listing-detector-dashboard"
    },
    {
        "project_title": "Breathing Reminder Engine for Autonomous Thoughtless Human Enhancement",
        "tagline": "In a highly intelligent world, AI can do everything. It can no longer just assist in creation, but can even remind you - are you breathing?",
        "authors": "DoNotGo",
        "summary": "Title: Local AI-Based Silence Detection and Interaction System\n\nThis project is a local, lightweight AI system that continuously monitors microphone input, detects prolonged silence, and responds with intelligent prompts generated by a local language model. The system architecture consists of the following functional modules:\n\nMicrophone Input Module\nCaptures real-time audio data from the user's microphone.\n\nAudio Sampling Module\nProcesses incoming audio signals into a form suitable for silence analysis. It ensures consistent sampling intervals and filters out irrelevant noise.\n\nSilence Detection Module\nAnalyzes the audio stream to detect periods of inactivity. If silence exceeds 10 seconds, it triggers a downstream response. This logic is configurable and designed for low-latency response.\n\nLocal Language Model Invocation Module\nUpon detecting prolonged silence, this module engages a local language model (e.g., Gemma or Llama.cpp) to generate contextually appropriate prompts. The model runs entirely offline to preserve privacy and reduce dependency on external APIs.\n\nOutput Interface Module\nThe generated response is delivered to the user through a terminal-based interface and/or voice feedback using a text-to-speech engine. This enables seamless, real-time interaction without relying on cloud infrastructure.\n\nUse Cases:\n\nProductivity assistants reminding users to stay focused\n\nSpeech therapy tools prompting vocal activity\n\nAmbient computing interfaces in quiet environments\n\nAI interaction demos for embedded or offline systems\n\nKey Characteristics:\n\nFully local processing; no network dependency\n\nModular and extensible architecture\n\nLow-resource and privacy-preserving\n\nDesigned for real-time, reactive AI behavior",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/afx9rqQRg_w",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/BREATHE"
    },
    {
        "project_title": "Checkstand - AI-Powered Financial Empowerment Through Receipt Intelligence",
        "tagline": "On-device Gemma 3n transforms paper receipts into financial insights, empowering users with privacy-first expense tracking",
        "authors": "Eido Askayo",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/gryphon2411/Checkstand",
        "demo_video_url": "https://www.youtube.com/watch?v=Pb0Qe0wS8Pk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/checkstand-ai-powered-financial-empowerment-throug"
    },
    {
        "project_title": "VISROC - Visual robotic controller for classrooms",
        "tagline": "Visroc is a android app which controls a off the shelf robot ( Siyeenove E-ARM ) using Gemma 3n for vision and reasoning.",
        "authors": "Prashobh Bala000",
        "summary": "In today’s fast-evolving technological landscape, equipping students with hands-on STEM experiences is essential. \n\nKey Innovations:\n\nDemocratizes Access: By turning everyday mobile phones into powerful control hubs, this approach eliminates the need for expensive robotics equipment upgrades. Schools can make their current robots more capable using devices already familiar to students, and my migrating existing robots to a esp32 board, to enable AI based training. \n\nBudget-Friendly Upgrades: Utilizing affordable, readily available components, this project modernizes robotics infrastructure, opening doors for all schools to offer advanced learning opportunities regardless of budget constraints.\n\nSeamless AI Integration: The solution incorporates artificial intelligence effortlessly into classroom robots. This unlocks dynamic features and creative problem-solving challenges, empowering students to dive deeper into coding and robotics. The entry barrier is lowered as there is no need for custom hardware or cloud access in classrooms. Students can install the app and edit the source code using Android studio. \n\nFosters STEM Engagement: With intuitive, visual interaction enabled by mobile controls, students become more engaged and inspired—from curious beginners in elementary labs to advanced learners in technology centers.\n\nScales Across Facilities: Designed to fit diverse educational settings, this mobile-based system maximizes technology investments by adapting from simple school labs to high-tech learning environments.\n\nFuture-Proof Learning: Keeping pace with rapid AI advances, the platform supports continuous upgrades and expansion, ensuring that educational robotics stay relevant and cutting-edge.\n\nMy email is prashobh07@gmail.com , please use email for contacts\n\n\nWhat challenges did we solve?\n\nCoding the Siyenoove robot in micropython was an unknown. This is because the robot came with firmware for Joy sticks and was written in python. Since the servos of the robot is not having any sensors, it was hard to write the code to initialize the bot and keep its movements smooth. we used threaded implementation in micropython to make smooth simultaneous movement for dance movements option. \n\nAndroid app of the Google Gallery edge was modified to enable the communication to robot. This is a new implementation which aligns to the robot's requirements for messages. \n\nWhy is this the right approach? \n\nBy enabling a general purpose board like ESP32, which costs around 5$ in larger units, and robot costing less than 50$ , we extend the reach to train students on AI and robotics to more than 3x number of students. \nModern robots with high programmability costs atleast 150$. Since the platform is completely open and Gemma 3n can run on any phone, additional hardware is not needed, saving costs exceeding 3x. \nLink to robot, on amazon\nhttps://a.co/d/bhidIE2",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/prashobhWJ/visroc/tree/main/Android/src/app/bot",
        "demo_video_url": "https://youtu.be/B25xzzx9ai4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/visroc-visual-robotic-controller-for-classrooms"
    },
    {
        "project_title": "Heat shock accident video detection（drowning detection）",
        "tagline": "AI-based drowning detection using the Gemma3n multimodal model",
        "authors": "Eiji-K",
        "summary": "Gemma3n Drowning Detection System Technical Report",
        "technologies": [],
        "source_code_url": "https://github.com/Eiji-Kb/in_this_corner",
        "demo_video_url": "https://youtu.be/F1NhjBNu-R8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/heat-shock-accident-video-detectiondrowning-detect"
    },
    {
        "project_title": "InstructAI",
        "tagline": "fully offline, AI-powered learning application",
        "authors": "Prashant Patil",
        "summary": "InstructAI – Technical Write-up\n\n\nDemo, APK & Source Code\n\nDemo Video: https://youtu.be/3XUDqbxLxNE?si=RjmUL-UWQUlVvxcZ\n\nAPK Download: https://github.com/prashantpatil0/InstructAI/releases/download/v1.0.0/InstructAI.apk\n\nSource Code: https://github.com/prashantpatil0/InstructAI\n\n\nOverview of InstructAI\n\nInstructAI is a fully offline, AI-powered learning application developed using Flutter and MediaPipe's GenAI framework. It empowers users to generate personalized course roadmaps and interactive lessons without needing an internet connection. The app uses Google’s Gemma 3n LLM (.task model) to run all inference directly on-device.\n\n\nArchitecture Overview\n\n1] Frontend – Flutter\n\nSplash screen for permission handling and model existence check\n\nDownload screen for manual .task model setup\n\nHome screen to browse recommended and custom course prompts\n\nRoadmap screen to display AI-generated learning paths\n\nLearn screen for module-wise lesson generation and content viewing\n\n\n2] Model Inference – MediaPipe GenAI (Native Android)\n\nModel (gemma-3n-E2B-it-int4.task) loaded using LlmInference\n\nModel interaction is handled via platform channels\n\nStreamed generation with LlmInferenceSession.receiveBroadcastStream()\n\nModel reset with _resetModelSession() to avoid context buildup\n\n\n3] Data Storage – Hive (Flutter)\n\nStores all saved course roadmaps and progress\n\nCourses can be resumed, regenerated, or continued at any time\n\nStorage is lightweight and entirely local\n\n\n- Core Functionality & Implementation\n\n1] Splash Screen (Model Check & Permission)\n\nChecks for internal storage permission\n\nUses Directory('/storage/emulated/0/InstructAI/model') to verify if the .task model file exists\n\nIf not found, navigates to the model download setup screen\n\n\n2] Model Download Setup\n\nCreates target folder: /storage/emulated/0/InstructAI/model\n\nDisplays download link for user to manually download the model:\n\n\nIncludes instructions to manually move the file into the correct path\n\n\n3] Model Inference Setup\n\nNative Android side loads the .task file using LlmInference.createFromFilePath(path)\n\nStreaming responses handled with:\n\n_streamChannel.receiveBroadcastStream({ \"prompt\": userPrompt })\n\nResets session after each inference:\n\ninferenceSession?.close()\ninferenceSession = LlmInferenceSession.create(inference)\n\n\n4] Roadmap Generation (Recommended/Custom)\n\nPrompts sent to the model using structured instruction format:\n\nCustom: \"Create a roadmap for learning [topic]...\"\n\nRecommended: Predefined prompts with course title and outcomes\n\n\nOutput parsed and saved locally with Hive for later retrieval\n\n\n5] Course Detail & Learning Flow\n\nDisplays full roadmap\n\nAllows selecting a module via dropdown\n\nCollects user input/focus area\n\nSends roadmap + selected module + user input as context for lesson generation\n\n\n6] Lesson Streaming\n\nFlutter UI displays token-by-token streamed lesson from model\n\nMarkdown output rendered with flutter_markdown\n\nStream cancellation or session reset occurs if user exits screen mid-inference\n\n\n\nHow We Used Gemma 3n\n\nWe used Gemma 3n E2B int4 quantized model in .task format\n\nIt is responsible for:\n\nGenerating compact, structured course roadmaps\n\nCreating detailed module lessons from roadmap and user input\n\n\nEntire inference happens offline using MediaPipe's LlmInference\n\nStreaming logic is used to provide real-time output for a responsive UX\n\nReset logic ensures no memory accumulation across multiple sessions\n\n\n\n\nChallenges We Overcame\n\n1. Manual Model File Setup\nDue to Android’s storage restrictions and model size, we designed a manual download+placement flow.\n\n\n2. Stable Streaming Inference\nPlatform channels were carefully managed to support real-time streaming without crashes or leaks.\n\n\n3. Model State Management\nSession reset logic was critical to avoid prompt accumulation or interference across multiple sessions.\n\n\n4. Polished Offline UI/UX\nDespite working offline, we focused on keeping the UI fast, responsive, and intuitive across screens.\n\n\n\n\nWhy These Technical Choices Were Right\n\nGemma 3n is a small, quantized LLM with impressive performance for on-device use.\n\nFlutter enabled fast and attractive UI development across platforms.\n\nMediaPipe Tasks allowed us to use native inference without building our own LLM runtime.\n\nHive ensured quick and reliable local storage with minimal overhead.\n\n\nThese choices enabled an AI learning app that works fully offline, is lightweight, and supports future extensibility—making it ideal for users with limited or no internet access.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/prashantpatil0/InstructAI",
        "demo_video_url": "https://youtu.be/3XUDqbxLxNE?si=RjmUL-UWQUlVvxcZ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/instructai"
    },
    {
        "project_title": "Dina: Document Intelligence Agent",
        "tagline": "A local-only open-souce document intelligence agent accessible to visually impaired users.",
        "authors": "Allister Beharry",
        "summary": "What it does\n\nDina is a local-only, open-source, cross-platform document intelligence agent that allows blind users to effectively navigate, understand, and query electronic and printed documents like letters, receipts, expense reports, invoices, tax forms, employee handbooks, training manuals, and other types of structured business documents which visually impaired employees must work with during their day-to-day activities.  Unllike apps like [Be My Eyes](https://www.bemyeyes.com/be-my-eyes-for-windows/) and [Google Lookout](https://play.google.com/store/apps/details?id=com.google.android.apps.accessibility.reveal&hl=en_US), Dina is specifically designed to help blind users understand, navigate and query the information in customer and internal documents and to use this information effectively in business processes and workflows and IT systems that may be inaccessible to blind employees.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/allisterb/Dina",
        "demo_video_url": "https://youtu.be/dYEP6nEo880",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/dina"
    },
    {
        "project_title": "DyslexAI: An Adaptive AI-Powered Literacy Companion",
        "tagline": "DyslexAI is your personal AI reading coach. It simplifies text, builds skills, and creates adaptive stories that learn and grow with you.",
        "authors": "Kieran McCarthy",
        "summary": "🧠 DyslexAI – Kaggle Hackathon Writeup\n\n> 💡 **Try it now**: [Download APK](https://github.com/SmiffyKMc/dyslexia-app/releases) | [Source Code](https://github.com/SmiffyKMc/dyslexia-app)",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/SmiffyKMc/dyslexia-app",
        "demo_video_url": "https://youtu.be/8Krt1CdYNd4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/dyslexai-an-adaptive-ai-powered-literacy-companion"
    },
    {
        "project_title": "Memory Palace: The First True AI Companion for Memory",
        "tagline": "An offline-first AI companion that helps those with memory loss reconnect with their memories and their families.",
        "authors": "Malek Gharsallah",
        "summary": "**Memory Palace: AI-Powered Family Memory System**\n\n**Author**: Malek Gharsallah  \n**Competition**: Google Gemma 3n Impact Challenge  \n**Categories**: Overall Track, Ollama Prize",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/GharsallahDev/memory-palace/",
        "demo_video_url": "https://youtu.be/K4teV0WZv60",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/memory-palace"
    },
    {
        "project_title": "Local Ollama Gemma 3N 4B LLM with Skin Cancer Detection & Health Stroke Eval",
        "tagline": "Offline web app enabling users to interact with a local LLM and perform preliminary skin lesion analysis",
        "authors": "Prototyping_AI",
        "summary": "",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/git-prototype-ai/kaggle/tree/main/Gemma3n_Challenge/AIChatbot",
        "demo_video_url": "https://youtu.be/Mr7RlW_o7Co",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/local-ollama-gemma-3n-4b-llm-with-skin-cancer-dete"
    },
    {
        "project_title": "Aura-Mind: An AI Field Guide for the Nigerian Farmer",
        "tagline": "An offline-first, on-device AI that empowers Nigerian farmers to diagnose maize diseases using just their phone's camera.",
        "authors": "Surfinia Burger",
        "summary": "#",
        "technologies": [
            "CUDA",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/surfiniaburger/tune",
        "demo_video_url": "https://www.youtube.com/watch?v=ZJlRoMf7KWo",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/aura-mind-an-ai-field-guide-for-the-nigerian-farme"
    },
    {
        "project_title": "PERSONALIZED LEARNING ASSISTANT",
        "tagline": "An Innovative Approach to Personal Development Guidance Using Natural Language Processing with Gemma 3N",
        "authors": "Veli Yikilmaz, Mustafa Bay, Harunn Gokce",
        "summary": "Project Description: A Step-by-Step User Journey\nWelcome to the Personalized Learning Assistant. Our goal is to transform the static, one-way learning process into a dynamic, interactive, and deeply personal dialogue between the user and our AI tutor, powered by Google's Gemma 3n.\n\nBelow, we will walk you through a complete user session to demonstrate how our application creates a unique learning path from scratch for any topic imaginable.\n\nThe Spark of Curiosity - Choosing a Topic\nThe user's journey begins on a clean, minimalist interface designed for a single purpose: to capture curiosity.\n\nEnter a Topic: The user types any subject they wish to learn into the text box. It can be anything from a scientific concept like \"Black Holes\" to a historical event like \"The French Revolution\".\n\nSelect a Language: The user chooses their preferred language for the entire experience (English or Turkish).\n\nInitiate the Journey: They click the \"Start Level Determination Quiz\" button. At this point, the application hands over the reins to Gemma 3n to begin the personalization process.\n\nThe Assessment - How Much Do You Know?\nInstead of immediately overwhelming the user with information, our assistant first seeks to understand them.\n\n1. Real-Time Quiz Generation: In just a few seconds, Gemma 3n generates a 3-question multiple-choice quiz tailored specifically to the chosen topic.\n\n2. Conceptual Questions: These are not simple trivia. The questions are designed to probe the user's foundational understanding of the subject's core concepts.\n\n3. Determining the Level: The user answers the questions. Their score (e.g., 0/3, 1/3, or 2+/3) is used to categorize their knowledge level as Beginner, Intermediate, or Advanced. This assessment is the cornerstone of the entire personalized experience.\n\nThe Custom Lesson - Content Tailored Just for You\nOnce the user's level is determined, the \"Topic Content\" tab comes to life. Gemma 3n now acts as a personal tutor and writes a comprehensive lesson from scratch, with its style and depth precisely calibrated to the user's level:\n\nA Beginner will receive a lesson rich with simple analogies, foundational definitions, and a clear, step-by-step introduction.\n\nAn Intermediate user will get more detailed explanations, exploring the nuances and connections within the topic.\n\nAn Advanced user will be presented with in-depth analysis, technical specifics, and comparative insights.\n\nThis ensures the user is always challenged but never overwhelmed, maximizing engagement and comprehension.\n\nReinforce and Explore - The Mini-Quiz and References\nA great lesson is supported by great tools. Alongside the custom lesson, two other tabs become available:\n\n📚 References Tab: Gemma 3n provides a curated list of reliable sources (websites, books, articles) for users who wish to continue their learning journey beyond the application.\n\n❓ Mini-Quiz Tab: To solidify what has just been taught, a new 10-question quiz is ready. After taking the quiz, the user receives an instant score. More importantly, for every incorrect answer, Gemma 3n provides a clear explanation of why the chosen option was wrong and why the correct one is right, turning mistakes into valuable learning opportunities.\n\nThe Deep Dive - An Interactive Dialogue\nThe learning doesn't end with the quiz. The final and most powerful step is the deep dive.\n\nAt the bottom of the \"Topic Content\" tab, a Q&A box invites the user to engage in a dialogue. They can now ask any follow-up questions they have about the material they just read. For example:\n\n\"Can you explain the concept of an event horizon in simpler terms?\"\n\n\"What was the main difference between the Girondins and the Jacobins?\"\n\n\"Why is this considered a key turning point?\"\n\nGemma 3n, acting as a Socratic learning partner, provides contextual answers based only on the information presented in the lesson, ensuring a focused and clarifying conversation. This transforms passive reading into active, inquiry-based learning.\n\nConclusion\nIn a single, seamless session, the user has gone from expressing a simple curiosity to having their knowledge assessed, receiving a bespoke lesson, testing their comprehension, and clarifying their doubts through a personal dialogue. This is the power of the Personalized Learning Assistant: a truly adaptive, effective, and private educational journey for anyone, anywhere.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Harungokc/Personalized-Learning-Assistant",
        "demo_video_url": "https://youtu.be/vgha1yleRRY?si=dX6vxOcXJozdWuec",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/personalized-learning-assistant"
    },
    {
        "project_title": "Gemma Vision",
        "tagline": "AI vision assistant for the blind ",
        "authors": "Tommaso Giovannini",
        "summary": "",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/TGTech06/gemma-vision/",
        "demo_video_url": "https://youtu.be/Fx6IuEggeac",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemma-vision"
    },
    {
        "project_title": "✨ MedReport ✨",
        "tagline": "Transforming Doctor-Patient Conversations into Professional Reports",
        "authors": "wouk1805",
        "summary": "**MedReport: Transforming Doctor-Patient Conversations into Professional Medical Reports**",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/wouk1805/medreport",
        "demo_video_url": "https://youtu.be/EsTw3E4DoC4",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/medreport"
    },
    {
        "project_title": "Gem Translate",
        "tagline": "An offline-first translation application for multilingual communication",
        "authors": "Nghi Huynh",
        "summary": "Gem Translate is a mobile application designed to make communicating across languages simple and effective.  Leveraging the powerful Gemma 3N model, it offers robust translation capabilities for both spoken and written input. Users can simply speak into their device or type text, and Gem Translate will provide the translated text, accompanied by synthesized speech output for comprehensive understanding. Gem Translate distinguishes itself as an offline solution, ensuring seamless communication without an internet connection, making it a great tool for travelers and anyone needing reliable translation on the go.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/nghi-huynh/GemmaTranslation",
        "demo_video_url": "https://youtu.be/es7hKiCUXdc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gem-translate"
    },
    {
        "project_title": "TutorLM - Your AI Learning Companion",
        "tagline": "Accessible AI Education for the World's 1.3 Billion Disconnected Children",
        "authors": "Ricardo Santana",
        "summary": "Internet Created One More Education Inequality\n\nOver 1.3 billion school-age children lack internet access at home, creating an unprecedented educational divide. This isn't about capabilities or curiosity, but about connection, the essential foundation for learning in the digital age.\n\nThe shocking reality:\n- **Two-thirds of children ages 3-17** are cut off from online educational resources\n- **Most vulnerable populations**: Rural communities, urban slums, refugee camps, and disaster-affected areas\n- **Sudden disconnection**: Even connected communities lose access during natural disasters, conflicts, or infrastructure failures\n- **The result**: A widening cycle of disadvantage that traps children in poverty and exclusion\n\nThe Text-Only Learning Trap\n\nFor centuries, learning has been trapped in static formats: books, websites, and now even ChatGPT, all deliver education through walls of text with minimal interactivity. Children learn by doing, creating, and exploring, yet our digital tools have regressed to glorified textbooks.\n\nTraditional educational technology falls short because it:\n- Requires constant internet connectivity\n- Delivers passive, one-size-fits-all content\n- Lacks real-time adaptation to student needs\n\nOur Solution: The Educational Internet in <6GB\n\nTutorLM transforms 44TB of educational content, the world's largest repository of human knowledge, into a 6GB package that can run on a solar-powered laptop anywhere on Earth. Using Gemma 3(n) trained on 11 trillion tokens, we've created an AI tutor that works completely offline.\n\nThis isn't just compression; it's educational revolution packaged for universal access.\n\nThe Magic of Multimodal, Offline AI: How It Works\n\nTutorLM replaces the passive screen with a Collaborative Human-AI Canvas, where learning becomes an interactive conversation between student and AI.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/ricardocsantana/tutorlm",
        "demo_video_url": "https://youtu.be/3SrcUKxUPIc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/tutorlm-your-ai-learning-companion"
    },
    {
        "project_title": "SaveMe - the digital twin rescue app",
        "tagline": "A multimodal Android app paired with a website providing expert-level answers on highly complex machinery in low-connectivity situations.",
        "authors": "FabianKs, Jahn Khan",
        "summary": "Product Idea\nSaveMe addresses a critical challenge faced by hardware and infrastructure companies: how to provide instant, reliable access to complex technical information when and where it’s most needed. In high-stakes environments - like bringing systems online in low-connectivity locations or managing emergencies in remote areas - technicians shouldn’t be flipping through manuals.\n\nSaveMe puts a fast, privacy-first AI assistant in every technician’s pocket. Working entirely offline, the app responds instantly via voice and text to spoken questions and visual input, offering concise, relevant answers on the spot.\n\nDuring the **envisioned online setup**, SaveMe collects and structures all relevant documentation, drawings, and procedures in a format optimized for multimodal AI models. This can be done by either fine-tuning the model or embedding the data as fast-searchable context. The result is a digital twin of the machinery - an intelligent, self-contained knowledge application - working locally on the user’s smartphone.\n\nThe **offline user experience** is designed to be natural and intuitive. A technician simply points their phone’s camera at the equipment, clicks the single app-button and asks a question. The app processes voice and visual input locally and delivers a clear, immediate response in both text and speech without requiring any network connection. During offline usage, the app consists of a single button only for extra user-friendly interaction.\n\nMinimum viable product\n\nFor our minimum viable product we minimized components to the essentials while preserving high user value and monetization potential.\n\nThe **setup process** is done on our website www.save-me.app. This website lets customers enter expert knowledge in plain text via a simple input box, saves this text to a private database and returns a unique 5-digit token. Entering that token in the Android app pulls the text as model context onto the device. Once the download is done, both model and context live *entirely offline* on the smartphone.\n\nThe **on-device usage** after model and text download consists of a single button, which takes an image and records voice. The latter is converted to text together with the input image, context and system prompt fed into Gemma 3n. The resulting text is displayed within the app and read out loudly. The predefined system prompt instructs the model to output precise answers based on the provided context.\n\nWith this foundation in place, SaveMe already delivers on its promise: in moments of need it provides precise, reliable answers with **complete data privacy**, powered by Google’s latest edge LLM. The next steps are clear — support richer data types, refine the user experience, and continue proving that the best help is the help you can carry in your pocket, wherever human engineering takes us. \n\nArchitectural decisions\n\nThe architectural decisions followed logically from MVP constraints and goals. On the website, which we built using PHP, HTML, CSS and MariaDB, expert knowledge can be entered into a text box. A token is generated and both text and token are stored in the database. When providing an available token, an API returns the corresponding text.\n\nThe Android app was built based on Google's AI edge gallery and MediaPipe with image and text input. Also, Gemma3n's lightweight, multimodal capabilities make it well-suited for this edge deployment. For voice generation we used Android's built-in text-to-speech system and for voice-to-text conversion we temporarily used the open source model Whisper with plans to migrate to MediaPipe's on-device audio support once stable.\n\nArchitectural details\n\nThe following description follows our public app repository at www.github.com/JahnKhan/saveme-app.\n\n#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/JahnKhan/saveme-app",
        "demo_video_url": "https://youtu.be/mjLnFTX5FRY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/saveme-the-digital-twin-rescue-app"
    },
    {
        "project_title": "AI Explainer",
        "tagline": "Inspired by How is it made TV show to help anyone quickly learn how things work ",
        "authors": "samu2505",
        "summary": "AI Explainer",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/samuellukudu/Gemma3n-AI-Explainer/tree/main",
        "demo_video_url": "https://www.youtube.com/watch?v=EzbJbv-MsiA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/ai-explainer"
    },
    {
        "project_title": "FieldScribe: A Multimodal AI Plant Doctor",
        "tagline": "FieldScribe: Giving a Voice to Farmers with Multimodal AI",
        "authors": "mayakaripel",
        "summary": "For millions of farmers, distance and communication are the hardest barriers to getting help. How do you describe a complex plant disease when you're miles from an expert and can't find the words?\nThis is the problem we set out to solve for the Google Gemma 3n Impact Challenge. Our project, FieldScribe, is an AI co-pilot that gives a voice to every farmer. It uses multimodal AI to see what they see (through an image) and hear what they say (through a voice note) to provide instant, expert-level analysis.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/mayakaripel/FieldScribe-Gemma3n-Hackathon",
        "demo_video_url": "https://youtu.be/T7KEHKBKKqs",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/fulfilling-the-gemma-3n-vision-fieldscribe-an-ai-p"
    },
    {
        "project_title": "KidWord: Dynamic & Personalised German Learning App for Kids",
        "tagline": "An innovative app that dynamically generates fun exercises and stories, enabling children to learn German, anytime, anywhere, even offline",
        "authors": "Pritam Biswas, Jolly Chowdhury",
        "summary": "#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/pritam-bs/the_kids_app",
        "demo_video_url": "https://youtu.be/exW3dcsbop0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/kidword-dynamic-and-personalised-german-learning-a"
    },
    {
        "project_title": "Local LLM NPC: Offline AI-Powered NPC Conversations for Education and Impact",
        "tagline": "A 2D educational game where NPCs teach farming through offline AI conversations powered by Gemma 3n and Ollama.",
        "authors": "Code Forge Temple",
        "summary": "Technical Writeup: Proof of Work",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/code-forge-temple/local-llm-npc",
        "demo_video_url": "https://youtu.be/kGyafSgyRWA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/local-llm-npc-offline-ai-npc-education"
    },
    {
        "project_title": "Leaf Disease Detection with Gemma3n Model",
        "tagline": "Using Deep Learning to Detect Early-Stage Plant Diseases from Images",
        "authors": "Srinivas T A",
        "summary": "Leaf Disease Detection with Gemma3n Model 🌿\n\nIn this project, I applied the Gemma3n deep learning model to detect leaf diseases from plant images. Using image classification, the model can identify early-stage plant diseases, helping farmers to take preventive measures before the disease spreads.\n\nKey Features:\nReal-time Disease Detection: Using deep learning, the model processes leaf images and classifies them into different disease categories.\n\nImage + Query = Answer + Audio: In addition to disease detection, the system can respond to user queries (e.g., \"What disease is on this leaf?\") by generating text-based answers and converting them into audio for easier understanding.\n\nData Augmentation: The model employs techniques like image augmentation to handle real-world variations in plant images.\n\nModel Fine-Tuning: By fine-tuning the Gemma3n model on plant disease datasets, the accuracy and generalization for agricultural use cases are optimized.\n\n🌍 Impact on Agriculture: This model can be a valuable tool for sustainable farming, helping farmers detect diseases early and mitigate crop loss.",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/0WfrLdBZfZE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/leaf-disease-detection-with-gemma3n-model"
    },
    {
        "project_title": "EduVerse - \"offline-first, culturally adaptive AI tutoring\"",
        "tagline": "\"Your Personal Learning Universe, Anywhere\"",
        "authors": "Aditya Katkar",
        "summary": "",
        "technologies": [
            "CUDA",
            "Flask",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Addyk-24/EduVerse",
        "demo_video_url": "https://youtu.be/E99L5BXhrmA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/eduverse"
    },
    {
        "project_title": "GEMMASRA: GEMMA 3n  Based Science Rationality Assistant ",
        "tagline": "GEMMASRA is  GEMMA 3n  Based Science Rationality Assistant to Promote Scientific Temper. ",
        "authors": "C R Suthikshn Kumar",
        "summary": "Scientific temper—the capacity for evidence-based reasoning, skepticism, and critical inquiry—is a fundamental attribute of modern citizenship and a constitutional duty in India. However, its development is often hindered by widespread pseudoscience, misinformation, and lack of access to scientific education. This project  presents the design and implementation of a Science Rationality Assistant (GEMMASRA) powered by Gemma Large Language Models (LLM) to address this gap. The assistant is designed to engage users in interactive dialogue, explain scientific concepts in accessible language, and critically evaluate unverified claims. It leverages Gemma 3n LLM capabilities in natural language understanding, multilingual generation, and context-sensitive reasoning to simulate scientific thinking processes. A custom prompt-engineering strategy and curated dataset of verified scientific content were used to align responses with rational inquiry principles. The assistant was evaluated on three dimensions: accuracy, engagement, and promotion of critical thinking. The Gemini Flash model was utilized in automated evaluation of the GEMMASRA.  Results indicate that the system significantly improved users' ability to differentiate between evidence-based and pseudoscientific claims.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://www.youtube.com/watch?v=kEqkhA3Kb2U",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemmasra-gemma-3n-based-science-rationality-assist"
    },
    {
        "project_title": "A Fine-Tuned Plant Disease Detection Gemma Model using Unsloth and Application ",
        "tagline": "Fine-tuning Gemma 3n for scientific plant disease analysis from images.",
        "authors": "Thomas Yiu",
        "summary": "We've developed a specialized version of Gemma that's been carefully trained to identify plant diseases using a dedicated dataset. This \"Plant Disease Gemma\" excels at the scientific analysis of plant images to find signs of sickness, addressing a common gap where most models lack accurate information in this area.\nUltimately, the goal of this project is to provide a specialized tool that can analyze an image of a plant and then output a detailed, text-based scientific assessment of any diseases it finds.\n\n\n#",
        "technologies": [
            "CUDA",
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/tomtyiu/Plant_Disease_analysis_writeup",
        "demo_video_url": "https://youtu.be/gGGBrlBbAPU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/a-fine-tuned-plant-disease-detection-model-and-app"
    },
    {
        "project_title": "NimittamAI",
        "tagline": "An AI Companion for Moments When Millions Converge",
        "authors": "Rithik Purohit, Mahesh Lambe, Ravikant Khamitkar, Padmanabh Kulkarni, KrishnaShingan1122",
        "summary": "Nimittam is an offline, privacy-preserving mobile application designed to assist pilgrims during large-scale events such as the Kumbh Mela, where connectivity is limited and crowd management is crucial. Built using Gemma 3N, Google’s lightweight on-device AI model, Nimittam offers a seamless, intelligent experience while strictly adhering to the principles of No Cloud, No Leaks, and No Latency.\n\nThe application serves two primary functions: navigation and contextual event assistance, both fully operational without internet access. With preloaded local maps and AI-powered camera-based landmark recognition, Nimittam helps users locate essential points of interest including toilets, medical tents, ghats, and lost and found centers without relying on GPS or mobile data. This offline navigation capability ensures that even in highly congested or remote areas, pilgrims can move confidently and safely throughout the event.\n\nIn addition, Nimittam provides real-time multilingual translation, leveraging Gemma 3N’s Universal Speech Model to break language barriers that commonly arise in diverse gatherings. Pilgrims can interact with the app through both text and images, enabling smooth communication across regional languages without the need for cloud services or server-based translation. Whether reading a sign, understanding a pamphlet, or interpreting directions from volunteers, users receive instant, on-device assistance tailored to their preferred language.\n\nThe app also addresses one of the most emotionally critical challenges of such events, the separation of individuals from their groups. Nimittam helps in reuniting lost elders and children, using a privacy-focused workflow that runs entirely offline, ensuring sensitive data such as images and identities remain secure.\n\nBy combining offline multimodal AI, a user-centric design, and a strong commitment to privacy, Nimittam transforms even the most basic smartphones into intelligent, context-aware assistants. It enhances not only safety and accessibility but also the overall spiritual experience of participants. As such, it stands as a meaningful and deployable solution aligned with the mission of the Gemma 3N Challenge to build responsible, real-world AI tools that work for everyone, everywhere.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/Nimittam/nimittam-app",
        "demo_video_url": "https://youtu.be/aEIQ1ODBoyQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/nimittamai"
    },
    {
        "project_title": "A.R.T.I.E. — AI Archery Coach (Gemma3n)",
        "tagline": "A.R.T.I.E.(Archery Real-Time Intelligence Engine)—revolutionizing coaching for 40K+ archers, saving $1.4M+, giving back to the sport I love!",
        "authors": "Ryan Mehra, Anshoo, Deepshikha Ch",
        "summary": "**A.R.T.I.E. — Revolutionizing Archery Coaching with AI**\n___\n\n#",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/L-Ccchpd0Is",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/a-r-t-i-e-ai-archery-coach-gemma3n"
    },
    {
        "project_title": "EngelBERT: The Thinking Operating System",
        "tagline": "Your AI. Your Data. Your Insights. Anywhere",
        "authors": "DavE, Epimorbit",
        "summary": "",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/daveAnalyst/EngelBERT",
        "demo_video_url": "https://youtu.be/LLRxykgu4zA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/wise-a-sovereign-ontology-driven-thinking-environm"
    },
    {
        "project_title": "Travel Buddy",
        "tagline": "Discover the world. Offline. In your language. ",
        "authors": "Tutkum AKYILDIZ",
        "summary": "Travel Buddy is an offline-first travel companion app, designed to provide AI-powered travel assistance using Google Gemma 3n. The app is engineered for travellers who need reliable, intelligent help during their trip even without internet connectivity. All core features are implemented to work offline, with a focus on Amsterdam for the MVP.\n\nA key innovation is the app’s on-device AI processing, which delivers complete offline functionality. This enables travelers to get assistance anywhere in the world, without data roaming costs or connectivity concerns. Users can interact in any language and easily request translations to or from the local language, breaking down language barriers and making travel more accessible and enjoyable.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/tutkumakyildiz/TravelBuddy",
        "demo_video_url": "https://youtu.be/Z4JCz_ihHAE",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/travel-buddy"
    },
    {
        "project_title": "PestEar: Offline Pest Detection Assistant Powered by Gemma 3n",
        "tagline": "An offline, voice-based pest identification app using Gemma 3n to assist farmers in remote areas.",
        "authors": "RAJESH CHOWDUVADA",
        "summary": "PestEar is an intelligent, offline-capable pest detection assistant powered by Gemma 3n, designed to support farmers and agricultural workers in identifying crop pests without needing internet access. By combining the power of on-device AI with an intuitive mobile interface, PestEar empowers users in remote or connectivity-limited areas to get instant, accurate pest diagnoses simply by capturing an image or describing symptoms through voice.\n\nThe app uses a lightweight, privacy-preserving version of Gemma 3n for multimodal processing, enabling it to interpret images, audio, and text queries locally. PestEar not only identifies the pest but also provides actionable suggestions on prevention and treatment—making modern AI assistance truly accessible to those who need it most.\n\nBuilt with scalability and ease-of-use in mind, PestEar aims to bridge the technological gap in rural agriculture, reduce crop losses, and support sustainable farming practices.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/NARB9FQhV8k",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/pestear-offline-pest-detection-assistant-powered-b"
    },
    {
        "project_title": "RescueGem: Offline Emergency Communicator (Gemma Demo on Kaggle)",
        "tagline": "Empowering crisis response with private, offline AI.",
        "authors": "Mohamed Nauman",
        "summary": "🆘 Project Description\n\nTitle: RescueGem: Offline Emergency Assistant Powered by Gemma 3n\n\nIn emergencies, access to timely information and communication can save lives—but internet access, cloud services, and even power may be unavailable when they're needed most.\n\nRescueGem is an offline-first, privacy-preserving AI assistant built on the Gemma 3n model, running locally via Ollama. It provides:\n\nReal-time Translation for multilingual coordination\nCrisis Knowledge Summarization for rapid decision-making\nLocal Q&A from a built-in emergency knowledge base\nAll features work entirely offline, with no internet, cloud dependency, or data leakage—making RescueGem ideal for use in disaster zones, remote communities, and humanitarian fieldwork.\n\nBy leveraging the compact power of Gemma 3n, RescueGem ensures that life-saving knowledge is always within reach—even when the network isn’t.\n\n\nabove video shows how run the model locally",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/5JBIzZm_RzA",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/rescuegem-offline-emergency-communicator-gemma-dem"
    },
    {
        "project_title": "LocalBuddy - your no-nonsense, private AI assistant",
        "tagline": "Offline, privacy-first AI assistant using Gemma 3n+Ollama, with memory, document context, and full local control—no internet needed.",
        "authors": "Momoboi, Priya@1022QQ//",
        "summary": "",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/ultimatemomo/LocalBuddy",
        "demo_video_url": "https://youtu.be/kU-qX5_BJf0",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/localbuddy-your-no-nonsense-private-ai-assistant"
    },
    {
        "project_title": "GemmaGuardian",
        "tagline": "Transforms any camera into an intelligent AI-powered surveillance system",
        "authors": "CloudJas",
        "summary": "🏆 GemmaGuardian: Intelligent Surveillance with Gemma Models",
        "technologies": [
            "Docker",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Cloud-Jas/GemmaGuardian",
        "demo_video_url": "https://www.youtube.com/watch?v=N7dvib4JkLM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemmaguardian"
    },
    {
        "project_title": "Murdock: AI Vision Assistant for the Visually Impaired",
        "tagline": "Give sight. With privacy. Anywhere.",
        "authors": "Lili Zhang Adams",
        "summary": "MURDOCK (original named SightSpeak, recently renamed to be Murdock) is an on-device, multilingual, multimodal AI assistant designed to empower blind and visually impaired users with real-time scene understanding, text translation, and voice-guided Q&A — all without requiring internet access. Inspired by the story of Daredevil’s Matt Murdock, this app brings super-sensing capabilities into the hands of real users, powered by the compact and efficient Gemma 3n model.\n\nUnlike cloud-based assistants, MURDOCK runs entirely offline, supporting seven languages — English, Chinese, Japanese, Korean, French, Dutch and German. It uses Google’s Gemma3n model via the MediaPipe LlmInference API to interpret camera input, voice commands, and text queries, delivering both natural voice and text responses in the user’s selected language.\n\nMURDOCK ensures complete data privacy, multilingual access, and reliable use in low-connectivity environments.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/lzadams/Murdock",
        "demo_video_url": "https://youtu.be/ZbWZUfhpSeY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sightspeak-ai-vision-for-the-visually-impaired"
    },
    {
        "project_title": "NexusRX AI",
        "tagline": "NexusRX is an AI health assistant that uses Gemma 3n via Ollama to give fast, accurate diagnoses and treatment suggestions.",
        "authors": "Ajashra Tripathi",
        "summary": "Frontend: HTML5, CSS3, JavaScript (Designed for a responsive, intuitive, and efficient user interface)\n\nBackend: Python (Flask Framework) (Orchestrating robust communication and data processing)\n\nCore AI Model: Gemma (Leveraged via Ollama for efficient, local Large Language Model inference)\n\nProject Overview:\nNexusRX is a smart desktop health assistant designed to give users fast, reliable, and privacy-focused medical support right from their own device. Built for simplicity and real-world usability, it acts as a helpful companion for understanding symptoms, exploring possible conditions, and receiving preliminary advice — all without needing an internet connection.\n\nPowered by Google’s Gemma 3n language model through Ollama, NexusRX delivers conversational and intelligent medical insights with impressive accuracy. Whether you're dealing with common symptoms or need a deeper explanation of a condition, NexusRX responds in real time with thoughtful, easy-to-understand guidance.\n\nWhat sets NexusRX apart is its offline-first architecture. Unlike typical health chatbots that rely on cloud servers and require sign-ins, NexusRX runs locally on your machine, giving you full control over your data. There’s no account needed, no data collection, and no background tracking — just fast, intelligent help when you need it most.\n\nThe user interface, built using CustomTkinter, is clean, modern, and focused on clarity. A straightforward chat experience makes it easy to describe symptoms or ask questions, while the AI assistant delivers information tailored to your input without jargon or filler.\n\nNexusRX was created for students, families, and even small clinics looking for a lightweight yet capable tool to support everyday health needs. It is not designed to replace medical professionals, but to serve as a first line of understanding — a way to feel more informed and less anxious before visiting a doctor.\n\nIn a time when accessible healthcare tools are more important than ever, NexusRX offers an approachable, private, and intelligent solution built for everyone.",
        "technologies": [
            "Flask",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/AJRGP3/NexusRX",
        "demo_video_url": "https://www.youtube.com/watch?v=xWbAp8ebG-o",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/nexusrx-ai"
    },
    {
        "project_title": "Health Compass",
        "tagline": "Navigate your health in the right direction, privately",
        "authors": "Barun Saha",
        "summary": "🔗 Gemma 3n Impact Challenge Deliverables At a Glance\n\n| Deliverable | Location |\n| --- | --- |\n| **The Video Demo** | https://youtu.be/_w0J7syoQEw  |\n| **Public Code Repo** | https://github.com/barun-saha/health-compass |\n| **The Technical Writeup** | This post.  |\n\n\nWindows [installation file for Health Compass](https://github.com/barun-saha/health-compass/releases/tag/v0.1.1) is available on GitHub. Please note that this file is **unsigned**. (Ollama needs to be installed separately; see the instructions on the GitHub project description.)\n\nIn addition, a separate screen recording of Health Compass demonstration (annotated; without audio) is also available for reference at https://youtu.be/6QMgsdwrSGc (also attached to this post as media at the end).\n\n---\n\n---\n\n\n🧭 Health Compass: Your Private Guide to Medical Clarity\n\n**Struggling to make sense of your medical test reports, but worried about sharing private health data with online AI tools?**\n*Health Compass* is here to help—with fully offline, secure, and intelligent explanations powered by Gemma 3n running locally via Ollama.\n\n**Key Features**:\n\n- 🔒 **Privacy First**: Your data never leaves your device. No uploads. No cloud processing. Health Compass runs entirely offline on your own computer, ensuring complete control and confidentiality.\n- 🧠 **Built on Open, Local AI**: Our AI engine—Gemma 3n—is optimized for offline inference via Ollama, combining robust language understanding with layout-aware medical reasoning. It reads your reports like a clinician would, surfacing what matters with clarity and care.\n- 📄 **Designed for Real-World Medical Data**: Whether it’s blood work, imaging summaries, or tabular test results, Health Compass preserves layout and structure for accurate context interpretation—thanks to its carefully benchmarked PDF extraction pipeline.\n\n🎯 **Why Choose Health Compass?**\n- No internet required—works entirely offline\n- Understands layout-heavy medical documents\n- Offers clear, empathetic explanations for non-experts\n- Transparent, modular, and open-source compliant\n\nWhether you're a patient seeking clarity or a caregiver looking to support loved ones, Health Compass empowers you to navigate complex health information with confidence—and total peace of mind.\n\n---\n\n\n🌱 Why Health Compass? The Frustration That Sparked It All\n\nMost people look at their medical test results and feel overwhelmed. Numbers like “CRP” or “T3 uptake” don’t mean much without clinical context—leaving patients confused, anxious, or forced to search the Web for answers. That's the first frustration.\n\nThe second? In today’s AI boom, many users attempt to upload their reports to cloud-based tools just to get some interpretation—accidentally exposing deeply personal health data in the process.\n\n**Health Compass was born to address both pains head-on**:  \n- It translates medical jargon into plain language summaries,  \n- _and_ does so entirely offline—no cloud calls, no data leakage, no GPU dependencies.\n\nAt the heart of this architecture is **Ollama**, a secure and lightweight local LLM runner. It powers **Gemma 3n** directly on the CPU, letting anyone with a modest laptop safely understand their health reports.\n\n**Health Compass uses Gemma 3n (specifically, Gemma 3n E2B) and Ollama because**:\n- Gemma 3n is small enough to run on CPU, yet powerful enough to correctly follow given instructions.\n- Gemma 3n can correctly identify user intents and relevant entities.\n- Having trained on a world body of knowledge, Gemma 3n can already answer most questions, including those related to the healthcare domain.\n- Gemma 3n is multilingual, making it a great choice for regions with rich linguistic diversity, such as India.\n- Ollama is one of the best and easiest solutions to run a Generative AI model locally, offline.\n- Ollama supports a large number of models, including the different generations of Gemma, which offers users the freedom of choice.\n\n---\n\n🌍 What's the Potential Impact?\n\nHealth Compass makes medical literacy personal, private, and local. Moreover, Health Compass is Open-Source.\n\n- Instead of uploading sensitive PDFs to an LLM server, users keep everything on-device.  \n- They can ask everyday questions—_“Is high creatinine bad?”_—and get conversational, grounded answers.\n- The system has no accounts, telemetry, or cloud APIs. It's engineered for **low-connectivity regions, privacy-sensitive users, and everyday accessibility.**\n\nBy wrapping Gemma 3n inside **Ollama’s containerized, on-device runtime**, Health Compass enables reasoning that’s safe, scalable, and empowering—even for those unfamiliar with AI or medicine.\n\n---\n\n🧠 Technical Concepts\n\nHealth Compass is more than a concept—it’s mature engineering. Here’s how it works:\n\n- **Gemma 3n E2B via Ollama**:  \n  - Ollama spins up Gemma 3n E2B locally via its CLI, with promptability and endpoint routing.\n  - Runs comfortably on CPU with token-efficient chunking for larger documents.\n- **Planner-Executor Agent Pattern**:\n  - User intent → structured JSON plan → agent executes → result rendered.\n  - Modular separation lets new tools drop in easily.\n- **PDF Interpretation**:\n  - Extracted using `pdftotext`, preserving document layout without cloud services.\n  - Summarized by Gemma 3n E2B, with agent fallback if context is lost or clarification is needed.\n- **Electron Shell & Secure IPC**:\n  - Prevents direct Node access from the UI.\n  - Uses preload scripts to bridge context securely and configure LLM selection per environment.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/barun-saha/health-compass",
        "demo_video_url": "https://youtu.be/6QMgsdwrSGc",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/health-compass"
    },
    {
        "project_title": "🌿 CropSenseAI - Clean AI Plant Health Advisor",
        "tagline": "Multimodal Plant Diagnosis with Image and Voice, Built for Lightweight Deployment",
        "authors": "Sheb Michel",
        "summary": "CropSenseAI: Offline Plant & Disease Identifier for Farmers, Plant lovers\n🎯 1. The Problem\nMillions of plants lovers as well as farmers lose crops yearly because:\n•\tThey can’t identify plant diseases accurately.\n•\tThey lack reliable means such as internet to look up solutions.\n•\tThey struggle to access expert advice.\n \n💡 2. The Solution\nan AI-powered open-source solution to be soon deployed in mobile app that will helps:\n✅ Identify plant species and diseases from photos\n✅ Receive treatment recommendations immediately\n✅ Listen to guidance in their language\n\n \n🛠️ 3. Core Features\n1.\tImage Capture\no\tFarmer takes a photo of the leaf or plant or upload from your saved photo.\n2.\tImage Classification\no\tGemma 3n classifies:\n\tDisease diagnosis (e.g., blight, rust, mosaic virus).\no\tOffline inference—no internet needed.\n3.\tTreatment Recommendation\no\tApp displays:",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/ShebMichel/CropSenseAI.git",
        "demo_video_url": "https://www.youtube.com/watch?v=aOsBeb2JDaY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/cropsenseai-clean-ai-plant-health-advisor"
    },
    {
        "project_title": "GlucoGuide: Smarter Food Choices with Gemma 3n",
        "tagline": "This project demonstrates how to build GlucoGuide, an AI-powered app that uses Google's on-device Gemma 3n model to analyze meals from text.",
        "authors": "Wonjun Choi",
        "summary": "GlucoGuide is an AI-powered nutrition assistant designed to help people, especially those managing diabetes, make confident and healthy food choices. This project demonstrates how to use Google's Gemma 3n, a fast and efficient on-device model, to analyze meals from either a photo or a simple text description.\n\nBy leveraging a carefully engineered system prompt, the application provides instant, structured, and actionable feedback directly on a mobile device, without needing an internet connection. The output includes the meal's predicted glucose impact, the reasoning behind it, and helpful management tips, turning a moment of dietary uncertainty into an opportunity for empowered decision-making.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/toonkev/GlucoGuide",
        "demo_video_url": "https://www.youtube.com/watch?v=2WCHUo_xjro",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/glucoguide-smarter-food-choices-with-gemma-3n"
    },
    {
        "project_title": "aura-os-your person ai",
        "tagline": "ai that you can you use to solve same project even offline",
        "authors": "nayituriki emmanuel",
        "summary": "this is the one ai for peaple mostly you can use it offline",
        "technologies": [],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/mFBi7yNCT4A",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/aura-os-your-person-ai"
    },
    {
        "project_title": "Gemm-Aid:Health AI",
        "tagline": "Local-first medical assistant built for remote areas with privacy in mind; includes memory, knowledge base, and emergency respones mode.",
        "authors": "Fuegoo",
        "summary": "Technical Overview: Gemm-Aid – A Local-First Medical Assistant Powered by Gemma 3n\n\n**Gemm-Aid** is a fully offline medical assistant designed for critical usability in both daily health tracking and emergency response. The system is structured around two operational modes: **Main Mode** and **Emergency Mode**, each optimized for distinct use cases and user contexts. Unlike typical LLM integrations that rely heavily on cloud infrastructure, Gemm-Aid was built from the ground up to function entirely on local hardware, leveraging **Gemma 3n:e4b** through **Ollama** to deliver fast, secure, and autonomous medical guidance without internet dependency.\n\n---",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/ffurkann/Gemma3n-Impact-Challenge",
        "demo_video_url": "https://www.youtube.com/watch?v=o3YtsWcDJqM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/gemm-aidhealth-ai"
    },
    {
        "project_title": "AptusTutor: On-Device AI for the Underserved Educator",
        "tagline": "Leveraging Gemma 3n's Architectural Efficiency for Private, Offline Classroom Analytics on Low-Cost Hardware",
        "authors": "Kwikiriza Mathias",
        "summary": "**Project Summary**\n\nAptusTutor is an Android application that leverages Google's Gemma 3n to deliver private, offline, AI-powered student assessment capabilities and classroom analytics directly on low-cost Android devices. The key vision of this project is to harness Gemma 3n’s capabilities to address a critical gap in global education: the limited availability of data-driven teaching tools in low-connectivity, resource-constrained environments. The report begins with a detailed justification for selecting the Gemma 3n E2B model variant, then details how AptusTutor leverages the model's novel, efficiency-focused architecture to bring state-of-the-art offline classroom analytics to a teacher with a low-end smartphone. Specifically, features such as Per-Layer Embeddings (PLE), 4-bit quantization, and conditional parameter loading have made it possible to run a powerful 5-billion parameter model within the tight memory and compute budget of a phone running on a MediaTek Helio G81 CPU with 4GB of RAM. Instead of choosing a high-end device with optimized AI runtimes, a low-end device was deliberately selected as a representative benchmark for the target environment. Beyond the implementation discussion, the report also transparently accounts for all the issues faced during project execution and details the steps taken. The result of this rigorous iteration is a functional application that serves as a blueprint for a new class of accessible and privacy-first educational technology designed to serve those most in need.\n\n**1. Project Vision & Impact: Democratizing Classroom Analytics**\n\nTeachers in many regions around the world, particularly in developing economies, face a significant technological disparity. Unlike their counterparts in developed regions, who benefit from a rich ecosystem of digital tools, these teachers often rely on what are commonly termed \"low-end\" mobile devices, characterized by limited RAM (often 1-4 GB), entry-level CPUs, and slow eMMC storage. This means that they hardly benefit from modern on-device AI features such as real-time vision and offline analytics, which are common on high-end devices with dedicated NPUs and optimized AI runtimes. This digital divide denies them powerful tools that could otherwise help manage workloads and personalize student feedback.\n\nTo directly address this challenge, I decided to embark on a project that brings the power of Google’s Gemma 3n to the palms of educators and students in possession of these low-end devices. Specifically, I decided to use a phone running on a MediaTek Helio G81 CPU with 4GB of RAM as a representative of all low-end devices in my target user environments [1]. As the video demo illustrates, if AptusTutor can perform reliably on this hardware, it can serve millions of educators in Africa, Asia, and other regions who use similar devices.\n\n**2. The Technological Catalyst: Why Gemma 3n is the Solution**\n\nThe feasibility of AptusTutor hinges entirely on the unique architectural innovations of Gemma 3n. On a low-end device, this model guarantees the following:\n- Memory Efficiency via Per-Layer Embeddings (PLE): On low-end devices, the primary obstacle to running LLMs is RAM. However, Gemma 3n’s PLE architecture is a key breakthrough since it can allow the model’s embedding parameters to be cached in the more abundant CPU-accessible RAM or local storage, rather than consuming precious, high-speed accelerator memory [2].\n- Conditional Parameter Loading for Multimodality: In the AptusTutor app, some key grading tasks will involve analyzing handwritten answers from images submitted by the students. Since Gemma 3n’s architecture allows for \"Conditional Parameter Loading,\" the app will load the powerful MobileNet-V5 vision encoder parameters into memory only when needed, i.e., when a student's submission includes an image. This architecture will help the app skip loading these parameters during text-only grading tasks, thus saving RAM [2].\n\nThese features will make it possible to run the multi-billion parameter model on the target hardware. Additionally, the model has two variants, i.e., the E4B and the E2B variants, and for this app, I selected the E2B variant and downloaded the gemma-3n-E2B-it-int4.task file from Kaggle due to the following two reasons:\n- Optimized On-Device Performance: Because the target hardware is resource-constrained, the E2B variant, due to its speed on consumer CPUs, was the best choice. Community benchmarks indicate it can achieve practical token generation speeds on consumer CPUs, making it significantly faster than its larger E4B counterpart [3].\n- Size: The ‘gemma-3n-E2B-it-int4.task’ file is 3.14 GB while the ‘gemma-3n-E4B-it-int4.task' file is 4.41 GB. On a resource-constrained device, a smaller model is better.\n\n**3. Architecture and Implementation**\n\nAptusTutor is a modern android app built using Kotlin, Jetpack Compose (with an MVVM architecture), the Google Gemma 3n model via the Google AI Edge SDK, the Google Nearby Connections API for offline networking, Hilt for dependency injection, Kotlin Coroutines and Flow for asynchronous operations, WorkManager for background tasks, and a persistence layer of Room and Jetpack DataStore.\n\n**4. The Engineering Journey**\n\nAfter implementing the core app architecture and ensuring that a tutor could reliably create classes, start sessions, send and receive student assessments, take attendance records, and also manually grade, the next step was to integrate the logic that allows them to grade student assessments by a simple click of a button. However, implementing this and ensuring that it works was a core task in realizing the dream set forward. The choices made and the challenges encountered in this journey are detailed below.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/MathiasKwi900/AptusTutor/releases/tag/v1.0.0",
        "demo_video_url": "https://youtu.be/Vqj4fbU_RQk",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/unlocking-offline-ai-power-with-aptustutor"
    },
    {
        "project_title": "1Student1School",
        "tagline": "A school of AI Agent tutors that provide personalized education.",
        "authors": "Shreyas Rao",
        "summary": "**Bridging the Quality Chasm in Education: The Personalized AI Tutor(s)**\n\nThe narrative surrounding modern education often focuses on access – has technology finally democratized knowledge? We can point to compelling evidence supporting this claim. Offline versions of platforms like Wikipedia offer vast information without connectivity, university lectures proliferate freely on YouTube for those without institutional access, and the declining cost of internet means it's a near-requisite even in remote areas.\n\nYet, despite these advancements making content readily available, many students are still falling behind due to issues that technology alone cannot resolve: the lack of quality teaching and effective tutoring. This paradox – abundant material but scarce skilled instruction – remains the central bottleneck hindering genuine learning and academic success for millions around the globe.\n\n#",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": "https://github.com/lindy-dev/1student1schl",
        "demo_video_url": "https://www.youtube.com/watch?v=BTqv0UU7sks",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/1student1school"
    },
    {
        "project_title": "Salud Andina: AI-Powered Healthcare Companion for Rural Ecuador",
        "tagline": "Empowering remote communities with offline AI health assessment, natural remedy guidance, and seamless doctor handoffs",
        "authors": "Danilo E Salazar",
        "summary": "Introduction\nIn rural Ecuador and across Latin America, healthcare access remains a critical challenge. Communities in the Andes and Amazon regions often live 3+ hours from the nearest medical facility, with government doctors making sporadic visits - sometimes only once per month. When illness strikes, families face impossible choices: undertake dangerous, expensive journeys to distant clinics or rely solely on traditional knowledge that may be incomplete.\nThis healthcare desert affects millions. According to WHO data, over 60% of rural Latin Americans lack regular access to healthcare professionals, leading to preventable complications, delayed diagnoses, and unnecessary suffering.",
        "technologies": [
            "FastAPI",
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/daniloedu/salud-andina",
        "demo_video_url": "https://youtu.be/pQVtzxP0NbQ",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/rural-health-ai-assistant"
    },
    {
        "project_title": "Trosyn Light – Offline AI Document Assistant",
        "tagline": " Trosyn Light is an offline-first AI document editor that summarizes, edits, and cleans up text securely using Gemma 3n—all on-device.",
        "authors": "Ssentongo Ivan",
        "summary": "** Trosyn Light**\n\nTrosyn Light transforms document workflows for NGOs, governments, healthcare organizations, and small enterprises operating in low-connectivity or privacy-sensitive environments. Built with Gemma 3n, all AI processing happens entirely on-device—no data ever leaves your machine.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/EFOo4AhrLVU",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/trosyn-light-offline-ai-document-assistant"
    },
    {
        "project_title": "PrivacyFirst.AI – An Offline, Modular AI Assistant for Secure, Accessible Intera",
        "tagline": "Empowering users with a fully offline, privacy-preserving AI assistant that delivers modular productivity, accessibility, and secure real-ti",
        "authors": "Narisetti Chaitanya naidu",
        "summary": "PrivacyFirst.AI is a fully offline, privacy-focused AI assistant designed to operate entirely on-device without sending any user data to the cloud. Built for accessibility and modular productivity, it serves multiple demographics—acting as a smart tutor for students, a productivity companion for professionals, and a real-time assistive interface for elderly users or those with disabilities.\n\nThe assistant integrates the Gemma 3n model locally via Ollama, and features a voice/text interface (Tauri + React), with installable plugins for tasks like note-taking, file summarization, and to-do management. A toggle lets users switch between offline secure mode and an optional online mode for general queries. This architecture ensures real-time utility while upholding full user control over their data.\n\nThe system is lightweight, extendable, and optimized for edge use cases where bandwidth, privacy, or compliance constraints exist. It offers a blueprint for ethical, responsible deployment of AI in critical, privacy-sensitive domains.",
        "technologies": [
            "Gemma 3n",
            "Ollama"
        ],
        "source_code_url": "https://github.com/Ncn914491/privacy_ai_assistant.git",
        "demo_video_url": "https://youtu.be/xuGHc2GyIwM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/privacyfirst-ai-an-offline-modular-ai-assistant-fo"
    },
    {
        "project_title": "CITE - Clinical Information Timeline Extractor",
        "tagline": "Using Large Language Models to visualize timelines for clinical narratives",
        "authors": "Aravind Mohanoor",
        "summary": "VAERS reports have a field called LAB_DATA which provides the results of diagnostic tests that were conducted on the patient. \n\nTo the best of my knowledge, this field has never been studied by the medical community for their VAERS research. \n\nOne of the big challenges, in my opinion, is that the LAB_DATA is often a \"wall of text\" which can be very hard for people to analyze. \n\nFor my project submission, I have built a tool which will help these medical researchers analyze LAB_DATA more easily by presenting the information in a timeline view. I use three different LLMs, including Gemma 3N 4B, to extract the relevant information and juxtapose them so that the researcher is able to get a clear picture of diagnostic events.",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": null,
        "demo_video_url": "https://youtu.be/_Q2ErhupNh8",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/cite"
    },
    {
        "project_title": "Offline Solar Assistant",
        "tagline": "An offline, bilingualAI-powered assistant for solar energy support, without internet connectivity",
        "authors": "shrweta",
        "summary": "Offline Solar Assistant ☀️ (English & Marathi)\nA fully offline, modular solar Q&A and troubleshooting tool for disaster and rural use, powered by local LLM (Gemma), translation, and knowledge base.\n\nFeatures\nRuns offline after setup\nMarathi & English Q&A\nTroubleshooter, Estimator, Disaster Guides, and more\nWeb and CLI interface\nHow to Run\nInstall Ollama, pull Gemma:7b-it\nInstall dependencies: pip install -r requirements.txt\nRun ollama serve\nRun python web_app.py",
        "technologies": [
            "Ollama"
        ],
        "source_code_url": "https://github.com/shrwetanaik/Offline_Solar_Assistant/",
        "demo_video_url": "https://youtu.be/QdJeyq9skaM",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/offline-solar-assistant"
    },
    {
        "project_title": "Kikko's Saga Forge: A Verifiable & Evolvable Knowledge RPG",
        "tagline": "Forge your verifiable saga with an on-device AI. A new game to combat digital amnesia and restore trust in AI. ",
        "authors": "Kurodo Henro Onsen",
        "summary": "#",
        "technologies": [
            "Gemma 3n"
        ],
        "source_code_url": "https://github.com/kurodohenroonsen/Kikko-s-Saga-Forge---The-Gemma-3n-Impact-Challenge",
        "demo_video_url": "https://youtu.be/GKSE5RrzzYY",
        "kaggle_url": "https://www.kaggle.com/competitions/google-gemma-3n-hackathon/writeups/KikkoSagaForge"
    }
]